1999.tmi-1.20,J98-1006,0,0.014156,", ACC stands for the accusative case. Parallels can be drawn between this general method and the conceptual similarity metric of Palmer & Wu (1995). 3 199 Table 3: A fragment of the case marker alternation matrix Note that in the current formulation, we give consideration only to verb sense disambiguation and choose not to independently disambiguate case fillers. Clearly, there is scope to augment this unidirectional verb-driven approach and potentially improve the precision of VSD through intra-case slot local and intra-clausal topical disambiguation techniques (Yarowsky 1994; Ng & Lee 1996; Leacock et al. 1998), as well as through consideration of domain (Wilks & Stevenson 1998), which are left as topics for future research. Case marker alternation Surface case alternates derive from the canonical case marker type(s), and argument status of the target case slot. In the current implementation, this is represented simply as a matrix of possible case marker alternations for each canonical case marker, for a given argument type. This matrix has been developed based around the relative degrees of freedom of case marker alternation outlined in Table 1, in that whereas a complement-type nominative case mar"
1999.tmi-1.20,1997.tmi-1.1,0,0.091756,"Missing"
1999.tmi-1.20,1997.tmi-1.18,0,0.0821482,"Missing"
1999.tmi-1.20,P96-1006,0,0.0173427,"s c1..n: 2 Here, ACC stands for the accusative case. Parallels can be drawn between this general method and the conceptual similarity metric of Palmer & Wu (1995). 3 199 Table 3: A fragment of the case marker alternation matrix Note that in the current formulation, we give consideration only to verb sense disambiguation and choose not to independently disambiguate case fillers. Clearly, there is scope to augment this unidirectional verb-driven approach and potentially improve the precision of VSD through intra-case slot local and intra-clausal topical disambiguation techniques (Yarowsky 1994; Ng & Lee 1996; Leacock et al. 1998), as well as through consideration of domain (Wilks & Stevenson 1998), which are left as topics for future research. Case marker alternation Surface case alternates derive from the canonical case marker type(s), and argument status of the target case slot. In the current implementation, this is represented simply as a matrix of possible case marker alternations for each canonical case marker, for a given argument type. This matrix has been developed based around the relative degrees of freedom of case marker alternation outlined in Table 1, in that whereas a complement-ty"
1999.tmi-1.20,P98-2228,0,0.0146882,"en this general method and the conceptual similarity metric of Palmer & Wu (1995). 3 199 Table 3: A fragment of the case marker alternation matrix Note that in the current formulation, we give consideration only to verb sense disambiguation and choose not to independently disambiguate case fillers. Clearly, there is scope to augment this unidirectional verb-driven approach and potentially improve the precision of VSD through intra-case slot local and intra-clausal topical disambiguation techniques (Yarowsky 1994; Ng & Lee 1996; Leacock et al. 1998), as well as through consideration of domain (Wilks & Stevenson 1998), which are left as topics for future research. Case marker alternation Surface case alternates derive from the canonical case marker type(s), and argument status of the target case slot. In the current implementation, this is represented simply as a matrix of possible case marker alternations for each canonical case marker, for a given argument type. This matrix has been developed based around the relative degrees of freedom of case marker alternation outlined in Table 1, in that whereas a complement-type nominative case marker (ga) can commonly alternate with any of a range of case markers i"
1999.tmi-1.20,H93-1052,0,0.0987019,"Missing"
1999.tmi-1.20,P94-1013,0,0.0127256,"onal constraints c1..n: 2 Here, ACC stands for the accusative case. Parallels can be drawn between this general method and the conceptual similarity metric of Palmer & Wu (1995). 3 199 Table 3: A fragment of the case marker alternation matrix Note that in the current formulation, we give consideration only to verb sense disambiguation and choose not to independently disambiguate case fillers. Clearly, there is scope to augment this unidirectional verb-driven approach and potentially improve the precision of VSD through intra-case slot local and intra-clausal topical disambiguation techniques (Yarowsky 1994; Ng & Lee 1996; Leacock et al. 1998), as well as through consideration of domain (Wilks & Stevenson 1998), which are left as topics for future research. Case marker alternation Surface case alternates derive from the canonical case marker type(s), and argument status of the target case slot. In the current implementation, this is represented simply as a matrix of possible case marker alternations for each canonical case marker, for a given argument type. This matrix has been developed based around the relative degrees of freedom of case marker alternation outlined in Table 1, in that whereas"
1999.tmi-1.21,1999.tmi-1.20,1,0.707675,"994). What is undoubtedly the most integral component of alternation description, however, is a listing of individual case slots and associated features. Case slots are presented in canonical ordering and annotated with: constituent structure (: cs), including case marker and an optional obligatoriness flag for Japanese, prepositional marker in the case of English, and a phrase-level part-of-speech; grammatical relation (:gs); case-role (:role — 24 roles), and argument status (:stat — 7 levels), based on the case grid representation and valency binding hierarchy proposed by Somers (1987) (see Baldwin & Tanaka (1999) for more detail); and an index back to the sense-level list of argument constraints (:sem-arg). The dictionary used in the Mikrokosmos project (Viegas et al. 1998), appears to have a comparable amount of information, but does not, as far as we are aware, treat the core meaning separately from its alternations. 214 Figure 6: The separated and relinked dictionary 5 Use in MT: the linking lexicon In order to use mono-lingual alternation-based lexicons for machine translation, it must be linked together. To do this we use a linking lexicon. The basic idea is that lexical choice is left to the gen"
1999.tmi-1.21,C94-1042,0,0.012763,"languages into a single system architecture. More information is kept in the monolingual dictionaries, which can be maintained by monolinguals. The linking lexicons are basically reversible, although it is likely that different constraints may be more useful for different directions. There will still be 2Cn2 linking lexicons for n languages, but the overhead for constructing a linking lexicon is considerably less than that for constructing a disambiguated transfer dictionary. In order to develop our architecture, we examined several existing resources: GoiTaikei (Ikehara et al. 1997), COMLEX (Grishman et al. 1994), WordNet (Fellbaum 1998), EVCA (English Verb Classes and Alternations: Levin (1993)) and Jing & McKeown's (1998) combined lexicon, which incorporates information from COMLEX, WordNet and EVCA. The remainder of this paper is structured as follows. Section 2 describes several linguistic resources. Section 3 discusses what the appropriate granularity is for monolingual senses. Section 4 describes the proposed dictionary architecture and the interrelation between the various levels of representation. Section 5 details a number of implementation issues related to the linking lexicon. 2 Linguistic"
1999.tmi-1.21,W98-0718,0,0.0141796,". For example, the word gather appears in three classes: the “Get” subclass of the “Verbs of Change 210 of Possession” family, the “Shake” class of the “Verbs of Combining and Attaching” and the “Herd” subclass of the “Verbs of Existence” family. Jing & McKeown’s (1998) combined lexicon Jing & McKeown’s (1998) dictionary incorporates syntactic frames from COMLEX and alternation pairs from EVCA into WordNet senses, along with frequency of occurrence of each sense in the Brown corpus. The combined dictionary has the strengths of all three resources, and has been successfully used in generation (Jing 1998). It has some rudimentary semantic constraints on arguments, but only at the level of something or somebody. There has been other research combining EVCA and WordNet, notably Kohl et al. (1998) and related work. In this work, frames are added to WordNet sense, along with prototypical fillers, to allow example sentences to be generated. Some semantic constraints are given on arguments, but they are still quite limited. 3 A definition of sense In order to avoid spurious ambiguities, we keep the number of senses to a minimum, as argued for by Wierzbicka (1996:244).2 This is in line with the curre"
1999.tmi-1.21,P98-1099,0,0.142868,"Missing"
1999.tmi-1.21,C94-2106,0,0.225129,"Missing"
1999.tmi-1.21,C98-2211,0,0.0521934,"presented in canonical ordering and annotated with: constituent structure (: cs), including case marker and an optional obligatoriness flag for Japanese, prepositional marker in the case of English, and a phrase-level part-of-speech; grammatical relation (:gs); case-role (:role — 24 roles), and argument status (:stat — 7 levels), based on the case grid representation and valency binding hierarchy proposed by Somers (1987) (see Baldwin & Tanaka (1999) for more detail); and an index back to the sense-level list of argument constraints (:sem-arg). The dictionary used in the Mikrokosmos project (Viegas et al. 1998), appears to have a comparable amount of information, but does not, as far as we are aware, treat the core meaning separately from its alternations. 214 Figure 6: The separated and relinked dictionary 5 Use in MT: the linking lexicon In order to use mono-lingual alternation-based lexicons for machine translation, it must be linked together. To do this we use a linking lexicon. The basic idea is that lexical choice is left to the generation stage, but constrained by the input text. This allows for flexible, fluent generation. There are also several practical advantages. The lexicon is easy to u"
1999.tmi-1.21,C94-1038,0,\N,Missing
1999.tmi-1.21,P98-2216,0,\N,Missing
1999.tmi-1.21,C98-1096,0,\N,Missing
2002.tmi-papers.2,1999.tmi-1.21,1,0.910795,"xample illustrates the nature of case marking variation, the principal form of morphological variation considered in this research. In its original form, the valency dictionary does not contain any explicitly-annotated alternations. Rather, alternants (e.g. akeru and aku) are described independently of one another, and any systematic variation that exists between them is incidental. The driving mechanism employed in the extraction of alternations is the assumption that the selectional restrictions associated with corresponding case slots are unchanged under alternation, originally proposed by Baldwin et al. (1999). That is, in the case of akeru/aku, the selectional restrictions associated with the direct object of akeru 1 Note that for the purposes of this paper, we do not make explicit reference to the non-lexical effects of alternation, such as transformation of grammatical relation. We focus instead on the surface manifestation of such effects, e.g. in the form of case marking alternation. 2 The following abbreviations are used in glosses: nom = nominative and acc = accusative. “opentrans ” are identical to those of the subject of aku “openintrans ”, onto which it maps under alternation. It is impor"
2002.tmi-papers.2,Y00-1002,1,0.85202,"classify these as synthetic. We make a number of assumptions about alternations in this research: 1. The selectional restrictions and lexical fillers on matching case slots are preserved under alternation 2. Alternations are monotonic in valency terms 3. A given alternation type has fixed direction The first of these assumptions states that corresponding case slots in the two alternants of a given alternation token, display the same selectional restrictions and lexical fillers. That is not to say that the same distribution of lexicalisations will be observable for the two case slots (e.g. see Baldwin & Tanaka (2000) for details of the impact of pragmatics and facilitation on argument realisation), but rather that they have the same basic scope for instantiation. The second assumption states that a given alternation type cannot involve both case slot insertion and deletion. That is, alternations must be strictly valency-reducing, valency-increasing or valency-preserving. A corollary of this assumption is that all case slots in at least one of the two alternant case frames must be linked to a case slot in the second alternant case frame (with all case slots in both case frames mapping to a case slot in the"
2002.tmi-papers.2,2002.tmi-papers.6,1,0.763787,"e most plausible (if any) alternation for each from the set of all possible valence-monotonic case slot mappings between them. We then analyse trends in the alternation data and feed this directly into the lexicon. Alternation candidates are scored by evaluating the quality of match of selectional restrictions on corresponding case slots. Case slot match quality is rated empirically in the manner described below. We could also use the existence of identical English translations as an indicator that two candidates are related by way of alternation (cf. the valency frame generation procedure of Fujita & Bond (2002)). For example, akeru and aku are both linked to the English translation open. 3.1 Scoring case slot matches The quality of match between case slots is quantitatively described by comparing the relative proximity of the selectional restrictions describing each, within the Goi-Taikei thesaurus tree. As stated above, selectional restrictions are provided as thesaurus node indices, and the greater the topological overlap between and conceptual cohesion within the regions described for the two case slots, the higher the match quality. This is intended to reflect the intuition that the higher the s"
2002.tmi-papers.2,1991.mtsummit-papers.16,0,0.0330506,"Missing"
2002.tmi-papers.2,H92-1086,0,0.0152486,"Missing"
2002.tmi-papers.2,A00-2034,0,0.0675442,"Missing"
2002.tmi-papers.2,C94-2106,0,0.0596368,"Missing"
2002.tmi-papers.2,C00-2108,0,0.0611112,"Missing"
2003.mtsummit-papers.50,briscoe-carroll-2002-robust,0,0.0206286,"er which to evaluate the method; and (d) an inventory of translation templates. 3.1 Corpus data The corpus data was taken from the 80m word written component of the British National Corpus (BNC, Burnard (2000)). The British National Corpus was chosen because of its size and domain-inspecificity. Corpus size will affect the relative coverage of fullyspecified translations, and the fact that the BNC covers a broad range of domains helps us to accurately capture the subcategorisation properties of a given word (in the form of , as described in Section 2). We dependency-parsed the BNC using RASP (Briscoe and Carroll, 2002), a tag sequence grammar-based stochastic parser. RASP captures noun-noun dependencies using the ncmod relation between a head noun and its noun dependent, optionally linked via a preposition or genitive construction. The noun-noun dependency structure of the NP the Jubjub bird’s relation to the frumious Bandersnatch, for example, would be captured by three ncmod relations: #    ncmod( ,bird,Jubjub) ncmod(POSS,relation,bird) ncmod(to,relation,Bandersnatch) which represent the NN, genitive and to-PP construction, respectively. Note that the head of the dependency relation occupies the"
2003.mtsummit-papers.50,C02-1011,0,0.453375,"lations which are syntactically unmarked, capture the basic semantics of the source language expression and from which the source language expression is recoverable with reasonable confidence. Examples include issue of blame and responsibility issue as alternative transsekiniN moNdai “liability islations for sue”. In this, we set ourselves apart from the highly subjective method of manual translation evaluation where bilingual annotators are presented with the source language expression and system output, and asked to rate the output for “plausibility” or “usability” (Tanaka and Matsuo, 1999; Cao and Li, 2002). By pre-generating our source language-recoverable translations (or L1-coverable translations) we remove the annotator from direct contact with our method and hence hope to make evaluation as objective as possible. Given that we are only ever required to evaluate translations generated by our method, we only consider those translation candidates our method can generate for a given source language expression using ALTDIC. To reduce the annotation overhead, we break the task down into two steps: (1) identify appropriate word-level translations for each member of the NN compound as conditioned b"
2003.mtsummit-papers.50,1999.tc-1.8,0,0.171841,"ompound noun MT task over a range of translation templates. They propose the use of “default constructions” for single words in a manner similar to that described in this research, but base determination of such defaults on English translations within a parallel corpus. Specifically, they take the Hansards corpus and observe which of ecology and ecological, e.g., occurs more often as a noun premodifier and use this information in generating ecological movement rather than ecology movement as a translation for the German Umweltbewegung. Unfortunately, no attempt is made to evaluate the method. Grefenstette (1999) uses web data to select English translations for compositional German and Spanish noun compounds, and achieves an impressive accuracy of 0.86–0.87. The translation task Grefenstette targets is intrinsically simpler than that described in this paper, however, in that he filters out the effects of translation template selection by considering only those compounds which translate into NN compounds in English. It is also possible that the historical relatedness of languages has an effect on the difficulty of the translation task, although further research would be required to confirm this predict"
2003.mtsummit-papers.50,1991.mtsummit-papers.16,0,0.255155,"Missing"
2003.mtsummit-papers.50,P03-1040,0,0.0717543,"-level translation lexicon and monolingual corpus data. Much work on the similar task of terminology translation has relied on parallel or comparable corpora (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Lee and Kim, 2002; Tanaka, 2002). We attempt to use only target language corpus evidence in the translation process, to reduce data sparseness and make the method as portable to novel domains as possible. We suggest that the method proposed in this research can be used as a specialist NN compound translation module in a full-scale MT system. This is supported by the finding of Koehn and Knight (2003) that, in the context of statistical MT, overall translation performance improves when noun phrases are translated independently of the sentential context. The remainder of this paper is structured as follows. Section 2 describes the proposed method, and Section 3 outlines the resources used in this research. Section 4 provides detailed evaluation of the proposed method. Section 5 contextualises this research with respect to previous work, and Section 6 concludes the paper. 2 Proposed method We translate NN compounds through the composition of word-level translations and a constructional trans"
2003.mtsummit-papers.50,E03-1073,0,0.0251545,"Missing"
2003.mtsummit-papers.50,C02-1046,0,0.0187978,"er method is preferred.     5  Related work One piece of research relatively closely related to our method is that of Cao and Li (2002), who use bilingual web data and various combinations of the EM algorithm, a naive Bayes classifier and TF-IDF to translate Chinese NN compounds into English. They report an impressive F-score of 0.73 over a dataset of 1000 instances, although they also cite a prior-based F-score (equivalent to our Baseline-1) of 0.70 for the task, such that the particular data set they are dealing with would appear to be less complex than that which we have targeted. Lee and Kim (2002) use definitions from a bilingual dictionary and target language corpora, and treat translation selection as disambiguation of a source word sense and selection of a target word. They report the accuracy of their method for nouns to be 0.55, although in the case of compound nouns it seems to be overkill to rely on such rich semantic resources to achieve relatively modest precision. Fung and McKeown (1997) extract terminology translations based on the assumption of crosslingual distributional similarity, as defined by seed and previously-extracted translation pairs across comparable corpora. Th"
2003.mtsummit-papers.50,W01-1413,0,0.115896,"Missing"
2003.mtsummit-papers.50,C92-4201,0,0.515004,"racy of their method for nouns to be 0.55, although in the case of compound nouns it seems to be overkill to rely on such rich semantic resources to achieve relatively modest precision. Fung and McKeown (1997) extract terminology translations based on the assumption of crosslingual distributional similarity, as defined by seed and previously-extracted translation pairs across comparable corpora. Their results show the usefulness of comparable corpora, but also underline the dependence of the method on closely-correlated crosslingual corpora, which can sometimes be an unreasonable expectation. Rackow et al. (1992) look at a German–English compound noun MT task over a range of translation templates. They propose the use of “default constructions” for single words in a manner similar to that described in this research, but base determination of such defaults on English translations within a parallel corpus. Specifically, they take the Hansards corpus and observe which of ecology and ecological, e.g., occurs more often as a noun premodifier and use this information in generating ecological movement rather than ecology movement as a translation for the German Umweltbewegung. Unfortunately, no attempt is ma"
2003.mtsummit-papers.50,P99-1067,0,0.10257,"Missing"
2003.mtsummit-papers.50,C02-1065,1,0.85518,"Missing"
2003.mtsummit-papers.50,W03-1803,1,0.839899,"ly 400,000 entries including more than 200,000 proper nouns. In order to make ALTDIC directly compatible with the BNC-derived RASP tuples, we: (a) converted any American spellings to British spellings, and (b) lemmatised the spelling-normalised words using morph and the POS tags supplied in ALTDIC. Spelling normalisation was based on simple lookup in the VARCON table of American-British spelling variants.4 3.3 Test data The primary test data used in this research comprises 500 Japanese NN compounds extracted from the 1996 Mainichi Shimbun Corpus (Mainichi Newspaper Co., 1996), as described in Tanaka and Baldwin (2003). We first segmented and tagged the corpus using ALTJAWS5 and then extracted out all NN bigrams adjoined by non-nouns. We next filtered off all NN compounds with a token occurrence of less than 10. As our test data, we took the 250 most frequent NN compounds, and a random selection of 250 NN compounds from the remainder of the extracted data.6 In order to evaluate translation accuracy over the test data, we generated a unique gold-standard translation for each Japanese NN compound to represent its optimally-general English translation. This was done with reference to the ALTDIC dictionary and"
2003.mtsummit-papers.50,1999.tmi-1.11,1,0.728525,"anslations, that is translations which are syntactically unmarked, capture the basic semantics of the source language expression and from which the source language expression is recoverable with reasonable confidence. Examples include issue of blame and responsibility issue as alternative transsekiniN moNdai “liability islations for sue”. In this, we set ourselves apart from the highly subjective method of manual translation evaluation where bilingual annotators are presented with the source language expression and system output, and asked to rate the output for “plausibility” or “usability” (Tanaka and Matsuo, 1999; Cao and Li, 2002). By pre-generating our source language-recoverable translations (or L1-coverable translations) we remove the annotator from direct contact with our method and hence hope to make evaluation as objective as possible. Given that we are only ever required to evaluate translations generated by our method, we only consider those translation candidates our method can generate for a given source language expression using ALTDIC. To reduce the annotation overhead, we break the task down into two steps: (1) identify appropriate word-level translations for each member of the NN compou"
2003.mtsummit-papers.50,W97-0119,0,\N,Missing
2018.gwc-1.19,W04-2209,0,0.0647191,"i “said term’s meaning” という意味は to iu imi wa “as for the said term’s meaning” の意味は/のいみは no imi wa “as for the meaning of” 169,756,339 19,134,679/1,207,555 5,360,613/167,095 4,544,800/10,364 51,726 1,979,108/1,169 Table 1: Google n-gram Corpus Frequencies of Text Patterns terms and are probably worth further investigation. 4.2 Testing Contexts of Known New Terms We also investigated the sorts of contexts in which known new terms are being used to see if any useful additional patterns could be identified. As an initial exploration 5 terms were chosen from recent additions to the JMdict database (Breen, 2004) which had been noted as popular new words/expressions. The 5 terms were: • マタハラ matahara abbreviation meaning “workplace discrimination against pregnant women”; • こじらせ女子 kojirase joshi “girl who has low self-esteem”; • ナマポ namapo slang for “welfare recipient” • 美魔女 bimajo “middle-aged woman who looks very young for her age” • 隠 れ メ タ ボ kakure metabo abbreviation meaning “normal weight obesity” 10 sentences for each term were extracted using a WWW search. While this is clearly a small number of samples, it emerged that there were relatively few of the と い う/と は/etc. sorts of patterns used; onl"
2018.gwc-1.19,P14-1119,0,0.0249622,"Singapore bond@ieee.org will do a WWW search for “⟨term⟩ とは”, etc. when encountering an unfamiliar term in order to identify cases where the term is being described, discussed or otherwise highlighted. The investigation broadly breaks into two components: a. the identification of the sorts of language patterns used to describe, discuss, highlight, etc. terms; b. the extraction and evaluation of terms so targeted by those language patterns. 2 Prior Work Research into the use of linguistic patterns in text to detect terms of interest has taken place in several contexts. In keyphrase extraction Hasan and Ng (2014) have produced a wide-ranging survey of the various techniques used in keyphrase extraction and their relative effectiveness, and Kim et al. (2013) evaluate the performance of a variety of supervised and unsupervised approaches. In term extraction, which is a major part of the broader field of terminology, usually in technical contexts (Kageura (2000)), Takeuchi et al. (2009) adapted the French ACABIT system, which detects morpho-syntactic sequences, to isolate terms in Japanese for later analysis. Le et al. (2013) used patterns of phrases to identify particular Japanese legal documents of int"
2018.gwc-1.19,W04-3230,0,0.036189,"Missing"
2018.gwc-1.19,sato-kaide-2010-person,0,0.0233685,"se was restricted to kanji sequences. The relationship between a text pattern and a term of interest is a form of collocation, i.e. lying between idiomatic expressions and free word combinations. In their survey of collocations in language processing, McKeown and Radev (2000) explore the role of the extraction of collocations in lexicography, although the focus is on the identification of general terms rather than those which are highlighted as being of interest. Prior published research into the use of Japanese text patterns which target general terms of interest appears to be quite limited. Sato and Kaide (2010) employed a related technique for extracting English–Japanese name pairs by scanning texts for nearby occurrences of Mr, Mrs, etc. and the Japanese equivalents, e.g. さん (san). 3 Text Corpora An essential element of the investigation is the availability of substantial quantities of Japanese text, preferably from a variety of sources. While there are number of Japanese corpora available for use in NLP work, most are actually quite small. In this study we used two text collections: a. the Kyoto WWW Corpus. This is a collection of 500 million Japanese sentences collected from WWW pages in 2004. Th"
2020.aacl-main.60,W05-0909,0,0.179523,"Missing"
2020.aacl-main.60,P16-1046,0,0.0901812,"Missing"
2020.aacl-main.60,N18-2097,0,0.0139307,"tractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-trained BERT models. We further conduct error analysis to better understand the limitat"
2020.aacl-main.60,N19-1395,0,0.0237846,"Missing"
2020.aacl-main.60,D18-1443,0,0.0119254,", and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) whic"
2020.aacl-main.60,P18-1013,0,0.0123031,"ext classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articl"
2020.aacl-main.60,D15-1229,0,0.0571084,"publicly available. Koto (2016) released a dataset for chat summarization by manually annotating chat logs from WhatsApp.14 However, this dataset contains only 300 documents. The largest summarization data to date is IndoSum (Kurniawan and Louvan, 2018), which has approximately 19K news articles with manually-written summaries. Based on our analysis, however, the summaries of IndoSum are highly extractive. Beyond Indonesian, there is only a handful of non-English summarization datasets that are of sufficient size to train modern deep learning summarization methods over, including: (1) LCSTS (Hu et al., 2015), which contains 2 million Chinese short texts constructed from the Sina Weibo microblogging website; and (2) ES-News (Gonzalez et al., 2019), which comprises 270k Spanish news articles with summaries. LCSTS documents are relatively short (less than 140 Chinese characters), while ES-News is not publicly available. Our goal is to create a benchmark corpus for Indonesian text summarization that is both large scale and publicly available. 7 Conclusion We release Liputan6, a large-scale summarization corpus for Indonesian. Our dataset comes with two test sets: a canonical test set and an “Xtreme”"
2020.aacl-main.60,N19-1260,0,0.0121347,"been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-trained BERT models. We further conduct error analysis to bett"
2020.aacl-main.60,L16-1129,1,0.80782,"n text summarization have largely been extractive and used small-scale datasets. Gunawan et al. (2017) developed an unsupervised summarization model over 3K news articles using heuristics such as sentence length, keyword frequency, and title features. In a similar vein, Najibullah (2015) trained a naive Bayes model to extract summary sentences in a 100-article dataset. 605 Aristoteles et al. (2012) and Silvia et al. (2014) apply genetic algorithms to a summarization dataset with less than 200 articles. These studies do not use ROUGE for evaluation, and the datasets are not publicly available. Koto (2016) released a dataset for chat summarization by manually annotating chat logs from WhatsApp.14 However, this dataset contains only 300 documents. The largest summarization data to date is IndoSum (Kurniawan and Louvan, 2018), which has approximately 19K news articles with manually-written summaries. Based on our analysis, however, the summaries of IndoSum are highly extractive. Beyond Indonesian, there is only a handful of non-English summarization datasets that are of sufficient size to train modern deep learning summarization methods over, including: (1) LCSTS (Hu et al., 2015), which contains"
2020.aacl-main.60,2020.coling-main.66,1,0.732015,"Missing"
2020.aacl-main.60,2020.acl-main.703,0,0.022553,"oder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization"
2020.aacl-main.60,W04-1013,0,0.131038,"trained with the Adam optimizer for 200,000 steps on 4×V100 16GB GPUs and evaluated every 10,000 steps. For summary generation, we use beam width = 5, trigram blocking, and a length penalty (Wu et al., 2016) to generate at least two sentences and at least 15 words (similar to the extractive model). Henceforth the abstractive model will be referred to as “B ERTA BS”. We additionally experiment with a third variant, “B ERT E XTA BS”, where we use the weights of the fine-tuned BERT in B ERT E XT for the encoder (instead of off-the-shelf BERT weights). 4 Experiment and Results We use three ROUGE (Lin, 2004) F-1 scores as evaluation metrics: R1 (unigram overlap), R2 (bigram overlap), and RL (longest common subsequence overlap). In addition, we also provide BERTS CORE (F-1), as has recently been used for machine translation evaluation (Zhang et al., 2020b).10 We use the development set to select the best checkpoint during training, and report the evaluation scores for the canonical and Xtreme test sets in Table 4. For both test sets, the summarization models are trained using the same training 10 set, but they are tuned with a different development set (see Section 2 for details). In addition to t"
2020.aacl-main.60,D19-1387,0,0.352871,"e the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More rece"
2020.aacl-main.60,K16-1028,0,0.0456162,"Missing"
2020.aacl-main.60,D18-1206,0,0.0848463,"e English datasets, including 320K CNN/Daily Mail document–summary pairs (Hermann et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstra"
2020.aacl-main.60,D15-1044,0,0.0733956,"h extractive and abstractive summarization models. 1 Introduction Despite having the fourth largest speaker population in the world, with 200 million native speakers,1 Indonesian is under-represented in NLP. One reason is the scarcity of large datasets for different tasks, such as parsing, text classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g."
2020.aacl-main.60,P17-1099,0,0.316542,"of large datasets for different tasks, such as parsing, text classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was Rush et al. (2015), where the authors proposed an encoder–decoder model with attention to generate headlines for English Gigaword documents (Graff et al., 2003). Subsequent studies introduced pointer networks (Nallapati et al., 2016b; See et al., 2017), summarization with content selection (Hsu et al., 2018; Gehrmann et al., 2018), graph-based attentional models (Tan et al., 2017), and deep reinforcement learning (Paulus et al., 2018). More recently, we have seen the widespread adoption 1 https://www.visualcapitalist.com/ 100-most-spoken-languages/. of pre-trained neural language models for summarization, e.g. BERT (Liu and Lapata, 2019), BART (Lewis et al., 2020), and PEGASUS (Zhang et al., 2020a). Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document–"
2020.aacl-main.60,P19-1212,0,0.021889,"et al., 2015) and 100k NYT articles (Sandhaus, 2008) which have been widely used in abstractive summarization research (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020a). News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries (Narayan et al., 2018), BIGPATENT is a summarization dataset for the legal domain (Sharma et al., 2019), Reddit TIFU is sourced from social media (Kim et al., 2019), and Cohan et al. (2018) proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-t"
2020.aacl-main.60,P17-1108,0,0.0354875,"Missing"
2020.aacl-main.60,2020.acl-main.450,0,0.0339976,"Missing"
2020.acl-main.261,Q18-1041,0,0.330064,"ould have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison t"
2020.acl-main.261,D19-1667,0,0.496977,"ally profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison term associated with the case. This model was constructed using a large-scale dataset of real-world Chinese court cases. Case Study in Ethical NLP Publication 2.1 Data ethics The first dimension to consider is data ethics: the data source and procedure used to construct a dataset have an immediate impact on the generalisabilty/interpretation of resu"
2020.acl-main.261,P16-2096,0,0.0285247,"decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can provide an overall prediction of the prison term associated with the case. This mod"
2020.acl-main.261,C18-1041,0,0.0498888,"f any mention of ethics approval is certainly troubling given the sensitivity of the data/task. The paper does briefly mention the possibility of demographic bias, without making any attempt to quantify or ameliorate any such bias. Privacy is an interesting question here, as we return to discuss under “data misuse” in Section 2.2, in addition to discussing the legality of using court documents for NLP research. Having said this, we acknowledge that similar datasets have been constructed and used by others (esp. Xiao et al. (2018)), including in major NLP conferences (e.g. Zhong et al. (2018), Hu et al. (2018)). However, this should never be taken as a waiver for data ethic considerations. Also notable here are court proceeding datasets such as that of Aletras et al. (2016), where the use case is 2909 the prediction of the violation of human rights (focusing on torture/degrading treatment, the right to a fair trial, and respect for privacy), which is more clearly aligned with “social good” (although there is more dataset documentation that could have been provided in that paper, along the lines described above). The conversation of what social good is, though, remains an open one (Green, 2019). In"
2020.acl-main.261,D17-1323,0,0.0213424,"been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines. 1 Introduction 2 NLP tools are increasingly being deployed in the wild with potentially profound societal implications. Alongside the rise in technical capabilities has been a growing awareness of the moral obligation of the field to self-assess issues including: dataset and system bias (Zhao et al., 2017), dataset ethics (Bender and Friedman, 2018), and dual use (Hovy and Spruit, 2016). More recently, there has also been vigorous debate on whether it is ethical for the community to work on certain topics or data types. This paper aims to investigate this issue, focused around the examination of a paper recently published at EMNLP 2019 on automatic prison term prediction by Chen et al. (2019). Specifically, the paper in question proposes a neural model which performs structured prediction of the individual charges laid against an individual, and the prison term associated with each, which can p"
2020.acl-main.261,D18-1390,0,0.184232,"g judges). The lack of any mention of ethics approval is certainly troubling given the sensitivity of the data/task. The paper does briefly mention the possibility of demographic bias, without making any attempt to quantify or ameliorate any such bias. Privacy is an interesting question here, as we return to discuss under “data misuse” in Section 2.2, in addition to discussing the legality of using court documents for NLP research. Having said this, we acknowledge that similar datasets have been constructed and used by others (esp. Xiao et al. (2018)), including in major NLP conferences (e.g. Zhong et al. (2018), Hu et al. (2018)). However, this should never be taken as a waiver for data ethic considerations. Also notable here are court proceeding datasets such as that of Aletras et al. (2016), where the use case is 2909 the prediction of the violation of human rights (focusing on torture/degrading treatment, the right to a fair trial, and respect for privacy), which is more clearly aligned with “social good” (although there is more dataset documentation that could have been provided in that paper, along the lines described above). The conversation of what social good is, though, remains an open one"
2020.acl-main.448,W14-3302,0,0.111226,"Missing"
2020.acl-main.448,E06-1032,0,0.372381,"Missing"
2020.acl-main.448,W14-3333,1,0.766456,"researchers use metric scores to compare pairs of MT systems, for instance when claiming a new state of the art, evaluating different model architectures, or even in deciding whether to publish. Basing these judgements on metric score alone runs the risk of making wrong decisions with respect to the true gold standard of human judgements. That is, while a change may result in a significant improvement in BLEU, this may not be judged to be an improvement by human assessors. Thus, we examine whether metrics agree with DA on all the MT systems pairs across all languages used in WMT 19. Following Graham et al. (2014), we use statistiMetric Dierence 5 BLEU NS Worse NS Better NS 0.0-2.5 2.5-5.0 5.0-7.5 7.5-15.0 15.0-30.0 30.0-200.0 −0.5 0.0 0.5 1.0 1.5 2.0 DA Dierence Figure 4: Pairwise differences in human DA evaluation (x-axis) compared to difference in metric evaluation (binned on y-axis; NS means insignificant metric difference). The colours indicate pairs judged by humans to be insignificantly different (cyan/light gray), significantly worse (red/dark gray on the left) and significantly better (green/dark gray on the right). 4990 cal significance tests to detect if the difference in scores (human or"
2020.alta-1.12,E17-2041,0,0.0271735,"legal judgements. Even though thousands of court 2 judgements are published in Australia every year, lawyers are only able to analyse small numbers of judgements, potentially missing broader trends hidden in the vast numbers of judgements that are published by the courts. There is a growing body of research at the intersection of Law and Natural Language Processing, including prediction of court opinion about a case (Chalkidis et al., 2019a; Aletras et al., 2016), classification of legal text by legal topics or issues (Soh et al., 2019; Chalkidis et al., 2019b), and legal entity recognition (Cardellino et al., 2017). However, our ultimate goal is to assist lawyers in identifying sections of judgements relevant to their case at hand, as well as bulk analysis of cases to identify relationships between factual patterns and decision outcomes. For this reason, we model our initial study on the sentence-by-sentence identification of argumentation zones within academic and scientific texts (Teufel et al., 2009; Guo et al., 2010). However, these zoning papers do not account for the complex document structure of legal judgements, which have the potential to be structured as multiple sub-documents within the one c"
2020.alta-1.12,P19-1424,0,0.0213888,"Missing"
2020.alta-1.12,W19-2209,0,0.0429227,"Missing"
2020.alta-1.12,N19-1423,0,0.00549075,"ight restrictions on republication. Republication of court judgments in an altered form, which our labelled dataset would be, is not allowed. Micro F1 Class R Macro F1 Model P RoBERTa BERT XLNet .64 .64 .65 .67 .70 .70 .65 .65 .66 .71 .70 .72 XLNet MajorityClass NBSVM .20 .55 .33 .56 .25 .55 .59 .63 XLNetcontext Model Table 1: Initial Performance Evaluation and (2) the NBSVM model proposed by Wang and Manning (2012), which combines a naive Bayes model with a support vector machine, using a bagof-words text representation. We compare this with a set of pre-trained language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019). We employ similar structures for these models: 12 layers of transformer blocks, a hidden layer size of 768d, and 12 attention heads. All models are trained by adding a 5 single hidden layer with softmax output. Initial Results We evaluate our models using Precision, Recall, and Macro-averaged and Microaveraged F1, showing the results in Table 1. The NBSVM model outperforms the majority class baseline by 0.30 in Macro F1. Using a pre-trained model further improves the performance, with XLNet increasing Macro F1 by 0.11 over the NBSVM b"
2020.alta-1.12,W10-1913,0,0.0348779,"(Chalkidis et al., 2019a; Aletras et al., 2016), classification of legal text by legal topics or issues (Soh et al., 2019; Chalkidis et al., 2019b), and legal entity recognition (Cardellino et al., 2017). However, our ultimate goal is to assist lawyers in identifying sections of judgements relevant to their case at hand, as well as bulk analysis of cases to identify relationships between factual patterns and decision outcomes. For this reason, we model our initial study on the sentence-by-sentence identification of argumentation zones within academic and scientific texts (Teufel et al., 2009; Guo et al., 2010). However, these zoning papers do not account for the complex document structure of legal judgements, which have the potential to be structured as multiple sub-documents within the one court decision (see Section 3). The overall goal of the project is to automate the extraction of information from legal judgements, to assist lawyers to more easily and quickly identify the type of information that they are looking for from a large number of judgements. The project also aims to enable the large-scale analysis of judgements by legal researchers in order to identify trends or patterns that may be"
2020.alta-1.12,2021.ccl-1.108,0,0.0291721,"Missing"
2020.alta-1.12,W19-2208,0,0.0191866,"(Politics By Aristotle), written 350 B.C.E, translated by Benjamin Jowett legal judgements. Even though thousands of court 2 judgements are published in Australia every year, lawyers are only able to analyse small numbers of judgements, potentially missing broader trends hidden in the vast numbers of judgements that are published by the courts. There is a growing body of research at the intersection of Law and Natural Language Processing, including prediction of court opinion about a case (Chalkidis et al., 2019a; Aletras et al., 2016), classification of legal text by legal topics or issues (Soh et al., 2019; Chalkidis et al., 2019b), and legal entity recognition (Cardellino et al., 2017). However, our ultimate goal is to assist lawyers in identifying sections of judgements relevant to their case at hand, as well as bulk analysis of cases to identify relationships between factual patterns and decision outcomes. For this reason, we model our initial study on the sentence-by-sentence identification of argumentation zones within academic and scientific texts (Teufel et al., 2009; Guo et al., 2010). However, these zoning papers do not account for the complex document structure of legal judgements, wh"
2020.alta-1.12,D09-1155,0,0.0521471,"opinion about a case (Chalkidis et al., 2019a; Aletras et al., 2016), classification of legal text by legal topics or issues (Soh et al., 2019; Chalkidis et al., 2019b), and legal entity recognition (Cardellino et al., 2017). However, our ultimate goal is to assist lawyers in identifying sections of judgements relevant to their case at hand, as well as bulk analysis of cases to identify relationships between factual patterns and decision outcomes. For this reason, we model our initial study on the sentence-by-sentence identification of argumentation zones within academic and scientific texts (Teufel et al., 2009; Guo et al., 2010). However, these zoning papers do not account for the complex document structure of legal judgements, which have the potential to be structured as multiple sub-documents within the one court decision (see Section 3). The overall goal of the project is to automate the extraction of information from legal judgements, to assist lawyers to more easily and quickly identify the type of information that they are looking for from a large number of judgements. The project also aims to enable the large-scale analysis of judgements by legal researchers in order to identify trends or pa"
2020.alta-1.12,P12-2018,0,0.00858273,"data; 4 We note that the dataset will not be made publicly available because the project team does not have the right to publish this data. Whilst court judgments are in the public domain, there are copyright restrictions on republication. Republication of court judgments in an altered form, which our labelled dataset would be, is not allowed. Micro F1 Class R Macro F1 Model P RoBERTa BERT XLNet .64 .64 .65 .67 .70 .70 .65 .65 .66 .71 .70 .72 XLNet MajorityClass NBSVM .20 .55 .33 .56 .25 .55 .59 .63 XLNetcontext Model Table 1: Initial Performance Evaluation and (2) the NBSVM model proposed by Wang and Manning (2012), which combines a naive Bayes model with a support vector machine, using a bagof-words text representation. We compare this with a set of pre-trained language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019). We employ similar structures for these models: 12 layers of transformer blocks, a hidden layer size of 768d, and 12 attention heads. All models are trained by adding a 5 single hidden layer with softmax output. Initial Results We evaluate our models using Precision, Recall, and Macro-averaged and Microaveraged F1, showing the results in"
2020.alta-1.14,P15-2030,0,0.0709999,"Missing"
2020.alta-1.14,N19-1423,0,0.102008,"cted hidden layers with a ReLU activation function before the final output layer. Note that we log-transform the signature count, consistent with previous work (Elnoshokaty et al., 2016; Proskurnia et al., 2017; Subramanian et al., 2018). 4.1.1 Text-only model We employ two different text-only model architectures: (1) a CNN regression model (Bitvai and 3 https://avaazdo.s3.amazonaws.com/do_ generic* (a) Text-only (b) Image-only (c) Combined model Figure 2: Overview of the models, where y denotes the signature count. Cohn, 2015) based on the method of Subramanian et al. (2018); and (2) a BERT (Devlin et al., 2019) regression model, where the [CLS] encoding of the final layer is used as the text representation. For our monolingual experiments over English petitions, for the CNN model we use GloVe (Pennington et al., 2014) word embeddings, and for BERT we use the pre-trained BERT-base English model (Devlin et al., 2019). For the multilingual experiments, we use the pre-trained multilingual BERT model (“mBERT”: Devlin et al. (2019)). An overview of the text model architecture is presented in Figure 2a. 4.1.2 Image-only model For the image-only model, we use InceptionResNet v2 (Szegedy et al., 2017) pre-tr"
2020.alta-1.14,P12-3005,1,0.784898,"Missing"
2020.alta-1.14,D14-1162,0,0.0837593,"017; Subramanian et al., 2018). 4.1.1 Text-only model We employ two different text-only model architectures: (1) a CNN regression model (Bitvai and 3 https://avaazdo.s3.amazonaws.com/do_ generic* (a) Text-only (b) Image-only (c) Combined model Figure 2: Overview of the models, where y denotes the signature count. Cohn, 2015) based on the method of Subramanian et al. (2018); and (2) a BERT (Devlin et al., 2019) regression model, where the [CLS] encoding of the final layer is used as the text representation. For our monolingual experiments over English petitions, for the CNN model we use GloVe (Pennington et al., 2014) word embeddings, and for BERT we use the pre-trained BERT-base English model (Devlin et al., 2019). For the multilingual experiments, we use the pre-trained multilingual BERT model (“mBERT”: Devlin et al. (2019)). An overview of the text model architecture is presented in Figure 2a. 4.1.2 Image-only model For the image-only model, we use InceptionResNet v2 (Szegedy et al., 2017) pre-trained on ImageNet, and extract the image representation from the penultimate layer. An overview of the image-only model is presented in Figure 2b. 4.1.3 Combined model In the combined model, we use text, image,"
2020.alta-1.14,P18-2030,1,0.917684,"1, opposing the Anti-Counterfeiting Trade Agreement (ACTA) and supporting a free and open internet. The popularity of a petition, in terms of the number of signatures it attracts, is critical to its success, and predicting popularity can help petition organizers to enhance their engagement strategy by optimising the petition content. In particular in this work, we target the task of predicting petition popularity at the time of submission (independent of any social media or direct signature signal). While existing work on petitions has focused on their text content (Elnoshokaty et al., 2016; Subramanian et al., 2018), images are also a key ingredient.1 Additionally, despite petitions being popular in many different languages, there has been 1 Timothy Baldwin♥ https://secure.avaaz.org/en/ community_petitions/how_to_create_ petition/ no work on multilingual modeling. From a social science viewpoint, multilingual analysis can contribute to an understanding of issues present in different languages (or regions). Previous research has shown that, other than petition content, metadata is also effective in modelling its popularity. Elnoshokaty et al. (2016) showed that the category of a petition has an influence"
2020.bionlp-1.11,W19-1909,0,0.0696569,"Missing"
2020.bionlp-1.11,S17-2001,0,0.0674623,") have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the i"
2020.bionlp-1.11,W05-1203,0,0.151145,"ely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS1 demonstrate substantial improvements, validating the utility of the proposed methods. 1 Introduction Semantic Textual Similarity (STS) is a language understanding task, involving assessing the degree of semantic equivalence between two pieces of text based on a graded numerical score (Corley and Mihalcea, 2005). It has application in tasks such as information retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regr"
2020.bionlp-1.11,N19-1423,0,0.214256,"referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (C"
2020.bionlp-1.11,I05-5002,0,0.0220521,"from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the impact of various pooling methods on"
2020.bionlp-1.11,D15-1181,0,0.0230962,"ion retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across f"
2020.bionlp-1.11,D17-1082,0,0.0158979,"ng the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home"
2020.bionlp-1.11,2021.ccl-1.108,0,0.124035,"Missing"
2020.bionlp-1.11,D19-1410,0,0.243996,"2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), perf"
2020.bionlp-1.11,P16-1009,0,0.0325008,"n. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using"
2020.bionlp-1.11,N16-1108,0,0.0388325,"Missing"
2020.bionlp-1.11,D19-1670,0,0.122152,"introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home). By contrast, replacing a complete mention of the concept can increase error propagation due to the prerequisite concept extraction and normalization. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improv"
2020.bionlp-1.11,N18-1101,0,0.0136766,"ed at improving downstream tasks indirectly by optimizing the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to af"
2020.bionlp-1.11,P19-1579,0,0.0156264,"NLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using a 12-layer base model initialized with pretrained weights. 3.1 Hierarchical Convolution (HConv) The resource-poor and concept-rich nature of clinical STS makes it difficult to train a large model endto-end on sentence pairs. To address this, most"
2020.bionlp-1.17,C08-1003,0,0.109274,"Missing"
2020.bionlp-1.17,W19-1909,0,0.0481317,"Missing"
2020.bionlp-1.17,W06-1615,0,0.168252,"l domains with unique vocabularies (Alsentzer et al., 2019; Lee et al., 2019). These models can also accomplish many tasks in an unsupervised manner. For example, Radford et al. (2019) showed that free text questions could be fed through a language model and generate the correct answer in many cases. In our experiments, we demonstrate the usefulness of contextualized language models by pre-training BERT on a large set of veterinary clinical records, and further explore its usefulness for domain adaptation through instance selection. Domain adaptation is a task which has a long history in NLP (Blitzer et al., 2006; Jiang and Zhai, 2007; Agirre and De Lacalle, 2008; Daum´e III, 2007). There has been further work demonstrating the usefulness of reducing the covariance between domains through adversarial learning (Li et al., 2018b). More recently, it has been shown that domain adversarial training can be effectively done using contextualized models, such as BERT, through using a two-step domain-discriminative data selection (Ma et al., 2019). We adapt these methods to our task to create a more generalizable SOURCE ( Ce f ov e c i n) TARGETY ( Ce p ha l e x i n ) TARGETZ ( Amoc y c i l l i nCl av u l on at"
2020.bionlp-1.17,U17-1008,1,0.832458,"ne with a variety of methods (Kiritchenko and Cherry, 2011; Goldstein et al., 2007; Li et al., 2018a). Additionally, classifying diseases and medications in clinical text has been addressed in shared tasks for human texts (Uzuner et al., 2010). Previous methods have also been explored for extracting the antimicrobials used, out of veterinary prescription labels, associated with the clinical records (Hur et al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expands on this work by linking the indication of use to an antimicrobial being administered for that diagnosis. Contextualized language models have recently gained much popularity due to their ability to greatly improve the representation of texts with fewer training instances, thereby transferring more efficiently between domains (Devlin et al., 2018; Howard and Ruder, 2018). Pre-training these language models on large amounts of text data specific to a given domain, such as clinical records or biomedical literature, has also been shown to further improve th"
2020.bionlp-1.17,P07-1033,0,0.354009,"Missing"
2020.bionlp-1.17,N19-1423,0,0.0691654,"Missing"
2020.bionlp-1.17,N13-1014,0,0.118762,"e used, with the form of drug administration (oral, injected, etc.) and different indications of use creating distinct contexts that can be seen as sub-domains. Therefore, models that allow for the transfer of knowledge between the sub-domains of the various antimicrobials are required to effectively label the indication of use. To explore the interaction between learning methods and the resource constraints on labeling, we develop models using the complete set of labels we had available, but also models derived using only labels that can be created within two hours, following the paradigm of Garrette and Baldridge (2013). Specifically, our work explores methods to improve the performance of classifying the indication for an antibiotic administration in veterinary records of dogs and cats. In addition to classifying the indication of use, we explore how data selection can be used to improve the transfer of knowledge derived from labeled data of a single antimicrobial agent to the context of other agents. We also release our code, and select pre-trained models used in this study at: https://github. com/havocy28/VetBERT. 157 Related Work Clinical coding of medical documents has been previously done with a variet"
2020.bionlp-1.17,P18-1031,0,0.0291686,"t al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expands on this work by linking the indication of use to an antimicrobial being administered for that diagnosis. Contextualized language models have recently gained much popularity due to their ability to greatly improve the representation of texts with fewer training instances, thereby transferring more efficiently between domains (Devlin et al., 2018; Howard and Ruder, 2018). Pre-training these language models on large amounts of text data specific to a given domain, such as clinical records or biomedical literature, has also been shown to further improve the performance in biomedical domains with unique vocabularies (Alsentzer et al., 2019; Lee et al., 2019). These models can also accomplish many tasks in an unsupervised manner. For example, Radford et al. (2019) showed that free text questions could be fed through a language model and generate the correct answer in many cases. In our experiments, we demonstrate the usefulness of contextualized language models b"
2020.bionlp-1.17,P07-1034,0,0.309145,"Missing"
2020.bionlp-1.17,S18-1190,0,0.0267164,"ained the prefixes clav or amoxyclav for amoxycillin clavulanate, ceph, rilex or kflex for cephalexin, and conv or cefov for cefovecin. These prefixes were sourced from a previous study exploring mention detection of antimicrobials (Hur et al., 2019). We signal the use of mention boundary embeddings with “+M” in the results tables. 4.2.2 Data augmentation Synonym-based data augmentation has been successfully applied to contexts including word sense disambiguation (Leacock and Chodorow, 1998), sentiment analysis (Li et al., 2017), text classification (Wei and Zou, 2019), and argument analysis (Joshi et al., 2018). We perform data augmentation on clinical notes by replacing synonyms using WordNet (Fellbaum, 160 S OURCE TARGET Y TARGET Z VetBERT+rank[linear] VetBERT+rank[linear]+A VetBERT+rank[linear]+M VetBERT+rank[linear]+M+A 74.3±0.2 75.8±1.3 73.4±0.9 75.7±0.8 76.6±3.0 81.0±2.6 77.1±1.9 81.0±2.8 66.9±2.2 63.7±1.4 65.9±2.4 63.8±3.5 VetBERT+rank[exp] VetBERT+rank[exp]+A VetBERT+rank[exp]+M VetBERT+rank[exp]+M+A 68.3±2.1 76.6±0.3 68.9±2.0 76.9±0.2 66.5±2.1 76.7±2.4 66.7±1.5 77.3±2.3 58.1±1.5 65.4±1.0 57.9±2.1 64.4±1.5 VetBERT+rank[rand] VetBERT+rank[rand]+A VetBERT+rank[rand]+M VetBERT+rank[rand]+M+A 73"
2020.bionlp-1.17,P11-1075,0,0.0329658,", our work explores methods to improve the performance of classifying the indication for an antibiotic administration in veterinary records of dogs and cats. In addition to classifying the indication of use, we explore how data selection can be used to improve the transfer of knowledge derived from labeled data of a single antimicrobial agent to the context of other agents. We also release our code, and select pre-trained models used in this study at: https://github. com/havocy28/VetBERT. 157 Related Work Clinical coding of medical documents has been previously done with a variety of methods (Kiritchenko and Cherry, 2011; Goldstein et al., 2007; Li et al., 2018a). Additionally, classifying diseases and medications in clinical text has been addressed in shared tasks for human texts (Uzuner et al., 2010). Previous methods have also been explored for extracting the antimicrobials used, out of veterinary prescription labels, associated with the clinical records (Hur et al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expand"
2020.bionlp-1.17,N18-2076,1,0.897088,"Missing"
2020.bionlp-1.17,E17-2004,1,0.874917,"Missing"
2020.bionlp-1.17,P19-1335,0,0.0420086,"Missing"
2020.bionlp-1.17,D19-6109,0,0.0362407,"Missing"
2020.bionlp-1.17,D19-1670,0,0.0288211,"re created by identifying strings that contained the prefixes clav or amoxyclav for amoxycillin clavulanate, ceph, rilex or kflex for cephalexin, and conv or cefov for cefovecin. These prefixes were sourced from a previous study exploring mention detection of antimicrobials (Hur et al., 2019). We signal the use of mention boundary embeddings with “+M” in the results tables. 4.2.2 Data augmentation Synonym-based data augmentation has been successfully applied to contexts including word sense disambiguation (Leacock and Chodorow, 1998), sentiment analysis (Li et al., 2017), text classification (Wei and Zou, 2019), and argument analysis (Joshi et al., 2018). We perform data augmentation on clinical notes by replacing synonyms using WordNet (Fellbaum, 160 S OURCE TARGET Y TARGET Z VetBERT+rank[linear] VetBERT+rank[linear]+A VetBERT+rank[linear]+M VetBERT+rank[linear]+M+A 74.3±0.2 75.8±1.3 73.4±0.9 75.7±0.8 76.6±3.0 81.0±2.6 77.1±1.9 81.0±2.8 66.9±2.2 63.7±1.4 65.9±2.4 63.8±3.5 VetBERT+rank[exp] VetBERT+rank[exp]+A VetBERT+rank[exp]+M VetBERT+rank[exp]+M+A 68.3±2.1 76.6±0.3 68.9±2.0 76.9±0.2 66.5±2.1 76.7±2.4 66.7±1.5 77.3±2.3 58.1±1.5 65.4±1.0 57.9±2.1 64.4±1.5 VetBERT+rank[rand] VetBERT+rank[rand]+A Ve"
2020.clinicalnlp-1.25,S15-2045,0,0.0502991,"Missing"
2020.clinicalnlp-1.25,S14-2010,0,0.0574759,"Missing"
2020.clinicalnlp-1.25,S16-1081,0,0.0499427,"Missing"
2020.clinicalnlp-1.25,S13-1004,0,0.0496199,"Missing"
2020.clinicalnlp-1.25,S12-1051,0,0.039632,"n STS-G + N2C2-STS train .894 .902 .830 .836 Eval set / Model Data STS-B dev: CLS-BERT CLS-BERT N2C2-STS test: HConvBERT HConvBERT Table 2: Pearson’s r and Spearman’s ρ evaluation on STS-B dev (upper half) and N2C2-STS test (bottom half), based on fine-tuning over STS-B train (5,749) and STS-G (28,518), for CLS-BERT and HConvBERT. clinical STS models? RQ2 How does low-quality training data impact clinical STS performance, vs. high-quality labelled data or no labelled data? Effect of Larger General STS Corpus. We source general-domain labelled data from: (1) SemEval-STS shared tasks 2012–2017 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017); and SICK-R (Marelli et al., 2014). This results in a total of 28,518 labelled sentence pairs, which we refer to as “STS-G”. We adapt a BERT encoder connected to a linear regression layer to fine-tune a general-domain STS model using STS-G, where the CLS-vector is used to represent the sentence pair (CLS-BERT). We compare this with a model trained only on STSB. We evaluate both models on STS-B dev (same setup as Section 6.1). For clinical STS, we employ a hierarchical convolution (HConv) model based on BERT (updating parameters of the last four layer"
2020.clinicalnlp-1.25,2020.acl-main.692,0,0.0204454,"or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts inevitably lead to performance drops (Gururangan et al., 2020). Therefore, we ask: RQ1 Can large-scale generaldomain labelled STS data be transferred to train Len Train Size Test Size MedSTS N2C2-STS 25.4 19.3 750 1642 318 412 Table 1: Clinical STS datasets. Train and Test Size = number of text pairs. Len = mean sentence length in tokens. Datasets and Tasks We select two available clinical STS benchmark datasets for evaluation: MedSTS (Wang et al., 20"
2020.clinicalnlp-1.25,W19-1909,0,0.0489185,"Missing"
2020.clinicalnlp-1.25,S17-2001,0,0.122684,"shop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts inevitably lead to performance drops (Gururangan et al., 2020). Therefore, we ask: RQ1 Can large-scale generaldomain labelled STS data be transferred to train Len Train Size Test Size MedSTS N2C2-STS 25.4 19.3 750 1"
2020.clinicalnlp-1.25,N19-1423,0,0.221766,"er to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to cova"
2020.clinicalnlp-1.25,2020.acl-main.740,0,0.0414559,"Missing"
2020.clinicalnlp-1.25,2021.ccl-1.108,0,0.0796948,"Missing"
2020.clinicalnlp-1.25,S14-2001,0,0.0322862,": CLS-BERT CLS-BERT N2C2-STS test: HConvBERT HConvBERT Table 2: Pearson’s r and Spearman’s ρ evaluation on STS-B dev (upper half) and N2C2-STS test (bottom half), based on fine-tuning over STS-B train (5,749) and STS-G (28,518), for CLS-BERT and HConvBERT. clinical STS models? RQ2 How does low-quality training data impact clinical STS performance, vs. high-quality labelled data or no labelled data? Effect of Larger General STS Corpus. We source general-domain labelled data from: (1) SemEval-STS shared tasks 2012–2017 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017); and SICK-R (Marelli et al., 2014). This results in a total of 28,518 labelled sentence pairs, which we refer to as “STS-G”. We adapt a BERT encoder connected to a linear regression layer to fine-tune a general-domain STS model using STS-G, where the CLS-vector is used to represent the sentence pair (CLS-BERT). We compare this with a model trained only on STSB. We evaluate both models on STS-B dev (same setup as Section 6.1). For clinical STS, we employ a hierarchical convolution (HConv) model based on BERT (updating parameters of the last four layers), where the model is first fine-tuned with STS-B, then N2C2STS is augmented"
2020.clinicalnlp-1.25,W19-5006,0,0.0717902,"al STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to covariate shift. To bridge this gap, a standard approach is to pretrain the LM on in-domain text, such as ClinicalBERT (Alsentzer et al., 2019) using MIMICIII (Johnson et al., 2016). However, existing research has tended to estimate effectiveness under the fine-tuning setting, rather than via inference tasks (Peng et al., 2019; Wang et al., 2020b). In this paper, we first evaluate domain pretraining approaches for clinical STS, with no labelled data. Based on the assumption that general STS models trained on large-scale STS datasets will perform reasonably well on clinical sentence pairs (Section 4), we then experiment with learning from the pseudo-labelled data (Section 5). Experimental results show both domain pretraining and pseudo-labelled data fine-tuning improve clinical STS, and the combination of the two achieves the best performance of r = 0.80 on N2C2-STS (Section 6.3). Further analysis shows that the sco"
2020.clinicalnlp-1.25,D14-1162,0,0.0835444,"specific text and task-specific labelled data (Gururangan et al., 2020; Peng et al., 2019). For this approach, however, domain-specific labelled data is required, an assumption that we seek to relax. 227 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy"
2020.clinicalnlp-1.25,D19-1173,0,0.0340273,"Missing"
2020.clinicalnlp-1.25,D19-1410,0,0.0111701,"a (Gururangan et al., 2020; Peng et al., 2019). For this approach, however, domain-specific labelled data is required, an assumption that we seek to relax. 227 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts i"
2020.clinicalnlp-1.25,D18-1187,0,0.0306164,"Missing"
2020.clinicalnlp-1.25,2020.bionlp-1.11,1,0.903317,"cale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to covariate shift. To bridge this gap, a standard approach is to pretrain the LM on in-domain text, such as ClinicalBER"
2020.clinicalnlp-1.25,2020.acl-main.414,0,0.0130279,"al model. We evaluate the approach on two clinical STS datasets, and achieve r = 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specifi"
2020.clinicalnlp-1.25,2020.tacl-1.8,0,0.0188378,"C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of cl"
2020.coling-main.330,S07-1109,0,0.0611646,"e simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution (Markert and Nissim, 2007) further catalyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC). Later work (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast, our approach is end-to-end: information"
2020.coling-main.330,N19-1423,0,0.038838,"resented as one-hot vectors and feed into two dense layers, for the left and right contexts, separately. By concatenating the four layers’ output and feeding it to a multi-layer perceptron, we get the final label. In line with the original paper, we use GloVe embeddings to represent the words (Pennington et al., 2014), set the window size to 5, use a dropout rate of 0.2, and trained the model for 5 epochs. To make the baseline model more competitive with our approach, we additionally experiment with a variant of the baseline where we replace the original GloVe embeddings with BERT embeddings (Devlin et al., 2019). We experimented with both BERT-base and BERT-large, but present results for BERT-base as we observed no improvement using the larger model. 4.3 Our Model We use BERT in three settings, for both the BERT-base (“BERT- BASE”) and BERT-large (“BERT- LG”) models: (1) fine-tuned over a given dataset, with no masking; (2) fine-tuned with data augmentation (see Section 3.3); and (3) fine-tuned using target word masking (see Section 3.4). We use the uncased model with a learning rate of 5e-5, and max sequence length of 256. For W I MC OR, we fine-tune for 1 epoch with a batch size of 64, and dropout"
2020.coling-main.330,S07-1033,0,0.205987,"they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution (Markert and Nissim, 2007) further catalyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC). Later work (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast, our approach is end-"
2020.coling-main.330,J91-1003,0,0.139155,"parser, and show that it boosts geoparsing performance. Third, we demonstrate that our method generalises better cross-domain, while being more data efficient. Finally, we conduct a detailed error analysis from the task rather than model perspective. Our code is available at: https://github.com/haonan-li/TWM-metonymy-resolution. 2 Related Work In early symbolic work, metonymy was treated as a syntactico-semantic violation (Hobbs Sr and Martin, 1987; Pustejovsky, 1991). As such, the resolution of metonymy was based on constraint violation, usually based on the selectional preferences of verbs (Fass, 1991; Hobbs et al., 1993). Markert and Nissim (2002) were the first to treat metonymy resolution as a classification task, based on corpus and linguistic analysis. They demonstrated that grammatical roles and syntactic associations are high-utility features, which they subsequently extended to include syntactic head–modifier relations and grammatical roles (Nissim and Markert, 2003). To tackle data sparseness, they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resou"
2020.coling-main.330,P17-1115,0,0.269365,"rk (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast, our approach is end-to-end: information is contained in the pretrained embeddings and language models only. Another difference is that we focus on the context of the PMW only, and not the PMW itself. More recently, in a departure from using ever-more hand-crafted features, Gritta et al. (2017) proposed a metonymy resolution approach based on basic parsing features and word embeddings. The main idea is to eliminate words that are superfluous to the task and keep only relevant words, by constructing a “predicate window” from the target word via a syntactic dependency graph. The classification of the target word is then based on the “predicate window”. Similar to us, they do not take the identity of the target word into consideration. However, we remove the dependency on a dependency parser, and more systematically generate a context representation by masking the target word within a"
2020.coling-main.330,P92-1047,0,0.113905,"Missing"
2020.coling-main.330,2020.lrec-1.697,0,0.0203901,"m the target word via a syntactic dependency graph. The classification of the target word is then based on the “predicate window”. Similar to us, they do not take the identity of the target word into consideration. However, we remove the dependency on a dependency parser, and more systematically generate a context representation by masking the target word within a pretrained language model. Researchers have released several datasets for metonymy resolution, including S EM E VAL (Nissim and Markert, 2003), R E L OCA R and C O NLL (Gritta et al., 2017), GWN (Gritta et al., 2019), and W I MC OR (Kevin and Michael, 2020). However, none of them have analyzed the data distribution and or generalisation across datasets. In this paper, we train our model on different datasets, and evaluate its transfer learning abilities. 3 Approach Formally, given a sentence and target word1 contained within it, metonymy resolution is the classification of whether the target word is a metonym or not, and what metonymic readings it has. 1 In practice, the target “word” can be made up of multiple tokens, either due to it containing multiple word pieces, or being a longer phrase such as New York. 3697 3.1 Motivation Due to the rela"
2020.coling-main.330,N18-2072,0,0.0269083,"transfer learning abilities. 3 Approach Formally, given a sentence and target word1 contained within it, metonymy resolution is the classification of whether the target word is a metonym or not, and what metonymic readings it has. 1 In practice, the target “word” can be made up of multiple tokens, either due to it containing multiple word pieces, or being a longer phrase such as New York. 3697 3.1 Motivation Due to the relatively small size of most existing metonymy resolution datasets, researchers have explored ways to compensate for the sparse training data, e.g. through data augmentation (Kobayashi, 2018; Wei and Zou, 2019). Modern pre-trained language models offer an alternative approach which performs well when fine-tuned over even small, task-specific datasets (Houlsby et al., 2019; Porada et al., 2019). Data sparseness may, however, still lead to overfitting. For example, in our metonymy resolution task, if the target word Vancouver appears only once during training, in the form of a metonymy, the model might overfit and always predict that Vancouver is a metonymy regardless of context. Intuitively, masking the target word during training can eliminate lexical bias and force the model to"
2020.coling-main.330,N15-1098,0,0.0715143,"Missing"
2020.coling-main.330,W02-1027,0,0.199574,"eoparsing performance. Third, we demonstrate that our method generalises better cross-domain, while being more data efficient. Finally, we conduct a detailed error analysis from the task rather than model perspective. Our code is available at: https://github.com/haonan-li/TWM-metonymy-resolution. 2 Related Work In early symbolic work, metonymy was treated as a syntactico-semantic violation (Hobbs Sr and Martin, 1987; Pustejovsky, 1991). As such, the resolution of metonymy was based on constraint violation, usually based on the selectional preferences of verbs (Fass, 1991; Hobbs et al., 1993). Markert and Nissim (2002) were the first to treat metonymy resolution as a classification task, based on corpus and linguistic analysis. They demonstrated that grammatical roles and syntactic associations are high-utility features, which they subsequently extended to include syntactic head–modifier relations and grammatical roles (Nissim and Markert, 2003). To tackle data sparseness, they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution"
2020.coling-main.330,S07-1007,0,0.217893,"ere the first to treat metonymy resolution as a classification task, based on corpus and linguistic analysis. They demonstrated that grammatical roles and syntactic associations are high-utility features, which they subsequently extended to include syntactic head–modifier relations and grammatical roles (Nissim and Markert, 2003). To tackle data sparseness, they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution (Markert and Nissim, 2007) further catalyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC)."
2020.coling-main.330,D09-1095,0,0.143724,"alyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC). Later work (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast, our approach is end-to-end: information is contained in the pretrained embeddings and language models only. Another difference is that we focus on the context of the PMW only, and not the PMW itself. More recently, in a departure from using ever-more hand-crafted features, Gritta et al. (2017) propose"
2020.coling-main.330,D12-1017,0,0.0985433,"k by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC). Later work (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast, our approach is end-to-end: information is contained in the pretrained embeddings and language models only. Another difference is that we focus on the context of the PMW only, and not the PMW itself. More recently, in a departure from using ever-more hand-crafted features, Gritta et al. (2017) proposed a metonymy resolutio"
2020.coling-main.330,S07-1101,0,0.0494169,"ckle data sparseness, they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution (Markert and Nissim, 2007) further catalyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking systems (Nicolae et al., 2007; Farkas et al., 2007; Brun et al., 2007) used features provided by the organisers, such as syntactic roles and morphological features. Most systems also used features from external resources such as WordNet, FrameNet, VerbNet, and the British National Corpus (BNC). Later work (Nastase and Strube, 2009; Nastase et al., 2012; Nastase and Strube, 2013) used the Wikipedia category network to capture the global context of PMWs, to complement local context features. All the above-mentioned approaches resolve metonymy by enriching the information about PMWs, in particular via resources. In contrast,"
2020.coling-main.330,P03-1008,0,0.437239,"rk, metonymy was treated as a syntactico-semantic violation (Hobbs Sr and Martin, 1987; Pustejovsky, 1991). As such, the resolution of metonymy was based on constraint violation, usually based on the selectional preferences of verbs (Fass, 1991; Hobbs et al., 1993). Markert and Nissim (2002) were the first to treat metonymy resolution as a classification task, based on corpus and linguistic analysis. They demonstrated that grammatical roles and syntactic associations are high-utility features, which they subsequently extended to include syntactic head–modifier relations and grammatical roles (Nissim and Markert, 2003). To tackle data sparseness, they further introduce simpler grammatical features by integrating a thesaurus. Much of this work has been preserved in recent work in the form of hand-engineered features and external resources. SemEval 2007 Task 8 on metonymy resolution (Markert and Nissim, 2007) further catalyzed interest in the task by releasing a metonymy dataset with syntatic and grammatical annotations, and fine-tuning the task definition and evaluation metrics. A range of learning paradigms (including maximum entropy, decision trees, and naive Bayes) were applied to the task. Top-ranking sy"
2020.coling-main.330,D14-1162,0,0.0908009,"to a particular place. 3699 SpaCy,3 and index the predicate window by the dependency head of the PMW. The output of the predicate window is then fed into two LSTM layers, one for the left context and one for the right context. The dependency relation labels of the content of the predicate window are represented as one-hot vectors and feed into two dense layers, for the left and right contexts, separately. By concatenating the four layers’ output and feeding it to a multi-layer perceptron, we get the final label. In line with the original paper, we use GloVe embeddings to represent the words (Pennington et al., 2014), set the window size to 5, use a dropout rate of 0.2, and trained the model for 5 epochs. To make the baseline model more competitive with our approach, we additionally experiment with a variant of the baseline where we replace the original GloVe embeddings with BERT embeddings (Devlin et al., 2019). We experimented with both BERT-base and BERT-large, but present results for BERT-base as we observed no improvement using the larger model. 4.3 Our Model We use BERT in three settings, for both the BERT-base (“BERT- BASE”) and BERT-large (“BERT- LG”) models: (1) fine-tuned over a given dataset, w"
2020.coling-main.330,N18-1202,0,0.130038,"Missing"
2020.coling-main.330,D19-6015,0,0.0131326,"hat metonymic readings it has. 1 In practice, the target “word” can be made up of multiple tokens, either due to it containing multiple word pieces, or being a longer phrase such as New York. 3697 3.1 Motivation Due to the relatively small size of most existing metonymy resolution datasets, researchers have explored ways to compensate for the sparse training data, e.g. through data augmentation (Kobayashi, 2018; Wei and Zou, 2019). Modern pre-trained language models offer an alternative approach which performs well when fine-tuned over even small, task-specific datasets (Houlsby et al., 2019; Porada et al., 2019). Data sparseness may, however, still lead to overfitting. For example, in our metonymy resolution task, if the target word Vancouver appears only once during training, in the form of a metonymy, the model might overfit and always predict that Vancouver is a metonymy regardless of context. Intuitively, masking the target word during training can eliminate lexical bias and force the model to learn to classify based on the context of use rather than the target word. 3.2 BERT for Word-level Classification For the tokenised sentence S = ht1 , t2 , ...tn i and target word hti , ...tj i with positio"
2020.coling-main.330,J91-4003,0,0.10735,"insic evaluation of location metonymy resolution, we include an extrinsic evaluation, where we incorporate a locative metonymy resolver into a geoparser, and show that it boosts geoparsing performance. Third, we demonstrate that our method generalises better cross-domain, while being more data efficient. Finally, we conduct a detailed error analysis from the task rather than model perspective. Our code is available at: https://github.com/haonan-li/TWM-metonymy-resolution. 2 Related Work In early symbolic work, metonymy was treated as a syntactico-semantic violation (Hobbs Sr and Martin, 1987; Pustejovsky, 1991). As such, the resolution of metonymy was based on constraint violation, usually based on the selectional preferences of verbs (Fass, 1991; Hobbs et al., 1993). Markert and Nissim (2002) were the first to treat metonymy resolution as a classification task, based on corpus and linguistic analysis. They demonstrated that grammatical roles and syntactic associations are high-utility features, which they subsequently extended to include syntactic head–modifier relations and grammatical roles (Nissim and Markert, 2003). To tackle data sparseness, they further introduce simpler grammatical features"
2020.coling-main.330,P93-1012,0,0.496595,"Missing"
2020.coling-main.330,P16-1158,1,0.84317,"Missing"
2020.coling-main.330,D19-1670,0,0.017799,"g abilities. 3 Approach Formally, given a sentence and target word1 contained within it, metonymy resolution is the classification of whether the target word is a metonym or not, and what metonymic readings it has. 1 In practice, the target “word” can be made up of multiple tokens, either due to it containing multiple word pieces, or being a longer phrase such as New York. 3697 3.1 Motivation Due to the relatively small size of most existing metonymy resolution datasets, researchers have explored ways to compensate for the sparse training data, e.g. through data augmentation (Kobayashi, 2018; Wei and Zou, 2019). Modern pre-trained language models offer an alternative approach which performs well when fine-tuned over even small, task-specific datasets (Houlsby et al., 2019; Porada et al., 2019). Data sparseness may, however, still lead to overfitting. For example, in our metonymy resolution task, if the target word Vancouver appears only once during training, in the form of a metonymy, the model might overfit and always predict that Vancouver is a metonymy regardless of context. Intuitively, masking the target word during training can eliminate lexical bias and force the model to learn to classify ba"
2020.coling-main.523,D19-1352,0,0.0214971,"KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concept ci . Each page is represented by its title, text (only for candidate generation), and multilingual aliases from Wikidata. We follow a two-stage retrieval procedure: (1) candidate generation, where an IR method (e.g. BM25) is used to retrieve related documents; and (2) reranking of t"
2020.coling-main.523,N19-1423,0,0.0533084,"and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concept ci . Each page is represented by its title, text (only for candidate generation), and multilingual aliases from"
2020.coling-main.523,P19-1335,0,0.176791,"Missing"
2020.coling-main.523,P18-1010,0,0.0261336,"an IR method (e.g. BM25) is used to retrieve related documents; and (2) reranking of the top k candidates via a learn-to-rank method (Liu, 2009). 3.1 Candidate Generation We index Wikipedia collection D using Lucene, and build query qi from UMLS to retrieve the top k=64 relevant pages. We use a Boolean disjunction between all alias terms in UMLS, and search in the title, text, and multilingual aliases fields in D. BM25 relies on exact term matches, and small variations can result in a mismatch. As a result, we also experimented with a character n-gram method (TFIDFchar ) successfully used in Murty et al. (2018) for candidate generation in medical entity linking. We build a bag of character n-grams (n ∈ [1, 5])) weighted by TF-IDF within term boundaries, and use cosine similarity between qi and each d ∈ D (excluding page text) to generate the top k=64 candidates. 3.2 Reranking We formulate the reranking task as passage pair binary classification (Nogueira and Cho, 2019), where the first passage is qi for concept ci from UMLS, and the second passage is the set of Wikipedia alias 5958 UMLS Concept UMLS Query Gold Wiki Candidate CUI: C0017168 GERD, Acid Reflux, oesofagusaandoening, . . . disorder of the"
2020.coling-main.523,P19-1492,0,0.0170411,"in more than 134 languages, compared to only a dozen languages in UMLS. Knowledge-base alignment: There are two main approaches for aligning knowledge bases such as Wikipedia and UMLS: (1) embedding-based alignment, and (2) string and semantic matching. In embedding-based methods, entity embeddings are learnt from text co-occurrence statistics or knowledgegraph (KG) relations separately, and are then aligned using a seed alignment dictionary (Mikolov et al., 2013; Chen et al., 2017), or adversarial learning (Qu et al., 2019). However, the text-based methods suffer from non-comparable corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly"
2020.coling-main.523,D19-1540,0,0.0219568,"rom non-comparable corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve pag"
2020.coling-main.523,P16-1162,0,0.133624,"Missing"
2020.coling-main.523,W18-2306,0,0.109033,"corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concep"
2020.coling-main.523,E14-1049,0,\N,Missing
2020.coling-main.523,D19-1274,0,\N,Missing
2020.coling-main.66,S16-1081,0,0.0255512,"ng it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work a"
2020.coling-main.66,J08-1001,0,0.0614843,"Missing"
2020.coling-main.66,P16-1046,0,0.0611105,"Missing"
2020.coling-main.66,N19-1423,0,0.364595,"formance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics"
2020.coling-main.66,P05-1045,0,0.0259592,"3 tags based on Indonesian tag definition of Adriani et al. (2009). For I NDO LEM, we use the Indonesian POS tagging dataset of Dinakaramani et al. (2014), and 5-fold partitioning of Kurniawan and Aji (2018).15 Named entity recognition (NER). Budi et al. (2005) was the first study on named entity recognition for Indonesian, where roughly 2,000 sentences from a news portal were annotated with three NE classes: person, location, and organization. In other work, Luthfi et al. (2014) utilized Wikipedia and DBPedia to automatically generate an NER corpus, and trained a model with Stanford CRF-NER (Finkel et al., 2005). Rachman et al. (2017) studied LSTM performance over 480 tweets with the same three named entity classes. None of these authors released the datasets used in the research. There are two publicly-available Indonesian NER datasets. The first, NER UI, comprises 2,125 sentences obtained via an annotation assignment in an NLP course at the University of Indonesia in 2016 (Gultom and Wibowo, 2017). The corpus has the same three named entity classes as its predecessors (Budi et al., 2005). The second, NER UGM, comprises 2,343 sentences from news articles, and was constructed at the University of Gaj"
2020.coling-main.66,Y12-1014,0,0.027676,"arch. There are two publicly-available Indonesian NER datasets. The first, NER UI, comprises 2,125 sentences obtained via an annotation assignment in an NLP course at the University of Indonesia in 2016 (Gultom and Wibowo, 2017). The corpus has the same three named entity classes as its predecessors (Budi et al., 2005). The second, NER UGM, comprises 2,343 sentences from news articles, and was constructed at the University of Gajah Mada (Fachri, 2014) based on five named entity classes: person, organization, location, time, and quantity. Dependency parsing. Kamayani and Purwarianti (2011) and Green et al. (2012) pioneered dependency parsing for the Indonesian language. Kamayani and Purwarianti (2011) developed languagespecific dependency labels based on 20 sentences, adapted from Stanford Dependencies (de Marneffe and Manning, 2016). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly avai"
2020.coling-main.66,P19-1356,0,0.0256687,"cally for Indonesian.7 However, there is no comprehensive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in NLP research based on pretrained models in the last few years. Although attention-based models are data- and GPU-hungry, the full attention mechanisms and parallelism offered by the transformer are highly compatible with the high levels of parallelism that GPU computation offers, and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie"
2020.coling-main.66,P19-1340,0,0.0276502,"ve at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #dev #test 5-Fold Evaluation 7,222 1,530 1,"
2020.coling-main.66,D19-1279,0,0.126981,"rdering task, the fine-tuning procedure is detailed in Table 2. For dependency parsing, we follow Nguyen and Nguyen (2020) in incorporating BERT into the BiAffine dependency parser (Dozat and Manning, 2017) by replacing the word embeddings with the corresponding contextualized representations. Specifically, we generate the BERT embedding of the first WordPiece token as the word embedding, and train the BiAffine parser in its default configuration. In addition, we also benchmark against a pre-existing fine-tuned version of M BERT trained over 75 concatenated UD datasets in different languages (Kondratyuk and Straka, 2019). For summarization, we follow Liu and Lapata (2019) in encoding the document by inserting the tokens [CLS] and [SEP] between sentences. We also apply alternating segment embeddings based on whether the position of a sentence is odd or even. On top of the pre-trained model, we use a second transformer encoder to learn inter-sentential relationships. The input is the encoded [CLS] representation, and the output is the extractive label y ∈ {0, 1} (1 = include in summary; 0 = don’t include). 7 Results Table 3 shows the results for POS tagging and NER. M BERT, M ALAY BERT, and I NDO BERT perform v"
2020.coling-main.66,2020.aacl-main.60,1,0.732015,"Missing"
2020.coling-main.66,L16-1129,1,0.857928,"to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a standardized corpus, Koto (2016) released a 300-document chat summarization dataset, and Kurniawan and Louvan (2018) released the IndoSum 19K document–summary dataset. At the time we carried out this work,20 IndoSum was the largest Indonesian summarization corpus in the news domain, manually constructed from CNN Indonesia21 and Kumparan22 documents. IndoSum is a single-document summarization dataset where each article has one abstractive summary. Kurniawan and Louvan (2018) released IndoSum together with the O RACLE — a set of extractive summaries generated automatically by maximizing ROUGE score between sentences of the art"
2020.coling-main.66,N16-1030,0,0.137754,"Missing"
2020.coling-main.66,larasati-2012-identic,0,0.0331061,"n English POS-tagged dataset and noisily projecting the POS tags from English to the Indonesian translations. 9 The existing implementation merges all documents into one text stream https://kompas.com 11 https://koran.tempo.co 12 https://liputan6.com 13 We checkpointed the model at 1M and 2M steps, and found that 2M steps yielded a lower perplexity over the dev set. 14 http://www.panl10n.net/ 10 759 To create a larger and more reliable corpus, Dinakaramani et al. (2014) published a manually-annotated corpus of 260K tokens (10K sentences). The text was sourced from the IDENTIC parallel corpus (Larasati, 2012), which was translated from data in the Penn Treebank corpus. The text is manually annotated with 23 tags based on Indonesian tag definition of Adriani et al. (2009). For I NDO LEM, we use the Indonesian POS tagging dataset of Dinakaramani et al. (2014), and 5-fold partitioning of Kurniawan and Aji (2018).15 Named entity recognition (NER). Budi et al. (2005) was the first study on named entity recognition for Indonesian, where roughly 2,000 sentences from a news portal were annotated with three NE classes: person, location, and organization. In other work, Luthfi et al. (2014) utilized Wikiped"
2020.coling-main.66,2020.acl-main.653,0,0.0297278,"formance by 20 points at the time of writing. In the cross-lingual setting, XGLUE (Liang et al., 2020) was introduced as a benchmark dataset that covers nearly 20 languages. Unlike GLUE, XGLUE includes language generation tasks such as question and headline generation. One of the largest cross-lingual corpora is dependency parsing provided by Universal Dependencies.4 It has consistent annotation of 150 treebanks across 90 languages, constructed through an open collaboration involving many contributors. Recently, other cross-lingual benchmarks have been introduced, such as Hu et al. (2020) and Lewis et al. (2020). While these three cross-lingual benchmarks contain some resources/datasets for Indonesian, the coverage is low and data is limited. Beyond the English and cross-lingual settings, ChineseGLUE5 is a comprehensive NLU collection for Mandarin Chinese, covering eight different tasks. For the Vietnamese language, Nguyen and Nguyen (2020) gathered a dataset covering four tasks (NER, POS tagging, dependency parsing, and language inference), and empirically evaluated them against a monolingual BERT. Elsewhere, there are individual efforts to maintain a systematic catalogue of tasks and datasets, and"
2020.coling-main.66,2020.emnlp-main.484,0,0.0446606,"Missing"
2020.coling-main.66,W04-1013,0,0.103901,"Missing"
2020.coling-main.66,D19-1387,0,0.327715,", and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #d"
2020.coling-main.66,P16-1100,0,0.0202437,"uate it over I NDO LEM, in addition to benchmarking it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datase"
2020.coling-main.66,2020.findings-emnlp.92,0,0.0698743,"ng provided by Universal Dependencies.4 It has consistent annotation of 150 treebanks across 90 languages, constructed through an open collaboration involving many contributors. Recently, other cross-lingual benchmarks have been introduced, such as Hu et al. (2020) and Lewis et al. (2020). While these three cross-lingual benchmarks contain some resources/datasets for Indonesian, the coverage is low and data is limited. Beyond the English and cross-lingual settings, ChineseGLUE5 is a comprehensive NLU collection for Mandarin Chinese, covering eight different tasks. For the Vietnamese language, Nguyen and Nguyen (2020) gathered a dataset covering four tasks (NER, POS tagging, dependency parsing, and language inference), and empirically evaluated them against a monolingual BERT. Elsewhere, there are individual efforts to maintain a systematic catalogue of tasks and datasets, and state-of-the-art methods for each across multiple languages,6 including one specifically for Indonesian.7 However, there is no comprehensive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in N"
2020.coling-main.66,P19-1442,0,0.022459,"19) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for high-resource languages such as English. I NDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely as a masked language model trained using the Huggingface8 framework, following the default configura4 https://universaldependencies.org/ https://github.com/ChineseGLUE/ChineseGLUE 6 https://github.com/sebastianruder/NLP-progress 7 https://github.com/kmkurn/id-nlp-resource 8 https://huggingface.co/ 5 758 Data #train #dev #test 5-Fold Evaluation 7,222 1,530 1,687 4,477 700 802 170 187 559 100 2,006 425"
2020.coling-main.66,Y17-1012,0,0.0171057,"es, and was constructed at the University of Gajah Mada (Fachri, 2014) based on five named entity classes: person, organization, location, time, and quantity. Dependency parsing. Kamayani and Purwarianti (2011) and Green et al. (2012) pioneered dependency parsing for the Indonesian language. Kamayani and Purwarianti (2011) developed languagespecific dependency labels based on 20 sentences, adapted from Stanford Dependencies (de Marneffe and Manning, 2016). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly available. The Universal Dependencies (UD) project16 has released two different Indonesian corpora of relatively small size: (1) 5,593 sentences of UD-Indo-GSD (McDonald et al., 2013);17 and (2) 1,000 sentences of UD-Indo-PUD (Zeman et al., 2018).18 Alfina et al. (2019) found that these corpora contain annotation errors and did not deal adequately with Indonesian morphology. They release"
2020.coling-main.66,P18-2124,0,0.034456,"n addition to benchmarking it against existing resources. Our experiments show that I NDO BERT achieves state-of-the-art performance over most of the tasks in I NDO LEM. 1 Introduction Despite there being over 200M first-language speakers of the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are ba"
2020.coling-main.66,D15-1044,0,0.0158694,"u, sorry, I am a new follower. the new school academic year is January, on the ﬁrst day I may get question about ""tukang keong"". what are you doing beautiful? Figure 1: Example for the next tweet prediction task. To the left is the original Indonesian version and to the right is an English translation. The tweet indicated in bold is the correct next tweet. simply count the proportion of positive and negative polarity aspects, and label the sentence based on the majority sentiment. We discard a review if there is a tie in positive and negative aspects. Summarization. From attention mechanisms (Rush et al., 2015; See et al., 2017) to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a s"
2020.coling-main.66,P17-1099,0,0.0134496,"w follower. the new school academic year is January, on the ﬁrst day I may get question about ""tukang keong"". what are you doing beautiful? Figure 1: Example for the next tweet prediction task. To the left is the original Indonesian version and to the right is an English translation. The tweet indicated in bold is the correct next tweet. simply count the proportion of positive and negative polarity aspects, and label the sentence based on the majority sentiment. We discard a review if there is a tie in positive and negative aspects. Summarization. From attention mechanisms (Rush et al., 2015; See et al., 2017) to pre-trained language models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English in terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry methods. Indonesian (single document) text summarization research has inevitably focused predominantly on extractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over a 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summarization over 3,075 news articles. As an attempt to create a standardized corpus,"
2020.coling-main.66,N19-1035,0,0.0239138,"ive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to fill with I NDO LEM. 3 I NDO BERT Transformers (Vaswani et al., 2017) have driven substantial progress in NLP research based on pretrained models in the last few years. Although attention-based models are data- and GPU-hungry, the full attention mechanisms and parallelism offered by the transformer are highly compatible with the high levels of parallelism that GPU computation offers, and have been shown to be highly effective at capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In particular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and Lample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as masked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and driven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin et al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al., 2019), and discourse analysis (Nie et al., 2019). However, this effect has been larg"
2020.coling-main.66,D19-1575,0,0.366639,"the Indonesian language, the language is underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics, to enhance reproducibility and robust benchmarking. These tasks are intended to span a broad range of morpho-synta"
2020.coling-main.66,N18-1101,0,0.0473251,"underrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In English, on the other hand, there are ever-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016; Rajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks to benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of which have contributed to rapid progress in the field in recent years. We attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM (“Indonesian Language Evaluation Montage”2 ), a comprehensive dataset encompassing seven NLP tasks and eight sub-datasets, five of which are based on previous work and three are novel to this work. As part of this, we standardize data splits and evaluation metrics, to enhance reproducibility and robust benchmarking. These tasks are intended to span a broad range of morpho-syntactic, semantic, and discourse analysis compet"
2020.coling-main.66,D19-1077,0,0.0231914,"prised of 3–5 tweets, and we model the task via multi-classification (with 5 classes/ranks). We perform inference based on P (r|t), where we decide the final rank based on the highest sum of probabilities from the exhaustive enumeration of document ranks. 763 6.2 BERT Benchmarks To benchmark I NDO BERT, we compare against two pre-existing BERT models: multilingual BERT (“M BERT”), and a monolingual BERT for Malay (“M ALAY BERT”).29 M BERT is trained by concatenating Wikipedia documents for 104 languages including Indonesian, and has been shown to be effective for zero-shot multilingual tasks (Wu and Dredze, 2019; Wang et al., 2019c). M ALAY BERT is a a publicly available model that was trained on Malay documents from Wikipedia, local news sources, social media, and some translations from English. We expect M ALAY BERT to provide better representations than M BERT for the Indonesian language, because Malay and Indonesian are mutually intelligible, with many lexical similarities, but noticeable differences in grammar, pronunciation and vocabulary. For the sequence labelling tasks (POS tagging and NER), sentiment analysis, NTP, and tweet ordering task, the fine-tuning procedure is detailed in Table 2. F"
2020.coling-main.66,K18-2001,0,0.0201268,"6). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency labels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a comparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al., 2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly available. The Universal Dependencies (UD) project16 has released two different Indonesian corpora of relatively small size: (1) 5,593 sentences of UD-Indo-GSD (McDonald et al., 2013);17 and (2) 1,000 sentences of UD-Indo-PUD (Zeman et al., 2018).18 Alfina et al. (2019) found that these corpora contain annotation errors and did not deal adequately with Indonesian morphology. They released a corrected version of UD-Indo-PUD by fixing annotations for reduplicated-words, clitics, compound words, and noun phrases. We include two UD-based dependency parsing datasets in I NDO LEM: (1) UD-Indo-GSD, and (2) the corrected version of UD-Indo-PUD. As our reference dependency parser model, we use the BiAffine dependency parser (Dozat and Manning, 2017), which has been shown to achieve strong performance for English. 4.2 Semantic Tasks Sentiment a"
2020.nlpcovid19-2.12,2020.nlpcovid19-acl.1,0,0.0599788,"Missing"
2020.nlpcovid19-2.12,N10-1012,1,0.61303,"coherent topics were further subdivided into specific and generic. This distinction is important as some topics can be highly coherent, but not informative. This is especially visible in datasets where the documents are homogeneous both in terms of style (scientific articles) and content (related to coronaviruses). For example, such topics as [research, study, approach ] or [coronavirus, virus, disease ] are coherent, but not representative of the content of the paper. In line with this, each topic was assigned one of three labels by the annotators: incoherent, specific, or generic. Following Newman et al. (2010), to evaluate coherence, annotators were asked to decide if each topic was meaningful and interpretable. To judge specificity, they were instructed to decide if a particular set of words is Word tokens Concepts Non-generic concepts Incoherent Generic Specific 11 3 2 7 6 3 7 16 20 Table 1: Number of incoherent, generic and specific topics identified in topic models of 25 topics built over different representations of the CORD-19 corpus likely to occur in the majority of COVID-19 related studies or not. Annotators were provided examples of incoherent, specific and generic topics related to COVID"
2020.nlpcovid19-2.12,W00-0901,0,0.519699,"Missing"
2020.nlpcovid19-2.12,C16-1050,0,0.0283846,"5 for each model, as the coherence structure (boilerplate sentences, section headings scores are close enough to each other at this point. such as Discussion, phrases such as in conclusion), or be included in informative sentences but not be 4.2 Document representation meaningful for the purposes of topic modelling. As We consider three different input representations adding all such words to a stop-word list would of the text for inferring the models: not be feasible, we filter the concepts based on their semantic type as defined in UMLS. Following • Word tokens: The input text was tokenised ShafieiBavani et al. (2016) and Plaza et al. (2011), using the NLTK Tokeniser6 . who used a similar approach to filter concepts for • Concepts: Documents are transformed into graph-based summarisation of medical documents, an unordered set of Unified Medical Language we exclude terms based on broad semantic types 4 5 including QUANTITATIVE CONCEPT (rate, unit), pypi.org/project/pycld2/ ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T.stopwords/ 6 nltk.org 7 metamap.nlm.nih.gov Figure 1: Coherence scores for different representations of the CORD-19 corpus. (characteristics, different), TEMPORAL CONCEPT (year, recent), F"
2021.eacl-main.116,D16-1245,0,0.0492171,"Missing"
2021.eacl-main.116,P16-1061,0,0.0711216,"Missing"
2021.eacl-main.116,D14-1162,0,0.0838093,"Missing"
2021.eacl-main.116,N18-1202,0,0.0683807,"Missing"
2021.eacl-main.239,D16-1120,0,0.171996,"Missing"
2021.eacl-main.239,D18-1002,0,0.451424,"optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protected attributes from being detected by the attacker). Preventing protected attributes from being detected tends to result in fairer models, as protected attributes will more likely be independent rather than confounding variables. Although this method leads to demonstrably less biased models, there are still limitations, most notably that significant protected information still remains in the model’s encodings and prediction outputs (Wang et al., 2019; Elazar and Goldberg, 2018). Many different approaches have been proposed to strengthen the attacker, including: increasing the discriminator hidden dimensionality; assigning different weights to the adversarial component during training; using an ensemble of adversaries with different initializations; and reinitializing the adversarial weights every t epochs (Elazar and Goldberg, 2018). Of these, the ensemble method has been shown to perform best, but independently-trained attackers can generally still detect private information after adversarial removal. In this paper, we adopt adversarial debiasing approaches and pre"
2021.eacl-main.239,D17-1169,0,0.057939,"sentiment classification, we merge the original 64 labels into two categories based on the results of hierarchical clustering. null-space, h∗M = PN (Alinear ) hM , where PN (Alinear ) is the null-space projection matrix of Alinear . In doing so, it becomes difficult for the protected attribute to be linearly identified from the projected hidden representations (h∗M ), and any linear ∗ ) trained on h∗ can thus main-task classifier (CM M be expected to make fairer predictions. 3 Experiments Fixed Encoder Following Elazar and Goldberg (2018) and Ravfogel et al. (2020), we use the DeepMoji model (Felbo et al., 2017) as a fixedparameter encoder (i.e. it is not updated during training). The DeepMoji model is trained over 1246 million tweets containing one of 64 common emojis. We merge the 64 emoji labels output by DeepMoji into two super-classes based on hierarchical clustering: ‘happy’ and ‘sad’. Models The encoder EM consists of a fixed pretrained encoder (DeepMoji) and two trainable fully connected layers (“Standard” in Table 1). Every linear classifier (C) is implemented as a dense layer. For protected attribute prediction, a discriminator (A) is a 3-layer MLP where the first 2 layers are collectively"
2021.eacl-main.239,N18-2076,1,0.751304,"Missing"
2021.eacl-main.239,2020.acl-main.647,0,0.448739,"s, resulting in less biased models than the standard ensemble-based adversarial method. According to Bousmalis et al. (2016), the difference loss has the additional advantage of also being minimized when hidden representations shrink to zero. Therefore, instead of minimizing the difference loss by learning rotated hidden representations (i.e., the same model), this method biases adversaries to have representations that are a) orthogonal, and b) low magnitude; the degree to which is given by weight decay of the optimization function. 2.3 INLP We include Iterative Null-space Projection (“INLP”: Ravfogel et al. (2020)) as a baseline method for mitigating bias in trained models, in addition to standard and ensemble adversarial methods. In INLP, a linear discriminator (Alinear ) of the protected attribute is iteratively trained from pre-computed fixed hidden representations (i.e., hM ) to project them onto the linear discriminator’s 2761 Model Accuracy↑ TPR Gap↓ TNR Gap↓ Leakage@h↓ Leakage@ˆ y↓ Random Fixed Encoder 50.00±0.00 61.44±0.00 0.00±0.00 0.52±0.00 0.00±0.00 17.97±0.00 — 92.07±0.00 — 86.93±0.00 Standard 71.59±0.05 31.81±0.29 48.41±0.27 85.56±0.20 70.09±0.19 INLP Adv Single Discriminator Adv Ensemble"
2021.eacl-main.239,P15-2079,0,0.0259523,"f reducing bias and the stability of training. 1 Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, where the attacker detects protected information in the representation of the encoder, and the optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protec"
2021.eacl-main.239,S18-2005,0,0.0428432,"Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, where the attacker detects protected information in the representation of the encoder, and the optimization of the encoder incorporates two parts: (1) minimizing the main loss, and (2) maximizing the attacker loss (i.e., preventing protected attributes from being detected by the attacker). Pre"
2021.eacl-main.239,P18-2005,1,0.88064,"minators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training. 1 Introduction While NLP models have achieved great successes, results can depend on spurious correlations with protected attributes of the authors of a given text, such as gender, age, or race. Including protected attributes in models can lead to problems such as leakage of personally-identifying information of the author (Li et al., 2018a), and unfair models, i.e., models which do not perform equally well for different sub-classes of user. This kind of unfairness has been shown to exist in many different tasks, including part-of-speech tagging (Hovy and Søgaard, 2015) and sentiment analysis (Kiritchenko and Mohammad, 2018). One approach to diminishing the influence of protected attributes is to use adversarial methods, where an encoder attempts to prevent a discriminator from identifying the protected attributes in a given task (Li et al., 2018a). Specifically, an adversarial network is made up of an attacker and encoder, whe"
2021.eacl-main.4,D19-1210,0,0.0281502,". We ensure the dimensionality of input features for the SVM and Faster R-CNN are the same (both are 1, 024), to remove this possible representational confound. Note that this is the externally pre-trained image model across all experiments, and that none of the text models in this first set of experiments involve pre-training on external resources (something we return to in Section 4). 4 https://dumps.wikimedia.org/other/ kiwix/zim/wikipedia/wikipedia_en_all_ maxi_2020-06.zim 5 https://github.com/openzim/libzim 6 Because it is quite difficult to find correspondences between images and texts (Hessel et al., 2019), image links extracted are “document-level”, instead of “sentence-level”. 7 When a document has less than 4 images, we pad the representation with blank images. Results and Analysis We report F1 scores over the test set in Table 2. The main finding is that images improve performance in all settings, for all languages and both image representations. S2V and BERT both perform worse than the simple bag of words, because of the limited training data in each case. We would, of course, expect the models 3 Baseline Experiments 43 Text BOW BOW BOW S2V S2V S2V BERT BERT BERT — SIFT+V R-CNN — SIFT+V R-"
2021.eacl-main.4,N19-1422,0,0.018591,"n answering (VQA) (Goyal et al., 2017; Johnson et al., 2017), visual commonsense reasoning (VCR) (Zellers et al., 2019; Geva et al., 2019), and multi-modal pretraining (Lu et al., 2019; Chen et al., 2019). While tasks such as VQA and VCR are multimodal in nature, there has been research on traditionally text-based tasks such as text classification (Shen et al., 2020; Huang, 2018) and word embedding learning (Bruni et al., 2014) which has demonstrated that the addition of images boosts performance. At the same time, however, there is evidence of images providing no additional information, e.g. Caglayan et al. (2019) show that MMT models learn to ignore visual content when trained 2 Task Description This research is couched in the context of a shared-task dataset released by the SHINRA project (Sekine et al., 2019), aimed at classifying Wikipedia pages into fine-grained entity classes.1 We chose this benchmark as many Wikipedia documents contain images, and data is provided for a total of 29 typologically-diverse languages.2 The task is not trivial as it involves classifying Wikipedia documents into a set of 219 classes, with the possibility of multiple labels for a given document.3 1 http://shinra-projec"
2021.eacl-main.4,J84-2007,0,0.732816,"Missing"
2021.eacl-main.4,N19-1423,0,0.0872578,"Missing"
2021.eacl-main.4,D18-1329,0,0.0197564,"complement the text and the Wikipedia page can be in one of a number of different languages. Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training, but when combined with BERT models pre-trained on largescale external data, images contribute nothing. 1 Introduction Combining data from multiple modalities (e.g., text, images, categorical metadata, or user interaction features) has become commonplace in artificial intelligence. In NLP, examples include multimodal machine translation (MMT) (Elliott et al., 2016; Elliott, 2018), visual question answering (VQA) (Goyal et al., 2017; Johnson et al., 2017), visual commonsense reasoning (VCR) (Zellers et al., 2019; Geva et al., 2019), and multi-modal pretraining (Lu et al., 2019; Chen et al., 2019). While tasks such as VQA and VCR are multimodal in nature, there has been research on traditionally text-based tasks such as text classification (Shen et al., 2020; Huang, 2018) and word embedding learning (Bruni et al., 2014) which has demonstrated that the addition of images boosts performance. At the same time, however, there is evidence of images providing no additional in"
2021.eacl-main.4,W16-3210,0,0.061992,"Missing"
2021.eacl-main.4,N18-1049,0,0.0429186,"Missing"
2021.eacl-main.60,D15-1263,0,0.0203611,"1: An example discourse tree, from the RST Discourse Treebank (elab = elaboration). Discourse analysis involves the modelling of the structure of text in a document. It provides a systematic way to understand how texts are segmented hierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two senten"
2021.eacl-main.60,E17-1028,0,0.447664,"ite unit is a supporting sentence for the nucleus unit and contains less prominent information. It is standard practice that the RST tree is trained and evaluated in a right-heavy binarized manner, resulting in three forms of binary nuclearity relationships between EDUs: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this work, eighteen coarse-grained relations are considered as discourse labels, consistent with earlier work (Yu et al., 2018).2 Work on RST parsing has been dominated by the bottom-up paradigm (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU ("
2021.eacl-main.60,C16-1179,0,0.111074,"ion boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, wh"
2021.eacl-main.60,P19-1356,0,0.0164152,"and (3) position. The input vector is computed by summing these three embeddings, and fed into BERT (initialized with bert-base). The output of BERT ... MLP Layer Encoder 3.2 ỹEm Figure 4: Architecture of the transformer model. In practice, 1 row of input can have more than two EDUs. gives us a contextualized embedding for each token, and we use the [CLS] embedding as the encoding for each EDU (gEj ). Unlike the LSTM model, we do not incorporate syntax embeddings into the transformer model as we found no empirical benefit (see Section 4.3). This observation is in line with other studies (e.g. Jawahar et al. (2019)) that have found BERT to implicit encode syntactic knowledge. For the segmenter we use a second transformer (initialized with random weights) to capture the inter-EDU relationships for sub-sequences of EDUs during iterative segmentation: {h0Em , .., h0En } = transformer({hEm , .., hEn }) y˜Ej = σ(MLP(h0Ej )) where y˜Ej gives the probability of a segmentation, and hEj is the concatenation of the output of BERT (gEj ) and the EDU type embedding (tEj ). 3.3 Nuclearity and Discourse Relation Prediction In Figure 5, we give an example of the iterative segmentation process to construct the RST tree"
2021.eacl-main.60,W01-1605,0,0.634548,"ntation order O to preserve as many subtrees as possible when an error occurs. We present pseudocode for the proposed dynamic oracle in Algorithm 1. The probability of using the ground truth segmentation or predicted segmentation during training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±"
2021.eacl-main.60,P14-1002,0,0.676293,"ierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a"
2021.eacl-main.60,N19-1423,0,0.20877,"ation for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (decoder) stack Kobayashi et al., 2020 BiLSTM (single EDU) Gated MLP grouped by paragraph/ sentence/edu BiLSTM (inter-grouped unit) BiLSTM (inter-EDU) BiLSTM (seq labelling) queue Ours (v2) BERT (partial doc) Splitting via Deep BiAfﬁne Scoring exhaustive search space Ours (v1) BiLSTM (single EDU) use an LSTM (Hochreiter and Schmidhuber, 1997) or pre-trained BERT (Devlin et al., 2019) as the segmenter, enhanced in a number of key ways. Our primary contributions are as follows: (1) we propose a novel top-down approach to RST parsing based on sequence labelling; (2) we explore both traditional sequence models such as LSTMs and also modern pre-trained encoders such as BERT; (3) we demonstrate that adding a weighting mechanism during the splitting of EDU sequences improves performance; and (4) we propose a novel dynamic oracle for training top-down discourse parsers. .... Transformer (seq labelling) queue Figure 2: Comparison of our top-down models with Zhang et al. (2020) and"
2021.eacl-main.60,J95-3005,0,0.408828,"6; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pr"
2021.eacl-main.60,P81-1022,0,0.528388,"Missing"
2021.eacl-main.60,P14-1048,0,0.377429,"texts are segmented hierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”"
2021.eacl-main.60,P13-1048,0,0.0250831,"to gold span boundaries in evaluation. In this paper, we propose a conceptually simpler top-down approach for RST parsing. The core idea is to frame the problem as a sequence labelling task, where the goal is to iteratively find a segmentation boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and lang"
2021.eacl-main.60,J15-3002,0,0.101885,"del in the remainder of our experiments. We next benchmark our models against state-ofthe-art RST parsers over the test set, as presented in Table 2 (original Parseval) and Table 5 (RSTParseval as additional result). Except Yu et al. (2018), all bottom-up results are from Morey et al. (2017). We present the labelled attachment decision performance for Yu et al. (2018) by running the code of the authors for three runs and taking 9 721 The result is consistent with the test set (see Appendix B) Method S N R F Bottom Up: Feng and Hirst (2014)*† Ji and Eisenstein (2014)*† Surdeanu et al. (2015)*† Joty et al. (2015)* Hayashi et al. (2016)* Li et al. (2016)* Braud et al. (2017)* Yu et al. (2018) (static)‡ Yu et al. (2018) (dynamic)‡ 68.6 64.1 65.3 65.1 65.1 64.5 62.7 71.1 71.4 55.9 54.2 54.2 55.5 54.6 54.0 54.5 59.7 60.3 45.8 46.8 45.1 45.1 44.7 38.1 45.5 48.4 49.2 44.6 46.3 44.2 44.3 44.1 36.6 45.1 47.4 48.1 Top Down: Zhang et al. (2020)* 67.2 55.5 45.3 44.3 Our model Transformer (static)‡ Transformer (dynamic)‡ LSTM (static)‡ LSTM (dynamic)‡ 70.6 70.2 72.7 73.1 59.9 60.1 61.7 62.3 50.6 50.6 50.5 51.5 49.0 49.2 49.4 50.3 78.7 66.8 57.1 #Docs #Spans (0, 50] 21 404 (50, 100] 9 (100, 150] (150, ∞) Type S N"
2021.eacl-main.60,W07-2215,0,0.0483007,"e, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et"
2021.eacl-main.60,U19-1010,1,0.885145,"Missing"
2021.eacl-main.60,C12-1059,0,0.0376299,"DU2:2 ) 3.4 n  X where yEi ∈ {0, 1} is the ground truth segmentation label, L(Em:n ) is the cross-entropy loss for an EDU sequence, S is the set of all EDU sequences (based on ground truth segmentation), and β is a scaling hyper-parameter. To summarize, the total training loss of our model is a (weighted) combination of segmentation loss (Lseg ) and nuclearity-discourse prediction loss (Lnuc+dis ): L = λ1 Lseg + λ2 Lnuc+dis 3.5 (2) Dynamic Oracle The training regimen for discourse parsing creates an exposure bias, where the parser may struggle to recover when it makes a mistake at test time. Goldberg and Nivre (2012) propose a dynamic oracle for transition-based dependency parsing to tackle this. The idea is to allow the model during training to use its predictions (instead of ground truth actions), and introduce a dynamic oracle to find the next best/optimal action sequences. It does so by comparing the current state of the constructed tree and the gold-standard tree, and aims to minimize the deviation. As the model is exposed to prediction errors during training time, it has a better chance of recovering from them at test time. We explore a similar idea, and propose a dynamic oracle for our top-down dis"
2021.eacl-main.60,D14-1220,0,0.0850361,"for the proposed dynamic oracle in Algorithm 1. The probability of using the ground truth segmentation or predicted segmentation during training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±0.4 52.1±0.4 51.8±0.8 configuration as for the LSTM model. We tuned the segmentation loss pena"
2021.eacl-main.60,W16-3616,0,0.0325496,"Missing"
2021.eacl-main.60,D16-1035,0,0.676564,"rse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a supporting sente"
2021.eacl-main.60,P19-1410,0,0.612704,"parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et al., 2019) have been shown to benefit a multitude of NLP tas"
2021.eacl-main.60,J01-2004,0,0.445184,"). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et"
2021.eacl-main.60,D19-1093,0,0.0152729,"pose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models (Radford et al., 2018; Devlin et al., 2019) have been shown to benefit a multitude of NLP tasks, including discourse analysis. For example, BERT models have been used for classifying discourse markers (Sileo e"
2021.eacl-main.60,P99-1054,0,0.47651,", 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; Roark, 2001; Frost et al., 2007), but relatively new to discourse parsing. Lin et al. (2019) propose a unified framework based on pointer networks for sentence-level discourse parsing, while Liu et al. (2019) employ hierarchical pointer network parsers. Morey et al. (2017) found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging.3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval). Pre-trained language models"
2021.eacl-main.60,D19-1387,0,0.0224675,"want to encode sequences of EDUs that span multiple sentences. In our case, EDU truncation is not an option (since that would produce an incomplete RST tree), and the average number of words per document in our data is 521 (741 word pieces after BERT tokenization), which is much larger than the 512 limit. We therefore break the document into a number of partial documents, each consisting of multiple sentences that fit into the 512 token limit. This way, we allow the model to capture the fine-grained wordto-word relationships across (most) EDUs. Each partial document is then processed based on Liu and Lapata (2019) trick where we use an alternating even/odd segmentation embedding to encode all the EDUs in a document. We illustrate this approach in Figure 4. First, all EDUs are formatted to start with [CLS] and end with [SEP], and words are tokenized using WordPiece. If the document has more than 512 tokens, we break it into multiple partial documents based on EDU boundaries, and pad accordingly (e.g. in Figure 4 we break the example document of 3 EDUs into 2 partial documents), and process each partial document independently with BERT. We also experimented with the second alternative by encoding each ED"
2021.eacl-main.60,P14-5010,0,0.00352362,"of evaluation, we use the standard metrics introduced by Marcu (2000): Span, Nuclearity, Relation, and Full. We report micro-averaged F-1 scores on labelled attachment decisions (original Parseval), following the recommendation of Morey et al. (2017). Additionally, we also present the evaluation with RSTParseval procedure in Appendix A. Table 1: Feature addition study over the development set to find the best configuration for our models. Presented results are the mean and standard deviation of the Full metric (micro-averaged F-score on labelled attachment decisions) over three runs. CoreNLP (Manning et al., 2014) is used for POS tagging.6 4.2 Model Configurations We experiment with two segmentation models — LSTM (Section 3.1) and transformer (Section 3.2) — both implemented in PyTorch framework.7 As EDUs are provided in the dataset, no automatic segmentation of EDU is required in our experiments. For the LSTM model, the dimensionality of the Bi-LSTMs in the encoder is 256, while the segmenter (Bi-LSTM4 ) is 128 (Figure 3). The embedding dimensions of words, POS tags, EDU type, and syntax features are 200, 200, 100, and 1,200, respectively, and we initialize words in EDU with GloVe embedding (Penningto"
2021.eacl-main.60,J93-2004,0,0.0781138,"g training is controlled by the hyper-parameter α ∈ [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when α = 0. 4 4.1 Experiments Data We use the English RST Discourse Treebank (Carlson et al., 2001) for our experiments, consistent with recent studies (Ji and Eisenstein, 2014; Li et al., 2014; Feng and Hirst, 2014; Yu et al., 2018). The dataset is based on the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as Yu et al. (2018), which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford 720 Variant Vanilla +Syntax +Penalty +Syntax+Penalty LSTM Transformer 48.4±0.5 50.0±0.7 49.6±0.5 51.6±0.1 51.3±0.2 51.9±0.4 52.1±0.4 51.8±0.8 configuration as for the LSTM model. We tuned the segmentation loss penalty hyperparameter β (Section 3.4) and the dynamic oracle hyper-parameter α (Section 3.5) based on the development set. Both the LSTM and tr"
2021.eacl-main.60,D17-1136,0,0.433734,"ting sentence for the nucleus unit and contains less prominent information. It is standard practice that the RST tree is trained and evaluated in a right-heavy binarized manner, resulting in three forms of binary nuclearity relationships between EDUs: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this work, eighteen coarse-grained relations are considered as discourse labels, consistent with earlier work (Yu et al., 2018).2 Work on RST parsing has been dominated by the bottom-up paradigm (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (d"
2021.eacl-main.60,P19-1442,0,0.0228327,"r EDUs) to produce the final encoding: ỹEn Segmenter MLP Layer BI-LSTM4 hEm ... hEn Get Sub-Sequence hE1 hE2 hE3 ... hEq BI-LSTM3 Encoder tE1 ... gE1 Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 xi = wi ⊕ pi tEq gEq w {aw 1 , .., ap } = Bi-LSTM1 ({x1 , .., xp }) Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 w1 w2 s1 p1 p2 {as1 , ..., asp } = Bi-LSTM2 ({s1 , .., sp }) w gEj = Avg-Pool({aw 1 , .., ap })⊕ Avg-Pool({as1 , .., asp }) ⊕ tEj ... w1 w2 p1 p2 s1 s2 Syntax Feature Syntax Feature Word and POS EDU1 s2 Word and POS ... EDUq Figure 3: Architecture of the LSTM model. 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019). To the best of our knowledge, however, pre-trained models have not been applied in the generation of full discourse trees, which we address here by experimenting with BERT for topdown RST parsing. 3 Top-down RST Parsing We frame RST parsing as a sequence labelling task, where given a sequence of input EDUs, the goal is to find a segmentation boundary to split the sequence into two sub-sequences. This is realized by training a sequence labelling model to predict a binary label for each EDU, and select the EDU with the highest probability to be the segmentation point. A"
2021.eacl-main.60,D14-1162,0,0.0922945,"l., 2014) is used for POS tagging.6 4.2 Model Configurations We experiment with two segmentation models — LSTM (Section 3.1) and transformer (Section 3.2) — both implemented in PyTorch framework.7 As EDUs are provided in the dataset, no automatic segmentation of EDU is required in our experiments. For the LSTM model, the dimensionality of the Bi-LSTMs in the encoder is 256, while the segmenter (Bi-LSTM4 ) is 128 (Figure 3). The embedding dimensions of words, POS tags, EDU type, and syntax features are 200, 200, 100, and 1,200, respectively, and we initialize words in EDU with GloVe embedding (Pennington et al., 2014).8 For hyper-parameters, we use the following: batch size = 4, gradient accumulation = 2, learning rate = 0.001, dropout probability = 0.5, and optimizer = Adam (with epsilon of 1e-6). The loss scaling hyper-parameters (Equation (2)), are tuned based on the development set, and set to λ1 = 1.0, and λ2 = 1.0. For the transformer model, the document length limit is set to 512 tokens, and longer documents are broken into smaller partial documents. As before, we truncate each EDU to the first 50 words. We initialize the transformer in the encoder with bert-base, and the transformer in the segmente"
2021.eacl-main.60,D19-1586,0,0.0205247,"the final encoding: ỹEn Segmenter MLP Layer BI-LSTM4 hEm ... hEn Get Sub-Sequence hE1 hE2 hE3 ... hEq BI-LSTM3 Encoder tE1 ... gE1 Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 xi = wi ⊕ pi tEq gEq w {aw 1 , .., ap } = Bi-LSTM1 ({x1 , .., xp }) Avg Pooling Avg Pooling BI-LSTM1 BI-LSTM2 w1 w2 s1 p1 p2 {as1 , ..., asp } = Bi-LSTM2 ({s1 , .., sp }) w gEj = Avg-Pool({aw 1 , .., ap })⊕ Avg-Pool({as1 , .., asp }) ⊕ tEj ... w1 w2 p1 p2 s1 s2 Syntax Feature Syntax Feature Word and POS EDU1 s2 Word and POS ... EDUq Figure 3: Architecture of the LSTM model. 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019). To the best of our knowledge, however, pre-trained models have not been applied in the generation of full discourse trees, which we address here by experimenting with BERT for topdown RST parsing. 3 Top-down RST Parsing We frame RST parsing as a sequence labelling task, where given a sequence of input EDUs, the goal is to find a segmentation boundary to split the sequence into two sub-sequences. This is realized by training a sequence labelling model to predict a binary label for each EDU, and select the EDU with the highest probability to be the segmentation point. After the sequence is seg"
2021.eacl-main.60,N19-1351,0,0.0300142,"Missing"
2021.eacl-main.60,N15-3001,0,0.053505,"Missing"
2021.eacl-main.60,P17-2029,0,0.0925473,"e propose a conceptually simpler top-down approach for RST parsing. The core idea is to frame the problem as a sequence labelling task, where the goal is to iteratively find a segmentation boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we Related Work Previous work on RST parsing has been dominated by bottom-up approaches (Hernault et al., 2010; Joty et al., 2013; Li et al., 2016; Braud et al., 2017; Wang et al., 2017). For example, Ji and Eisenstein (2014) introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. Braud et al. (2016) propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere, Yu et al. (2018) showed that implicit syntax features extracted from a dependency parser (Dozat and Manning, 2017) are highly effective for discourse parsing. Top-down parsing is well established for constituency parsing and language modelling (Johnson, 1995; Roark and Johnson, 1999; R"
2021.eacl-main.60,C18-1047,0,0.0811348,"e relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis (Bhatia et al., 2015) and abstractive summarization (Koto et al., 2019). Rhetorical Structure Theory (RST; Mann and Thompson (1988)) is one of the most widely used discourse theories in NLP (Hernault et al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2016; Yu et al., 2018). RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units (EDUs). EDUs are typically clauses of a sentence. Non-terminal nodes in the tree represent discourse unit relations. In Figure 1, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a “satellite” points to its “nucleus”. The satellite unit is a supporting sentence for the nucleu"
2021.eacl-main.60,2020.acl-main.569,0,0.621727,"t al., 2010; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Braud et al., 2017; Morey et al., 2017; Yu et al., 2018). These methods produce very competitive benchmarks, but in practice it is not a straightforward 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown 2 Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html 1 Introduction 715 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 715–726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics BiAfﬁne Attention Zhang et al., 2020 BiGRU (single EDU) BiGRU (inter-EDU) CNN (kernel=2) BiGRU (ﬁnal encoder) GRU (decoder) stack Kobayashi et al., 2020 BiLSTM (single EDU) Gated MLP grouped by paragraph/ sentence/edu BiLSTM (inter-grouped unit) BiLSTM (inter-EDU) BiLSTM (seq labelling) queue Ours (v2) BERT (partial doc) Splitting via Deep BiAfﬁne Scoring exhaustive search space Ours (v1) BiLSTM (single EDU) use an LSTM (Hochreiter and Schmidhuber, 1997) or pre-trained BERT (Devlin et al., 2019) as the segmenter, enhanced in a number of key ways. Our primary contributions are as follows: (1) we propose a novel top-down approach"
2021.emnlp-main.155,D16-1120,0,0.0412668,"Missing"
2021.emnlp-main.155,D19-1176,0,0.0258959,"ote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019),"
2021.emnlp-main.155,D19-1236,0,0.0278975,"grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), dat"
2021.emnlp-main.155,N19-1423,0,0.0289859,"Missing"
2021.emnlp-main.155,D18-1002,0,0.0428755,"Missing"
2021.emnlp-main.155,P19-1339,0,0.0282322,"ed experiments that the proposed approaches help mitigate both class imbalance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wic"
2021.emnlp-main.155,2021.eacl-main.239,1,0.819973,"ic mean of performance and fairness for Table 2, and use the results from the original paper for Table 1. We select all other models by first selecting from models with F-score at least as high 5 Acknowledgement as INLP, and then selecting the one with the lowWe thank Xudong Han for the discussions and inest GAP. We include a recent adversarial model in the varying stereotyping experiments, which per- puts. This work was funded in part by the Australian Government Research Training Program formed strongly on the class-balanced emoji data Scholarship, and the Australian Research Council. (ADV: Han et al. (2021)). Our results on varying stereotyping levels in Table 1 show that the vanilla baseline drops in performance more sharply than most proposed models, References Pinkesh Badjatiya, Manish Gupta, and Vasudeva and results in the most unfair predictions by a large Varma. 2019. Stereotypical bias removal for hate margin. LDAMiw , LDAMadv , and ADV retain speech detection task using knowledge-based genhigh F-scores but drop in fairness with increaseralizations. In The World Wide Web Conference, ing stereotyping, while INLP exhibits the opposite WWW 2019, San Francisco, CA, USA, May 13-17, pattern. LD"
2021.emnlp-main.155,2020.emnlp-main.154,0,0.0335943,"ance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kenne"
2021.emnlp-main.155,2020.findings-emnlp.269,0,0.021587,"ge detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggravated by as “stereotyping” whereby one or more private at- the presence of class imbalance"
2021.emnlp-main.155,D19-1578,0,0.0201251,"majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi"
2021.emnlp-main.155,2020.acl-main.483,0,0.0270419,"2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is ofte"
2021.emnlp-main.155,2020.acl-main.45,0,0.189224,"Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggrav"
2021.emnlp-main.155,P18-2005,1,0.928794,"thin each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbala"
2021.emnlp-main.155,N19-1063,0,0.0234726,"w through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.1 C Figure 1: Example of a two-class problem where grey and blue points denote majority and minority classes, respectively, and circles and squares denote two sub-groups. Imbalanced learning methods such as LDAM (Cao et al., 2019) maximise the (soft-)margin for minority classes and do not consider sub-groups within each class. bias towards demographic groups based on gender, disability, race or religion (Caliskan et al., Class imbalance is common in many NLP tasks, in- 2017; May et al., 2019; Garimella et al., 2019; cluding machine reading comprehension (Li et al., Nangia et al., 2020), and bias towards individu2020), authorship attribution (Caragea et al., 2019), als (Prabhakaran et al., 2019). Methods to mitigate toxic language detection (Breitfeller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 20"
2021.emnlp-main.155,2020.acl-main.647,0,0.0212734,"qualised odds measure considering both TPR and TNR of classifiers, in order to address scenarios where certain subgroups are predicted more often with some classes (see Section 1). We report fairness as 1−GAP, such that higher numbers are better, and a perfectly fair model achieves 1− GAP = 1. We compare our methods against the following benchmarks: LDAM: the original LDAM model (Cao et al., 2019). LDAMcw : a variant of LDAM with instance reweighting by inverse class proportion (Cao et al., 2019). 3.1 Model Comparison We include simulated experimental settings with the emoji dataset following Ravfogel et al. (2020) where they keep the class proportions balanced, but vary group proportions (stereotyping). In our work, we systematically vary both class imbalance and stereotyping, in order to assess the robustness of the models wrt class imbalance and fairness individually. We explore three settings: varying both dimensions at the same time (Figure 2), controlling for class imbalance and vary stereotyping (Table 1), and controlling for stereotyping while varying class imbalance (Table 2). We simultaneously vary stereotyping and class imbalance in the emoji dataset, exploring several settings: • Original: t"
2021.emnlp-main.155,2021.emnlp-main.193,1,0.734072,"Missing"
2021.emnlp-main.155,D19-1670,0,0.0165387,"eller et al., 2019), these biases include data augmentation (Badjatiya and text classification (Tian et al., 2020). A et al., 2019), adversarial learning (Li et al., 2018), skewed class distribution hurts the performance instance weighting based on group membership of deep learning models (Buda et al., 2018), and (Kamiran and Calders, 2011), regularization (Wick approaches such as instance weighting (Lin et al., et al., 2019; Kennedy et al., 2020), and explicit sub2017; Cui et al., 2019; Li et al., 2020), data aug- space removal (Bolukbasi et al., 2016; Ravfogel mentation (Juuti et al., 2020; Wei and Zou, 2019), et al., 2020). and weighted max-margin (Cao et al., 2019) are This paper draws a connection between classcommonly used to alleviate the problem. imbalanced learning and stereotyping bias. Most Bias in data often also manifests as skewed dis- work has focused on class-imbalanced learning and tributions, especially when considered in combi- bias mitigation as separate problems, but the unfairnation with class labels. This is often referred to ness caused by social biases is often aggravated by as “stereotyping” whereby one or more private at- the presence of class imbalance (Yan et al., 2020)."
2021.emnlp-main.193,2021.eacl-main.239,1,0.694825,"Missing"
2021.emnlp-main.193,2020.lrec-1.180,0,0.0531121,"Missing"
2021.emnlp-main.193,P18-2005,1,0.835554,"cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al., 2020). Notably, algorithms can be fair towards independent groups, but not 2 Background towards all intersectional groups. Despite this, debiasing approaches within NLP have so far been Debiasing with respect to more than a single proevaluated only using independent grou"
2021.emnlp-main.193,N19-1062,0,0.0196669,"ions trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al., 2020). Notably, algorithms can be fair towards independent groups, but not 2 Background towards all intersectional groups. Despite this, debiasing approaches within NLP have so far been Debiasing with respect to more than a single proevaluated only using independent group fairness tected attribute (|Z |&gt; 1) requires grouping data 2"
2021.emnlp-main.193,N19-1423,0,0.0316514,"Missing"
2021.emnlp-main.193,N19-1063,0,0.0522268,"approaches based on independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and"
2021.emnlp-main.193,P19-1339,0,0.140807,"n independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et a"
2021.emnlp-main.193,2020.emnlp-main.154,0,0.369684,"e prone to fairness gerrymandering than methods using intersectional groups? • How do INLP and bias-constrained approaches, and their extensions to handle intersectional groups, fare compare in terms of both predictive accuracy and fairness? Text data reflects the social and cultural biases in the world, and NLP models and applications trained on such data have been shown to reproduce and amplify those biases. Discrimination has been identified across diverse sensitive attributes including gender, disability, race, and religion (Caliskan et al., 2017; May et al., 2019; Garimella et al., 2019; Nangia et al., 2020; Li et al., 2020). While early work focused on debiasing typically binarized protected attributes in isolation (e.g., age, gender, or race; Caliskan et al. (2017)), more recent work has adopted a more realistic scenario with multiple sensitive attributes (Li et al., 2018) or attributes covering several classes (Manzini et al., 2019). In the context of multiple protected attributes, gerrymandering refers to the phenomenon where an attempt to make a model fairer towards some group results in increased unfairness towards another group (Buolamwini and Gebru, 2018; Kearns et al., 2018; Yang et al."
2021.emnlp-main.193,2020.acl-main.647,0,0.175149,"B D  male (M) female (F) Figure 1: Group intersection and gerrymandering: A=white male, B=male person of colour, C=white female, D=female person of colour. when modelling datasets with multiple attributes, disregarding intersectional subgroups defined by combinations of sensitive attributes (see Figure 1). The primary goal of this work is to evaluate independent and intersectional identity debiasing approaches in relation to fairness gerrymandering for text classification tasks. To this end, we evaluate bias-constrained models (Cotter et al., 2019b) and iterative nullspace projection (INLP; Ravfogel et al. (2020)), a post-hoc debiasing method which we extend to handle intersectional groups. The constrained model jointly optimizes model performance and model fairness, while INLP seeks to learn a hidden representation which is independent of the protected attributes. INLP does not consider the trade-off between accuracy and fairness, but rather it iteratively maximizes fairness in an unconstrained fashion. In this work, we address the following questions: • Are debiasing approaches based on independent groups more prone to fairness gerrymandering than methods using intersectional groups? • How do INLP a"
2021.emnlp-main.833,W19-1909,0,0.0368597,"Missing"
2021.emnlp-main.833,Q17-1010,0,0.128687,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.118,0,0.0605214,"Missing"
2021.emnlp-main.833,P11-2008,0,0.0642259,"Missing"
2021.emnlp-main.833,2020.aacl-main.60,1,0.761881,"e backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, f"
2021.emnlp-main.833,2020.coling-main.66,1,0.878257,"e backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, f"
2021.emnlp-main.833,N18-1088,0,0.0516435,"Missing"
2021.emnlp-main.833,2021.ccl-1.108,0,0.0807347,"Missing"
2021.emnlp-main.833,2020.emnlp-demos.2,0,0.134861,"f modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embeddings is taining high-quality contextualized representations for specific domains/data sources such as biomedi- more efficient than pretraining from scratch, and more effective than initializing based on word2vec cal, social media, and legal, remains a challenge. Previous studies (Alsentzer et al., 2019; projections (Poerner et al., 2020). We use I N DO BERT (Koto et al., 2020b), a monolingual Chalkidis et al., 2020; Nguyen et al., 2020) have BERT for Indonesian as the domain-general model shown that for domain-specific text, pretraining to develop a pretrained domain-specific model I N from scratch outperforms off-the-shelf BERT. As an alternative approach with lower cost, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, ficial language of the 5th most populous nation, Inalthough sti"
2021.emnlp-main.833,D11-1141,0,0.189252,"Missing"
2021.emnlp-main.833,S17-2088,0,0.0817598,"Missing"
2021.emnlp-main.833,P16-1162,0,0.142671,"Missing"
2021.emnlp-main.833,W16-3919,0,0.0363854,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.129,0,0.0298012,"othy Baldwin School of Computing and Information Systems The University of Melbourne ffajri@student.unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net Abstract term in biology. To tackle this problem, Poerner et al. (2020); Tai et al. (2020) proposed simple We present I NDO BERT WEET, the first largemethods to domain-extend the BERT vocabulary: scale pretrained model for Indonesian Twitter Poerner et al. (2020) initialize new vocabulary usthat is trained by extending a monolinguallying a learned projection from word2vec (Mikolov trained Indonesian BERT model with additive et al., 2013), while Tai et al. (2020) use random inidomain-specific vocabulary. We focus in particular on efficient model adaptation under votialization with weight augmentation, substantially cabulary mismatch, and benchmark different increasing the number of model parameters. ways of initializing the BERT embedding layer New vocabulary augmentation has been also confor new word types. We find that initializing ducted for language-adaptive pretraining, mainly with the average BERT subword embedding based on multilingual BERT (M BERT). For inmakes pretraining five times faster, and is more stance, Chau et al. (2020) replace 99 “u"
2021.emnlp-main.833,S18-1005,0,0.0535351,"Missing"
2021.emnlp-main.833,2020.findings-emnlp.240,0,0.0215308,"umber of model parameters. ways of initializing the BERT embedding layer New vocabulary augmentation has been also confor new word types. We find that initializing ducted for language-adaptive pretraining, mainly with the average BERT subword embedding based on multilingual BERT (M BERT). For inmakes pretraining five times faster, and is more stance, Chau et al. (2020) replace 99 “unused” effective than proposed methods for vocabuWordPiece tokens of M BERT with new comlary adaptation in terms of extrinsic evaluation 1 over seven Twitter-based datasets. mon tokens in the target language, while Wang et al. (2020) extend M BERT vocabulary with non1 Introduction overlapping tokens (|VM BERT − Vnew |). These two approaches use random initialization for new WordTransformer-based pretrained language models (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., Piece token embeddings. In this paper, we focus on the task of learning 2019; Radford et al., 2019) have become the backan Indonesian BERT model for Twitter, and show bone of modern NLP systems, due to their success across various languages and tasks. However, ob- that initializing domain-specific vocabulary with average-pooling of BERT subword embe"
2021.emnlp-main.833,2020.aacl-main.85,0,0.596838,"st, Gururan- DO BERT WEET for Indonesian Twitter. gan et al. (2020) demonstrated that domain adapThere are two primary reasons to experiment tive pretraining (i.e. pretraining the model on target with Indonesian Twitter. First, despite being the ofdomain text before task fine-tuning) is effective, ficial language of the 5th most populous nation, Inalthough still not as good as training from scratch. donesian is underrepresented in NLP (notwithstandThe main drawback of domain-adaptive pretrain- ing recent Indonesian benchmarks and datasets ing is that domain-specific words that are not in the (Wilie et al., 2020; Koto et al., 2020a,b)). Secpretrained vocabulary are often tokenized poorly. ond, with a large user base, Twitter is often utilized For instance, in B IO BERT (Lee et al., 2019), Im- to support policymakers, business (Fiarni et al., munoglobulin is tokenized into {I, ##mm, ##uno, 2016), or to monitor elections (Suciati et al., 2019) ##g, ##lo, ##bul, ##in}, despite being a common or health issues (Prastyo et al., 2020). Note that 1 most previous studies that target Indonesian TwitCode and models can be accessed at https:// github.com/indolem/IndoBERTweet ter tend to use traditional machine l"
2021.findings-acl.41,D16-1120,0,0.359067,"Missing"
2021.findings-acl.41,N19-1423,0,0.0759813,"Missing"
2021.findings-acl.41,D17-1169,0,0.060443,"Missing"
2021.findings-acl.41,2021.eacl-main.239,1,0.65361,"iments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates two parts: (1) predicting the main task label (ˆ yi = C(E(xi ))); and (2) preventing protected attributes from being identified by the discriminator. An important limitation of previous adversarial debiasing work is that training instances must be a"
2021.findings-acl.41,P15-1073,0,0.030859,"Data We use three datasets for different purposes: main task training, adversarial training, and out-ofdomain evaluation. Following Li et al. (2018), we train a biLSTM POS tagging model on the English Web Treebank (Bies et al., 2012), comprising 13.5k POStagged sentences without protected labels. To evaluate model performance and fairness, we use the TrustPilot English POS-tagged dataset (Hovy and Søgaard, 2015), consisting of 600 sentences with both POS labels and binary author-age labels (over45-year-old and under-35-year-old). To train the discriminator, we use unlabelled TrustPilot data (Hovy, 2015), which consists of 156.5k English reviews with author-age labels. Based on protected-label predictability estimation (Algorithm 1), we examine 4 subsampling strategies: (1) “random”, based on random-sampling; (2) “largest leakage”, select instances with the highest predictability (intuitively the most biased instances); (3) “smallest leakage”, select instances where the predictability is below a majority-class baseline; and (4) “absolute leakage”, a combination of largest and smallest leakage where equal number of instances from the largest leakage sampling and the smallest leakage sampling a"
2021.findings-acl.41,P15-2079,0,0.115954,", or only available in limited numbers. In this paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the repres"
2021.findings-acl.41,2020.lrec-1.180,0,0.0112631,"paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A("
2021.findings-acl.41,S18-2005,0,0.0142028,"eeds only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the in- and crossdomain effectiveness of our method through a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates"
2021.findings-acl.41,P18-2005,1,0.845604,"a range of experiments. 1 Introduction Protected attributes such as user gender can act as confounding variables in models, and spurious correlations with task response variables can lead to unfair predictions, as seen in tasks such as partof-speech tagging (Hovy and Søgaard, 2015), hate speech detection (Huang et al., 2020), and sentiment analysis (Kiritchenko and Mohammad, 2018). Adversarial methods are a popular method for mitigating bias associated with protected attributes, wherein the encoder attempts to prevent a discriminator from identifying protected attributes (Zhang et al., 2018; Li et al., 2018; Han et al., 2021). An adversarial network consists of a discriminator A and an encoder E. Each input xi is required to be annotated with both a main task label yi and protected attribute label gi , and the discriminator identifies protected information in the representation generated by the encoder (ˆ gi = A(hi )). The objective of the encoder training incorporates two parts: (1) predicting the main task label (ˆ yi = C(E(xi ))); and (2) preventing protected attributes from being identified by the discriminator. An important limitation of previous adversarial debiasing work is that training"
2021.findings-acl.71,2020.acl-main.175,0,0.0243273,"omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisat"
2021.findings-acl.71,N19-1074,0,0.0273267,"n and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words lear"
2021.findings-acl.71,2020.emnlp-main.751,0,0.147623,"-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics p"
2021.findings-acl.71,D15-1013,0,0.0597712,"EN), Indonesian (ID), French (FR), Turkish (TR), Mandarin Chinese (ZH), Russian (RU), German (DE), and Spanish (ES); (2) we evaluate an extensive range of traditional and model-based metrics, and find BERTScore to be the best metric for evaluating both focus and coverage; and (3) we release a manually-annotated multilingual resource for summarization evaluation comprising 4,320 annotations. Data and code used in this paper is available at: https://github.com/ fajri91/Multi SummEval. 2 Related Work As with much of NLP, research on automatic summarization metrics has been highly Englishcentric. Graham (2015) comprehensively evaluated 192 ROUGE variations based on the DUC2004 (English) dataset. Bhandari et al. (2020) released a new (English) evaluation dataset by annotating CNN/DailyMail using simplified Pyramid (Nenkova and Passonneau, 2004). First, semantic content units (SCUs) were manually extracted from the reference, and crowd-workers were then asked to count the number of SCUs in the system summary. Their annotation procedure does not specifically consider focus, but is closely related to the coverage aspect of our work. Similarly, Fabbri et al. (2020) annotated the (English) CNN/DailyMail"
2021.findings-acl.71,P18-1063,0,0.0234824,"., 2010) and RESA (Cohan and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacre"
2021.findings-acl.71,N15-1124,1,0.837557,"Missing"
2021.findings-acl.71,L16-1130,0,0.017336,"erlap, and the longest common subsequence (“LCS”) between reference and system summaries. In other work, Saggion et al. (2010) investigated coverage, responsiveness, and pyramids for several extractive models in English, French, and Spanish. To the best of our knowledge, we are the first to systemically quantify the panlinguistic efficacy of evaluation metrics for modern summarization systems. 3 Evaluation Metrics We assess a total of 19 different evaluation metrics that are commonly used in summarization research (noting that lesser-used metrics such as FRESA (Saggion et al., 2010) and RESA (Cohan and Goharian, 2016) are omitted from this study). ROUGE (Lin, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2"
2021.findings-acl.71,P19-1330,0,0.112854,"oped for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization"
2021.findings-acl.71,D15-1229,0,0.0202696,"TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated b"
2021.findings-acl.71,2020.aacl-main.60,1,0.788782,"m largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage are more interpretab"
2021.findings-acl.71,2020.coling-main.66,1,0.74331,"m largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage are more interpretab"
2021.findings-acl.71,2020.findings-emnlp.360,0,0.0260883,"aturing summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no systematic validation of its efficacy across other languages. The questions we ask in this study, therefore, are twofold: (1) How well do existing automatic metrics perform over languages other than English? and (2) What automatic metric works best across different languages? In this paper, we examine content-based summarization evaluation from the aspects of precision and recall, in the form of focus and coverage to compare system-generated summaries to groundtruth summaries (see Figure 1). As advocated by Koto et al. (2020a), focus and coverage a"
2021.findings-acl.71,2020.tacl-1.20,1,0.822724,"rately high. However, the agreement does vary, and it affects the interpretation of the correlation scores when we assess the automatic metrics later. Note also that we get the lowest score for English, meaning the results for non-English languages are actually more 16 We approved all HITs with at least 30 minutes working time and a minimum quality control score of 5, irrespective of whether they passed the higher quality-control threshold required for the ground truth. 804 17 In MTurk, we did not set a specific location for Chinese because we found there are no workers in China. 18 We follow Lau et al. (2020) in computing one-vs-rest correlation: we randomly isolate a worker’s score (for each sample) and compare it against the mean score of the rest using Pearson’s r, and repeat this for 1000 trials to get the mean correlation. robust.19 Although focus and coverage are positively correlated in Table 1, the distribution of scores varies quite a bit between languages: English annotation variance is higher than the other languages, and has the lowest correlation between focus and coverage (r = 0.57); for French, Russian, and Spanish, summaries generally have low focus and coverage (for more details,"
2021.findings-acl.71,W07-0734,0,0.446038,"ght different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et a"
2021.findings-acl.71,W04-1013,0,0.287754,"zation corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Ran"
2021.findings-acl.71,W02-0406,0,0.277386,"d reliable to evaluate focus and coverage based on the source document than the ground-truth summary, we use the ground-truth summary in this research for the following reasons. First, historically, validation of automatic evaluation metrics for summarization has been based primarily on ground-truth summaries (not source documents). Second, ROUGE (Lin, 801 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 801–812 August 1–6, 2021. ©2021 Association for Computational Linguistics 2004) was initially motivated and assessed based on coverage over the DUC datasets3 (Lin and Hovy, 2002) using annotations based on reference summaries (not source documents). Third, although it is certainly possible to generate different summaries for the same source document, we argue that the variance in content is actually not that great, especially for single-document summarization. Lastly, basing human evaluation (of focus and coverage) on the source article leads to more complicated annotation schemes, and has been shown to yield poor annotations (Nenkova and Passonneau, 2004; Fabbri et al., 2020). In summary, this paper makes three contributions: (1) we carry out the first systematic att"
2021.findings-acl.71,D19-1387,0,0.0152401,"nto 1 task, as the only thing that differentiates them is the ordering of the system and reference summaries, which is opaque to the annotators.14 Workers responded by scoring based via a slider button (continuous scale of 1–100).15 For each HIT, we create 10 samples for quality control: 5 samples are random pairs (should be 7 https://github.com/AIPHES/emnlp19-moverscore https://github.com/Tiiiger/bert score 9 Note that both multilingual BERT and XLM were explicitly trained over all eight target languages used in this paper. 10 English, Indonesian and Chinese summaries were generated with the Liu and Lapata (2019) model, and the Dong et al. (2019) model was used for the MLSUM-based languages. 8 803 11 Summaries for all datasets except LCSTS were sourced from the authors of the dataset. For LCSTS, we trained the two models ourselves based on the training data. 12 BERT-based summaries are representative of transformerbased model, and the ROUGE score gap over state-of-the-art models (Zhang et al., 2020a) for English is only ∼2 points. 13 https://www.mturk.com 14 For focus, the first text is the reference and the second text the system summary; for coverage, the order is reversed. 15 See Appendix for the M"
2021.findings-acl.71,2020.acl-main.448,1,0.706094,"verage (for more details, see scatterplots of focus-coverage in Figure 2 of the Appendix). 5.2 Correlation with Automatic Evaluation In Table 2 we present the Pearson correlation between the human annotations and various automatic metrics, broken down across language and focus vs. coverage, and (naively) aggregated across languages in the form of the average correlation. We also include the one-vs-rest annotator correlation (Section 5.1) in the last row, as it can be interpreted as the average performance of a single annotator. Recognizing the sensitivity of Pearson’s correlation to outliers (Mathur et al., 2020), we manually examined the distribution of scores for all language– system combinations for outliers (and present all scatterplots in Figure 2 of the Appendix). The general pattern is consistent across languages: BERTScore performs better than other metrics in terms of both focus and coverage. This finding is consistent with that of Fabbri et al. (2020) wrt expert annotations of relevance (interpreted as the harmonic mean of our focus and coverage). ROUGE-1 and ROUGE-L are overall the best versions of ROUGE, while BLEU-4 performs the worst. For coverage, METEOR tends to be competitive with ROU"
2021.findings-acl.71,N04-1019,0,0.0578396,"ation evaluation methods developed for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English. 1 Figure 1: Illustration of focus and coverage. Introduction Although manual evaluation (Nenkova and Passonneau, 2004; Hardy et al., 2019) of text summarization is more reliable and interpretable, most research on text summarization employs automatic evaluations such as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated ext"
2021.findings-acl.71,P02-1040,0,0.119558,"in, 2004) measures the lexical overlap between the system and reference summary; based on the findings of Graham (2015), we consider 7 variants in this paper: ROUGE1 (unigram), ROUGE-2 (bigram), ROUGE-3 (trigram), ROUGE-L (LCS), ROUGE-S (skipbigram), ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisation problem.7 We adapt use the"
2021.findings-acl.71,P19-1502,0,0.0173613,"h as ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), MoverScore (Zhao et al., 2019), and BERTScore (Zhang et al., 2020b) because they are time- and cost-efficient. In proposing these metrics, the authors measured correlation with human judgments based on English datasets that are not representative of modern summarization systems. For instance, Lin (2004) use DUC1 2001–2003 for ROUGE (meaning summaries were generated with largely outdated extractive summarization systems); Zhao et al. (2019) use the TAC2 dataset for MoverScore (again, featuring summaries from largely defunct systems; see Peyrard (2019) and Rankel et al. (2013)); and Zhang et al. (2020b) developed BERTScore based on a machine translation corpus (WMT). In contemporaneous work, Bhandari et al. (2020) address this issue by annotating English CNN/DailyMail summaries produced by recent summarization models, and found disparities over results from TAC. 1 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/ Equally troublingly, ROUGE has become the default summarization evaluation metric for languages other than English (Hu et al., 2015; Scialom et al., 2020; Ladhak et al., 2020; Koto et al., 2020b), despite there being no s"
2021.findings-acl.71,W18-6319,0,0.0218519,"ROUGE-SU (skip-bigram plus unigram), and ROUGE-W (weighted LCS).4 METEOR (Lavie and Agarwal, 2007) performs word-to-word matching based on word-alignment, and was originally developed for MT but has recently been used for summarization evaluation (See et al., 2017; Chen and Bansal, 2018; Falke and Gurevych, 2019; Amplayo and Lapata, 2020).5 BLEU (Papineni et al., 2002) is a precisionbased metric originally developed for MT, which measures the n-gram match between the reference and system summary. Based on the findings of Graham (2015), we use BLEU-4 according to the SacreBLEU implementation (Post, 2018).6 MoverScore (Zhao et al., 2019) measures the Euclidean distance between two contextualized BERT representations, and relies on soft alignDUC 2001, 2002, 2003 802 4 https://github.com/bheinzerling/pyrouge http://www.cs.cmu.edu/∼alavie/METEOR/ 6 https://github.com/mjpost/sacrebleu 5 ments of words learned by solving an optimisation problem.7 We adapt use the default configuration (n-gram=1) over 5 different pre-trained models, as detailed below. Note that MoverScore is symmetric (i.e. MoverScore(x, y) = MoverScore(y, x)), and as such is not designed to separately evaluate precision and recall."
2021.findings-emnlp.249,D15-1075,0,0.0328244,"Missing"
2021.findings-emnlp.249,P18-1015,0,0.0254825,"ey elements of contrastive learning are: (1) the construction of positive and negative samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generation, Liu et al. (20"
2021.findings-emnlp.249,N19-1423,0,0.036348,"Missing"
2021.findings-emnlp.249,2021.emnlp-main.552,0,0.062778,"Missing"
2021.findings-emnlp.249,2020.acl-main.703,0,0.391352,"on, and show that our model has on heavily entity-centric tasks such as FEVER potential commercial value. (Thorne et al., 2018). However, while using sparse vector space retrieval models can retrieve relevant 1 Introduction prototypes that contain a set of concepts, there can Pre-trained language models have achieved impres- be significant domain mismatches between the resive results across a wide range of NLP tasks (De- trieved results and target distribution, making it vlin et al., 2019; Yang et al., 2019; Sun et al., 2019; difficult for generation models to bridge between Liu et al., 2019; Lewis et al., 2020a; Qi et al., 2020; prototypes and targets. We argue that a two-stage He et al., 2020b). However, their ability to accu- retrieval strategy alleviates this issue by combining rately reflect factual knowledge or perform logi- sparse vector space search and dense representacal inference is still limited. To investigate the tion filters. First, a sparse vector retrieval model is used to find passage candidates with high coverage ∗ Work done during an internship at Microsoft Research of concept words, and then a dense vector-based Asia. † Corresponding author. filter is applied to score the candid"
2021.findings-emnlp.249,2020.findings-emnlp.165,0,0.0605969,"Missing"
2021.findings-emnlp.249,2020.acl-main.228,0,0.01997,"samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generation, Liu et al. (2020) which is computationally prohibitive. Moco (He propose a knowledge graph-augmented lang"
2021.findings-emnlp.249,2020.emnlp-main.550,0,0.0380335,"Missing"
2021.findings-emnlp.249,2021.ccl-1.108,0,0.0595026,"Missing"
2021.findings-emnlp.249,P02-1040,0,0.109512,"emory bank is set to 4096, and the momentum coefficient is set to 0.999. 4.3.2 Baselines We use several state-of-the-art pre-trained language generation models as baselines: GPT-2 (Radford et al., 2019), BERT-Gen (Bao et al., 2020), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), T5 (Raffel et al., 2020), and BART (Lewis et al., 2020a). All models are fine-tuned in seq2seq mode. We also compare our model with two strong baselines that use external knowledge: EKI (Fan et al., 2020) and KG-BART (Liu et al., 2020). 4.3.3 Evaluation Metrics To evaluate generation performance, we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), in addition to evaluation metrics for captioning tasks, namely CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). As all metrics score the output in the range [0, 100], we also present the average score across all metrics. 2923 Model ROUGE-2/L BLEU-3/4 METEOR CIDEr SPICE Overall GPT-2 (Radford et al., 2019) BERT-Gen (Bao et al., 2020) UniLM (Dong et al., 2019) UniLM-v2 (Bao et al., 2020) T5 (Raffel et al., 2020) BART (Lewis et al., 2020a) 17.18 18.05 21.48 18.24 22.01 22.23 39.28 40.49 43.87 40.62 42.97 41.98 30.70 30.40"
2021.findings-emnlp.249,2020.findings-emnlp.217,1,0.868036,"model has on heavily entity-centric tasks such as FEVER potential commercial value. (Thorne et al., 2018). However, while using sparse vector space retrieval models can retrieve relevant 1 Introduction prototypes that contain a set of concepts, there can Pre-trained language models have achieved impres- be significant domain mismatches between the resive results across a wide range of NLP tasks (De- trieved results and target distribution, making it vlin et al., 2019; Yang et al., 2019; Sun et al., 2019; difficult for generation models to bridge between Liu et al., 2019; Lewis et al., 2020a; Qi et al., 2020; prototypes and targets. We argue that a two-stage He et al., 2020b). However, their ability to accu- retrieval strategy alleviates this issue by combining rately reflect factual knowledge or perform logi- sparse vector space search and dense representacal inference is still limited. To investigate the tion filters. First, a sparse vector retrieval model is used to find passage candidates with high coverage ∗ Work done during an internship at Microsoft Research of concept words, and then a dense vector-based Asia. † Corresponding author. filter is applied to score the candidates, and filter 2"
2021.findings-emnlp.249,W18-5713,0,0.0203819,"ter vision. The two key elements of contrastive learning are: (1) the construction of positive and negative samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generati"
2021.findings-emnlp.249,P18-1238,0,0.0426037,"Missing"
2021.findings-emnlp.249,N19-1421,0,0.0397719,"Missing"
2021.findings-emnlp.249,N18-1101,0,0.0590496,"Missing"
2021.findings-emnlp.249,D18-1009,0,0.056409,"Missing"
2021.findings-emnlp.249,N18-1074,0,0.0277987,"Missing"
2021.findings-emnlp.414,Q18-1041,0,0.137596,"descriptive, and for ensuring a match with the target user demographics, if the data or models derived from it are meant for real-world use (Rogers, 2021). Much of NLP data is “convenience samples” of naturally-occurring data, which reflects a world riddled with inequalities and social biases. For instance, GPT-2 training data was scraped from links shared on Reddit, reflecting the worldview, language, and interests of predominantly young white men (Bender et al., 2021). Creation of ‘perfect’ and representative samples is not necessarily possible, but documenting the lacunae and omissions is (Bender and Friedman, 2018). • Consider the potential for exposure. Social media storms are a force to be reckoned with, which can equally serve as an accountability instrument for public figures and companies (Rost et al., 2016; Neu et al., 2019), or a means of identity-based harrassment (Ortiz, 2020; Waisbord, 2020). Taking data from a moment in time and then ‘baking it in’ presents many problems, potentially leading to inaccurate or harmful representations of the individual (e.g. if they later retract a comment, or appeal a legal case and have the decision reversed). • Consider the potential for misuse. The responsib"
2021.findings-emnlp.414,2020.acl-main.485,0,0.0975314,"Missing"
2021.findings-emnlp.414,Q18-1018,0,0.0177871,"general scientific methodology. It requires that researchers focus on the more robust observations, and guards against falsification and fabrication (as such results would not be reproducible). Most experiments and studies are performed once, under unique conditions, which means that it is difficult to guarantee that the results are valid and trustworthy. In NLP, to reproduce an experiment one would need both the implementation of a given system, and the data on which it would run. The increased focus on code availability in NLP research (Wieling et al., 2018; Raff, 2019; Dodge et al., 2019; Crane, 2018, inter alia) leads to increased expectations for data availability.7 Two recent taxonomies of reproducibility in NLP (Cohen et al., 2018; Tatman et al., 2018) both consider data availability a necessary precondition. This is completely fair when the data is e.g. synthetic or collected with informed consent for this use, but in many other cases the principle of reproducibility is inherently in tension with the other principles, as will be discussed in Section 3. Transparency “in relation to data subject” is the 2.5 Do No Harm first GDPR principle cited in Figure 1: the people The final princip"
2021.findings-emnlp.414,D19-1224,0,0.0522933,"Missing"
2021.findings-emnlp.414,D14-1162,0,0.0839949,"Missing"
2021.findings-emnlp.414,2021.acl-long.170,1,0.887825,"such as fixed splits of the They must also be made aware of their rights, data, the choice of evaluation metric(s), and potentially even including the right to request rectification or erasure pre-processing strategies. 4824 et al., 2014; Flick, 2016), but there are also bestpractice recommendations for data collection: • Document the population from whom the data comes. This is necessary both for understanding the linguistic data, if the study goals are descriptive, and for ensuring a match with the target user demographics, if the data or models derived from it are meant for real-world use (Rogers, 2021). Much of NLP data is “convenience samples” of naturally-occurring data, which reflects a world riddled with inequalities and social biases. For instance, GPT-2 training data was scraped from links shared on Reddit, reflecting the worldview, language, and interests of predominantly young white men (Bender et al., 2021). Creation of ‘perfect’ and representative samples is not necessarily possible, but documenting the lacunae and omissions is (Bender and Friedman, 2018). • Consider the potential for exposure. Social media storms are a force to be reckoned with, which can equally serve as an acco"
2021.findings-emnlp.414,D19-5005,1,0.721886,"king it in’ presents many problems, potentially leading to inaccurate or harmful representations of the individual (e.g. if they later retract a comment, or appeal a legal case and have the decision reversed). • Consider the potential for misuse. The responsible thing to do before commencing a project is to think through what a bad actor could do with its results. *ACL conferences are increasingly requiring all submissions to consider possible misuse8 and how the possible harms should be mitigated. In some cases it may not be safe to release even the annotation guidelines, much less the data (Rogers et al., 2019). 3 Tension between Data Collection Principles While all of the above principles are important, there is a tug-of-war between them, as well as tensions with other factors in the research environment. The reproducibility principle aims to maximise data sharing in the interest of open 8 science, while the others all limit it from ethical and legal perspectives. The transparency principle means that the data processors have to disclose what they are doing with the data, but that complicates the protection of trade secrets. The privacy principle is sometimes fundamentally at odds with public inter"
2021.findings-emnlp.414,2021.findings-acl.414,0,0.0620012,"Missing"
2021.findings-emnlp.414,2021.naacl-main.295,0,0.0910275,"Missing"
2021.findings-emnlp.414,E17-2106,0,0.032584,"Missing"
2021.mrl-1.2,E17-1088,0,0.0199535,"s the ISO-639-3 language code for Na. 6 8 They are tuned on a small subset of de-en or fr-en data. Note that Griko is not included in ISO-639-3, and “grk” is an arbitrary (non-assigned) designator used in this paper. 9 18 Na raw cln njɤ˧ |ɑ˩ʁo˧ |ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩! njɤ˧ ɑ˩ʁo˧ ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩ English raw cln It (i.e. this story) is only what (we’ve) heard our great-grandmother tell. it is only what heard our great grandmother tell Table 2: An example Na–English parallel sentence before and after pre-processing (“raw” vs. “cln”) processing code used in Adams et al. (2017) with minor modifications.10 3.1.2 common problems for endangered languages. 3.2 Baselines Shipibo-Konibo We compare our model against various crosslingual models that are trained on a parallel corpus. First, we compare our model against a recently-proposed word-alignment model based on mBERT (Dou and Neubig, 2021).12 It fine-tunes mBERT on parallel corpora using various crosslingual objectives, and achieves state-of-the-art performance on word alignment tasks across many language pairs. We also include Levy et al. (2017), Luong et al. (2015a), and Sabet et al. (2020) as recent word embedding"
2021.mrl-1.2,2020.lrec-1.356,0,0.0222913,"word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthography in Griko, both of which are 10 We use white space as the word delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al."
2021.mrl-1.2,N19-1391,0,0.0192352,"l too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko– Italian, Shipibo-Konibo–Spanish and Na–{French, Chinese, English}. For the Griko–Italian pair, we evaluate models on a cross-lingual word alignment task and report alignment accuracy (1−AER). We use the gold alignments manually annotated over the 330 Griko–Italian sentences. To produce alignments using Giza++ and Fast Align, we train them on the 330 sentences with or without additional 10k sen"
2021.mrl-1.2,W08-0336,0,0.148786,"Missing"
2021.mrl-1.2,C18-1214,0,0.0233141,"monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an SOV language with very rich morphology (Valenzuela, 1997; Vasquez et al., 2018), whereas Spanish is an SVO language. 3.1.3 Griko Griko is a Greek dialect spoken in southern Italy, and “severely endangered” according to UNESCO. There is no large-scale monolingual corpus of Griko, but there are two Griko–Italian parallel corpora (Zanon Boito et al., 2018; Anastasopoulos et al., 2018), with the smaller one including gold word alignment annotations. However, Griko has never had a consistent orthography, and hence its tokenisation and word segmentation differ across these corpora: the smaller data set is based on orthographic conventions from Italian, while the larger one follows the concept of a phonological word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthograph"
2021.mrl-1.2,2020.emnlp-main.42,0,0.0366598,"Missing"
2021.mrl-1.2,P18-1073,0,0.0194224,"presentations even in extremely lowresource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri"
2021.mrl-1.2,J82-2005,0,0.663283,"Missing"
2021.mrl-1.2,Q19-1038,0,0.0362526,"Missing"
2021.mrl-1.2,Q17-1010,0,0.0443519,"posed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). Whil"
2021.mrl-1.2,W03-0301,0,0.414717,"Missing"
2021.mrl-1.2,P19-1314,0,0.0598251,"Missing"
2021.mrl-1.2,W15-1521,0,0.227358,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,D15-1166,0,0.0790593,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,N18-1031,0,0.0197724,"Text (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). While they apply separate output layers to the weighted average of word embeddings and hidden states, we have found that sharing the same output layer (E t ) performed the best, suggesting that the optimal model architecture is different between word and sentence translations. 17 where F(·) denotes the subword encoding function; Zk denotes the k-th subword embedding and Q(wi ) denotes the indices of the subwords included in wi . The subword embeddings Z are shared among all languages, capturing orthographic similarities across languages. For the encoding function F(·), we experiment with two"
2021.mrl-1.2,L18-1697,0,0.0167371,"we train them on the same data, and align each word in a sentence to the closest word in its translation using static or contextualised word embeddings.18 To calculate word similarity, we use cross-domain similarity local scaling (Conneau et al., 2018): 1 ∑ CSLS(x, y) = 2 cos(x, y) − cos(x, yt ) K yt ∈NT (x) − 1 K ∑ each source word in a bilingual dictionary, we extract the k nearest words from the whole target vocabulary and see whether they are listed as translations in the dictionary. We set k to 1 or 5, and report P@1 and P@5. For evaluation, we use a Shipibo-Konibo–Spanish dictionary20 (Maguiño-Valencia et al., 2018) and Na–French– Chinese–English dictionaries (Michaud, 2018). Based on extracting words that are present in the parallel corpora, we identified 79, 262, 215 and 87 word pairs for Shipibo-Konibo–Spanish, Na–French, Na–Chinese, and Na–English.21 To perform BLI with GIZA++ and Fast Align, we use their source-to-target probability table. We also try using the result of bidirectional word alignments, aligning each word to the most frequently aligned words to it.22 For the neural baselines and our model, we use static word embeddings and employ CSLS to measure the word embedding similarities. To obt"
2021.mrl-1.2,J03-1002,0,0.0434436,"d BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speakers in the Amazon region of Peru (Vasquez et al., 2018), and is “definitely endangered” according to the UNESCO’s Atlas of the World’s Languages in Danger (Moseley, 2010). There is no large monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an"
2021.mrl-1.2,P19-1312,0,0.0201844,"Sabet et al. (2020) as recent word embedding baselines, which we denote as SENTID, BIVEC and BIS2V, respectively. All of these baselines are very similar in terms of methodology: SENTID trains a Skip-Gram model that predicts a sentence ID (which is assigned to each set of parallel sentences) from the component words; BIVEC trains a Skip-Gram model that predicts the context cross-lingually based on the wordalignment information;13 and BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speake"
2021.mrl-1.2,2020.acl-main.156,0,0.0813123,"Missing"
2021.mrl-1.2,W03-0320,0,0.167078,"Missing"
2021.mrl-1.2,N18-1202,0,0.055646,"r¯is + hi in Eqn. (5) before the linear transformation, with the dropout rate all set to 0.5. We show that this strong regularisation leads to better cross-lingual representations. denotes a one-hot vector. In cross-lingual tasks, we employ ris and usi as the static and contextualised word embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi ,"
2021.mrl-1.2,E17-2025,0,0.0608051,"Missing"
2021.mrl-1.2,P19-1300,1,0.812151,"delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko–"
2021.mrl-1.2,2020.sltu-1.13,0,0.0499318,"Missing"
2021.mrl-1.2,D18-1034,0,0.0217819,", our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri−1 ), t p(BOS, y1t ..., yM )= M ∏ ← − t s p(yM −i |h M −i , u ),"
2021.mrl-1.2,P18-2037,0,0.0387565,"Missing"
2021.mrl-1.2,N15-1104,0,0.0870183,"Missing"
2021.mrl-1.2,W17-2619,0,0.0616142,"Missing"
2021.mrl-1.2,J96-1001,0,0.692304,"and report the best score; we fine-tune the mBERT baseline for 40,000 steps26 with 20 checkpoints, and train SENTID and BIS2V for 1,000 epochs with 100 checkpoints to ensure convergence. For BIVEC, we increase the training corpus size by 20 times by duplicating the sentences and train the model for 50 epochs with 50 checkpoints.27 For our model, on the other hand, we use a simple early-stopping criterion that doesn’t require external data. First, we build a pseudo bilingual dictionary from the training data. To retrieve pseudo bilingual word pairs, we compute the Dice Coeﬀicient (Dice, 1945; Smadja et al., 1996) and extract pairs of words that appear ≥ 3 times in each language and whose Dice Coeﬀicient is ≥ 0.8 across two languages. We perform model selection based on the BLI performance on this pseudo dictionary. 3.5 Word src–tgt nru–en nru–fr nru–zh +SWave +SWcnn bi multi bi mult bi multi 30.2 27.3 31.6 34.2 28.7 36.9 32.0 29.9 36.7 37.8 29.8 40.1 35.6 32.1 38.5 40.5 30.7 40.8 Table 4: Our model performance (P@1) on BLI when the model is trained on two and four languages (“bi” vs. “multi”). All scores are averaged over three runs. our model. Compared to the neural baselines, our model performs bett"
2021.mrl-1.2,2020.acl-main.146,0,0.0389663,"Missing"
2021.mrl-1.2,P17-1179,0,0.0515976,"Missing"
2021.mrl-1.2,2020.emnlp-main.43,0,0.0841257,"Missing"
2021.naacl-main.175,2020.acl-main.194,0,0.11453,"ress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence policies like renewhave been combined with great success, e.g. Xie ables or carbon taxes are not worth it. et al. (2020) used BERT along with consistency reg• Justification by Comparison (Justify ularization on unlabeled data, Croce et al. (2020) POLICY): our actions are not as important extended the fine-tuning process of BERT to a genas other countries which pollute more, or there erative adversarial setting, and Chen et al. (2020) are other more important issues than global 2168 warming. Examples of these 7 neutralization techniques are given in Table 2. CCS texts often use multiple NT together in their narrative (hence motivating a multilabel classification task), as seen in the second example in Table 1 where Condemn (POLICY) is used to blame the alarmist greens and Deny-Responsibility (SCIENCE) is used to highlight that global warming is a natural cycle. Similarly, in third example as well we see Condemn (POLICY) is used to accuse the IPCC (Intergovernmental Panel on Climate Change,3 in conjunction with Deny-Respons"
2021.naacl-main.175,D18-1217,0,0.056925,"Missing"
2021.naacl-main.175,P19-1590,0,0.0170891,"sed NLP modSCIENCE): there is no evidence of els is the strong dependency on labelled data. To climate change and no climate change tackle this, one approach is apply transfer learning victims; total denial of any global warming. from pretrained language models (Radford et al., • Condemnation of the Condemner (Condemn POLICY): climate change is 2019; Peters et al., 2018; Yang et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019). Anmisrepresented by scientists or manipulated other approach is semi-supervised learning. Yang by politicians, the media, environmentalists, et al. (2017) and Gururangan et al. (2019) emetc. ployed variational autoencoders, and Clark et al. • Appeal to Higher Loyalties (Loyalties (2018) leveraged cross-view training using a mixPOLICY): economic progress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence policies like renewhave been combined with great success, e.g. Xie ables or carbon taxes are not worth it. et al. (2020) used BERT along with consistency reg• Justification by Comparison (Justify ularization on unlabeled data, Croce et al. (2020) POLI"
2021.naacl-main.175,2020.acl-main.740,0,0.0462508,"Missing"
2021.naacl-main.175,2020.findings-emnlp.296,0,0.294732,"operated at the article level, relate to the SCIENCE frame, and the last three to and focused on binary detection (presence vs. ab- the POLICY frame, as indicated): sence) (Barr´on-Cedeno et al., 2019; Rashkin et al., • Denial of Responsibility 2017). Da San Martino et al. (2019) argued for (Deny-Responsibility SCIENCE): the need for finer granularity in propaganda detecclimate change is happening, but is a natural tion, both in terms of propaganda sub-types and cycle and human are not responsible. fragment-level detection. In a similar vein, Naka• Denial of Injury1 (Deny-Injury1 mura et al. (2020) proposed fine-grained classes SCIENCE): there are no significant harms of fake news to differentiate between misleading, attributable to climate change, and claims are manipulated, or totally false content. More recently generally overstated. in the climate change domain, Luo et al. (2020) re• Denial of Injury2 (Deny-Injury2 leased a stance-annotated dataset for global warmSCIENCE): there are benefits in rising ing, and proposed an opinion framing task to study rising C02 levels which have a positive effect discourse used in the debate around global warmon the environment. ing. • Denial of Vi"
2021.naacl-main.175,D17-1317,0,0.0529276,"Missing"
2021.naacl-main.175,2020.lrec-1.755,0,0.0845038,"Missing"
2021.naacl-main.175,N18-1202,0,0.0403638,"s in rising ing, and proposed an opinion framing task to study rising C02 levels which have a positive effect discourse used in the debate around global warmon the environment. ing. • Denial of Victim (Deny-Victim One challenge in building supervised NLP modSCIENCE): there is no evidence of els is the strong dependency on labelled data. To climate change and no climate change tackle this, one approach is apply transfer learning victims; total denial of any global warming. from pretrained language models (Radford et al., • Condemnation of the Condemner (Condemn POLICY): climate change is 2019; Peters et al., 2018; Yang et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019). Anmisrepresented by scientists or manipulated other approach is semi-supervised learning. Yang by politicians, the media, environmentalists, et al. (2017) and Gururangan et al. (2019) emetc. ployed variational autoencoders, and Clark et al. • Appeal to Higher Loyalties (Loyalties (2018) leveraged cross-view training using a mixPOLICY): economic progress and develture of labelled and unlabelled data. More recently, opment are more important than action on pretrained models and semi-supervised learning climate change, and hence"
2021.naacl-main.301,D17-1168,0,0.023274,"rmalized Scores 0.8 Accuracy Discourse Connective 0.6 0.4 Spearman Rank Accuracy 1.0 10 12 2 4 6 8 Layer 10 12 Figure 2: Probing task performance on English for each of the seven tasks, plus the average across all tasks. For BART and T5, layers 7–12 are the decoder layers. All results are averaged over three runs, and the vertical line for each data point denotes the standard deviation (noting that most results have low s.d., meaning the bar is often not visible). or, or although (Nie et al., 2019), representing the conceptual relation between the sentences/clauses. monsense and storytelling (Chaturvedi et al., 2017; Liu et al., 2018). 4 4. RST nuclearity prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the nucleus/satellite status of each (see Figure 1). 5. RST relation prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the relation that holds between them (see Figure 1). 6. RST elementary discourse unit (EDU) segmentation. Chunk a concatenated sequence of EDUs into its component EDUs. Experimental Setup We summarize all data (sources, number of labels,"
2021.naacl-main.301,2020.coling-main.66,1,0.859942,"next sentence. The preceding context takes the form of between 2 and 8 sentences, but the candidates are always single sentences. We outline the 7 pretrained models in Table 1. They comprise 4 encoder-only models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), and ELECTRA (Clark 2. Sentence ordering. We shuffle 3–7 sentences et al., 2020); 1 decoder-only model: GPT-2 (Radand attempt to reproduce the original order. ford et al., 2019); and 2 encoder–decoder modThis task is based on Barzilay and Lapata els: BART (Lewis et al., 2020) and T5 (Raffel (2008) and Koto et al. (2020), and is assessed et al., 2019). To reduce the confound of model size, based on rank correlation relative to the origiwe use pretrained models of similar size (∼110m nal order. model parameters), with the exception of ALBERT which is designed to be lighter weight. All mod3. Discourse connective prediction. Given two els have 12 transformer layers in total; for BART sentences/clauses, the task is to identify an and T5, this means their encoder and decoder have appropriate discourse marker, such as while, 6 layers each. Further details of the models are 2 provided in the Supplementary Material."
2021.naacl-main.301,2020.acl-main.703,0,0.24701,"s of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not capture. Probing tasks offer a means to perform fine-grained analysis of the capabilities of such models, but most existing work has focused on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing h"
2021.naacl-main.301,D14-1224,0,0.074932,"Missing"
2021.naacl-main.301,P18-2045,1,0.8568,"acy Discourse Connective 0.6 0.4 Spearman Rank Accuracy 1.0 10 12 2 4 6 8 Layer 10 12 Figure 2: Probing task performance on English for each of the seven tasks, plus the average across all tasks. For BART and T5, layers 7–12 are the decoder layers. All results are averaged over three runs, and the vertical line for each data point denotes the standard deviation (noting that most results have low s.d., meaning the bar is often not visible). or, or although (Nie et al., 2019), representing the conceptual relation between the sentences/clauses. monsense and storytelling (Chaturvedi et al., 2017; Liu et al., 2018). 4 4. RST nuclearity prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the nucleus/satellite status of each (see Figure 1). 5. RST relation prediction. For a given ordered pairing of (potentially complex) EDUs which are connected by an unspecified relation, predict the relation that holds between them (see Figure 1). 6. RST elementary discourse unit (EDU) segmentation. Chunk a concatenated sequence of EDUs into its component EDUs. Experimental Setup We summarize all data (sources, number of labels, and data split) in"
2021.naacl-main.301,2021.ccl-1.108,0,0.0758504,"Missing"
2021.naacl-main.301,P11-1015,0,0.0839363,"s with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not"
2021.naacl-main.301,2020.emnlp-main.552,0,0.0509812,"Missing"
2021.naacl-main.301,N16-1098,0,0.199386,", 2020) N/A #Labels: 15 Split: 900/148/159 (4) RST Nuclearity (5) RST Relation RST-DT (Carlson et al., 2001) #Labels (nuc/rel): 3/18 Split: 16903/1943/2308 CDTB (Li et al., 2014) #Labels (nuc/rel): 3/4 Split: 6159/353/809 Potsdam Commentary (Bourgonje and Stede, 2020) #Labels (nuc/rel): 3/31 Split: 1892/289/355 (6) RST EDU Segmentation RST-DT (Carlson et al., 2001) Split: 312/35/38 docs Potsdam Commentary RST-Spanish Treebank CDTB (Li et al., 2014) (Bourgonje and Stede, 2020) (da Cunha et al., 2011) Split: 2135/105/241 p’graphs Split: 131/20/25 docs Split: 200/34/30 docs (7) Cloze Story Test (Mostafazadeh et al., 2016) Split: 1683/188/1871 N/A N/A RST-Spanish Treebank (da Cunha et al., 2011) #Labels (nuc/rel): 3/29 Split: 2042/307/421 N/A Table 2: A summary of probing tasks and datasets for each of the four languages. “Split” indicates the number of train/development/test instances. Nuclearity and Relation prediction: cause (N) (S) elab (S) EDU1 EDU3 (N) EDU2 ⇨ ⇨ text1 |text2 nuclearity, relation EDU1 |EDU2 SN, elab EDU1 EDU2 |EDU3 NS, cause ⇨ EDU segmentation: EDU1 EDU2 EDU3 EDU1 |EDU2 |EDU3 ⇨ Figure 1: Illustration of the RST discourse probing tasks (Tasks 4–6). To summarize, we introduce 7 discourse-rela"
2021.naacl-main.301,D19-6108,0,0.0176823,"as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020) has raised questions about what precise aspects of language these models do and do not capture. Probing tasks offer a means to perform fine-grained analysis of the capabilities of such models, but most existing work has focused on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing how sentences are connected and describing the overall structure of a document (Mann and Thompson, 1986). A number of studies have used pretrained models to classify discourse markers (Sileo et al., 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019), but few (Koto et al., to appear) have systematically investigated the ability of pretrained models to model discourse 1 structure. Further"
2021.naacl-main.301,D19-1448,0,0.0621788,"Missing"
2021.naacl-main.301,2020.blackboxnlp-1.3,0,0.0261906,"model, leaving open the question of how well these findings generalize to other models with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse units. The remarkable development of pretrained language models (Devlin et al., 2019; Lewis et al., 2020; Lan et al., 2020)"
2021.naacl-main.301,P18-2119,0,0.0125827,"cept sentence ordering and EDU segmentation as a classification problem, and evaluate using accuracy. During fine-tuning, we add an MLP layer on top of the pretrained model for classification, and only update the MLP parameters (all other layers are frozen). We use the [CLS] embedding for BERT and ALBERT following standard practice, while for other models we perform average pooling to obtain a vector for each sentence, and concatenate them as the input to the MLP.3 7. Cloze story test. Given a 4-sentence story context, pick the best ending from two possible options (Mostafazadeh et al., 2016; Sharma et al., 2018). This task is harder than NSP, as it requires an understanding of com3851 3 BERT and ALBERT performance with average pooling For sentence ordering, we follow Koto et al. (2020) and frame it as a sentence-level sequence labelling task, where the goal is to estimate P (r|s), where r is the rank position and s the sentence. The task has 7 classes, as we have 3–7 sentences (see Section 3). At test time, we choose the label sequence that maximizes the sequence probability. Sentence embeddings are obtained by average pooling. The EDU segmentation task is also framed as a binary sequence labelling t"
2021.naacl-main.301,D19-1586,0,0.0239393,"on sentence-level analysis such as syntax (Hewitt and Manning, 2019; Jawahar et al., 2019; de Vries et al., 2020), entities/relations (Papanikolaou et al., 2019), and ontological knowledge (Michael et al., 2020). Less is known about how well such models capture broader discourse in documents. Rhetorical Structure Theory is a framework for capturing how sentences are connected and describing the overall structure of a document (Mann and Thompson, 1986). A number of studies have used pretrained models to classify discourse markers (Sileo et al., 2019) and discourse relations (Nie et al., 2019; Shi and Demberg, 2019), but few (Koto et al., to appear) have systematically investigated the ability of pretrained models to model discourse 1 structure. Furthermore, existing work relating to For example, they only consider discourse relation labels discourse probing has typically focused exclusively and ignore nuclearity. 3849 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3849–3864 June 6–11, 2021. ©2021 Association for Computational Linguistics Probing Task English Chinese German Spanish (1) 4-way NSP (2) Sen"
2021.naacl-main.301,N19-1351,0,0.0631861,"Missing"
2021.naacl-main.301,2020.emnlp-main.70,0,0.0359673,"= sentence order prediction, “LM” = language model, “DISC” = discriminator, and “DAE” = denoising autoencoder. on the BERT-base model, leaving open the question of how well these findings generalize to other models with different pretraining objectives, for different languages, and different model sizes. Our research question in this paper is: How much discourse structure do layers of different pretrained language models capture, and do the findings generalize across languages? There are two contemporaneous related studies that have examined discourse modelling in pretrained language models. Upadhye et al. (2020) analyzed how well two pretrained models capture referential biases of different classes of English verbs. Zhu et al. (2020) applied the model of Feng and Hirst (2014) to parse IMDB documents (Maas et al., 2011) into discourse trees. Using this (potentially noisy) data, probing tasks were conducted by mapping attention layers into single vectors of document-level rhetorical features. These features, however, are unlikely to capture all the intricacies of inter-sentential abstraction as their input is formed based on discourse relations1 and aggregate statistics on the distribution of discourse"
2021.nllp-1.23,2020.acl-main.647,0,0.041008,"Missing"
2021.nllp-1.23,W19-2208,0,0.0655402,", but also the risks associated with naive use of model predictions, including fairness across different user demographics. 1 Introduction which we can make incremental measurable improvements. Text classification of any real-world data can be a challenge for many reasons. In the case of legal text classification, the classes themselves or the legal categorisation of a possible case, can vary from organisation to organisation, and also from court to court; there is no universally agreed-upon set of areas of law neatly defined into a taxonomy (Goncalves and Quaresma, 2005; Sulea et al., 2017a; Soh et al., 2019; Tuggener et al., 2020). Furthermore, a case can span multiple areas of law — for example, a FAMILY LAW matter could also fall under the umbrella of G UARDIANSHIP AND AD MINISTRATION , or a C HARITIES LAW issue may also have aspects regarding E MPLOYEES AND VOLUNTEERS. In addition to the issues surrounding the inherent fuzziness of legal categories, the descriptions of legal issues themselves exhibit a range of language styles: those who seek free legal help are not versed in the legal domain, and may have varying linguistic styles, reflecting their social, cultural, and educational backgroun"
2021.nllp-1.23,N19-1423,0,0.00538772,"ckgrounds, generalise well from small amounts of curated data, and potentially dynamically interact with the help-seeker to clarify the nature of the case. However, in this preliminary work, our aim is to develop initial models as 1 a means to ascertain what biases manifest in our https://www.acnc.gov.au/charity/ given data, and to have a workable model upon 232d6dcbcaa1550da90f825fe6fab643#history 217 Natural Legal Language Processing Workshop 2021, pages 217–227 November 10, 2021. ©2021 Association for Computational Linguistics and 5, which describe the initial fine-tuned B ERT classifiers (Devlin et al., 2019) on the small curated help-seeker data informally describing issues in their own words on matters they believe require legal assistance. As a starting point, we wanted to leverage the patterns of language usage encoded in B ERT given our relatively small data set. The main risk is that while robust results can be achieved by fine-tuning over relatively little labelled data in this manner, the data used in developing the pre-trained models can lead to these models implicitly capturing a variety of biases about the world (Bender et al., 2021). In Section 6, we reveal how these biases manifest fo"
2021.nllp-1.23,2020.lrec-1.155,0,0.0802432,"Missing"
2021.nllp-1.24,Q18-1041,0,0.0222124,"al cases, in which there is a significant issue for NLP techniques, as discussed by Higgins et al. (2020). 7 Conclusion 8 There is also a third task of case importance regression, on a scale of 1–4, where no explicit judgment prediction is made. We have introduced a new large-scale data set for legal judgment prediction, derived from domain 235 name dispute cases from the World Intellectual Property Organisation where a complainant has applied for a domain name to be transferred or cancelled on the grounds of trademark infringement. The data set is released with a data statement as defined by Bender and Friedman (2018) in the Appendix. In the context of binary classification of the complaint being accepted or denied, we provided results for a number of benchmark NLP methods. In terms of macro-averaged f-score (to counter the effects of class imbalance), we showed that B ERT with fine-tuning performed the best, including over the subset of cases where a response has been submitted. However, we equally showed that there is considerable room for improvement in the results, and plenty of room for further work on this task. In terms of future work, we aim to further build on the models presented in this paper, a"
2021.nllp-1.24,P19-1424,0,0.236188,"nternet Corporation for Assigned Names and Numbers (ICANN), introduced a fast and low-cost mandatory arbitration system as an alternative to the slow and expen1 sive litigation of cybersquatting disputes — the https://www.icann.org/resources/ Uniform Domain Name Dispute Resolution Policy pages/policy-2012-02-25-en 228 Natural Legal Language Processing Workshop 2021, pages 228–238 November 10, 2021. ©2021 Association for Computational Linguistics interest to the task. While we are by no means the first to propose the task of legal judgment prediction (Aletras et al., 2016; Zhong et al., 2018a; Chalkidis et al., 2019; Zhong et al., 2020), previous work has focused almost exclusively on criminal or human rights cases, which we argue have sensitivities and ethical dimensions that are yet to be fully quantified (Leins et al., 2020; Tsarapatsanis and Aletras, 2021). Part of our intention with this paper is to introduce a “lower-stakes” task/data set in terms of its direct and indirect implications for individuals and civil liberties, which is far from “solved” in terms of current NLP capabilities. In the introduction of this new task, we outline in Section 2 the process of how a complaint is dealt with once i"
2021.nllp-1.24,W14-4012,0,0.0249319,"Missing"
2021.nllp-1.24,2020.emnlp-main.21,0,0.0410158,"Missing"
2021.nllp-1.24,N19-1423,0,0.0111084,"e.eng.unimelb.edu. ing the earliest data, followed by the development, au/tbaldwin/resources/wipo/; Code: https://github.com/vihikan/ and finally the test data). automatic-resolution-of-domain-name-disputes Each case is represented as a JSON file with 6 https://commoncrawl.org 7 seven fields: (1) the (binary) outcome decision; https://nlp.stanford.edu/projects/ (2) the URL of the original case description; (3) glove/ 232 trained using the Adam optimizer (Kingma and Ba, 2017) with a learning rate of 0.001. We iterate through 20 epochs with a batch size of 64. We next train 3 variants of B ERT (Devlin et al., 2019): (1) OO B ERT (out-of-the-box B ERT); (2) FT B ERT (fine-tuned B ERT); and (3) L EGAL B ERT, a B ERT model pre-trained on legal documents (Chalkidis et al., 2020) which we fine-tune over our data. The difference between OO B ERT and FT B ERT is that we additionally pre-train FT B ERT over the non-test data using the masked language model objective. By default, the B ERT base model has 12 transformer layers, and in this experiment, we freeze the first eight layers, in addition to the embeddings layer. The classification layer has 64 units followed by a dropout layer with value of 0.2, and an o"
2021.nllp-1.24,P82-1020,0,0.783565,"Missing"
2021.nllp-1.24,2020.emnlp-main.102,0,0.0544898,"Missing"
2021.nllp-1.24,2020.acl-main.261,1,0.851728,"Missing"
2021.nllp-1.24,P12-3005,1,0.81733,"Missing"
2021.nllp-1.24,D14-1162,0,0.0872026,"ses that accept or deny the complainant’s claims. 4.1 Data Preparation the WIPO case number; (4) the name of the complainant; (5) the name of the respondent; (6) the title of the case; and (7) the textual content of the case, structured based on headings and section boundaries. All of the data described as well as the 5 code is available for download. 4.2 Method We first developed two RNN-based models: (1) B I GRU (a bidirectional GRU: Cho et al. (2014)); and (2) B I LSTM (a bidirectional LSTM: Hochreiter and Schmidhuber (1997)). For both of these models we use G LOVE pretrained word vectors (Pennington et al., 2014), derived from 42 billion tokens of 6 7 Common Crawl with 300-dimension vectors. For both models the pretrained G LOVE embeddings are not updated in the training phase (i.e. they are set to ‘untrainable’). The B I GRU and B I LSTM models are built using the layers module in Tensorflow’s Keras API, with the pretrained embeddings being passed through a SpatialDropout layer with value 0.2, followed by either an LSTM or GRU layer with 128 units and 0.2 dropout. It is then followed by the second (reverse-direction) layer but with 64 units, a 0.2 dropout layer, and finally a dense layer that has 3 u"
2021.nllp-1.24,2020.acl-main.280,0,0.0117549,"ty party. The data set has since been used 234 M ODEL R ESPONSE N O R ESPONSE ACC P R F ACC P R F Majority 0.692 0.346 0.500 0.409 0.982 0.491 0.500 0.495 L OG R EG 0.828 0.828 0.827 0.820 0.753 0.759 0.775 0.779 0.982 0.982 0.888 0.759 0.710 0.584 0.769 0.625 0.692 0.831 0.834 0.770 0.808 0.808 0.738 0.806 0.798 0.750 0.803 0.802 0.982 0.982 0.982 0.769 0.763 0.753 0.586 0.598 0.573 0.628 0.642 0.611 SVM OO B ERT FT B ERT L EGAL B ERT Table 5: Breakdown of results based on response vs. no response widely as a benchmark data set for reasoning and legal NLP (Yang et al., 2019; Li et al., 2019; Xu et al., 2020; Zhong et al., 2020). For English, the most popular data set is perhaps that of Chalkidis et al. (2019) based on over 11k European Court of Human Rights (“ECtHR”) cases, in the form of two tasks: (1) a binary classification task: does the case violate any of the 66 articles and protocols of the ECtHR; and (2) a multi-label classification task: which, if any, of the 66 articles 8 and protocols does a given case violate. Other data sets include a French data set of 126k cases from the French Supreme Court (Sulea ¸ et al., 2017), an English data set of 5k cases from the UK Court of Appeal (Stric"
2021.nllp-1.24,D18-1390,0,0.0430995,"Missing"
2021.nllp-1.24,sulea-etal-2017-predicting,0,0.0672739,"Missing"
2021.nllp-1.24,2021.findings-acl.314,0,0.0339832,"Missing"
2021.sigdial-1.16,P05-1018,0,0.386402,"w the superiority of our proposed method in in- and cross-domain settings. The utility of our method is also verified over a multi-document summarisation task. 1 Introduction and Background Document coherence understanding plays an important role in natural language understanding, where a coherent document is connected by rhetorical relations, such as contrast, elaboration, narration, and justification, allowing us to communicate cooperatively in understanding one another. In this work, we measure the ability of models to capture document coherence in the strictest setting: sentence ordering (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Prabhumoye et al., 2020), a task of ordering an unordered bag of sentences from a document, aiming to maximise document coherence. The task of sentence ordering is to restore the original order for a given bag of sentences, based on the coherence of the resulting document. The ability of a model to reconstruct the original sentence order is a demonstration of its capacity to capture document coherence. Figure 1 presents such an example, where the (shuffled) sentences are from a paper abstract discussing the relationship between word informative"
2021.sigdial-1.16,J08-1001,0,0.44027,"and cross-domain settings. The utility of our method is also verified over a multi-document summarisation task. 1 Introduction and Background Document coherence understanding plays an important role in natural language understanding, where a coherent document is connected by rhetorical relations, such as contrast, elaboration, narration, and justification, allowing us to communicate cooperatively in understanding one another. In this work, we measure the ability of models to capture document coherence in the strictest setting: sentence ordering (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Prabhumoye et al., 2020), a task of ordering an unordered bag of sentences from a document, aiming to maximise document coherence. The task of sentence ordering is to restore the original order for a given bag of sentences, based on the coherence of the resulting document. The ability of a model to reconstruct the original sentence order is a demonstration of its capacity to capture document coherence. Figure 1 presents such an example, where the (shuffled) sentences are from a paper abstract discussing the relationship between word informativeness and pitch prominence, and the gold-standard"
2021.sigdial-1.16,D19-1371,0,0.0236356,"the same architecture and shared word embeddings to obtain representations, and the segment representations are concatenated together to feed into a linear layer and softmax layer. We use 300d pre-trained GloVe word embeddings (Pennington et al., 2014) with updating, LSTM cell size of 128, and train with a mini-batch size of 128 for 10 epochs (with early stopping) and learning rate of 1e-3. BERT: predict the relative order from the “CLS” token using pre-trained BERT (Devlin et al., 2019), or alternatively ALBERT (Lan et al., 2020) (due to its specific focus on document coherence) or SciBERT (Beltagy et al., 2019) (due to the domain fit with the datasets). For BERT and ALBERT, we use the base uncased version,1 and finetune for 2 epochs in each case with a learning rate of {5e-5, 5e-6}. BERTSON (Cui et al., 2020): the current SOTA for sentence ordering, in the form of a BERT-based generative model which feeds representations of each sentence (given the context of the full document) into a self-attention based paragraph encoder to obtain the document representation, which is used to initialise the initial state of an LSTMbased pointer network. During decoding, a deep relational module is integrated with"
2021.sigdial-1.16,P19-1606,0,0.0174804,"is a demonstration of its capacity to capture document coherence. Figure 1 presents such an example, where the (shuffled) sentences are from a paper abstract discussing the relationship between word informativeness and pitch prominence, and the gold-standard sentence ordering is (4, 5, 1, 7, 3, 2, 6). Furthermore, the task of sentence ordering is potentially beneficial for downstream tasks such as multi-document summarisation (Nallapati Figure 1: An example of shuffled sentences from the same document. et al., 2017), storytelling (Fan et al., 2019; Hu et al., 2020), cooking recipe generation (Chandu et al., 2019), and essay scoring (Tay et al., 2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either"
2021.sigdial-1.16,D18-1465,0,0.0120847,"role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Association for Computational Linguistics tive approaches, Cui et al. (2020) obtain sentence and paragraph representations from BERT (Devlin et al., 2019) and then use a pointer network to decode the sentence ordering for a given paragraph, whereas Yin et al. (2019) use a graph-based"
2021.sigdial-1.16,2020.emnlp-main.511,0,0.0422243,"Missing"
2021.sigdial-1.16,N19-1423,0,0.116896,"work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Association for Computational Linguistics tive approaches, Cui et al. (2020) obtain sentence and paragraph representations from BERT (Devlin et al., 2019) and then use a pointer network to decode the sentence ordering for a given paragraph, whereas Yin et al. (2019) use a graph-based neural network over sentences and entities. The shortcoming of generative methods is the difficulty in obtaining good paragraph representations, especially for longer paragraphs. To mitigate this, various attention mechanisms have been explored (Cui et al., 2018; Wang and Wan, 2019; Kumar et al., 2020). Discriminative approaches, on the other hand, can readily capture the relative order between sentence pairs, and paragraph decoding can then be achieved through met"
2021.sigdial-1.16,N07-1055,0,0.25819,"oposed method in in- and cross-domain settings. The utility of our method is also verified over a multi-document summarisation task. 1 Introduction and Background Document coherence understanding plays an important role in natural language understanding, where a coherent document is connected by rhetorical relations, such as contrast, elaboration, narration, and justification, allowing us to communicate cooperatively in understanding one another. In this work, we measure the ability of models to capture document coherence in the strictest setting: sentence ordering (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Prabhumoye et al., 2020), a task of ordering an unordered bag of sentences from a document, aiming to maximise document coherence. The task of sentence ordering is to restore the original order for a given bag of sentences, based on the coherence of the resulting document. The ability of a model to reconstruct the original sentence order is a demonstration of its capacity to capture document coherence. Figure 1 presents such an example, where the (shuffled) sentences are from a paper abstract discussing the relationship between word informativeness and pitch promin"
2021.sigdial-1.16,P11-2022,0,0.0334738,"hermore, the task of sentence ordering is potentially beneficial for downstream tasks such as multi-document summarisation (Nallapati Figure 1: An example of shuffled sentences from the same document. et al., 2017), storytelling (Fan et al., 2019; Hu et al., 2020), cooking recipe generation (Chandu et al., 2019), and essay scoring (Tay et al., 2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of"
2021.sigdial-1.16,P19-1102,0,0.02675,"0.0 λ=0.3 λ=0.5 λ=0.7 λ=1.0 91.28 91.02 91.94 69.97 70.88 71.76 55.76 57.45 58.30 41.55 44.03 44.85 20.24 23.89 24.67 proposed method, in addition to results in a crossdomain setting and for multi-document summarisation. Table 4: Coherence scores for reordered summaries. “allpairs” indicates ALBERT-allpairs and “adjonly” indicates ALBERT-adjonly (our model). maries generated by an extractive multi-document summarisation system. Following Yin et al. (2020), we finetune ALBERT-allpairs and ALBERTadjonly over 500 reference summaries randomly sampled from a large-scale news summarisation dataset (Fabbri et al., 2019). We then generate extractive summaries from DUC 2004 documents (Task 2) with TextRank (Barrios et al., 2016), and use ALBERT-allpairs and ALBERT-adjonly to reorder the summaries. To evaluate the coherence of generated summaries, Nayeem and Chali (2017) and Yin et al. (2020) use the weighted sum of cosine similarity and named entity similarity,3 defined as: 1 X Sim(si , si+1 ), n−1 i=1 Sim(si , si+1 ) = λ ∗ NESim(si , si+1 ) +(1 − λ) ∗ Sim(si , si+1 ), where n is the number of sentences, Sim(si , si+1 ) is the cosine similarity over representations (sum of word embeddings) of adjacent sentence"
2021.sigdial-1.16,P19-1254,0,0.0306658,"Missing"
2021.sigdial-1.16,P13-1010,0,0.0251851,"allapati Figure 1: An example of shuffled sentences from the same document. et al., 2017), storytelling (Fan et al., 2019; Hu et al., 2020), cooking recipe generation (Chandu et al., 2019), and essay scoring (Tay et al., 2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Associat"
2021.sigdial-1.16,J06-4002,0,0.222309,"Missing"
2021.sigdial-1.16,D14-1218,0,0.0199551,"oring (Tay et al., 2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Association for Computational Linguistics tive approaches, Cui et al. (2020) obtain sentence and paragraph representations from BERT (Devlin et al., 2019) and then use a pointer network to decode the"
2021.sigdial-1.16,D17-1019,0,0.0172501,"2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Association for Computational Linguistics tive approaches, Cui et al. (2020) obtain sentence and paragraph representations from BERT (Devlin et al., 2019) and then use a pointer network to decode the sentence ordering for"
2021.sigdial-1.16,N16-1167,0,0.0178663,"ce ordering is potentially beneficial for downstream tasks such as multi-document summarisation (Nallapati Figure 1: An example of shuffled sentences from the same document. et al., 2017), storytelling (Fan et al., 2019; Hu et al., 2020), cooking recipe generation (Chandu et al., 2019), and essay scoring (Tay et al., 2018; Li et al., 2018), where document coherence plays an important role. Traditional approaches to sentence ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting o"
2021.sigdial-1.16,W17-2407,0,0.0149788,"reordered summaries. “allpairs” indicates ALBERT-allpairs and “adjonly” indicates ALBERT-adjonly (our model). maries generated by an extractive multi-document summarisation system. Following Yin et al. (2020), we finetune ALBERT-allpairs and ALBERTadjonly over 500 reference summaries randomly sampled from a large-scale news summarisation dataset (Fabbri et al., 2019). We then generate extractive summaries from DUC 2004 documents (Task 2) with TextRank (Barrios et al., 2016), and use ALBERT-allpairs and ALBERT-adjonly to reorder the summaries. To evaluate the coherence of generated summaries, Nayeem and Chali (2017) and Yin et al. (2020) use the weighted sum of cosine similarity and named entity similarity,3 defined as: 1 X Sim(si , si+1 ), n−1 i=1 Sim(si , si+1 ) = λ ∗ NESim(si , si+1 ) +(1 − λ) ∗ Sim(si , si+1 ), where n is the number of sentences, Sim(si , si+1 ) is the cosine similarity over representations (sum of word embeddings) of adjacent sentences, and NESim(si , si+1 ) measures the fraction of shared named entities between adjacent sentences. Higher values indicate better performance. Table 4 shows the results for different λ values (different emphasis on shared named entities). We can see tha"
2021.sigdial-1.16,D19-1232,0,0.0175606,"nce ordering used hand-engineered features to capture document coherence (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Mesgar and Strube, 2016), e.g. using an entity matrix (Barzilay and Lapata, 2005, 2008) or graph (Guinaudeau and Strube, 2013) to represent entity transitions across sentences, and maximising transition probabilities between adjacent sentences. Neural work has modelled the task either generatively (Li and Hovy, 2014; Li and Jurafsky, 2017; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Wang and Wan, 2019; Oh et al., 2019; Cui et al., 2020; Yin et al., 2020; Kumar et al., 2020) or discriminatively (Chen et al., 2016; Prabhumoye et al., 2020). As example genera154 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 154–160 July 29–31, 2021. ©2021 Association for Computational Linguistics tive approaches, Cui et al. (2020) obtain sentence and paragraph representations from BERT (Devlin et al., 2019) and then use a pointer network to decode the sentence ordering for a given paragraph, whereas Yin et al. (2019) use a graph-based neural network over sentences and en"
2021.sigdial-1.16,D14-1162,0,0.0937479,"). • Kendall’s Tau (τ ): rank-based correlation between between the predicted and correct order (Lapata, 2006). 3.3 Model Configuration We benchmark against Prabhumoye et al. (2020), using a range of text encoders, each of which is trained separately over allpairs and adjonly data. LSTM: each segment is fed into a separate biLSTM (Hochreiter and Schmidhuber, 1997) with the same architecture and shared word embeddings to obtain representations, and the segment representations are concatenated together to feed into a linear layer and softmax layer. We use 300d pre-trained GloVe word embeddings (Pennington et al., 2014) with updating, LSTM cell size of 128, and train with a mini-batch size of 128 for 10 epochs (with early stopping) and learning rate of 1e-3. BERT: predict the relative order from the “CLS” token using pre-trained BERT (Devlin et al., 2019), or alternatively ALBERT (Lan et al., 2020) (due to its specific focus on document coherence) or SciBERT (Beltagy et al., 2019) (due to the domain fit with the datasets). For BERT and ALBERT, we use the base uncased version,1 and finetune for 2 epochs in each case with a learning rate of {5e-5, 5e-6}. BERTSON (Cui et al., 2020): the current SOTA for sentenc"
2021.sigdial-1.16,2020.acl-main.248,0,0.145735,"The utility of our method is also verified over a multi-document summarisation task. 1 Introduction and Background Document coherence understanding plays an important role in natural language understanding, where a coherent document is connected by rhetorical relations, such as contrast, elaboration, narration, and justification, allowing us to communicate cooperatively in understanding one another. In this work, we measure the ability of models to capture document coherence in the strictest setting: sentence ordering (Barzilay and Lapata, 2005; Elsner et al., 2007; Barzilay and Lapata, 2008; Prabhumoye et al., 2020), a task of ordering an unordered bag of sentences from a document, aiming to maximise document coherence. The task of sentence ordering is to restore the original order for a given bag of sentences, based on the coherence of the resulting document. The ability of a model to reconstruct the original sentence order is a demonstration of its capacity to capture document coherence. Figure 1 presents such an example, where the (shuffled) sentences are from a paper abstract discussing the relationship between word informativeness and pitch prominence, and the gold-standard sentence ordering is (4,"
2021.sigdial-1.16,P19-1067,0,0.0188374,"). In this work, we experiment with j ∈ {0, 1, 2} (i.e. sentence unigrams, bigrams, and trigrams), resulting in (at most) 6(n − 2) instances for a paragraph with n sentences (noting that the segment cannot extend beyond the extremities of the document). At test time, following Prabhumoye et al. (2020), we predict the relative order of each sentence pair (only sentence unigram), then order the sentences with topological sort. We also trialled other training methods — including regressing over the distance between two sentences, and training with constraints over sentence triplets inspired from Xu et al. (2019a) in computer vision — but observed no improvement. 3 3.1 Experiments Datasets We perform experiments over six publicly available datasets from Logeswaran et al. (2018) and Xu et al. (2019b), resp.: • NeurIPS, ACL, and NSF: abstracts from NeurIPS papers, ACL papers, and NSF grants (ave. sentences = 6.2, 5.0, and 8.9, resp.). • Athlete, Artist, and Institution: paragraphs with &gt;10 sentences from Wikipedia articles of athletes, artists, and educational institutions (ave. sentences ≈ 12). 155 3.2 Evaluation Metrics Following previous work, we use 4 evaluation metrics (higher is better in each ca"
baldwin-awab-2006-open,A97-1004,0,\N,Missing
baldwin-awab-2006-open,briscoe-carroll-2002-robust,0,\N,Missing
baldwin-etal-2002-enhanced,J97-4001,0,\N,Missing
baldwin-etal-2002-enhanced,W99-0902,1,\N,Missing
baldwin-etal-2004-road,copestake-flickinger-2000-open,1,\N,Missing
baldwin-etal-2004-road,C02-2025,1,\N,Missing
baldwin-etal-2004-road,P03-1059,1,\N,Missing
bilac-etal-2004-evaluating,C02-1140,1,\N,Missing
C00-1006,W99-0902,1,\N,Missing
C00-1006,C92-4203,0,\N,Missing
C00-1006,C94-1101,0,\N,Missing
C00-1006,C94-1014,0,\N,Missing
C02-1140,baldwin-etal-2002-enhanced,1,0.673826,"ack) transliteration. 2.2 Generating and grading readings In order to generate a set of plausible readings we ﬁrst extract all dictionary entries containing kanji, and for each entry perform the following steps: 1. Segment the kanji string into minimal morphophonemic units3 and align each resulting unit with the corresponding reading. For this purpose, we modiﬁed the TF-IDF based method proposed by Baldwin and Tanaka (2000) to accept bootstrap data. 2. Perform conjugational, phonological and morphological analysis of each segment–reading pair and standardize the reading to canonical form (see Baldwin et al. (2002) for full details). In particular, we consider gemination (onbin) and sequential voicing (rendaku) as the most commonly-occurring phonological alternations in kanji compound formation (Tsujimura, 1996)4 . The canonical reading for a given seg3 A unit is not limited to one character. For example, verbs and adjectives commonly have conjugating suﬃces that are treated as part of the same segment. 4 In the previous example of happyou “announcement” the underlying reading of individual characters are hatsu and hyou respectively. When the compound is formed, hatsu seg correctly identify word bounda"
C02-1140,C00-1050,0,0.0669998,"Missing"
C02-1140,C94-1032,0,0.0431085,"Missing"
C02-1140,C96-2206,0,0.0543478,"Missing"
C02-1140,J98-4003,0,\N,Missing
C08-1073,P01-1014,0,\N,Missing
C08-1073,W99-0405,0,\N,Missing
C08-1131,W04-2105,0,\N,Missing
C08-1131,W04-2106,0,\N,Missing
C08-1131,C04-1041,0,\N,Missing
C08-1131,P08-2024,0,\N,Missing
C08-1131,P06-1036,0,\N,Missing
C08-1131,W02-1118,0,\N,Missing
C10-1065,W08-0312,0,0.0183476,"Missing"
C10-1065,W97-0703,0,0.0194392,"is found to achieve the highest correlation with human annotations. We also provide evidence that the degree of semantic similarity varies with the location of the partially-matching component words. 1 Introduction Keyphrases are noun phrases (NPs) that are representative of the main content of documents. Since they represent the key topics in documents, extracting good keyphrases benefits various natural language processing (NLP) applications such as summarization, information retrieval (IR) and question-answering (QA). Keyphrases can also be used in text summarization as semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases supplement full-text indexing and assist users in creating good queries. In the past, a large body of work on keyphrases has been carried out as an extraction task, utilizing three types of cohesion: (1) document cohesion, i.e. cohesion between documents and keyphrases (Frank et al., 1999; Witten et al., 1999; Despite recent successes in keyphrase extraction (Frank et al., 1999; Turney, 2003; Park et al., 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), current work is hampered by the inflexibility of stan"
C10-1065,S10-1004,1,0.888563,"Missing"
C10-1065,N03-1020,0,0.0479685,"etween the reference and candidate translations. The difference is that it allows for more match flexibility, including stem variation and WordNet synonymy. The basic metric is based on the number of mapped unigrams found between the two strings, the total number of unigrams in the translation, and the total number of unigrams in the reference. NIST (Martin and Przybocki, 1999) is once again similar to BLEU, but integrates a proportional difference in the co-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basi"
C10-1065,W09-0404,0,0.0450082,"Missing"
C10-1065,2001.mtsummit-papers.68,0,0.0658224,"Missing"
C10-1065,C08-2021,0,0.031443,"Missing"
C10-1065,C02-1073,0,0.0347311,"-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basic ROUGE metric, and unigrams (i.e. ROUGE-1). 3.2 R-precision In order to analyze near-misses in keyphrase extraction evaluation, Zesch and Gurevych (2009) proposed R-precision, an n-gram-based evaluation metric for keyphrase evaluation.3 R-precision contrasts with the majority of previous work on keyphrase extraction evaluation, which has used semantic similarity based on external resources 3 Zesch and Gurevych’s R-precision has nothing to do with the informat"
C10-1065,C08-1122,0,0.0279896,"Missing"
C10-1065,R09-1086,0,0.579699,"rithm. Also, computing algorithm is closer than effective grid to the same goldstandard keyphrase. From these observations, we infer that n-gram-based evaluation metrics can be applied to evaluating keyphrase extraction, but also that candidates with the same relative n-gram overlap are not necessarily equally good. Our primary goal is to test the utility of n-gram based evaluation metrics to the task of keyphrase extraction evaluation. We test the following evaluation metrics: (1) evaluation metrics from MT and multi-document summarization (BLEU, NIST, METEOR and ROUGE); and (2) R-precision (Zesch and Gurevych, 2009), an n-gram-based evaluation metric developed specifically for keyphrase extraction evaluation which has yet to be evaluated against humans at the extraction task. Secondarily, we attempt to shed light on the bigger question of whether it is feasible to expect that n-gram-based metrics without access to external resources should be able to capture subtle semantic differences in keyphrase candidates. To this end, we experimentally verify the impact of lexical overlap of different types on keyphrase similarity, and use this as the basis for proposing a variant of R-precision. In the next section"
C10-1065,W04-3252,0,\N,Missing
C10-1065,C02-1142,0,\N,Missing
C10-1065,P02-1040,0,\N,Missing
C10-2069,E09-1013,0,0.0280579,"ts above the baseline of simply selecting the highest-ranked topic word. This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres. 1 trout ﬁsh ﬂy ﬁshing water angler stream rod ﬂies salmon Introduction In the short time since its inception, topic modelling (Blei et al., 2003) has become a mainstream technique for tasks as diverse as multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). For many of these tasks, the multinomial topics learned by the topic model can be interpreted natively as probabilities, or mapped onto a pre-deﬁned discrete class set. However, for tasks where the learned topics are provided to humans as a ﬁrst-order output, e.g. for use in document collection analysis/navigation, it can be difﬁcult for the end-user to interpret the rich statistical information encoded in the topics. This research is concerned with making topics more readily human interpretable, b"
C10-2069,P10-1044,0,0.0307269,"Missing"
C10-2069,N09-1041,0,0.0366841,"a reranking model, we are able to consistently achieve results above the baseline of simply selecting the highest-ranked topic word. This is the case both when training in-domain over other labelled topics for that topic model, and cross-domain, using only labellings from independent topic models learned over document collections from different domains and genres. 1 trout ﬁsh ﬂy ﬁshing water angler stream rod ﬂies salmon Introduction In the short time since its inception, topic modelling (Blei et al., 2003) has become a mainstream technique for tasks as diverse as multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). For many of these tasks, the multinomial topics learned by the topic model can be interpreted natively as probabilities, or mapped onto a pre-deﬁned discrete class set. However, for tasks where the learned topics are provided to humans as a ﬁrst-order output, e.g. for use in document collection analysis/navigation, it can be difﬁcult for the end-user to interpret the rich statistical information encoded in the topics. This research is concerned wi"
C10-2069,N10-1012,1,0.787706,"sk is thus termed best word or most representative word selection, as we are selecting the label from the closed set of the top-N topic words in that topic. Naturally, not all topics are equally coherent, however, and the lower the topic coherence, the more difﬁcult the label selection task becomes. For example: oct sept nov aug dec july sun lite adv globe appears to conﬂate months with newspaper names, and no one of these topic words is able to capture the topic accurately. As such, our methodology presupposes an automatic means of rating topics for coherence. Fortunately, recent research by Newman et al. (2010) has shown that this is achievable at levels approaching human performance, meaning that this is not an unreasonable assumption. Labelling topics has applications across a diverse range of tasks. Our original interest in the 605 Coling 2010: Poster Volume, pages 605–613, Beijing, August 2010 problem stems from work in document collection visualisation/navigation, and the realisation that presenting users with topics natively (e.g. as represented by the top-N terms) is ineffective, and would be signiﬁcantly enhanced if we could automatically predict succinct labels for each topic. Another appli"
C10-2069,N04-1041,0,0.0140641,"arginal probability, including when training the ranking model on labelled topics from other document collections. Magatti et al. (2009) proposed a method for labelling topics induced by hierarchical topic modelling, based on ontological alignment with the Google Directory (gDir) hierarchy, and optionally expanding topics based on a thesaurus or WordNet. Preliminary experiments suggest the method has promise, but the method crucially relies on both a hierarchical topic model and a pre-existing ontology, so has limited applicability. Over the general task of labelling a learned semantic class, Pantel and Ravichandran (2004) proposed the use of lexico-semantic patterns involving each member of that class to learn a (usually hypernym) label. The proposed method was shown to perform well over the semantically homogeneous, ﬁne-grained clusters learned by CBC (Pantel and Lin, 2002), but for the coarse-grained, heterogeneous topics learned by topic modelling, it is questionable whether it would work as well. The ﬁrst works to report on human scoring of topics were Chang et al. (2009) and Newman et al. (2010). The ﬁrst study used a novel but synthetic intruder detection task where humans evaluate both topics (that had"
C10-2069,D09-1098,0,0.0135524,"e ranking of words based on these probabilities indicates the importance of a word in a topic, and it is also a feature that we use for predicting the most representative word. We also observe that sometimes the most representative words are generalized concepts of other words. As such, hypernym relations could be another feature that may be relevant to predicting the best word. To this end, we use WordNet to ﬁnd hypernym relations between pairs of words in a topic and obtain a set of boolean-valued relationships for each topic word. Our last feature is the distributional similarity scores of Pantel et al. (2009), as trained over Wikipedia.1 This takes the form of representing the distributional similarity between each pairing of terms sim(wi |wj ); if wi is not in the top-200 most similar terms for a given wj , we assume it to have a similarity of 0. While the above features can be used alone to get a ranking on the ten topic words, we can also use various combinations of features in a reranking model such as support vector regression (SVMrank : Joachims (2006)). Applying the features described above — conditional probabilities, PMI, WordNet hypernym relations, the topic model word rank, and Pantel’s"
C10-3010,P09-1030,0,0.0688511,"Missing"
C10-3010,2007.mtsummit-papers.53,0,0.0317976,"naries), it does not scale to the long tail of one-off dictionaries constructed by lexicographers using ad hoc formats, as detailed in Section 2.2. lextract is an attempt to semiautomate this process, as detailed in Section 3. Inference of new translation edges is nontrivial, because lexical ambiguity degenerates the quality of indirect translations as the paths through intermediate languages grow longer. PanDictionary is an attempt to infer a denser translation graph from PanLex combining translations from many resources based on path redundancy, evidence of ambiguity, and other information (Sammer and Soderland, 2007; Mausam et al., 2009; Mausam et al., 2010). PanLex is more than a collection, or docbase, of independent resources. Its value in translation inference depends on its ability to combine facts attested by multiple resources into a single graph, in which lemmata from multiple resources that are substantively identical are recognized as identical. The obstacles to such integration of heterogeneous lexical data are substantial. They include: (1) ad hoc formatting, including format changes between portions of a resource; (2) erratic spacing, punctuation, capitalization, and line wrapping; (3) undoc"
C10-3010,2009.mtsummit-papers.15,1,0.907082,"Missing"
C10-3010,P09-2049,0,\N,Missing
C12-1064,W03-0109,0,0.0523152,"onger or more homogeneous document sets. In contrast, social media data consists of terse noisy texts, presenting a challenge for these approaches. For instance, any reliance on named entity recognition is thwarted by the unedited nature of social media data, where spelling and capitalisation are much more ad hoc than in edited document collections. The spatial data mining community has tended to approach the task via identifying geographical references in documents (also known as geoparsing: Leidner and Lieberman (2011)). Methods range from naive gazetteer matching and rule-based approaches (Bilhaut et al., 2003), to machine learning-based methods (mainly based on named entity recognition: Qin et al. (2010); Gelernter and Mushegian (2011)). The principal drawback of these methods is that they rely on explicit mentions of addresses or formal placenames in the text, rather than words which are more informally associated with a place. In social media data, we can’t rely on a given user mentioning an address or formal placename, severely limiting the coverage of such methods.3 There has been a limited amount of work on geolocation prediction based on social network analysis (Backstrom et al., 2010), but s"
C12-1064,D10-1124,0,0.559157,"Missing"
C12-1064,P12-3005,1,0.118954,"Missing"
C12-1064,D12-1137,0,0.504242,"kstrom et al., 2010), but social networks are dynamic and the data is often hard to obtain. In terms of text-based geolocation prediction, Cheng et al. (2010) estimate the city-level user geolocation for the continental US with a simple probabilistic model, which they complement with strictly local words and smoothing. Compared with their approach, our LIW selection requires no explicit training data and is more flexible. Wing and Baldridge (2011) use KL-divergence (Kullback and Leibler, 1951) to measure the similarity between different geo-grids specified by geospatial coordinates. Recently, Roller et al. (2012) extend this idea using a KD-tree-based adaptive grid and grid centroids, achieving state-of-the-art geolocation prediction results. Li et al. (2011) investigated the prediction of Places of Interest (POIs) based on linear rank combination of content and temporal factors. Kinsella et al. (2011) compare a variety of geolocation prediction classification models at different location granularities. Adams and Janowicz (2012) utilise external geo-reference data to infer locations. Mahmud et al. (2012) combine timezone information and content-based classifiers in a hierarchical model for geolocation"
C12-1064,P11-1096,0,0.497895,"rowing popularity of social media, massive volumes of user-generated data are produced everyday, e.g. in the form of Twitter messages (tweets) and Facebook updates.1 This data provides many new opportunities and challenges for natural language processing. One such challenge is geolocation prediction: predicting the geolocation of a message or user based on their social media posts. In this paper, we focus on user-level geolocation based on the aggregated body of tweets from a user, and estimate the user’s location at the city level. As is well established in previous work (Cheng et al., 2010; Wing and Baldridge, 2011; Kinsella et al., 2011), it is reasonable to assume that user posts in social media reflect their geospatial locum, because lexical priors differ from region to region. For example, a user in London is much more likely to talk about Piccadilly and British than a user in New York or Beijing. That is not to say that those terms are uniquely associated with London, of course: British could be used by a user outside of the UK to discuss something relating to the UK. However, the use of a range of such terms with high relative frequency is strongly indicative of the fact that a user is located in"
C12-1093,P12-1056,0,0.0639088,"at it fails to capture the fact that multiple terms may be involved with the same event (Zanzotto et al., 2011), and requires that at least one term undergoes a sufficiently high jump in relative frequency that the event can be identified. Topic models have been proposed as a means of better capturing events, by way of learning clusters of terms that are associated with a given event, as well as modelling changes in term co-occurrence rather than just term frequency. Most work based on topic modelling has been in the form of retrospective event detection models, however (Kireyev et al., 2009; Diao et al., 2012). Moving to the more general area of the machine learning, several online topic models have been proposed (Hoffman et al., 2010; AlSumait et al., 2008). Hoffman et al. (2010) introduced an online LDA variant that uses variational Bayes as the approximate posterior inference algorithm. The model that is closest in spirit to what we propose is On-Line LDA (OLDA) (AlSumait et al., 2008). Using collapsed Gibbs sampling for approximate inference, OLDA processes documents in an on-line fashion by resampling topic assignments for new documents using parameters from a previously learnt model. We retur"
C12-1093,D11-1024,0,0.0327199,"Missing"
C12-1093,N10-1012,1,0.0594681,"Missing"
C12-1093,N10-1021,0,0.0855016,"Missing"
C12-1093,D12-1134,0,0.0399574,"wish to detect events happening presently in 1 http://webtrends.about.com/od/twitter/a/why_twitter_uses_for_twitter.htm 1520 our time, however, we require on-line event detection models. An example application where real-time responsiveness is critical is earthquake detection (Sakaki et al., 2010), and trend analysis also clearly requires on-line processing in order to be of use (Mathioudakis and Koudas, 2010). Most on-line approaches, however, use a relatively simple keyword-based methodology over a pre-defined set of keywords (Culotta, 2010; Lampos and Cristianini, 2010; Weng and Lee, 2011; Zhao et al., 2012) rather than tackling the more challenging task of open-world event detection. Real-time first story detection (Petrovi´c et al., 2010; Osborne et al., 2012) is the task of detecting the mentions of a breaking story as close as possible in time to its first mention. Here, the system should ideally pick up on the breaking story within seconds or minutes of its first mention in order to have impact, e.g. as an alert system for a newswire agency or intelligence organisation. As such, the methods that are standardly applied to the task tend to be based on analysis of local “burstiness” in the data"
C12-1093,D11-1061,0,\N,Missing
C12-1127,E99-1015,0,\N,Missing
C12-1127,S10-1004,1,\N,Missing
C12-1127,S10-1041,0,\N,Missing
C12-1127,S10-1030,0,\N,Missing
C12-1167,P08-1081,0,0.0243099,"arsing as a joint link and dialogue act classification task, by using CRFSGD (Bottou, 2011) and MaltParser (Nivre et al., 2007). They also demonstrated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al.,"
C12-1167,D10-1084,1,0.902682,"inux Information Access by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological order"
C12-1167,W10-2923,1,0.877252,"inux Information Access by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological order"
C12-1167,U08-1009,0,0.0882953,"Missing"
C12-1167,C04-1128,0,0.0258603,"as Kim et al. (2010b), but different parsing approaches. Specifically, Wang et al. (2011) approached thread discourse structure parsing as a joint link and dialogue act classification task, by using CRFSGD (Bottou, 2011) and MaltParser (Nivre et al., 2007). They also demonstrated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level"
C12-1167,C10-2133,0,0.0271101,"g et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). Moreover Wang and Rosé (2010) demonstrated that initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) from online forums have the potential to enhance thread summarisation and automatically generate knowledge bases for Community Question Answering (cQA) services such as Yahoo! Answers. Furthermore, Kim et al. (2006) showed that dialogue acts can be used to classify student online discussions in web-enhanced courses. Specifically, they use dialogue acts to identify discussion threads that may have"
C12-1167,D11-1002,1,0.802154,"cess by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological ordering. In the annotated"
C12-1167,N10-1097,0,0.030598,"trated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). Moreover Wang and Rosé (2010) demonstrated that initiation–response pairs (e.g."
C12-1167,C00-2137,0,0.0123633,"xperiments over the full 250-thread ILIAD dataset. In both cases, various combinations of the features introduced in Section 4 are used. To generate these features, both the gold-standard LinkDAs and the automatically predicted ones are used. All our Solvedness classification experiments were carried out based on stratified 10-fold crossvalidation. The results are evaluated using classification accuracy (AC C). As our baselines, we use a majority classifier (ZeroR), as well as the best Solvedness classifier provided by Baldwin et al. (2007) (ADCS). As mentioned earlier, randomised estimation (Yeh, 2000) (at a significance level of p < 0.05) is used throughout the paper for statistical significance testing. 2746 Feature Category Baseline DA-only LinkDA-based System/feature(s) ZeroR ADCS LastPostDA LastNonInitDA HasResolution LastPostDA +LastNonInitDA LastPostDA +HasResolution LastNonInitDA +HasResolution AllDAFeat LastPairDA LastSubthreadDA AllLinkDAFeat AllDAFeat +AllLinkDAFeat AC C g old AC Caut o .779 .788 .833∗ .775 .766 .792 .779 .779 .834∗ .779 .883∗ .775 .874∗ .792 .883∗ .779 .851∗ .792 .833∗ .779 .833∗ .792 .865∗ .792 Table 3: Results over ILIAD222 , using discourse structure features"
C12-1167,N10-1142,0,\N,Missing
C14-1154,E09-1013,0,0.0784519,"Missing"
C14-1154,P13-1141,0,0.0154691,"pproaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but identifies the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further extensions"
C14-1154,Y11-1028,1,0.838437,"about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches"
C14-1154,cook-stevenson-2010-automatically,1,0.937154,"d in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not"
C14-1154,W11-2508,0,0.203893,"senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic"
C14-1154,S13-2049,0,0.0594711,"Missing"
C14-1154,H05-1053,1,0.75209,"nexpensively constructed in the future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports 1632 Method NoveltyDiff NoveltyLLR NoveltyRatio RelevanceAuto RelevanceManual Rank SumDiff,auto Rank SumDiff,manual Upper-bound Baseline F-score BNC–ukWaC SiBol/Port 0.57 0.29 0.67 0.28 0.66 0.28 0.48 0.24 0.45 0.27 0.72 0.30 0.72 0.29 0.72 0.42 0.36 0.20 Table 2: Token-level F-score for the BNC–ukWaC and SiBol/Port datasets using variants of Novelty, Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown. and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed very high NoveltyRatio for many distractors (selected in a similar way to our other experiments). Unlike the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to cooccur with very different words in the corpora, and NoveltyRatio will consequently be high. To address vocabulary differences between corpora, in their experiments on identifying lexical semantic differences between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word to those with mod"
C14-1154,S13-2051,1,0.89377,"Missing"
C14-1154,S13-2039,1,0.871209,"Missing"
C14-1154,E12-1060,1,0.877532,"s for identifying new word-senses could benefit applied NLP by helping to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in addition to new words themselves; methods which identify new word-senses could therefore also help to keep dictionaries current. In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses. Specifically, we consider the identification of word-senses that are not attested in a reference corpus, taken to represent standard usage, but that are attested in a focus corpus of newer texts. Lau et al. (2012) introduced the task of novel sense identification. They presented a method for identifying novel word-senses — described here in Section 4 — and evaluated this method on a very small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al. (2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new wordsenses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
C14-1154,J07-4005,1,0.808136,"d senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but ide"
C14-1154,S13-2035,0,0.0486422,"Missing"
C14-1154,P11-2053,0,0.252002,"nge in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains."
C14-1154,W09-0214,0,0.490803,"arger than has been used in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergon"
C14-1154,W11-1102,0,0.0956413,"Missing"
C16-1046,W02-2001,1,0.607985,"ining (but holding out the surprise language), we find that we are able to “smooth” annotation differences between languages. 2 Related Work There is a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for ide"
C16-1046,U09-1006,1,0.744627,"a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaus"
C16-1046,C14-1071,0,0.019567,"identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulaic language”, i.e. idiomatic expressions with statistically-marked properties in a given corpus — blurred in the sense that many MWEs are not statistically marked, and also that they include formulaic expressions such as in this paper that are not formally MWEs. Also related is recent work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015)."
C16-1046,D14-1082,0,0.00863418,"igher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr e"
C16-1046,P12-1022,0,0.0203088,"ata, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al.,"
C16-1046,P11-1061,0,0.0194027,"ties across token instances. However, our model also predicts that hNOUN, ccomp, ADJi is an MWE. 6.2 Experiment II: Learning without gold standard dependency relations In our second experiment, we evaluate under the more realistic task setting of there being no gold standard treebank in the target language. Instead, we use the cross-lingual parser proposed by Duong et al. (2015) to parse the corpus in the target language (see Section 2). Note that we still use gold-standard POS tags, but this isn’t entirely unrealistic given the relative maturity of methods for inducing universal POS taggers (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Duong et al., 2014). Obviously, due to the fact that the parser has no access to dependency annotations in the target language, the parser output will be noisy. However, this emulates a true surprise language setup, where we 476 Language Baseline Reranker PMI Baseline + gold De 0.816 0.736 0.804 0.797 Sv 0.511 0.514 0.512 0.750 Da 0.481 0.428 0.567 0.846 It 0.637 0.631 0.803 0.885 Es 0.546 0.610 0.494 0.879 Fr 0.656 0.684 0.634 0.799 Hr 0.487 0.461 0.575 0.696 Bg 0.554 0.645 0.471 0.917 Hu 0.406 0.470 0.588 0.457 Fa 0.467 0.549 0.521 0.919 Ga 0.586 0.673 0.687 0.799"
C16-1046,W09-2903,0,0.0234383,"rases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blur"
C16-1046,D14-1096,1,0.933826,"s-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr es it da nl sv de en 0.64 0.56 0.48 0.40 0.32 0.24 0.16 0.00 0.56 0.48 0.40 0.32 0.24 0.16 0.08 en de sv nl da it es fr bg hr fi fa eu id he en de sv nl da no it es fr hr et fi hu fa hi ga eu id 0.08 0.64 (a) compound 0.00 (b) mwe Figure 1: Cross-lingual similarity of MWE pat"
C16-1046,K15-1012,1,0.869192,"e able to inventorise the MWE types in the language (Oard, 2003; Maynard et al., 2003)? Here, there is little expectation of success without an automatic method for determining the inventory and relative frequency of MWEs in a given language. This provides the motivation for this paper: can we develop a method for automatically profiling the MWE inventory of a novel language based simply on a monolingual corpus of that language, and a treebank in a second language such as English? We carry out this research in the Universal Dependency (“UD”) framework (Nivre et al., 2016), using the method of Duong et al. (2015) to induce a delexicalised dependency parser for the surprise language, based on a supervised parsing model for a language such as English where we have a well-developed treebank in the UD. Given the parser output over a monolingual corpus in the surprise language, we then apply one of two methods to extract our MWE profile: (1) a baseline method, where we simply 471 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 471–481, Osaka, Japan, December 11-17 2016. extract out delexicalised dependency tuples of relation type mwe or co"
C16-1046,N13-1014,0,0.0207183,"rpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr es it da nl sv de en 0.64 0.56 0.48 0.40 0.32 0.24 0.16 0.00 0.56 0.48 0.40 0.32 0.24 0.16 0.08 en de sv nl da it es fr bg hr fi fa eu id he en de sv nl da no it es fr hr et fi hu fa hi ga eu id 0.08 0.64 (a) compound 0.00 (b) mwe Figure 1: Cross-lingual"
C16-1046,W03-1502,0,0.114981,"Missing"
C16-1046,E03-1073,0,0.065874,"g the supervised reranking method, however, and incorporating more and more languages for training (but holding out the surprise language), we find that we are able to “smooth” annotation differences between languages. 2 Related Work There is a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the type"
C16-1046,C10-2078,0,0.0279912,"004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on"
C16-1046,W97-0311,0,0.272816,"articular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Close"
C16-1046,W06-2405,0,0.0769814,"Missing"
C16-1046,P12-1066,0,0.0214297,"exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulaic language”, i.e. idiomatic expressions with statistically-marked properties in a given corpus — blurred in the sense that many MWEs are not statistically marked, and also that they include formulaic expressions such as in this paper that are not formally MWEs. Also related is recent work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed metho"
C16-1046,C12-1127,1,0.805624,"ained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al., 2005). Note here that the combination of highly-productive"
C16-1046,petrov-etal-2012-universal,0,0.313646,"nt work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on obse"
C16-1046,W12-3311,0,0.0251613,"of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annota"
C16-1046,Q14-1016,0,0.0125712,", or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulai"
C16-1046,S13-1038,0,0.0601992,"Missing"
C16-1046,W04-2118,0,0.0183763,"e proportion of MWE tokens 3 We discarded Hindi despite the high proportion of MWEs because: (1) it only covered compound relations, and has no mwe relations, and (2) it has a low number of distinct MWE patterns (23), and as such appeared skewed in its annotation. 2 473 are: NOUN–NOUN; PROPN–PROPN (i.e. proper noun dependencies, which should be annotated with the name relation rather than compound, according to the annotation guidelines); and VERB– NOUN, which includes LVCs. There are also other noticeable patterns such as VERB–ADV(erb) and VERB–ADP(osition), corresponding to VPCs (Schulte im Walde, 2004; Baldwin, 2005). mwe patterns are more diverse than compound patterns: compound patterns mostly involve nouns and verbs, while mwe patterns involve a diverse range of POS types, such as ADP–ADP or ADV–ADV, and pairings including CONJ(unctions) or SCONJ (subordinating conjunctions). We additionally measured the similarity between the MWE pattern probability distribution of the different languages using Jensen–Shannon divergence, as shown in Figure 1 for all languages in UD. To make comparison between related languages easier, we clustered the languages by language family. According to Figure 1"
C16-1046,N13-1126,0,0.0328538,"Missing"
C16-1046,W03-1803,1,0.611439,"2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al., 2005). Note here that the combination of highly-productive MWE types can vary greatly across languages: English is rich with compound nouns and LVCs are also common, but lacks compound verbs; Persian is rich with LVCs and adjective–noun compounds, but has very few compound nouns and compound verbs; and Japanese is rich with LVCs and compound nouns and verbs, but adjective–noun MWEs"
C16-1046,W06-1204,0,0.0390327,"E patterns, certainly no annotated MWE data, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages s"
C16-1046,L16-1262,0,\N,Missing
C16-1091,N13-1016,0,0.089093,"el, in particular, the topics themselves, which provide the primary insights into the document collection. The de facto topic representation has been a simple term list, in the form of the top-10 terms in a given topic, ranked in descending order of Pr(wj |ti ). The cognitive overhead in interpreting the topic presented as a list of terms can be high, and has led to interest in the task of generating labels for topics, e.g. in the form of textual descriptions (Mei et al., 2007; Lau et al., 2011; Kou et al., 2015), visual representations of the topic words (Smith et al., to appear), or images (Aletras and Stevenson, 2013). In the former case, for example, rather than the top-10 terms of hschool, student, university, college, teacher, class, education, learn, high, program i, a possible textual label could be simply EDUCATION. Recent work has shown that, in the context of a timed information retrieval (IR) task, automatically-generated textual labels are easier for humans to interpret than the top-10 terms, and lead to equivalent-quality relevance judgements (Aletras et al., 2014). Despite this, the accuracy of state-of-the-art topic generation methods is far from perfect, providing the motivation for this work"
C16-1091,D08-1038,0,0.048096,"of interpreting these topics for end-users, we propose labelling a topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet al"
C16-1091,P03-1054,0,0.0052759,"5 In addition to doc2vec, we also experiment with word2vec to generate embeddings for Wikipedia titles. By treating titles as a single token (e.g. concatenating financial crisis into financial crisis) and greedily tokenising the text of all of the Wikipedia articles, we can then generate word embeddings for the titles. For word2vec, we use the skip-gram implementation exclusively.6 For both doc2vec and word2vec, we first pre-process English Wikipedia,7 using Wiki Extractor to clean and extract Wikipedia articles from the original dump.8 We then tokenise words with the Stanford CoreNLP Parser (Klein and Manning, 2003), and lowercase all words. We additionally filter out articles where the article body is made up of less than 40 words, and also disambiguation pages. We also remove titles whose length is longer than 4 words, as they are often too specific or inappropriate as topic labels (e.g. List of Presidents of the United States). For word2vec, we remove any parenthesised sub-component of an article title — e.g. in the case of Democratic Party (United States), we remove (United States) to generate Democratic Party — as we would not expect to find verbatim usages of the full title. This has the potential"
C16-1091,W16-1609,1,0.806617,"of which is no longer publicly available), limiting the general-purpose utility of the method. We propose an alternative approach: precomputing distributed representations of the topic terms and article titles using word2vec and doc2vec. To this end, we train a doc2vec model on the English Wikipedia articles, and represent the embedding of a Wikipedia title by its document embedding. As doc2vec runs word2vec internally, word embeddings are also learnt during the training. Given the top-N topic terms, the topic embedding is represented by these terms’ word embeddings. Based on the findings of Lau and Baldwin (2016) that the simpler dbow has less parameters, trains faster, and performs better than dmpv in several extrinsic tasks, we experiment only with dbow.4 In terms of hyper-parameter settings, we follow the recommendations of Lau and Baldwin (2016).5 In addition to doc2vec, we also experiment with word2vec to generate embeddings for Wikipedia titles. By treating titles as a single token (e.g. concatenating financial crisis into financial crisis) and greedily tokenising the text of all of the Wikipedia articles, we can then generate word embeddings for the titles. For word2vec, we use the skip-gram im"
C16-1091,P11-1154,1,0.376587,"to a human user, a fundamental concern is the best way of presenting the rich information generated by the topic model, in particular, the topics themselves, which provide the primary insights into the document collection. The de facto topic representation has been a simple term list, in the form of the top-10 terms in a given topic, ranked in descending order of Pr(wj |ti ). The cognitive overhead in interpreting the topic presented as a list of terms can be high, and has led to interest in the task of generating labels for topics, e.g. in the form of textual descriptions (Mei et al., 2007; Lau et al., 2011; Kou et al., 2015), visual representations of the topic words (Smith et al., to appear), or images (Aletras and Stevenson, 2013). In the former case, for example, rather than the top-10 terms of hschool, student, university, college, teacher, class, education, learn, high, program i, a possible textual label could be simply EDUCATION. Recent work has shown that, in the context of a timed information retrieval (IR) task, automatically-generated textual labels are easier for humans to interpret than the top-10 terms, and lead to equivalent-quality relevance judgements (Aletras et al., 2014). De"
C16-1091,N10-1012,1,0.906287,"topic with a succinct phrase that summarises its theme or idea. Using Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet allocation (LDA: Blei et al. (2003)) or extensions thereof, but more re"
C16-1091,P16-1110,0,0.0650891,"Wikipedia document titles as label candidates, we compute neural embeddings for documents and words to select the most relevant labels for topics. Compared to a state-of-the-art topic labelling system, our methodology is simpler, more efficient, and finds better topic labels. 1 Introduction Topic models are a popular approach to detecting trends and traits in document collections, e.g. in tracing the evolution of a scientific field through its publications (Hall et al., 2008), enabling visual navigation over search results (Newman et al., 2010a), interactively labelling document collections (Poursabzi-Sangdeh et al., 2016), or detecting trends in text streams (Wang and McCallum, 2006; AlSumait et al., 2008). They are typically unsupervised, and generate “topics” ti in the form of multinominal distributions over the terms wj of the document collection (Pr(wj |ti )), and topic distributions for each document dk in the collection, in the form of a multinomial distribution over topics (Pr(ti |dk )). Traditionally, this has been carried out based on latent Dirichlet allocation (LDA: Blei et al. (2003)) or extensions thereof, but more recently there has been interest in deep learning approaches to topic modelling (Ca"
C16-1091,P11-1039,0,\N,Missing
C16-1294,P16-2013,0,0.0124843,"of statistical significance of a difference in dependent correlations with human assessment for competing out-of-English human-targeted metrics; a green cell denotes a significant increase in correlation with human assessment for the metric in a given row over the metric in a given column at p &lt; 0.05. the decision to include fluency should be weighed against the degree to which reference bias is actually present in the adequacy evaluation. If there is strong reference bias, for example, it would be highly advisable to include both fluency and adequacy but for a smaller number of translations. Fomicheva and Specia (2016) investigate bias in monolingual evaluation of MT and conclude reference bias to be a serious issue, with human annotators strongly biased by the reference translation provided. Their conclusion was based on estimation of confidence intervals for Kappa coefficients by means of an unconventional resampling technique, where samples used to estimate confidence intervals were smaller than the original sample size, drawn without replacement, and averaged; this diverges from standard methods of confidence interval estimation, such as bootstrap resampling. As a result, the reported confidence limits"
C16-1294,W13-2305,1,0.859772,"H−BLEU H.TER H−CDER H.CDER H−BLEU H.BLEU H−PER H.WER H.CDER H.TER H.BLEU H.PER CS-EN Figure 1: Significance test results (Williams test) of statistical significance of a difference in dependent correlations with human assessment for competing to-English human-targeted metrics; a green cell denotes a significant increase in correlation with human assessment for the metric in a given row over the metric in a given column at p &lt; 0.05. reference translation. Although certainly not to the same extreme, human assessment of MT that employs a reference translation could incur similar reference bias. Graham et al. (2013) provide discussion on how reference bias could exist for direct assessment of translation adequacy, and, as a solution, propose inclusion of a separate reference-free fluency assessment, to provide a component of the evaluation that cannot be biased in any way by a reference translation. However, it is inevitably the case that resources available to conduct human evaluation are limited, and therefore a trade-off exists between adding fluency assessments of translations and numbers of translations we can feasibly include. For example, in our earlier human evaluation of HTER, it would have been"
C16-1294,N15-1124,1,0.925892,"Translation Adequacy Overall in this work, human assessment was carried out for two distinct data sets, firstly a nine language pair data set to re-evaluate HTER and subsequently an English to Spanish data set (see Section 4.2). Below we describe human assessment of the nine language pair data set. Human direct assessment (DA) of the adequacy of translations was collected by means of a 100-point continuous rating scale via crowd-sourcing. Large numbers of repeat human judgments for translations were collected on Amazon Mechanical Turk, by way of re-implementation and minor adaptation of 3126 Graham et al. (2015).2 Translations were sampled at random from all systems competing in WMT-13 translation task for Czech-to-English (CS-EN), German-to-English (DE-EN), Spanish-to-English (ESEN), French-to-English (FR-EN), Russian-to-English (RU-EN), English-to-German (EN-DE), Englishto-Spanish (EN-ES), English-to-French (EN-FR) and English-to-Russian (EN-RU). Human assessors rated the adequacy of translations in a monolingual human evaluation by comparing the meaning of the original generic human reference translation (rendered in gray) with a given MT output translation (rendered in black) by stating the degre"
C16-1294,P15-1174,1,0.940182,"gth of correlation with human assessment. For example, when the most widely applied human-targeted metric, human-targeted translation edit rate (HTER) was first proposed, Snover et al. (2006) reported that “HTER is more highly correlated to human judgments than BLEU or HBLEU” (p. 230), and hence, since 2006, it is HTER, as opposed to HBLEU, that is employed in MT as a human assessment substitute. 2 Relevant Work HTER is generally regarded as a valid substitute for human assessment. In MT QE, for example, HTER scores are used to evaluate systems in large-scale shared tasks (Bojar et al., 2015; Graham, 2015). If a metric such as HTER is to be relied upon as a substitute for human assessment, it is important that trust in the metric is well-placed to avoid inaccuracies in empirical evaluations. For instance, Bojar et al. (2013) and Graham (2015) make the assumption that HTER provides a valid representation of translation quality and subsequently employ HTER scores as a gold standard representation when evaluating QE systems, ultimately leading to rankings for competing systems. If HTER scores do not in fact provide a valid representation of translation quality, however, system rankings are likely"
C16-1294,P02-1040,0,0.115521,"in the target language of the MT output in question. All human post-editors were unaware of the purpose of the post-editing work and experiments. Annotators were shown the reference and MT output with post-editing instructions as follows:4 ∗∗ Making as few changes as possible ∗∗ , correct the hypothesis segment to make it (a) have the same meaning as the reference segment; (b) grammatically correct. 3.3 HTER Re-evaluation Results Table 2 shows correlations achieved by a range of human-targeted metric formulations with human assessment, including human-targeted versions of segment-level BLEU (Papineni et al., 2002; Koehn et al., 2007), TER (Snover et al., 2006), WER, PER and CDER (Leusch and Ney, 2008). Correlations with human assessment achieved by HTER are, for three of the nine language pairs we evaluate, below a Pearson correlation of 0.6. In addition, comparison of correlations achieved by HTER and HBLEU reveal a higher correlation achieved by HBLEU for five of the nine language pairs we evaluate. As recommended by Graham et al. (2015), we test for significance of difference in dependent correlations using Williams test, as shown in Figures 1 and 2. For two language pairs, DE-EN and FR-EN, correla"
C16-1294,2006.amta-papers.25,0,0.757095,"terparts, the scores they produce are nonetheless still partly automatic. Given the vast number of possible methods of comparing a given MT output with its (human-targeted) reference translation, it is necessary to provide evidence that any given choice of human-targeted metric provides the best formulaic substitute for human assessment. As with fully automatic metrics, human-targeted metrics are themselves evaluated by strength of correlation with human assessment. For example, when the most widely applied human-targeted metric, human-targeted translation edit rate (HTER) was first proposed, Snover et al. (2006) reported that “HTER is more highly correlated to human judgments than BLEU or HBLEU” (p. 230), and hence, since 2006, it is HTER, as opposed to HBLEU, that is employed in MT as a human assessment substitute. 2 Relevant Work HTER is generally regarded as a valid substitute for human assessment. In MT QE, for example, HTER scores are used to evaluate systems in large-scale shared tasks (Bojar et al., 2015; Graham, 2015). If a metric such as HTER is to be relied upon as a substitute for human assessment, it is important that trust in the metric is well-placed to avoid inaccuracies in empirical e"
C16-1294,P07-2045,0,\N,Missing
C18-1085,baccianella-etal-2010-sentiwordnet,0,0.021975,"ethod applies to word vectors directly via the word embedding layer of the neural network in the context of training a sentiment analyzer, rather than over pre-trained word vectors without explicit task-based training. Our method uses a feedforward neural network to encode sentiment information, as distinct from prior work, which has used cosine similarity or Euclidean distance in the objective function to model word embedding (dis)similarity. 3 Methods Sentiment lexicons are considered to be a critical component of sentiment analysis. Such external knowledge resources — such as SentiWordNet (Baccianella et al., 2010) and the extended version of Affective Norms of English Words (E-ANEW: Warriner et al. (2013)) — are often used to provide more accurate information about the polarity of a word. Encoding sentiment knowledge into word vectors has been proven to be an effective way to enhance the performance of sentiment analysis. 3.1 Encoding Method A high-level overview of methods for encoding external knowledge into word vectors for CNN classifiers is presented in Figure 1. Word vectors are used to initialize word embeddings in a CNN classifier. The parameters of the CNN and components for fine-tuning the wo"
C18-1085,P98-1013,0,0.4111,"tp:// 997 Proceedings of the 27th International Conference on Computational Linguistics, pages 997–1007 Santa Fe, New Mexico, USA, August 20-26, 2018. 2 2.1 Related Work Encoding External Knowledge into Word Vectors Existing approaches to leveraging external knowledge for word embedding learning for natural language processing fall into two categories: (1) encoding external knowledge during the word vector learning stage; and (2) encoding external knowledge into pre-trained word vectors. Both styles of approach make use of similar linguistic resources such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998), the Paraphrase Database (“PPDB”: Ganitkevitch et al. (2013)), or BabelNet (Navigli and Ponzetto, 2012). Methods in the first category usually change the objective function of the language model or add regularization terms into the original objective function. Yu and Dredze (2014) combine CBOW (Mikolov et al., 2013) with word relations extracted from WordNet and PPDB. Xu et al. (2014) regard relational knowledge and categorical knowledge as learning regularizers, and combine them with the skip-gram objective function. Bian et al. (2014) also combine the objective function of CBOW with externa"
C18-1085,S17-2094,0,0.0735761,"rchive/p/word2vec/ 6 https://github.com/nmrksic/attract-repel 1003 SemEval2016 SemEval2017 MR SST-2 word2vec (Mikolov et al., 2013) attract-repel-pos-neg (Mrkˇsi´c et al., 2017) attract-repel-ant-syn (Mrkˇsi´c et al., 2017) PARAGRAM (Wieting et al., 2015) counter-fitting (Mrkˇsi´c et al., 2016) 0.608 0.608 0.608 0.599 0.603 0.647 0.641 0.626 0.632 0.627 0.792 0.792 0.785 0.781 0.780 0.837 0.836 0.817 0.839 0.838 DURING+SentiNet AFTER+SentiNet 0.601 0.613 0.641 0.648 0.790 0.796 0.842 0.844 SwissCheese (Deriu et al., 2016) SENSEI-LIF (Rouvier and Favre, 2016) UNIMELB (Xu et al., 2016) BB twtr (Cliche, 2017) CNN-non-static (Kim, 2014) Re(word2vec) (Yu et al., 2017) 0.633 0.630 0.617 — — — — — — 0.685 0.685 — — — — — — — — — — — — 0.879 Table 3: Experiment results of word vectors on the four Sentiment Classification Datasets. The measurement of the second and third columns is Macro F1. The measurement of the fourth and fifth columns is Accuracy. • PARAGRAM: The word vectors of PARAGRAM, generated by encoding PPDB into word2vec word embeddings, based on the pre-trained PARAGRAM word embedding set.7 • counter-fitting: The word vectors of counter-fitting, generated by encoding an antonym/synonym lexi"
C18-1085,J81-4005,0,0.740656,"Missing"
C18-1085,S16-1173,0,0.041044,"Missing"
C18-1085,C14-1008,0,0.0406011,"a probabilistic document model and a sentiment component — to jointly learn word vectors. The probabilistic document model does not require labelled data. The sentiment component uses document-level sentiment annotations to constrain words expressing similar sentiment to have similar representations. Tang et al. (2014) changed the objective function of the C&W (Collobert et al., 2011) model to produce sentiment-specific word vectors for Twitter sentiment analysis, by leveraging large volumes of distant-supervised tweets. In order to capture morphological and shape information from words, dos Santos and Gatti (2014) concatenate character-level embeddings and word-level embeddings to form 998 a combined word representation for sentiment analysis. Severyn and Moschitti (2015) refine pre-trained word vectors through a CNN based on distant-supervised data. Zhou et al. (2016) introduced three kinds of word vectors as features into their sentiment classifier. One is general purpose, and the the other two are trained by leveraging sentiment information. However, their feature selection experiments indicate that the task-specific trained word vectors do not improve the performance of their system substantially."
C18-1085,N15-1184,0,0.109311,"Missing"
C18-1085,N13-1092,0,0.0299771,"Missing"
C18-1085,J15-4004,0,0.0226709,"ctors (as in most scenarios, it is undesirable for antonyms to be closely related) (Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). In order to solve this problem, antonym lexicons have been used to fine-tune pre-trained word vectors. Mrkˇsi´c et al. (2016) present a method called counter-fitting to inject antonymy and synonymy constraints into word vectors trained with GloVe (Pennington et al., 2014) and PARAGRAM (Wieting et al., 2015). The adapted word vectors trained with PARAGRAM achieve the second-highest SimLex-999 (Hill et al., 2015) score. Mrkˇsi´c et al. (2017) extend this previous work using negative sampling, to force synonym pairs to be closer to each other than to their negative examples, and forcing antonyms pairs to be further away from each other than from their negative examples. Encoding external knowledge into word vectors has shown to be effective for improving pre-trained word vectors for intrinsic evaluation such as WordSim-353 (Finkelstein et al., 2002) and SimLex-999. It has also shown to be effective for improving extrinsic tasks such as dialogue state tracking (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 20"
C18-1085,D15-1242,0,0.0150813,"han to their negative examples, and forcing antonyms pairs to be further away from each other than from their negative examples. Encoding external knowledge into word vectors has shown to be effective for improving pre-trained word vectors for intrinsic evaluation such as WordSim-353 (Finkelstein et al., 2002) and SimLex-999. It has also shown to be effective for improving extrinsic tasks such as dialogue state tracking (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017), sentiment analysis (Faruqui et al., 2015; Wieting et al., 2015; Yu et al., 2017), document classification (Kiela et al., 2015), and word sense disambiguation (Rothe and Sch¨utze, 2015). The above two kinds of methods both encode external knowledge into word vector space before applying word vectors in downstream tasks. Our method encodes a sentiment lexicon into word vectors when fine-tuning the word vectors in a downstream task. 2.2 Adapting Word Vectors for Sentiment Analysis There are many methods of adapting word vectors for sentiment analysis. Maas et al. (2011) combine two components — a probabilistic document model and a sentiment component — to jointly learn word vectors. The probabilistic document model does"
C18-1085,D14-1181,0,0.254944,"racy of sentiment analysis compared to a number of benchmark methods. 1 Introduction Sentiment analysis plays an important role in many real-world applications. The objective of sentiment classification is to classify a sentence, message or document according to sentiment, often in the form of ordinal regression (e.g. positive vs. neutral vs. negative). In recent years, deep neural networks, such as convolutional neural networks (“CNNs”), have been widely used for sentiment classification. A simple CNN trained over pre-trained word vectors has been shown to achieve highly competitive results (Kim, 2014). Learning task-specific vectors through fine-tuning may offer further gains in performance, and this is the primary focus of this paper. Separately, there has been recent work on methods for learning word embeddings based not just on textual contexts, but also external knowledge bases (Wieting et al., 2015; Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017). This has also been applied to sentiment classification (Rouvier and Favre, 2016; Yu et al., 2017), with empirical results indicating that expli"
C18-1085,P14-2050,0,0.0299972,"gs. Faruqui et al. (2015) use synonym relations extracted from WordNet and other resources to construct an undirected graph. They then retrofit the undirected graph to pre-trained word vectors to obtain new word vectors, under the constraint that the resulting vectors should be close to the vectors of their neighbours in the semantic graph. Antonyms are generally close in vector space, presenting a problem when learning general-purpose word vectors (as in most scenarios, it is undesirable for antonyms to be closely related) (Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). In order to solve this problem, antonym lexicons have been used to fine-tune pre-trained word vectors. Mrkˇsi´c et al. (2016) present a method called counter-fitting to inject antonymy and synonymy constraints into word vectors trained with GloVe (Pennington et al., 2014) and PARAGRAM (Wieting et al., 2015). The adapted word vectors trained with PARAGRAM achieve the second-highest SimLex-999 (Hill et al., 2015) score. Mrkˇsi´c et al. (2017) extend this previous work using negative sampling, to force synonym pairs to be closer to each other than to their negative exa"
C18-1085,P11-1015,0,0.0646003,"al., 2017; Vuli´c et al., 2017), sentiment analysis (Faruqui et al., 2015; Wieting et al., 2015; Yu et al., 2017), document classification (Kiela et al., 2015), and word sense disambiguation (Rothe and Sch¨utze, 2015). The above two kinds of methods both encode external knowledge into word vector space before applying word vectors in downstream tasks. Our method encodes a sentiment lexicon into word vectors when fine-tuning the word vectors in a downstream task. 2.2 Adapting Word Vectors for Sentiment Analysis There are many methods of adapting word vectors for sentiment analysis. Maas et al. (2011) combine two components — a probabilistic document model and a sentiment component — to jointly learn word vectors. The probabilistic document model does not require labelled data. The sentiment component uses document-level sentiment annotations to constrain words expressing similar sentiment to have similar representations. Tang et al. (2014) changed the objective function of the C&W (Collobert et al., 2011) model to produce sentiment-specific word vectors for Twitter sentiment analysis, by leveraging large volumes of distant-supervised tweets. In order to capture morphological and shape inf"
C18-1085,N16-1018,0,0.0290492,"Missing"
C18-1085,Q17-1022,0,0.0224321,"Missing"
C18-1085,S16-1001,0,0.018006,"ere θ 1 , θ 2 , b1 and b2 are trainable parameters, and σ is the sigmoid function. The reason for using a feedforward neural network is that it has the same structure as standard word embedding training models such as CBOW, skip-gram and C&W (see Section 2.1).2 4 Experiments 4.1 Experimental Setup Experiments are conducted over four popular, publicly-available sentence-level sentiment classification datasets: • SemEval2016: The dataset of the SemEval-2016 Message Polarity Classification task. The goal is to classify a given Twitter message according to positive, negative or neutral sentiment (Nakov et al., 2016). • SemEval2017: The dataset of the SemEval-2017 Message Polarity Classification task. The task formulation is exactly the same as SemEval2016 and the same training data is used, with new test data (Rosenthal et al., 2017). 2 When training SentiNet with a mini-batch sentences, the words of the mini-batch are classified into positive, neutral and negative according to their sentiment score in the sentiment lexicon. It takes two uniform sample steps to sample a word. First, the word kind (positive, neutral and negative) is sampled uniformly. Then a word is sampled uniformly from the words of the"
C18-1085,P05-1015,0,0.289759,"Missing"
C18-1085,D14-1162,0,0.0922094,"use synonym relations extracted from WordNet and other resources to construct an undirected graph. They then retrofit the undirected graph to pre-trained word vectors to obtain new word vectors, under the constraint that the resulting vectors should be close to the vectors of their neighbours in the semantic graph. Antonyms are generally close in vector space, presenting a problem when learning general-purpose word vectors (as in most scenarios, it is undesirable for antonyms to be closely related) (Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). In order to solve this problem, antonym lexicons have been used to fine-tune pre-trained word vectors. Mrkˇsi´c et al. (2016) present a method called counter-fitting to inject antonymy and synonymy constraints into word vectors trained with GloVe (Pennington et al., 2014) and PARAGRAM (Wieting et al., 2015). The adapted word vectors trained with PARAGRAM achieve the second-highest SimLex-999 (Hill et al., 2015) score. Mrkˇsi´c et al. (2017) extend this previous work using negative sampling, to force synonym pairs to be closer to each other than to their negative examples, and forcing antonym"
C18-1085,P15-1173,0,0.0506626,"Missing"
C18-1085,S16-1030,0,0.347145,"e-trained word vectors has been shown to achieve highly competitive results (Kim, 2014). Learning task-specific vectors through fine-tuning may offer further gains in performance, and this is the primary focus of this paper. Separately, there has been recent work on methods for learning word embeddings based not just on textual contexts, but also external knowledge bases (Wieting et al., 2015; Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017). This has also been applied to sentiment classification (Rouvier and Favre, 2016; Yu et al., 2017), with empirical results indicating that explicitly embedding sentiment resources can improve the performance of sentiment analysis. Existing methods for encoding external knowledge into word vectors are generally trained independently of the downstream task. In order to leverage sentiment lexicons for sentiment analysis, we propose a novel method to combine a feedforward neural network (denoted “SentiNet”) with a CNN classifier to encode sentiment knowledge into word vectors during training. The method tunes word vectors through the CNN and SentiNet, based on independent inf"
C18-1085,D13-1170,0,0.0154743,"Missing"
C18-1085,P14-1146,0,0.138984,"n downstream tasks. Our method encodes a sentiment lexicon into word vectors when fine-tuning the word vectors in a downstream task. 2.2 Adapting Word Vectors for Sentiment Analysis There are many methods of adapting word vectors for sentiment analysis. Maas et al. (2011) combine two components — a probabilistic document model and a sentiment component — to jointly learn word vectors. The probabilistic document model does not require labelled data. The sentiment component uses document-level sentiment annotations to constrain words expressing similar sentiment to have similar representations. Tang et al. (2014) changed the objective function of the C&W (Collobert et al., 2011) model to produce sentiment-specific word vectors for Twitter sentiment analysis, by leveraging large volumes of distant-supervised tweets. In order to capture morphological and shape information from words, dos Santos and Gatti (2014) concatenate character-level embeddings and word-level embeddings to form 998 a combined word representation for sentiment analysis. Severyn and Moschitti (2015) refine pre-trained word vectors through a CNN based on distant-supervised data. Zhou et al. (2016) introduced three kinds of word vector"
C18-1085,P17-1006,0,0.0237323,"Missing"
C18-1085,Q15-1025,0,0.106229,"inal regression (e.g. positive vs. neutral vs. negative). In recent years, deep neural networks, such as convolutional neural networks (“CNNs”), have been widely used for sentiment classification. A simple CNN trained over pre-trained word vectors has been shown to achieve highly competitive results (Kim, 2014). Learning task-specific vectors through fine-tuning may offer further gains in performance, and this is the primary focus of this paper. Separately, there has been recent work on methods for learning word embeddings based not just on textual contexts, but also external knowledge bases (Wieting et al., 2015; Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017). This has also been applied to sentiment classification (Rouvier and Favre, 2016; Yu et al., 2017), with empirical results indicating that explicitly embedding sentiment resources can improve the performance of sentiment analysis. Existing methods for encoding external knowledge into word vectors are generally trained independently of the downstream task. In order to leverage sentiment lexicons for sentiment analysis, we propose a novel method to c"
C18-1085,S16-1027,1,0.935027,"https://code.google.com/archive/p/word2vec/ 6 https://github.com/nmrksic/attract-repel 1003 SemEval2016 SemEval2017 MR SST-2 word2vec (Mikolov et al., 2013) attract-repel-pos-neg (Mrkˇsi´c et al., 2017) attract-repel-ant-syn (Mrkˇsi´c et al., 2017) PARAGRAM (Wieting et al., 2015) counter-fitting (Mrkˇsi´c et al., 2016) 0.608 0.608 0.608 0.599 0.603 0.647 0.641 0.626 0.632 0.627 0.792 0.792 0.785 0.781 0.780 0.837 0.836 0.817 0.839 0.838 DURING+SentiNet AFTER+SentiNet 0.601 0.613 0.641 0.648 0.790 0.796 0.842 0.844 SwissCheese (Deriu et al., 2016) SENSEI-LIF (Rouvier and Favre, 2016) UNIMELB (Xu et al., 2016) BB twtr (Cliche, 2017) CNN-non-static (Kim, 2014) Re(word2vec) (Yu et al., 2017) 0.633 0.630 0.617 — — — — — — 0.685 0.685 — — — — — — — — — — — — 0.879 Table 3: Experiment results of word vectors on the four Sentiment Classification Datasets. The measurement of the second and third columns is Macro F1. The measurement of the fourth and fifth columns is Accuracy. • PARAGRAM: The word vectors of PARAGRAM, generated by encoding PPDB into word2vec word embeddings, based on the pre-trained PARAGRAM word embedding set.7 • counter-fitting: The word vectors of counter-fitting, generated by encoding"
C18-1085,P14-2089,0,0.165159,"positive vs. neutral vs. negative). In recent years, deep neural networks, such as convolutional neural networks (“CNNs”), have been widely used for sentiment classification. A simple CNN trained over pre-trained word vectors has been shown to achieve highly competitive results (Kim, 2014). Learning task-specific vectors through fine-tuning may offer further gains in performance, and this is the primary focus of this paper. Separately, there has been recent work on methods for learning word embeddings based not just on textual contexts, but also external knowledge bases (Wieting et al., 2015; Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017). This has also been applied to sentiment classification (Rouvier and Favre, 2016; Yu et al., 2017), with empirical results indicating that explicitly embedding sentiment resources can improve the performance of sentiment analysis. Existing methods for encoding external knowledge into word vectors are generally trained independently of the downstream task. In order to leverage sentiment lexicons for sentiment analysis, we propose a novel method to combine a feedforward"
C18-1085,D17-1056,0,0.382112,"s been shown to achieve highly competitive results (Kim, 2014). Learning task-specific vectors through fine-tuning may offer further gains in performance, and this is the primary focus of this paper. Separately, there has been recent work on methods for learning word embeddings based not just on textual contexts, but also external knowledge bases (Wieting et al., 2015; Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017). This has also been applied to sentiment classification (Rouvier and Favre, 2016; Yu et al., 2017), with empirical results indicating that explicitly embedding sentiment resources can improve the performance of sentiment analysis. Existing methods for encoding external knowledge into word vectors are generally trained independently of the downstream task. In order to leverage sentiment lexicons for sentiment analysis, we propose a novel method to combine a feedforward neural network (denoted “SentiNet”) with a CNN classifier to encode sentiment knowledge into word vectors during training. The method tunes word vectors through the CNN and SentiNet, based on independent information from supe"
C18-1085,S16-1040,0,0.0213144,"ment to have similar representations. Tang et al. (2014) changed the objective function of the C&W (Collobert et al., 2011) model to produce sentiment-specific word vectors for Twitter sentiment analysis, by leveraging large volumes of distant-supervised tweets. In order to capture morphological and shape information from words, dos Santos and Gatti (2014) concatenate character-level embeddings and word-level embeddings to form 998 a combined word representation for sentiment analysis. Severyn and Moschitti (2015) refine pre-trained word vectors through a CNN based on distant-supervised data. Zhou et al. (2016) introduced three kinds of word vectors as features into their sentiment classifier. One is general purpose, and the the other two are trained by leveraging sentiment information. However, their feature selection experiments indicate that the task-specific trained word vectors do not improve the performance of their system substantially. Ren et al. (2016) use a recursive autoencoder to learn topic-enhanced word vectors, based on the assumption that the same word can vary in sentiment according to topic. Rouvier and Favre (2016) proposed three kinds of word embeddings — lexical embeddings, part"
copestake-etal-2002-multiword,W98-0707,0,\N,Missing
copestake-etal-2002-multiword,P97-1018,1,\N,Missing
D07-1050,W07-1204,1,0.843758,"the SemCor corpus (Fellbaum, 1998) which is annotated with parse trees and WordNet senses, but it is fairly small, and does not explicitly include any structural semantic information. Therefore, we decided to construct and use a treebank with both syntactic information (e.g. HPSG parses) and lexical semantic information (e.g. sense tags): the Hinoki treebank (Bond et al., 2004). This can be used to train word sense disambiguation and parse ranking models using both syntactic and lexical semantic features. In this paper, we discuss only word sense disambiguation. Parse ranking is discussed in Fujita et al. (2007). 2 The Hinoki Corpus The Hinoki corpus consists of the Lexeed Semantic Database of Japanese (Kasahara et al., 2004) and corpora annotated with syntactic and semantic infor477 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 477–485, Prague, June 2007. 2007 Association for Computational Linguistics mation. 2.1 Lexeed Lexeed is a database built from on a dictionary, which defines word senses used in the Hinoki corpus and has around 49,000 dictionary definition sentences and 46,000 example sentences wh"
D07-1050,P05-1050,0,0.0210164,"Missing"
D07-1050,P03-1054,0,0.00495422,"del using only bag of words and n-gram features. 1 Introduction Recently, significant improvements have been made in combining symbolic and statistical approaches to various natural language processing tasks. In parsing, for example, symbolic grammars are being combined with stochastic models (Riezler et al., 2002; Oepen et al., 2002; Malouf and van Noord, 2004). Statistical techniques have also been shown to be useful for word sense disambiguation (Stevenson, 2003). However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. Klein and Manning (2003) show that much of the gain in statistical parsing using lexicalized models comes from the use of a small set of function words. Features based on general relations provide little improvement, presumably because the data is too sparse: in the Penn treebank normally used to train and test statistical parsers stocks and skyrocket never appear together. They note that this should motivate the use of similarity and/or class based approaches: the superordinate concepts capital (⊃ stocks) and move upward (⊃ sky rocket) frequently appear together. However, there has been little success in this area t"
D07-1050,J03-4004,0,0.0237773,"parsers stocks and skyrocket never appear together. They note that this should motivate the use of similarity and/or class based approaches: the superordinate concepts capital (⊃ stocks) and move upward (⊃ sky rocket) frequently appear together. However, there has been little success in this area to date. For example, Xiong et al. (2005) use semantic knowledge to parse Chinese, but gain only a marginal improvement. Focusing on WSD, Stevenson (2003) and others have shown that the use of syntactic information (predicate-argument relations) improve the quality of word sense disambiguation (WSD). McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. However, there is still little work on combining WSD and parse selection. We hypothesize that one of the reasons for the lack of success is that there has been no resource annotated with both syntactic (or structural semantic information) and lexical semantic information. For English, there is the SemCor corpus (Fellbaum, 1998) which is annotated with parse trees and WordNet senses, but it is fairly small, and does not explicitly include any structural semantic information. Therefore, we decided to construct and u"
D07-1050,C02-2025,0,0.023192,"Missing"
D07-1050,P02-1035,0,0.0501663,"Missing"
D07-1050,W02-1210,0,0.0181814,"were then hand verified. Goi-Taikei has about 400,000 words including proper nouns, most nouns are classified into about 2,700 semantic classes. These semantic classes are arranged in a hierarchical structure (11 levels). The Goi-Taikei Semantic Class for untenshu “chauffeur” is shown in Figure 1: hC292:driveri at level 9 which is subordinate to hC4:personi. þ U3 2.5 Syntactic and Structural Semantics Annotation Syntactic annotation is done by selecting the best parse (or parses) from the full analyses derived by a broad-coverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. As the grammar is based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994) it is possible to simultaneously annotate syntactic and semantic structure without overburdening the annotator. Using a grammar enforces treebank consistency — all sentences annotated are guaranteed to have well I NDEX POS   L EX -T YPE  FAMILIARITY          S ENSE 1      þU3  untenshu noun noun-lex 6.2 [1–7] (≥ 5)  D EFINITION    E XAMPLE    H"
D07-1050,W06-0608,1,0.894014,"Missing"
D07-1050,N04-4003,0,0.0206929,"tion 4. In the initial state, some of the semantic features, e.g. semantic collocations (SEM-Col) and word sense extensions for semantic dependencies (SEM-Dep) are not available, since no word senses for polysemous words have been determined. It is not practical to count all combinations of word senses for target words, therefore, we first try to decide the sense for that word which is most plausible among all the ambiguous words, then, disambiguate the next word by using the sense. We use the beam search algorithm, which is similar to that used for decoder in statistical machine translation (Watanabe, 2004), for finding the plausible combination of word sense tags. We trained and tested on the Lexeed Dictionary Definition (LXD-DEF) and Example sections (LXD-EX) of the Hinoki corpus (Bond et al., 2007). These have about 75,000 definition and 46,000 example sentences respectively. Some 54,000 and 36,000 sentences of them are treebanked, i.e., they have the syntactic trees and structural semantic information. We used these sentences with the complete information and selected 1,000 sentences out of each sentence class as test sets (LXD-DEFtest , LXD-EXtest ), and the remainder is combined and used a"
D07-1050,I05-1007,0,\N,Missing
D10-1068,C08-1008,0,0.0162961,"ant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implementation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant of t"
D10-1068,J99-2004,0,0.126197,"o get multiple gold trees for a given sentence. Table 5 shows the results for these two methods. All the results show an improvement over the baseline, with all but the F-score for the Edges method of tc-006 being at a level of statistical significance.5 The only statistically significant difference between the Edges and Branching methods is over the jhpstgt data set. While improvement over random is encouraging, the results were still uninspiring and so we moved on to slightly more complex methods, described in the next section. 5 Supertagging Experiments The term supertags was first used by Bangalore and Joshi (1999) to describe fine-grained part of speech tags which include some structural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5 All statistical significance tests in these experim"
D10-1068,W02-1502,0,0.0310782,"tistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. Bender (2008) showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time. However, the Grammar Matrix can only help with the first stage of parsing. The statistical model used in the second stage of parsing (ie parse selection) requires a treebank to learn the features, but as we explain in Section 2, the treebanks are created by parsing, preferabl"
D10-1068,P08-1111,0,0.0187453,"nd pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. Bender (2008) showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time. However, the Grammar Matrix can only help with the first stage of parsing. The statistical model used in the second stage of parsing (ie parse selection) requires a treebank to learn the features, but as we explain in Section 2, the treebanks are created by parsing, preferably with a statistical model. In this work, we look at methods for bootstrapping the production of these statistical models without having an annotated treebank. Sinc"
D10-1068,W95-0101,0,0.154041,"s tried earlier. This suggested that, at least with a reasonably accurate tagger, this was a viable strategy for training a model. With no significant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov M"
D10-1068,P06-2006,0,0.0202805,"ndard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMN A evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMN A metric matches triples consisting of predicate names and the argument type that connects them.4 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the test data using that model; and (4) evaluate the accuracy of the top analyses. The differences lay in how the ‘correct’ analyses are selected"
D10-1068,W97-1502,0,0.0607223,"2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants (Carter, 1997). Whenever the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annotation decisions. This treebanking process not only produces gold standard trees, but also a set of non-gold trees which provides the negative training data necessary for a discriminative maximum entropy model. The standard process for creating a parse selection model is: 1. parse the training set, recording up to 500 highest-ranking parses for each sentence; 2. treebank the training set; 3. extract features from the gold and non-gold parses; 4. learn feature weights using the TADM"
D10-1068,P05-1022,0,0.109138,"inative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar enginee"
D10-1068,N06-1019,0,0.0266598,"dividing by the number of edges, to give an average edge weight for an analysis. All analyses that had the best analysis score for a sentence were designated ‘gold’. Since it was possible for multiple analyses to have the same score, there could be multiple gold analyses for any one sentence. If all the analyses had the same score, this sentence could not be used as part of the training data. This method has the effect of selecting the parse(s) most like all the others, by some definitions the centroid of the parse forest. This has some relationship to the partial training method described by Clark and Curran (2006), where the most frequent dependencies where used to train a model for the C&C CCG parser. In that case, however, the dependencies were extracted only from analyses that matched the gold standard supertag sequence, rather than the whole parse forest. 698 Test Set tc-006 jhpstgt catb Exact Match Edges Branching 17.48 21.35 15.27 17.53 9.36 10.86 F-score Edges Branching 0.815 0.822 0.766 0.780 0.713 0.712 Table 5: Accuracy for each test set, measured both as percentage of sentences that exactly matched the gold standard, and f-score over elementary dependencies. The second heuristic we tried is"
D10-1068,P07-1032,0,0.0142854,"r are under-resourced, we can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them"
D10-1068,J07-4004,0,0.0284858,"r are under-resourced, we can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them"
D10-1068,W07-2202,0,0.106501,"parse forest. This 5 All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with p < 0.05. work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ‘gold’ parse(s) that are then used to train a model, as in the previous sections. 5.1 Gold Supertags In order to test the viability of this method, we first experimented using gold standard tags, extracted from"
D10-1068,J07-3004,0,0.0173416,"useful metric for parse selection evaluation, but it is very blunt-edged, and gives no way of evaluating how close the top parse was to the gold standard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMN A evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMN A metric matches triples consisting of predicate names and the argument type that connects them.4 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the"
D10-1068,D07-1031,0,0.0215829,"or Japanese. However, this insignificant tagger accuracy decrease for Japanese produced a significant increase in parser accuracy, while a more pronounced tagger accuracy decrease had no significant effect on parser accuracy in English. Language Japanese English Initial counts 84.4 71.7 EM trained 83.3 64.6 Table 9: Tagger accuracy over the training data, using both the initial counts and the EM trained models. There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al. (2010b). There is also the issue of whether tag accuracy is the best measure for indicating potential parse accuracy. The Japanese parsing results are already equivalent to those achieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and ou"
D10-1068,W02-2018,0,0.0111363,"the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annotation decisions. This treebanking process not only produces gold standard trees, but also a set of non-gold trees which provides the negative training data necessary for a discriminative maximum entropy model. The standard process for creating a parse selection model is: 1. parse the training set, recording up to 500 highest-ranking parses for each sentence; 2. treebank the training set; 3. extract features from the gold and non-gold parses; 4. learn feature weights using the TADM toolkit.1 (Malouf, 2002) The useful training data from this process is the parses from those sentences for which: more than one parse was found; and at least one parse has been annotated as correct. That is, there needs to be both gold and non-gold trees for any sentence to be used in training the discriminative model. 1 http://tadm.sourceforge.net/ 695 There are two issues with this process for new grammars. Firstly, treebanking takes many personhours, and is hence both time-consuming and expensive. Complicating that is the second issue: N best parsing requires a statistical model. While it is possible to parse exha"
D10-1068,N06-1020,0,0.157859,"Missing"
D10-1068,J94-2001,0,0.236533,"heuristic methods tried earlier. This suggested that, at least with a reasonably accurate tagger, this was a viable strategy for training a model. With no significant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order H"
D10-1068,J08-1002,0,0.0834645,"can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process"
D10-1068,P08-1006,0,0.0142317,"thods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants"
D10-1068,A00-2022,0,0.0380747,"reebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses o"
D10-1068,P10-1051,0,0.277963,"+morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implementation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant of the Expectation-Maximisation (EM) algor"
D10-1068,C10-1106,0,0.103691,"+morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implementation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant of the Expectation-Maximisation (EM) algor"
D10-1068,D08-1050,0,0.11598,"5 All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with p < 0.05. work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ‘gold’ parse(s) that are then used to train a model, as in the previous sections. 5.1 Gold Supertags In order to test the viability of this method, we first experimented using gold standard tags, extracted from the gold standard parses"
D10-1068,W02-1210,0,0.0368199,"uracy as an upperbound. The English Resource Grammar (ERG: Language Sentences Japanese English 6769 4855 Average words 10.5 9.0 Average parses 49.6 59.5 Test Set Language Sentences tc-006 jhpstgt catb Table 1: Initial model training data, showing the average word length per sentence, and also the ambiguity measured as the average number of parses found per sentence. Flickinger (2002)) is an HPSG-based grammar of English that has been under development for many person years. In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002). In both cases, we use grammar versions from the “Barcelona” release, from mid-2009. 3.1 Training Data Both of our grammars come with statistical models, and the parsed data and gold standard annotations used to create these models are freely available. As we are trying to simulate a fully unsupervised setup, we didn’t want any influence from these earlier models. Hence, in our experiments we used the parsed data from those sentences that received less than 500 parses and ignored any ranking, thus annulling the effects of the statistical model. This led to a reduced data set, both in the numb"
D10-1068,C00-2137,0,0.0266421,"ural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5 All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with p < 0.05. work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500"
D10-1068,W07-2207,0,0.379796,"equires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these HPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collab"
D10-1084,P02-1048,0,0.0245783,"icipant’s message(s) in a turn. 863 al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-"
D10-1084,W98-0319,0,0.661934,"Missing"
D10-1084,W10-2923,1,0.878643,". Previously, Ivanovic (2008) explored Boolean 1- 864 Our motivation for using structural information as a feature is that the location of an utterance can be a strong predictor of the dialogue act. That is, dialogues are sequenced, comprising turns (i.e. a given user is sending text), each of which is made up of one or more messages (i.e. strings sent by the user). Structured classification methods which make use of this sequential information have been applied to related tasks such as tagging semantic labels of key sentences in biomedical domains (Chung, 2009) and post labels in web forums (Kim et al., 2010). Based on the nature of live chats, we observed that the utterance position in the chat, as well as in a turn, plays an important role when identifying its dialogue act. For example, an utterance such as Hello will occur at the beginning of a chat while an utterance such as Have a nice day will typically appear at the end. The position of utterances in a turn can also help identify the dialogue act; i.e. when there are several utterances in a turn, utterances are related to each other, and thus examining the previous utterances in the same turn can help correctly predict the target utterance."
D10-1084,N04-3002,0,0.0102816,"lassifying Dialogue Acts in One-on-one Live Chats Su Nam Kim,♠ Lawrence Cavedon♥ and Timothy Baldwin♠ ♠ Dept of Computer Science and Software Engineering, University of Melbourne ♥ School of Computer Science and IT, RMIT University sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net Abstract linguistic research on the topic. There has been substantially more work done on dialogue and dialogue corpora, mostly in spoken dialogue (e.g. Stolcke et al. (2000)) but also multimodal dialogue systems in application areas such as telephone support service (Bangalore et al., 2006) and tutoring systems (Litman and Silliman, 2004). Spoken dialogue analysis introduces many complications related to the error inherent in current speech recognition technologies. As an instance of written dialogue, an advantage of live chats is that recognition errors are not such an issue, although the nature of language used in chat is typically ill-formed and turn-taking is complicated by the semi-asynchronous nature of the interaction (e.g. Werry (1996)). We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate"
D10-1084,P98-2188,0,0.105999,"n derived from utterances (Sections 4.2 and 4.3). 2 Related Work While there has been significant work on classifying dialogue acts, the bulk of this has been for spoken dialogue. Most such work has considered: (1) defining taxonomies of dialogue acts; (2) discovering useful features for the classification task; and (3) experimenting with different machine learning techniques. We focus here on (2) and (3); we return to (1) in Section 3. For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et 1 An utterance is the smallest unit to deliver a participant’s message(s) in a turn. 863 al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spo"
D10-1084,J00-3003,0,\N,Missing
D10-1084,P06-1026,0,\N,Missing
D10-1084,C98-2183,0,\N,Missing
D11-1002,W04-3240,0,0.462342,"Missing"
D11-1002,D09-1047,0,0.0158785,"on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles i"
D11-1002,P08-1081,0,0.0130061,"ove the classification accuracy for postlevel tasks. Initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) from online forums have the potential to enhance thread summarisation or automatically generate knowledge bases for Community Question Answering (cQA) services such as Yahoo! Answers. While initiation– response pair identification has been explored as a pairwise ranking problem (Wang and Ros´e, 2010), question–answer pair identification has been approached via the two separate sub-tasks of question classification and answer detection (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009). Our thread discourse structure prediction task includes joint classification of post roles (i.e. dialogue acts) and links, and could potentially be performed at the sub-post sentence level to extract initiation–response pairs. 3 Task Description and Data Set The main task performed in this research is joint classification of inter-post links (Link) and dialogue acts (DA) within forum threads. In this, we assume that a post can only link to an earlier post (or a virtual root node), and that dialogue acts are labels on edges. It is possible for there to be multiple edges fro"
D11-1002,W05-1504,0,0.0477988,"Missing"
D11-1002,P08-1095,0,0.0229703,"010a), demonstrating the generalisability of the original method. In both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of c"
D11-1002,N09-1037,0,0.0208178,"different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the predictions of a multi-layer perceptron classifier on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-"
D11-1002,J86-3001,0,0.379024,", but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivan"
D11-1002,D10-1084,1,0.517285,"assessment (Lui and Baldwin, 2009). We aim to move beyond simple threading, to predict not only the links between posts, but also show the manner of each link, in the form of the discourse structure of the thread. In doing so, we hope to be able to perform richer visualisation of thread structure (e.g. highlighting the key posts which appear to have led to a successful resolution to a problem), and more finegrained weighting of posts in threads for search purposes. To illustrate the task, we use an example thread, made up of 5 posts from 4 distinct participants, from the CNET forum dataset of Kim et al. (2010b), as shown in Figure 1. The discourse structure of the thread is modelled as a rooted directed acyclic graph 13 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13–25, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics full threads, indicating that the method is applicable to in-situ thread classification. Finally, we investigate the role of user-level features in discourse structure analysis. Ø 0+Question-Question User A Post 1 HTML Input Code ...Please can someone tell me how to create an input box that a"
D11-1002,W10-2923,1,0.839716,"Missing"
D11-1002,W10-1915,0,0.0189845,"Missing"
D11-1002,W02-0216,0,0.0351975,"neralisability of the original method. In both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication includin"
D11-1002,U10-1009,1,0.684369,"tion (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles in web forums have also been studied (Chan and Hayes, 2010; Chan et al., 2010). Threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). These results su"
D11-1002,D07-1013,1,0.806323,"the joint classification predictions, and performed a similar breakdown of posts 20 for Link and DA; the results are presented in Figure 3. It is clear that the anomaly for CRFSGD comes from the DA component, due to there being greater predictability in the dialogue for final posts in a thread (users tend to confirm a successful resolution of the problem, or report on successful external reproduction of the solution). MaltParser seems less adept at identifying that a post is at the end of a thread, and predicting the dialogue act accordingly. This observation is congruous with the findings of McDonald and Nivre (2007) that errors propagate, due to MaltParser’s greedy inference strategy. The higher results for Link are to be expected, as throughout the thread, most posts tend to link locally. XXX B/down XXX XXX Test [1, 2] [1, 4] [1, 6] [1, 8] [All] [1, 2] [1, 4] [1, 6] [1, 8] [All] .947/.947 .946/.947 .946/.947 .946/.947 .946/.946 — .836/.841 .840/.841 .840/.841 .840/.838 — — .800/.794 .800/.794 .800/.791 — — — .780/.769 .776/.767 — — — .756/.738 Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of different lengths (indicated in the rows), broken d"
D11-1002,E06-1011,0,0.053421,"Missing"
D11-1002,H05-1066,0,0.17872,"Missing"
D11-1002,N06-1047,0,0.022453,"-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different su"
D11-1002,W03-3017,1,0.730692,". One feature of MaltParser that makes it well suited to our task is that it is possible to define feature models of arbitrary complexity for each token. In presenting the thread data to MaltParser, we represent the nulllink from the initial post of each thread, as well as any disconnected posts, as the root. To the best of our knowledge, there is no past work on using dependency parsing to learn thread discourse structure. Based on extensive experimentation, we determined that the MaltParser configuration that obtains the best results for our task is the Nivre algorithm in arc-standard mode (Nivre, 2003; Nivre, 2004), using LIBSVM (Chang and Lin, 2011) with a linear kernel as the learner, and a feature model with exhaustive combinations of features relating to the features and predictions of the first/top 17 three tokens from both “Input” and “Stack”.3 As such, MaltParser is actually unable to predict any non-projective structures, as experiments with algorithms supporting non-projective structures invariably led to lower results. In our choice of parsing algorithm, we are also unable to detect posts with multiple heads, but can potentially detect disconnected sub-graphs. 4.2 Features The fe"
D11-1002,W04-0308,1,0.0531983,"of MaltParser that makes it well suited to our task is that it is possible to define feature models of arbitrary complexity for each token. In presenting the thread data to MaltParser, we represent the nulllink from the initial post of each thread, as well as any disconnected posts, as the root. To the best of our knowledge, there is no past work on using dependency parsing to learn thread discourse structure. Based on extensive experimentation, we determined that the MaltParser configuration that obtains the best results for our task is the Nivre algorithm in arc-standard mode (Nivre, 2003; Nivre, 2004), using LIBSVM (Chang and Lin, 2011) with a linear kernel as the learner, and a feature model with exhaustive combinations of features relating to the features and predictions of the first/top 17 three tokens from both “Input” and “Stack”.3 As such, MaltParser is actually unable to predict any non-projective structures, as experiments with algorithms supporting non-projective structures invariably led to lower results. In our choice of parsing algorithm, we are also unable to detect posts with multiple heads, but can potentially detect disconnected sub-graphs. 4.2 Features The features used in"
D11-1002,P95-1005,0,0.304332,"Missing"
D11-1002,C08-1095,0,0.0412607,"Missing"
D11-1002,W09-3813,0,0.0170508,"et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the p"
D11-1002,W04-2319,0,0.0142908,"o a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interaction"
D11-1002,C10-2133,0,0.0513622,"ies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles in web forums have also been studied (Chan and Hayes, 2010; Chan et al., 2010). Threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). These results suggest that the thread structural representation used in this research, which includes both linking struc15 ture and the dialogue act associated with each link, could potentially provide even greater leverage in these retrieval tasks. Another related research area is post-level classification, such as general post quality classification (Weimer et al., 2007; Weimer and Gurevych, 2007; Wanas et al., 2008; Lui and Baldwin, 2009), and post descriptiveness in particular domains (e.g. medical forums: Leaman et al. (2010))"
D11-1002,N03-1030,0,0.021924,"c graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their a"
D11-1002,J00-3003,0,0.332415,"Missing"
D11-1002,W05-0636,0,0.0238895,"nd dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the predictions of a multi-layer perceptron classifier on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-o"
D11-1002,A97-1011,0,0.0698392,"Missing"
D11-1002,N10-1097,0,0.161975,"Missing"
D11-1002,P07-2019,0,0.0814572,"ation predictions, and performed a similar breakdown of posts 20 for Link and DA; the results are presented in Figure 3. It is clear that the anomaly for CRFSGD comes from the DA component, due to there being greater predictability in the dialogue for final posts in a thread (users tend to confirm a successful resolution of the problem, or report on successful external reproduction of the solution). MaltParser seems less adept at identifying that a post is at the end of a thread, and predicting the dialogue act accordingly. This observation is congruous with the findings of McDonald and Nivre (2007) that errors propagate, due to MaltParser’s greedy inference strategy. The higher results for Link are to be expected, as throughout the thread, most posts tend to link locally. XXX B/down XXX XXX Test [1, 2] [1, 4] [1, 6] [1, 8] [All] [1, 2] [1, 4] [1, 6] [1, 8] [All] .947/.947 .946/.947 .946/.947 .946/.947 .946/.946 — .836/.841 .840/.841 .840/.841 .840/.838 — — .800/.794 .800/.794 .800/.791 — — — .780/.769 .776/.767 — — — .756/.738 Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of different lengths (indicated in the rows), broken d"
D11-1002,P07-2032,0,0.0159489,"Missing"
D11-1002,J05-2005,0,0.0318554,"n both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 20"
D11-1002,C00-2137,0,0.0329904,"Missing"
D12-1039,P06-2005,0,0.594423,"andwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 3 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness clas"
D12-1039,P11-1040,0,0.00874909,"h other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-t"
D12-1039,P00-1037,0,0.0191066,"in lexical normalisation, t is assumed to be an 422 OOV token, relative to a fixed dictionary. In practice, not all OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as ph"
D12-1039,C10-2022,0,0.146671,"Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as 426 consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and (nite, night) are at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, mixed with lexical variants like (Facebook, speech) and (Youtube, web). In ranking by IV word frequency, w"
D12-1039,W09-2010,1,0.797376,"ntities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts,"
D12-1039,P11-2008,0,0.0600356,"Missing"
D12-1039,P11-2102,0,0.00976165,"ore and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws"
D12-1039,W11-2210,0,0.196949,"toria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods often utilise complex models and struggle to differentiate bet"
D12-1039,P11-1038,1,0.0891507,"Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods often utilise complex m"
D12-1039,P11-1016,0,0.0175407,"-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2"
D12-1039,P06-1129,0,0.0608485,"Missing"
D12-1039,P98-2127,0,0.0455843,"therwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et"
D12-1039,P11-2013,0,0.380884,"logs Bo Han,♠♥ Paul Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods"
D12-1039,P11-1037,0,0.607442,"logs Bo Han,♠♥ Paul Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods"
D12-1039,P12-1109,0,0.520743,"scovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-to-one normalisation in which one OOV token is normalised to one IV word. Naturally, not all OOV words in microblogs are lexical variants of IV words: named entities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics tingui"
D12-1039,I11-1062,1,0.735848,"ax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API.1 From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length ≥ 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, following settings in Han and Baldwi"
D12-1039,N10-1020,0,0.0428163,"Missing"
D12-1039,D11-1141,0,0.0566316,"f representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were"
D12-1039,P02-1019,0,0.0120776,"OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various"
D12-1039,C00-2137,0,0.0417631,"onaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p &lt; 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. 6.2.3 Hybrid Approaches The methods of Gouws et al. (2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionarybased approaches; this is largely caused by lexical variant detection errors.8 Using all dictionaries in combination with these methods — HB-dict+GHM-dict+S-dict+GHM-norm and HBdict+GHM-dict+S-dict+HB-norm — gives some improvements, but the false alarm rates remain high. Despite the limitations of a pure dictionary-based approach to normalisation — dis"
D12-1039,C98-2122,0,\N,Missing
D14-1020,2003.mtsummit-papers.32,0,0.160669,"metrics is Spearman’s rank correlation with human judgments (Melamed et al., 2003), which measures the relative degree of monotonicity between the metric and human scores in the range [−1, 1]. The standard justification for calculating correlations over ranks rather than raw scores is to: (a) reduce anomalies due to absolute score differences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent W MT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – A MBER (Chen et al., 2012) and T ERRORCAT (Fishel et al., 2012) – over the W MT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The l"
D14-1020,W11-2108,0,0.0308069,"Missing"
D14-1020,J82-2005,0,0.753631,"Missing"
D14-1020,W12-3103,0,0.0750178,"Missing"
D14-1020,P03-1021,0,0.0374185,"contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson:"
D14-1020,W12-3104,0,0.0139142,"ferences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent W MT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – A MBER (Chen et al., 2012) and T ERRORCAT (Fishel et al., 2012) – over the W MT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric invol"
D14-1020,2001.mtsummit-papers.68,0,0.0757134,"trics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha,"
D14-1020,W11-2107,0,0.0185794,"Missing"
D14-1020,W12-3106,0,0.0555225,"Missing"
D14-1020,W12-3105,0,0.0303018,"Missing"
D14-1020,W05-0908,0,0.0192495,"the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson: ● −2 −2 ● −3 −3 ● −3 −2 −1"
D14-1020,N03-1010,0,0.0412204,"). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.97"
D14-1020,W14-3333,1,0.762543,"presented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson: ● −2 −2 ● −3 −3 ● −3 −2 −1 0 1 2 3 −3 Human −2 −"
D14-1020,W04-3250,0,0.168479,"e system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson:"
D14-1020,N04-1022,0,0.0167647,"to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0"
D14-1020,N03-2021,0,\N,Missing
D14-1020,P02-1040,0,\N,Missing
D14-1189,W03-1812,1,0.691138,"between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013). The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003), Bannard (2006)), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011; Salehi and Cook, 2013), or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006). In this paper, we focus on the binary classification of MWE types relative to each component of the 1792 Proceedings of the 2014 Conference on Empirical Methods in Natural Language"
D14-1189,C10-3010,1,0.886304,"Missing"
D14-1189,J09-1005,1,0.884205,"ts about spelling, but not those which contain only bee. For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or app"
D14-1189,S13-1039,1,0.885672,"ither research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zett"
D14-1189,E14-1050,1,0.85897,"Missing"
D14-1189,I11-1102,1,0.882813,"Missing"
D14-1189,kamholz-etal-2014-panlex,0,0.0141321,"gs of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792–1797, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics MWE. The work that is perhaps most closely related to this paper is that of Salehi and Cook (2013) and Salehi et al. (2014), who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014). However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of Salehi and Co"
D14-1189,D13-1145,0,0.0604532,"bee. For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar"
D14-1189,I11-1024,0,0.0698704,"ents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwi"
D16-1087,Q15-1018,0,0.0281204,"lts. Similar conclusions were reached by Yosinski et al. (2014), who investigated the transferability of features from a deep neural network trained over the ImageNet data set. Sutton and McCallum (2005) investigated how the target task affects the source task, and demonstrated that decoding for transfer is better than no transfer, and joint decoding is better than cascading decoding. Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch¨utze, 2015). Having a fine-grained entity typology has been shown to improve other tasks such as relation extraction (Ling and Weld, 2012) and question answering (Lee et al., 2007). Nevertheless, for many social media-based or security-related applications, we don’t have access to a high-coverage knowledge base, meaning distant supervision is not appropriate. 3 Transfer Learning for NER Our proposed approach TransInit consists of three steps: (1) we train a linear-chain CRF on a large 900 source-domain corpus; (2) we learn the correlation between source NE types and targ"
D16-1087,U15-1010,1,0.897344,"Missing"
D16-1087,P08-1029,0,0.0624189,"Missing"
D16-1087,W11-0207,0,0.0319156,"Missing"
D16-1087,D10-1098,0,0.0572535,"Missing"
D16-1087,P05-1045,0,0.0187102,"Missing"
D16-1087,P15-1046,0,0.198749,"ing sentences. 899 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 899–905, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Related work The main scenario where transfer learning has been applied to NER is domain adaptation (Arnold et al., 2008; Maynard et al., 2001; Chiticariu et al., 2010), where it is assumed that the label set Y is the same for both the source and target corpora, and only the domain varies. In our case, however, both the domain and the label set differ across datasets. Similar to our work, Kim et al. (2015) use transfer learning to deal with NER data sets with different label distributions. They use canonical correlation analysis (CCA) to induce label representations, and reduce the problem to one of domain adaptation. This supports two different label mappings: (i) to a coarse label set by clustering vector representations of the NE types, which are combined with mention-level predictions over the target domain to train a target domain model; and (ii) between labels based on the k nearest neighbours of each label type, and from this transferring a pre-trained model from the source to the target"
D16-1087,K15-1009,1,0.837303,", we split the training set of CADEC and I2B2 into 10 partitions based on a log scale, and created 10 successively larger training sets by merging these partitions from smallest to largest (with the final merge resulting in the full training set). For all methods, we report the macro-averaged F1 over only the NE classes that are novel to the target domain. 4.2 Baselines We compare our methods with the following two in-domain baselines, one cross-domain data-based method, and three cross-domain transfer-based benchmark methods. BOW: an in-domain linear-chain CRF with handcrafted features, from Qu et al. (2015). Embed: an in-domain linear-chain CRF with handcrafted features and pre-trained word embeddings, from Qu et al. (2015). LabelEmbed: take the labels in the source and target domains, and determine the alignment based on the similarity between the pre-trained embeddings for each label. 902 CCA: the method of Kim et al. (2015), where a one-to-one mapping is generated between source and target NE classes using CCA and k-NN (see Section 2). TransDeepCRF: A three-layer deep CRF. The bottom layer is a linear layer initialised with Ws from the source domain-trained CRF. The middle layer is a hard tan"
D16-1087,E12-2015,0,0.0354781,"Missing"
D16-1087,H05-1094,0,0.161623,"s, which are combined with mention-level predictions over the target domain to train a target domain model; and (ii) between labels based on the k nearest neighbours of each label type, and from this transferring a pre-trained model from the source to the target domain. They showed their automatic label mapping strategies attain better results than a manual mapping, with the pre-training approach achieving the best results. Similar conclusions were reached by Yosinski et al. (2014), who investigated the transferability of features from a deep neural network trained over the ImageNet data set. Sutton and McCallum (2005) investigated how the target task affects the source task, and demonstrated that decoding for transfer is better than no transfer, and joint decoding is better than cascading decoding. Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch¨utze, 2015). Having a fine-grained entity typology has been shown to improve other tasks such as relation extraction (Ling and Weld, 2012) and question answe"
D16-1087,P10-1040,0,0.236192,"Missing"
D16-1087,D15-1083,0,0.151134,"amed Entity Recognition for Novel Types by Transfer Learning Lizhen Qu1,2 , Gabriela Ferraro1,2 , Liyuan Zhou1 , Weiwei Hou1 , Timothy Baldwin1,3 DATA61, Australia The Australian National University 3 The University of Melbourne 1 2 {lizhen.qu,gabriela.ferraro,joe.zhou}@data61.csiro.au houvivid2013@gmail.com, tb@ldwin.net Abstract data, we will be hampered in our ability to reliably learn feature weights. Second, the absence of target NE types in the source domain makes transfer difficult, as we cannot directly apply a model trained over the source domain to the target domain. Alvarado et al. (2015) show that even if the NE label set is identical across domains, large discrepancies in the label distribution can lead to poor performance. In named entity recognition, we often don’t have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume n"
D16-1087,P13-4023,0,0.0164495,"ieving the best results. Similar conclusions were reached by Yosinski et al. (2014), who investigated the transferability of features from a deep neural network trained over the ImageNet data set. Sutton and McCallum (2005) investigated how the target task affects the source task, and demonstrated that decoding for transfer is better than no transfer, and joint decoding is better than cascading decoding. Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities (Ling and Weld, 2012; Dong et al., 2015; Yosef et al., 2013; Althobaiti et al., 2015; Yaghoobzadeh and Sch¨utze, 2015). Having a fine-grained entity typology has been shown to improve other tasks such as relation extraction (Ling and Weld, 2012) and question answering (Lee et al., 2007). Nevertheless, for many social media-based or security-related applications, we don’t have access to a high-coverage knowledge base, meaning distant supervision is not appropriate. 3 Transfer Learning for NER Our proposed approach TransInit consists of three steps: (1) we train a linear-chain CRF on a large 900 source-domain corpus; (2) we learn the correlation between"
D16-1207,P15-2030,1,0.794831,"inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-Representation. robustness of classifiers t"
D16-1207,N13-1037,0,0.02851,")3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability α ∈ {0, 0.1, 0.2, 0.3}. We also experimented with adding different levels of Gaussian noise to the sentence embeddings ES , but found the results to be largely consistent with those for word dropout noise, and therefore we have omitted these results from the paper. To directly test the robustness under a more realistic setting, we additionally perform cross-domain evaluation, where we train a model on one dataset 2 For d"
D16-1207,P11-1038,1,0.806331,"set (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability α ∈ {0, 0.1, 0.2, 0.3}. We also experimented with adding different levels of Gaussian noise to the sentence embeddings ES , but found the results to be largely consistent with those for word dropout noise, and therefore we have omitted these results from the paper. To directly test the robustness under a more realistic setting, we additionally perform cross-domain evaluation, where we train a model on"
D16-1207,P14-1062,0,0.056791,"limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-R"
D16-1207,D14-1181,0,0.373064,"network. Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders. Where our proposed approach differs is that we train models using first-order derivatives of the training loss as part of a regularization term, necessitating second-order derivatives for computing the gradient. We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014). In this, we show that our method is superior to dropout (Srivastava et al., 2014) and a baseline method using MAP training. 2 Training for Robustness Our method introduces a regularization term during training to ensure model robustness. We develop our approach based on a general class of parametric mod1979 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1979–1985, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics els, with the following structure. Let x be the input, which is a sequence of (discrete) words, repr"
D16-1207,P05-1015,0,0.132058,"ngs can be found in Section 3.2. The feature vector h is fed into a final softmax layer with a linear transform to generate a probability distribution over labels ypred = softmax(w · h + b) , where w and b are parameters. Finally, the model minimizes the loss of the cross-entropy between the ground-truth and the model prediction, 1981 L = CrossEntropy(ytrue , ypred ), for which we use stochastic gradient descent. 3 Datasets and Experimental Setups We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-l"
D16-1207,D13-1170,0,0.00926157,"stribution over labels ypred = softmax(w · h + b) , where w and b are parameters. Finally, the model minimizes the loss of the cross-entropy between the ground-truth and the model prediction, 1981 L = CrossEntropy(ytrue , ypred ), for which we use stochastic gradient descent. 3 Datasets and Experimental Setups We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy. 3.1 Noisifying the Data Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability"
D16-1207,P14-2105,0,0.0312831,"ty to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1 1 Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization. Fawzi et al. (2015) provided a theoretical framework for analyzing the 1 Implementation available at https://github.com/ lrank/Robust-Representation. rob"
D17-1016,C12-1064,1,0.916873,"ng one of the cities. In the machine learning literature, 1 Code available at https://github.com/afshinrahimi/geomdn 168 Assuming sufficient training samples containing the term Norwalk in the two main, a trained classification model would, given this term as input, predict a probability distribution over all regions, and assign higher probabilities to the regions containing the two major cities. The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierar"
D17-1016,E14-1011,0,0.0540804,"ave no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness. 2.2 itude/longitude coordinate pair). The probability mass function is given by: Lexical Dialectology The traditional linguistic approach to lexical dialectology is to find the geographical distributions of known contrast sets such as {you, yall, yinz}: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, to appear). This usually involves surveying a large geographically-uniform sample of people from different locations and analysing where each known alternative is used more frequently. Then, the coordinates are clustered heuristically into dialect regions, based on the lexical choices of users in each region relative to the contrast set. This processing is very costly and time-consuming, and relies critically on knowing the lexical alternatives a priori. For example, it would require a priori knowledge of the fact that people in different regions of"
D17-1016,D10-1124,0,0.850034,"Missing"
D17-1016,N10-1038,0,0.0931546,"Missing"
D17-1016,W15-1527,0,0.486489,"Missing"
D17-1016,P17-2033,1,0.579699,"Missing"
D17-1016,N15-1153,1,0.872945,"Missing"
D17-1016,D12-1137,0,0.661056,"tput layer, and only use the input to predict π, the mixture probabilities, using a SoftMax layer. We 4 Continuous Representation of Location Experiments We apply the two described MDN models on two widely-used geotagged Twitter datasets for geolocation, and compare the results with state-of-the-art classification and regression baselines. Also, we use the mixture of Gaussian representation of location to predict dialect terms from coordinates. 170 4.1 Data In our experiments, we use two existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), and (2) T WITTER -US (Roller et al., 2012). Each dataset has fixed training, devel4.2 output layer: term probabilities tanh hidden layer Gaussian layer: K Gaussian components input: location coordinates (a) Predict text given location output layer: mixing coefficients π1 . . . πk tanh hidden layer input: Text BoW (b) Predict location given text Figure 1: (a) The lexical dialectology model using a Gaussian representation layer. (b) MDN-SHARED geolocation model where the mixture weights π are predicted for each sample, and µ and Σ are parameters of the output layer, shared between all samples. opment and test partitions, and a user is r"
D17-1016,P14-1109,0,0.0382902,"Missing"
D17-1016,P11-1096,0,0.434275,"/geomdn 168 Assuming sufficient training samples containing the term Norwalk in the two main, a trained classification model would, given this term as input, predict a probability distribution over all regions, and assign higher probabilities to the regions containing the two major cities. The challenge, though, is that the coordinates in the training data must first be partitioned into regions using administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), a uniform grid (Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness."
D17-1016,D14-1039,0,0.722211,"(Serdyukov et al., 2009), or a clustering method such as a k-d tree (Wing and Baldridge, 2011) or K-means (Rahimi et al., to appear). The cluster/region labels can then be used as targets. Once we have a prediction about where a user is more likely to be from, there is no more information about the coordinates inside the predicted region. If a region that contains Wyoming is predicted as the home location of a user, we have no idea which city or county within Wyoming the user might be from, unless we retrain the model using a more fine-grained discretisation or a hierarchical discretisation (Wing and Baldridge, 2014), which is both time-consuming and challenging due to data sparseness. 2.2 itude/longitude coordinate pair). The probability mass function is given by: Lexical Dialectology The traditional linguistic approach to lexical dialectology is to find the geographical distributions of known contrast sets such as {you, yall, yinz}: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, to appear). This usually involves surveying a large geographically-uniform sample of people from different locations and analysing where each kn"
D17-1262,W07-0718,0,0.0556468,"ors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst. Within this set-up, when presented with the same pair of MT output translations, human assessors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability"
D17-1262,E06-1032,0,0.0800317,"s of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of com"
D17-1262,1993.eamt-1.1,0,0.575525,"Missing"
D17-1262,P16-2013,0,0.447493,"y legitimate variations in scoring strategies. Despite efforts to avoid bias in Graham et al. (2013), since DA is a monolingual evaluation of MT that operates via comparison of MT output with a reference translation, it is therefore still possible, while avoiding other sources of bias, that DA incurs reference bias where the level of superficial similarity of translations with reference translations results in an unfair gain, or indeed an unfair disadvantage for systems that yield translations that legitimately deviate from the surface form of reference translations. Following this intuition, Fomicheva and Specia (2016) carry out an investigation into bias in monolingual evaluation of MT and conclude that in a monolingual setting, human assessors of MT are strongly biased by the reference translation. In this paper, we provide further analysis of experiments originally provided in Fomicheva and Specia (2016), in addition to further investigation into the degree to which the intuition about reference bias can be supported. 2 Background Fomicheva and Specia (2016) provide an investigation into reference bias in monolingual evaluation of MT. 100 Chinese to English MT output translations are assessed by 25 human"
D17-1262,W13-2305,1,0.950422,"ors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessme"
D17-1262,E14-1047,1,0.88998,"Missing"
D17-1262,N15-1124,1,0.852927,"an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessment. A reference translation is displayed to the human assessor (rendered in gray) and below it the MT output (in black), with the human judge asked to state the degree to which they agree that The black text adequately expresses the meaning of the gray text in English.1 The motivation behind 2476 1 Instructions are translated into a given target language. Proceedings of the 2017 Conference on Empirical Methods in Natural Language P"
D17-1262,P02-1040,0,0.111277,"ongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranki"
D17-1306,W10-0701,0,0.06039,"Missing"
D17-1306,N15-1124,1,0.894493,"r all aspects. Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2). Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects. 3.3 Machine Translation Adequacy When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output. In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100point sliding scale which is initialised at the mid point. There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration. They are given no other instructions or All β1 β2 Good Bad ∗ −0.03 −0.01 −0.04∗ 0.45∗∗∗ 0.66∗∗∗ 0.23∗∗∗ Table 2: Autocorrelation coefficient β1 for the A F FECTIVE dataset. guidelines. In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire da"
D17-1306,N06-2015,0,0.0446363,"Missing"
D17-1306,W06-3114,0,0.0379656,"Missing"
D17-1306,Q14-1025,0,0.0373302,"Missing"
D17-1306,N06-2031,0,0.0189771,"Missing"
D17-1306,D08-1027,0,0.204451,"Missing"
D17-1306,E12-2021,0,0.107327,"Missing"
D18-1098,W13-0102,0,0.414812,"offers little insight into the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intr"
D18-1098,K17-1022,1,0.383063,"this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intruder topic, and are tasked to find the intruder. Success in the task demonstrat"
D18-1098,D14-1181,0,0.00303579,"2: mpGOLD vs. System Scores at the model level are triples of (di , tji , yij ) — essentially the task is formulated as a binary classification problem. The architecture of our network is given in Figure 1. The input to our model is a document–topic pair, with each represented as a sequence of words. These words are mapped to embeddings, via embedding matrix W ∈ R|V |×d , where V is the vocabulary and d the dimensionality of the embeddings. The document embeddings Ed ∈ Rk×d (k = document length) and topic embeddings Et ∈ Rm×d (m = number of topic words) are processed via convolutional layers (Kim, 2014; Severyn and Moschitti, 2015) to produce two hidden representations for the document and topic. The convolution operation is performed using feature maps of varying size followed by a max-pooling operation to produce a constant-length vector. The document and topic hidden representations are concatenated and fed to 2 dense layers and ultimately reduced to a sigmoid-activated score. 4.3.1 External IR Feature A good topic model learns common themes in the document collection. A limitation of our network is the lack of global- or collection-level information (as the input consists of only a docu"
D18-1098,E14-1056,1,0.936639,"he utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intruder topic, and ar"
D18-1098,D11-1024,0,0.233888,"Missing"
D18-1098,N10-1012,1,0.863488,"ell topic words relate to each other, but offers little insight into the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a set of topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation. 1 Introduction Topic models have traditionally been evaluated using model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015). In earlier work (Bhatia et al., 2017), we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document collection. We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of Chang et al. (2009). In the original topic intrusion setup, users are presented with a document, a set of a"
D18-1098,D14-1162,0,0.0811572,"performs mpORIG and mp by a substantial margin, and also has a score close to human judgements. We can attribute this to the fact that nss provides more nuanced system predictions (over the full range [0, 1]), whereas mp tends to be binary.4 For our experiments, we train the model on outputs from all topics models over one dataset, and test it on the other (cross-domain training). We use a single channel for the convolutional networks, pad the documents as necessary (k = 200), and use the top-10 words to represent a topic (i.e. m = 10). Word embeddings are initialised using pre-trained GloVe (Pennington et al., 2014) vectors (d = 100), and their weights are fixed during training. We use kernel windows of width = {3, 5, 7} with 100 feature maps each and two (fully-connected) hidden layers, with dimensionality of 50 and 10. We use a dropout rate of 0.5, 0.5 and 0.25 after the document, topic and first hidden layer, respectively. We set the batch size to 100, and use Adam as the optimizer with a learning rate of 0.001. For activation functions, we use ReLU for the fully-connected layers and sigmoid for the final layer. To reduce variance, we run the models with 8 different seeds for initialisation and take t"
D19-1182,J08-4004,0,0.107717,"Missing"
D19-1182,D18-1217,0,0.0449086,"ches based on a uni-modal probability distribution (e.g., Poisson) as output (da Costa et al., 2008; Beckham and Pal, 2017) can be seen as related to the former approach (Imani and White, 2018) where the discrete probability mass function replaces the histogram density. We propose to use a uni-modal distributional loss-based ordinal regression for pledge specificity prediction. Secondly, as it is difficult to obtain large amounts of labeled data, existing approaches have used semi-supervised learning (Li and Nenkova, 2015; Subramanian et al., 2019). Here we use a cross-view training approach (Clark et al., 2018; Subramanian et al., 2019), where we enforce consensus between the intermediate class distributions or the final real-valued output. 3 Pledge Specificity Dataset We annotated 22 election manifestos from the Australian Labor and Liberal parties, covering eleven Australian federal election cycles from 1730 Category Definition Example # Not a pledge Provide facts; greetings; approval or criticism of policies 1 Rhetorical pledge Based on moral values and applies to all irrespective of the party Specify intangible goals, and also not the ways to achieve them Commit to the maintenance of currently"
D19-1182,S17-2088,0,0.0601742,"Missing"
D19-1182,L16-1620,0,0.055798,"Missing"
D19-1182,D17-1231,0,0.0224685,"inal regression output of the teacher model (f (x)) to fit an auxiliary model, thereby enforcing consensus using a squared loss, MSE(Eqθ [Y], Eqω [Y]) where Y is a fixed class vector; denoted as “LUMSE ”. K LD : an intermediate distribution over targets qθ (Y|s) is used to fit an auxiliary model, qω (Y|s), by minimising the Kullback-Leibler (KL) divergence, KL(qθ (Y|s), qω (Y|s)); denoted as “LUKLD ”.2 E MD : qθ (Y|s) is again used to fit the auxiliary model, qω (Y|s), by minimising the earth 2 We incorporate context in the form of information from adjacent sentences following the approach of Liu et al. (2017): for each training sentence, we Semi-supervised Learning This is the closest setting to Subramanian et al. (2019), which minimizes KL divergence between output distributions in a classification setting. But the overall objective is different in our case, in that we have an expectation layer over q to obtain the target regression output. 1733 MMAE ρ Majority Length Speciteller NNREG biGRUREG biGRUCLASS biGRUREGl1 3 2.05 1.83 2.17 1.99 0.21 0.18 0.33 0.47 0.40 0.46 Ordinal Ordinal Ordinal Ordinal biGRUC ATEGORICAL biGRUB INOMIAL biGRUP OISSON biGRUG AUSS 1.80 1.78 1.90 1.72 0.48 0.48 0.41 0.49"
D19-1182,I11-1068,0,0.0354047,"on the association between text specificity and communication style. In terms of automated specificity analysis, Cook (2016) found specificity in the context of congressional hearings to vary between speakers belonging to the same vs. different ideologies. Namely, it was shown that specificity increases as the ideological distance between the committee chair and the witness decreases. Subramanian et al. (2019) addressed two levels of pledge specificity, as part of speech act classification task. Specificity has 1 https://github.com/shivashankarrs/ Pledge-Specificity also been studied in news (Louis and Nenkova, 2011) and classroom discussion domains (Luo and Litman, 2016; Lugini and Litman, 2017). These studies have dealt with a restrictive coarse-level analysis (2–3 categories), whereas a fine-grained scale better captures and allows for comparison of election manifestos (Pomper and Lederman, 1980). Gao et al. (2019) was the first attempt at fine-grained text specificity prediction, in the context of social media posts. Here, we target the novel task of fine-grained pledge specificity prediction, which can be used in a range of downstream applications, including capturing party priorities (salience) and"
D19-1182,W17-5006,0,0.0214945,"automated specificity analysis, Cook (2016) found specificity in the context of congressional hearings to vary between speakers belonging to the same vs. different ideologies. Namely, it was shown that specificity increases as the ideological distance between the committee chair and the witness decreases. Subramanian et al. (2019) addressed two levels of pledge specificity, as part of speech act classification task. Specificity has 1 https://github.com/shivashankarrs/ Pledge-Specificity also been studied in news (Louis and Nenkova, 2011) and classroom discussion domains (Luo and Litman, 2016; Lugini and Litman, 2017). These studies have dealt with a restrictive coarse-level analysis (2–3 categories), whereas a fine-grained scale better captures and allows for comparison of election manifestos (Pomper and Lederman, 1980). Gao et al. (2019) was the first attempt at fine-grained text specificity prediction, in the context of social media posts. Here, we target the novel task of fine-grained pledge specificity prediction, which can be used in a range of downstream applications, including capturing party priorities (salience) and ideological position across election cycles. All the text specificity analysis wo"
D19-1182,N18-1202,0,0.130423,"Missing"
D19-1182,N18-1178,1,0.863075,"r sparse supervision scenarios using the teacher– student framework; and (3) we evaluate the utility of pledge specificity towards ideology prediction, and provide further qualitative analysis by correlating model predictions with party-specific issue salience across major policy areas. 2 Related Work Political manifesto text analysis is a relatively novel application, at the intersection of Political Science and NLP. Research has focused primarily on fine-grained policy topic classification and overall ideology prediction tasks (Volkens et al., 2017; Verberne et al., 2014; Zirn et al., 2016; Subramanian et al., 2018). Most work dealing with pledge specificity analysis in manifestos has been based on manual analysis, as outlined in Section 1. Specificity is a pragmatic property of text which has been studied across various fields of research. In cognitive linguistics, Dixon (1987) showed that specificity of information in text impacts reading comprehension speed. In Political Science, it has been used to analyze salience, party position and post-election policy framing (see Section 1). There has also been research on the association between text specificity and communication style. In terms of automated sp"
D19-1182,S19-1030,1,0.753661,"l scientists have long studied how specific pledges translate into government programs and actual policy (Royed, 1996; Thomson, 2001; Naurin, 2011; Schermann and Ennser-Jedenastik, 2014). Other work relates specific pledges to the issue clarity of a political party Issue clarity has also been shown to be influenced by a party’s ideological position and its role in government (Praprotnik, 2017). Although pledge specificity prediction is an important task for the analysis of party position, priorities, and post-election policy framing, to date, almost all research has relied on manual analysis. Subramanian et al. (2019) is a recent exception to this, in performing speech act classification over political campaign text, where the class schema includes the distinction between specific and vague pledges (binary specificity class). In this paper, we perform fine-grained pledge specificity prediction, which is more expressive than binary levels (Li et al., 2016; Gao et al., 2019). We use a class schema proposed by Pomper and Lederman (1980) as detailed in Table 1, which captures seven levels of specificity, forming a nonlinear increasing order of commitment and specificity (Pomper and Lederman, 1980). Given the n"
D19-5525,P13-1004,0,0.0328243,"ce of a GP model was studied in both underestimate and overestimate scenarios. Beck and Cohn (2017); Beck (2017) employ GPs to model text representation noise in emotion analysis, where Pearson’s correlation and NLPD are used as the evaluation metrics. Our work is different from these two studies as we model the subjectivity of quality assessment explicitly, which can mimic people’s different opinions over the quality of a document. There is also a rich body of work on identifying trustworthy annotators and predicting the correct underlying labels from multiple annotations (Hovy et al., 2013; Cohn and Specia, 2013; Passonneau and Carpenter, 2014; Graham et al., 2017; Paun et al., 2018). For example, Hovy et al. (2013) propose an item-response model to identify trustworthy annotators and predict the true labels of instances in an unsupervised way. However, our task is to measure the uncertainty of a model over its predictions (as distinct from attempting to learn the “true” label for an instance from potentially biased/noisy annotations), in addition to correctly predicting the gold label, in the context of assessing the quality of Wikipedia articles. Additionally, we have a rich representation of the d"
D19-5525,P82-1020,0,0.819772,"Missing"
D19-5525,N13-1132,0,0.0302399,"alue. The performance of a GP model was studied in both underestimate and overestimate scenarios. Beck and Cohn (2017); Beck (2017) employ GPs to model text representation noise in emotion analysis, where Pearson’s correlation and NLPD are used as the evaluation metrics. Our work is different from these two studies as we model the subjectivity of quality assessment explicitly, which can mimic people’s different opinions over the quality of a document. There is also a rich body of work on identifying trustworthy annotators and predicting the correct underlying labels from multiple annotations (Hovy et al., 2013; Cohn and Specia, 2013; Passonneau and Carpenter, 2014; Graham et al., 2017; Paun et al., 2018). For example, Hovy et al. (2013) propose an item-response model to identify trustworthy annotators and predict the true labels of instances in an unsupervised way. However, our task is to measure the uncertainty of a model over its predictions (as distinct from attempting to learn the “true” label for an instance from potentially biased/noisy annotations), in addition to correctly predicting the gold label, in the context of assessing the quality of Wikipedia articles. Additionally, we have a rich"
D19-5525,Q14-1025,0,0.0230591,"udied in both underestimate and overestimate scenarios. Beck and Cohn (2017); Beck (2017) employ GPs to model text representation noise in emotion analysis, where Pearson’s correlation and NLPD are used as the evaluation metrics. Our work is different from these two studies as we model the subjectivity of quality assessment explicitly, which can mimic people’s different opinions over the quality of a document. There is also a rich body of work on identifying trustworthy annotators and predicting the correct underlying labels from multiple annotations (Hovy et al., 2013; Cohn and Specia, 2013; Passonneau and Carpenter, 2014; Graham et al., 2017; Paun et al., 2018). For example, Hovy et al. (2013) propose an item-response model to identify trustworthy annotators and predict the true labels of instances in an unsupervised way. However, our task is to measure the uncertainty of a model over its predictions (as distinct from attempting to learn the “true” label for an instance from potentially biased/noisy annotations), in addition to correctly predicting the gold label, in the context of assessing the quality of Wikipedia articles. Additionally, we have a rich representation of the data point (i.e. document) that w"
D19-5525,Q18-1040,0,0.0138437,"s. Beck and Cohn (2017); Beck (2017) employ GPs to model text representation noise in emotion analysis, where Pearson’s correlation and NLPD are used as the evaluation metrics. Our work is different from these two studies as we model the subjectivity of quality assessment explicitly, which can mimic people’s different opinions over the quality of a document. There is also a rich body of work on identifying trustworthy annotators and predicting the correct underlying labels from multiple annotations (Hovy et al., 2013; Cohn and Specia, 2013; Passonneau and Carpenter, 2014; Graham et al., 2017; Paun et al., 2018). For example, Hovy et al. (2013) propose an item-response model to identify trustworthy annotators and predict the true labels of instances in an unsupervised way. However, our task is to measure the uncertainty of a model over its predictions (as distinct from attempting to learn the “true” label for an instance from potentially biased/noisy annotations), in addition to correctly predicting the gold label, in the context of assessing the quality of Wikipedia articles. Additionally, we have a rich representation of the data point (i.e. document) that we are attempting to label, whereas in wor"
D19-5525,D14-1162,0,0.0832321,"proposed methods into perspective, we compare their performance with these neural models in terms of point estimates. Specifically, we use the mean of the distributions for both GP and RF as predictions and use standard regression metrics (RMSE and Pearson’s r correlation) to assess the performance of our models against BI LSTM. For the GP model, we restrict our evaluation in this section (and in the remainder of this paper) to the one with an RBF kernel, as it performed significantly better in Section 4.1. We compare with two BI LSTM models: (1) with pre-trained word embeddings, using GloVe (Pennington et al., 2014) (“BI LSTM+ ” hereafter); and with randomly initialised word embeddings (“BI LSTM− ” hereafter). See Shen et al. (2017) for a detailed description of all hyperparameters. 4.3 Prediction of Secondary Labels As explained in Section 2.2, the inconsistent articles are ones where the primary label is in disagreement with the secondary ones, from different Wikipedia Projects. In this section, we assess how our models fare under a transfer scenario, where the goal is to predict these secondary labels. Such a scenario can be useful, for instance, if we want to incorporate information from Projects to"
D19-5525,U17-1005,1,0.796981,"Train Table 1: A breakdown of our Wikipedia dataset. ity predictions as close as possible to the primary labels while also providing uncertainty estimates of such predictions: lower uncertainty over consistent articles and higher uncertainty over inconsistent articles. The dataset is then stratified into training, development, and test sets, as detailed in Table 1. 3 Methods A key aspect of the task is the ordinal nature of the quality labels, e.g., a Start article is close in quality to a C, but much worse than an FA. Surprisingly though, most previous studies (Dang and Ignat, 2016a,b, 2017; Shen et al., 2017) formulate the problem as multi-class classification and use accuracy as the evaluation metric. Such modelling and evaluation procedures completely disregard the ordinal nature of the labels, which in turn does not correspond to real world scenarios: the cost of mispredicting a Start article as C is different to mispredicting it as an FA (standard classification metrics such as accuracy assume equal cost for all mispredictions). To better address the scenarios we are interested in, we treat quality assessment as a regression problem, in terms of both modelling and evaluation. In order to do th"
E12-1060,S07-1002,0,0.718641,"assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple"
E12-1060,E09-1013,0,0.822927,"s a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighte"
E12-1060,cook-stevenson-2010-automatically,1,0.882901,"hese senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to"
E12-1060,de-marneffe-etal-2006-generating,0,0.00362124,"Missing"
E12-1060,H92-1045,0,0.355926,"ruction of semantic space models, e.g. for WSD. Based on these findings, we include dependency relations as additional features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1 Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2 We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T , the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P (t = z|d) then; (2) draw a word w from the chosen topic P (w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P (w|d) = T ∑ P (w|t = z)P (t = z|d). z=1 In stand"
E12-1060,W11-2508,0,0.0612863,"t 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new"
E12-1060,S10-1079,0,0.195775,"Missing"
E12-1060,S10-1011,0,0.457638,"rm of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple topics, with each documen"
E12-1060,D10-1012,0,0.0734893,"sk. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use t"
E12-1060,S07-1037,0,0.0562894,"Missing"
E12-1060,J07-2002,0,0.0570721,"Missing"
E12-1060,W09-0214,0,0.117489,"Missing"
E12-1060,J98-1004,0,0.680031,"sense detection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and"
E12-1060,W11-1102,0,0.612441,"Missing"
E12-2014,P11-1038,1,0.769199,"roblem. At present, lexical normalisation is an optional plug-in for post-processing messages. A further issue related to noisy tokens is that it is possible that a relevant tweet might contain a variant of a query term, but not that query term itself. In future versions of the system we therefore aim to use query expansion to generate noisy versions of query terms to retrieve additional relevant tweets. We subsequently intend to perform lexical normalisation to evaluate the precision of the returned data. The present lexical normalisation used by our system is the dictionary lookup method of Han and Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). Ultimately, however, we are interested in performing contextsensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). This method will allow us to target a wider variety of noisy tokens such as typos (e.g. earthquak “earthquake”), abbreviations (e.g. lv “love”), phonetic substitutions (e.g. b4 “before”) and vowel lengthening (e.g. goooood “good”). 3.4 Geolocation A vital component of event detection is the determination of where the event is h"
E12-2014,I11-1062,1,0.814867,"led and asynchronous. Below, we describe details of the various modules of the system. 3.1 Twitter Querying When the user inputs a set of keywords, this is issued as a disjunctive query to the Twitter Streaming API, which returns a streamed set of results in JSON format. The results are parsed, and piped through to the language filtering, lexical normalisation, and geolocation modules, and finally stored in a flat file, which the GUI interacts with. 3.2 Language Filtering For language identification, we use langid.py, a language identification toolkit developed at The University of Melbourne (Lui and Baldwin, 2011).3 langid.py combines a naive Bayes classifier with cross-domain feature selection to provide domain-independent language identification. It is available under a FOSS license as a stand-alone module pre-trained over 97 languages. langid.py has been developed specifically to be able to keep pace with the speed of messages through the Twitter “garden hose” feed on a single-CPU machine, making it particularly attractive for this project. Additionally, in an in-house evaluation over three separate corpora of Twitter data, we have found langid.py to be overall more accurate than other state-ofthe-a"
E12-2014,D11-1141,0,0.0183463,"ion, by combining the total set of messages from a given user into a single combined message. Given a message m, the task is to find arg maxi P (loci |m) where each loci is a grid cell on the map. Based on Bayes’ theorem and standard assumptions in the naive Bayes formulation, this is transformed into: arg max P (loci ) i v Y P (wj |loci ) j To avoid zero probabilities, we only consider tokens that occur at least twice in the training data, and ignore unseen words. A probability is calculated for the most-probable location by normalising over the scores for each loci . We employ the method of Ritter et al. (2011) to tokenise messages, and use token unigrams as features, including any hashtags, but ignoring twitter mentions, URLs and purely numeric tokens. We 6 Alternatively, we could consider a hybrid approach of user- and message-level geolocation prediction, especially for users where we have sufficient training data, which we plan to incorporate into a future version of the system. 71 0.40 Prediction Accuracy 0.35 • the probability of the predicated geolocation ● ● ● • the text of the tweet ● ● ● ● 0.30 ● ● ● ● 0.15 0.20 0.25 ● 10000 20000 30000 40000 Feature Number Figure 2: Accuracy of geolocatio"
E14-1047,N12-1017,0,0.0150144,"if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual human-targeted reference translations which are then used as the basis for computing the translation error rate. HyTER, on the other hand, is a tool that facilitates creation of very large numbers of reference translations (Dreyer and Marcu, 2012). Although both approaches increase fairness compared to automatic metrics that use a single generic reference translation, even human post-editors will inevitably vary in the way they post-edit translations, and the process of creating even a single new reference transRecent human evaluation of machine translation has focused on relative preference judgments of translation quality, making it difficult to track longitudinal improvements over time. We carry out a large-scale crowd-sourcing experiment to estimate the degree to which state-of-theart performance in machine translation has increase"
E14-1047,J08-4004,0,0.0485306,"human assessor to specify how strongly they agree or disagree with that statement. The scale and labels can then be held constant across experimental set-ups for all attributes evaluated – meaning that if the scale is still biased in some way it will be equally so across all set-ups. 3 Assessor Consistency One way of estimating the quality of a human evaluation regime is to measure its consistency: whether or not the same outcome is achieved if the same question is asked a second time. In MT, annotator consistency is commonly measured using Cohen’s kappa coefficient, or some variant thereof (Artstein and Poesio, 2008). Originally developed as a means of establishing assessor independence, it is now commonly used in the reverse sense, with high numeric values being used as evidence of agreement. Two different measurements can be made – whether a judge is consistent with other judgments performed by themselves (intraannotator agreement), and whether a judge is consistent with other judges (inter-annotator agreement). Cohen’s kappa is intended for use with categor2 This dimension of the assessment is similar but not identical to the monolingual adequacy assessment in early NIST evaluation campaigns (NIST, 200"
E14-1047,W13-2305,1,0.581936,"is presented with a set of judgments for translations from two systems, one of which is known to produce better translations than the other, the mean score for the better system will be significantly higher than that of the inferior system. Assumption B is the basis of our quality-control mechanism, and allows us to distinguish between Turkers who are working carefully and those who are merely going through the motions. We use a 100-judgment HIT structure to control same-judge repeat items and deliberately-degraded system outputs (bad reference items) used for workerintrinsic quality control (Graham et al., 2013). bad reference translations for fluency judgments are created as follows: two words in the translation are randomly selected and randomly re-inserted elsewhere in the sentence (but not as the initial or final words of the sentence). Since adding duplicate words will not degrade adequacy in the same way, we use an alternate method to create bad reference items for adequacy judgments: we randomly delete a short sub-string of length proportional to the length of the original translation to emulate a missing phrase. Since 446 Si 70 randomly selected system outputs made up of roughly equal proport"
E14-1047,W05-0909,0,0.0473555,"r each language pair, a 100-translation HIT was constructed by randomly selecting translations from the pool of (3003 + 2007) × 2 that were available, and this results in apparently fewer assessments for the 2007 test set. In fact, numbers of evaluated translations are relative to the size of each test set. Average z scores for each system are also presented, based on the mean and standard deviation of all assessments provided by an individual worker, with positive values representing deviations above the mean of workers. In addition, we include mean B LEU (Papineni et al., 2001) and M ETEOR (Banerjee and Lavie, 2005) automatic scores for the same system outputs. The C URR benchmark shows fluency scores that are 5.9 points higher on the 2007 data set than they are on the 2012 test data, with a larger difference in adequacy of 8.3 points. As such, the 2012 test data is more challenging than the 2007 test data. Despite this, both fluency and adequacy scores for the best system in 2012 have increased by 4.5 and 2.0 points respectively, amounting to estimated average gains of 10.4 points in fluency and 10.3 points in adequacy for state-of-the-art MT across the seven language pairs. Looking at the standardized"
E14-1047,P11-1023,0,0.0255157,"Missing"
E14-1047,W13-2203,0,0.0479048,"m for human evaluation is WMT shared tasks, where assessments have (since 2007) taken the form of ranking five alternate system outputs from best to worst (Bojar et al., 2013). This method has been shown to produce more consistent judgments compared to fluency and adequacy judgments on a five-point scale (CallisonBurch et al., 2007). However, relative preference judgments have been criticized for being a simplification of the real differences between translations, not sufficiently taking into account the large number of different types of errors of varying severity that occur in translations (Birch et al., 2013). Relative preference judgments do not take into account the degree to which one translation is better than another – there is no way of knowing if a winning system produces far better translations than all other systems, or if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual"
E14-1047,W11-2101,0,0.0426779,"Missing"
E14-1047,2001.mtsummit-papers.68,0,0.0585449,"o test sets (C URR07 , C URR12 ). For each language pair, a 100-translation HIT was constructed by randomly selecting translations from the pool of (3003 + 2007) × 2 that were available, and this results in apparently fewer assessments for the 2007 test set. In fact, numbers of evaluated translations are relative to the size of each test set. Average z scores for each system are also presented, based on the mean and standard deviation of all assessments provided by an individual worker, with positive values representing deviations above the mean of workers. In addition, we include mean B LEU (Papineni et al., 2001) and M ETEOR (Banerjee and Lavie, 2005) automatic scores for the same system outputs. The C URR benchmark shows fluency scores that are 5.9 points higher on the 2007 data set than they are on the 2012 test data, with a larger difference in adequacy of 8.3 points. As such, the 2012 test data is more challenging than the 2007 test data. Despite this, both fluency and adequacy scores for the best system in 2012 have increased by 4.5 and 2.0 points respectively, amounting to estimated average gains of 10.4 points in fluency and 10.3 points in adequacy for state-of-the-art MT across the seven langu"
E14-1047,W07-0718,0,0.365461,"ly qualified workers. With this set-up in place for adequacy, we also re-introduce a fluency assessment. Fluency ratings can be carried out without the presence of a reference translation, reducing any remnant bias towards reference translations in the evaluation setup. That is, we propose a judgment regime in which each task is presented as a two-item fluency and adequacy judgment, evaluated separately, and with adequacy restructured into a monolingual “similarity of meaning” task. When fluency and adequacy were originally used for human evaluation, each rating used a 5point adjective scale (Callison-Burch et al., 2007). However, adjectival scale labels are problematic and ratings have been shown to be highly dependent on the exact wording of descriptors (Seymour et al., 1985). Alexandrov (2010) provides a summary of the extensive problems associated with the use of adjectival scale labels, including bias resulting from positively- and negatively-worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings. It is often the case, however, that the question could be restructured so that the rating scale no longer require"
E14-1047,2006.amta-papers.25,0,0.208879,"erent types of errors of varying severity that occur in translations (Birch et al., 2013). Relative preference judgments do not take into account the degree to which one translation is better than another – there is no way of knowing if a winning system produces far better translations than all other systems, or if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual human-targeted reference translations which are then used as the basis for computing the translation error rate. HyTER, on the other hand, is a tool that facilitates creation of very large numbers of reference translations (Dreyer and Marcu, 2012). Although both approaches increase fairness compared to automatic metrics that use a single generic reference translation, even human post-editors will inevitably vary in the way they post-edit translations, and the process of creating even a single new reference transRecent human e"
E14-1047,W12-3102,0,\N,Missing
E14-1047,P02-1040,0,\N,Missing
E14-1047,W13-2201,0,\N,Missing
E14-1050,W11-0815,0,0.161446,"Missing"
E14-1050,E03-1073,0,0.0405212,"ce language distributional similarity, but when combined with Results All experiments are carried out using 10 iterations of 10-fold cross validation, randomly partitioning the data independently on each of the 10 iterations, and averaging across all 100 test partitions in our presented results. In the case of CS L2N and other methods that make use of it (i.e. CS L1 +L2N and CS all ), the languages selected for a given training fold are then used to compute the compositionality scores for the instances in the test set. Figures 3a, 3b and 3c are histograms of the number of times 7 Although see Lapata and Lascarides (2003) for discussion of the difficulty of reliably identifying low-frequency English noun compounds. 477 25 20 18 20 16 Frequency Frequency 14 15 10 12 10 8 6 5 4 2 0 0 5 10 15 20 0 0 25 5 10 bestN 15 20 25 best N (a) ENC (b) EVPC 20 18 16 Frequency 14 12 10 8 6 4 2 0 0 5 10 15 20 25 best N (c) GNC Figure 3: Histograms displaying how many times a given N is selected as the best number of languages over each dataset. For example, according to the GNC chart, there is a peak for N = 2, which shows that over 100 folds, the best-2 languages achieved the highest correlation on 18 folds. Method CS L1 CS L"
E14-1050,W03-1812,1,0.673121,"ither the tokenlevel (over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts. Identifying token instances of MWEs is not always easy, especially when the component words do not occur sequentially. For"
E14-1050,P99-1041,0,0.0380202,"te-of-the-art results over two datasets. 2 Related Work Most recent work on predicting the compositionality of MWEs can be divided into two categories: language/construction-specific and general-purpose. This can be at either the tokenlevel (over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its compone"
E14-1050,C10-3010,1,0.556517,"Missing"
E14-1050,W03-1810,0,0.829157,"Missing"
E14-1050,W03-1809,1,0.715081,"160 English verb particle constructions (VPCs), from the work of Bannard (2006). In this dataset, a verb particle construction consists of a verb (the head) and a prepositional particle (e.g. hand in, look up or battle on). For each component word (the verb and particle, respectively), multiple annotators were asked whether the VPC entails the component word. In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC. That is, following Bannard et al. (2003), we only consider the compositionality of the verb component in our experiments (and as such α = 1 in Equation 1). One area of particular interest with this dataset will be the robustness of the method to function words (the particles), both under translation and in terms of calculating distributional similarity, although the findings of Baldwin (2006) for English prepositions are at least encouraging in this respect. Additionally, English VPCs can occur in “split” form (e.g. put your jacket on, from our earlier example), which will complicate identification, and the verb component will often"
E14-1050,D13-1060,0,0.0109709,"ing MWE “identification” to be a token-level disambiguation task, and MWE “extraction” to be a type-level lexicon induction task. 472 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472–481, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics tional scores may not be reliable. Additionally, for morphologically-rich languages, it can be difficult to predict the different word forms a given MWE type will occur across, posing a challenge for our requirement of no language-specific preprocessing. Pichotta and DeNero (2013) proposed a tokenbased method for identifying English phrasal verbs based on parallel corpora for 50 languages. They show that they can identify phrasal verbs better when they combine information from multiple languages, in addition to the information they get from a monolingual corpus. This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction. Having said that, the general applicability of the method is questionable — there are many parallel corpora involving English,"
E14-1050,W12-3311,0,0.167418,"Missing"
E14-1050,N10-1029,0,0.0743601,"Missing"
E14-1050,I11-1024,0,0.72515,"over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts. Identifying token instances of MWEs is not always easy, especially when the component words do not occur sequentially. For example consider put"
E14-1050,W03-1806,0,0.0157759,"Missing"
E14-1050,S13-1039,1,0.760884,"rasal verbs based on parallel corpora for 50 languages. They show that they can identify phrasal verbs better when they combine information from multiple languages, in addition to the information they get from a monolingual corpus. This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction. Having said that, the general applicability of the method is questionable — there are many parallel corpora involving English, but for other languages, this tends not to be the case. Salehi and Cook (2013) proposed a generalpurpose type-based approach using translation data from multiple languages, and string similarity between the MWE and each of the component words. They use training data to identify the best-10 languages for a given family of MWEs, on which to base the string similarity, and once again find that translation data improves their results substantially. Among the four string similarity measures they experimented with, longest common substring was found to perform best. Their proposed method is general and applicable to different families of MWEs in different languages. In this p"
E14-1050,W01-0513,0,0.131714,"and German noun compounds. We show that the estimation of compositionality is improved when using translations into multiple languages, as compared to simply using distributional similarity in the source language. We further find that string similarity complements distributional similarity. 1 Compositionality of MWEs Multiword expressions (hereafter MWEs) are combinations of words which are lexically, syntactically, semantically or statistically idiosyncratic (Sag et al., 2002; Baldwin and Kim, 2009). Much research has been carried out on the extraction and identification of MWEs1 in English (Schone and Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009) and other languages (Dias, 2003; Evert and Krenn, 2005; Salehi et al., 2012). However, considerably less work has addressed the task of predicting the meaning of MWEs, especially in non-English languages. As a step in this direction, the focus of this study is on predicting the compositionality of MWEs. An MWE is fully compositional if its meaning is predictable from its component words, and it is non-compositional (or idiomatic) if not. For example, stand up “rise to one’s feet” is composi1 In this paper, we follow Baldwin and Kim (2009) in considering MWE"
E14-1050,S13-1038,0,0.403379,"Missing"
E14-1056,W13-0102,0,0.849832,"d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. (2010) introduced the notion of topic “coherence”, a"
E14-1056,C10-2069,1,0.630341,"1.2 million New York Times articles from 1994 to 2004 (from the English Gigaword); and (2) W IKI -F ULL, which contains 3.3 million English Wikipedia articles (retrieved November 28th 2009).2 The rationale for choosing the New York Times and English Wikipedia as the reference corpora is to ensure domain consistency with the word intrusion dataset; the full collections are used to more robustly estimate lexical probabilities. 4 task is that it requires human annotations, therefore preventing large-scale evaluation. We begin by proposing a methodology to fully automate the word intrusion task. Lau et al. (2010) proposed a methodology that learns the most representative or best topic word that summarises the semantics of the topic. Observing that the word intrusion task — the task of detecting the least representative word — is the converse of the best topic word selection task, we adapt their methodology to automatically identify the intruder word for the word intrusion task, based on the knowledge that there is a unique intruder word per topic. The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the topN topic word"
E14-1056,N13-1016,0,0.19099,"d found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the counts of all associated words before and after the sampling of a new topic assignment in the Gibbs sampler. This variant of topic model was shown to produce more coherent topics than LDA based on the log conditional probability coherence measure. Aletras and Stevenson (2013a) introduced distributional semantic similarity methods for computing coherence, calculating the distributional similarity between semantic vectors for the top-N topic words using a range of distributional similarity measures such as cosine similarity and the Dice coefficient. To construct the semantic vector space for the topic words, they used English Wikipedia as the reference corpus, and collected words that co-occur in a window of ±5 words. They showed that their method correlates well with the observed coherence rated by human judges. (2010) introduced the notion of topic “coherence”, a"
E14-1056,P11-1154,1,0.788243,"ic method for estimating topic coherence based on pairwise pointwise mutual information (PMI) between the topic words. Mimno et al. (2011) similarly introduced a methodology for computing coherence, replacing PMI with log conditional probability. Musat et al. (2011) incorporated the WordNet hierarchy to capture the relevance of topics, and in Aletras and Stevenson (2013a), the authors proposed the use of distributional similarity for computing the pairwise association of the topic words. One application of these methods has been to remove incoherent topics before generating labels for topics (Lau et al., 2011; Aletras and Stevenson, 2013b). Ultimately, all these methodologies, and also the word intrusion approach, attempt to assess the same quality: the human-interpretability of topics. The relationship between these methodologies, however, is poorly understood, and there is no consensus on what is the best approach for computing the semantic interpretability of topic models. This is a second contribution of this paper: we perform a systematic empirical comparison of the different methods and find appreciable differences between them. We further go on to propose an improved formulation of Newman e"
E14-1056,C12-1093,1,0.34226,"(2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2"
E14-1056,E12-1060,1,0.435095,"(2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2"
E14-1056,N09-1041,0,0.0355475,"valuation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics word that does not belong to the topic in question. Newman e"
E14-1056,D11-1024,0,0.908556,"t al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10 topic words) on a 3-point scale based on how coherent the topic words are (i.e. their observed coherence). They proposed several ways of automating the estimation of the observed coherence, and ultimately found that a simple method based on PMI term co-occurrence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al. (2011) proposed a closely-related method for evaluating semantic coherence, replacing PMI with log conditional probability. Rather than using Wikipedia for sampling the word cooccurrence counts, Mimno et al. (2011) used the topic-modelled documents, and found that their measure correlates well with human judgements of observed coherence (where topics were rated in the same manner as Newman et al. (2010), based on a 3-point ordinal scale). To incorporate the evaluation of semantic coherence into the topic model, the authors proposed to record words that co-occur together frequently, and update the co"
E14-1056,D08-1038,0,0.0958689,"nstration that we can automate the method of Chang et al. (2009) at near-human levels of accuracy, as a result of which we can perform automatic evaluation of the human-interpretability of topics, as well as topic models. There has been prior work to directly estimate the human-interpretability of topics through automatic means. For example, Newman et al. Introduction Topic modelling based on Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and related methods is increasingly being used in user-focused tasks, in contexts such as the evaluation of scientific impact (McCallum et al., 2006; Hall et al., 2008), trend analysis (Bolelli et al., 2009; Lau et al., 2012a) and document search (Wang et al., 2007). The LDA model is based on the assumption that document collections have latent topics, in the form of a multinomial distribution of words, which is typically presented to users via its top-N highestprobability words. In NLP, topic models are generally used as a means of preprocessing a document collection, and the topics and per-document topic allocations are fed into downstream applications such as document summarisation (Haghighi and Vanderwende, 2009), novel word sense detection methods (Lau"
E14-1056,N10-1012,1,0.862842,"e, 2009), novel word sense detection methods (Lau et al., 2012b) and machine translation (Zhao and Xing, 2007). In fields such as the digital humanities, on the other hand, human users interact directly with the output of topic models. It is this context of topic modelling for direct human consumption that we target in this paper. 530 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530–539, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics word that does not belong to the topic in question. Newman et al. (2010) capture topic interpretability using a more direct approach, by asking human users to rate topics (represented by their top-10 topic words) on a 3-point scale based on how coherent the topic words are (i.e. their observed coherence). They proposed several ways of automating the estimation of the observed coherence, and ultimately found that a simple method based on PMI term co-occurrence within a sliding context window over English Wikipedia produces the consistently best result, nearing levels of interannotator agreement over topics learnt from two distinct document collections. Mimno et al."
E14-4042,I13-1041,1,0.827887,"ultiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is that there are strong user-level lexical semantic priors in Twitter, equivalent in strength to document-level 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, c Got"
E14-4042,S13-2049,0,0.0833979,"word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised firs"
E14-4042,2011.mtsummit-papers.59,0,0.0862472,"Missing"
E14-4042,H05-1053,0,0.11944,"ul in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational natu"
E14-4042,N13-1121,0,0.0640041,"Missing"
E14-4042,C12-1093,1,0.84043,"OLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is th"
E14-4042,P12-3005,1,0.763775,"er text. 3 case form match rule charge function panel sign deal issue paper track Table 1: The 20 target nouns used in this research 3.1 Data Sampling We sampled tweets from a crawl made using the Twitter Streaming API from January 3, 2012 to February 29, 2012. The web corpus was built from ukWaC (Ferraresi et al., 2008), which was based on a crawl of the .uk domain from 2007. In contrast to ukWaC, the tweets are not restricted to documents from any particular country. For both corpora, we first selected only the English documents using langid.py, an off-theshelf language identification tool (Lui and Baldwin, 2012). We next identified documents which contained nominal usages of the target words, based on the POS tags supplied with the corpus in the case of ukWaC, and the output of the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) in the case of Twitter. For Twitter, we are interested in not just the overall lexical distribution of each target noun, but also per-user lexical distributions. As such, we construct two Twitter-based datasets: (1) T WITTER RAND , a random sample of 100 usages of each target noun; and (2) T WITTER USER , 5 usages of each target noun from each member of a random sample"
E14-4042,P09-1002,0,0.161799,"Missing"
E14-4042,U12-1006,1,0.822745,"Missing"
E14-4042,J07-4005,0,0.0941266,"Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identifie"
E14-4042,W04-0807,0,0.0528238,"Missing"
E14-4042,H92-1045,0,0.674404,"ristic (Gale et al., 1992). This has potential implications for future applications over Twitter which attempt to move beyond a simple string-based meaning representation to explicit lexical semantic analysis. In recent years, microblogs such as Twitter have emerged as a new communication channel. Twitter in particular has become the target of a myriad of content-based applications including trend analysis and event detection, but there has been little fundamental work on the analysis of word usage patterns in this text type. In this paper — inspired by the one-sense-perdiscourse heuristic of Gale et al. (1992) — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven"
E14-4042,S13-2035,0,0.0612337,"d predominantly to comment on a favourite sports team or political events, and as such is domain-driven. Alternatively, it can perhaps be explained by the “reactive” nature of Twitter, in that posts are often emotive responses to happenings in a user’s life, and while different things excite different individuals, a given individual will tend to be excited by events of similar kinds. Clearly more research is required to test these hypotheses. One highly promising direction for this research would be to overlay analysis of sense distributions with analysis of user profiles (e.g. Bergsma et al. (2013)), and test the impact of geospatial and sociolinguistic factors on sense preferences. We would also like to consider the impact of time on the one sense per tweeter heuristic, and consider whether “one sense per Twitter conversation” also holds. To summarise, we have investigated sense distributions in Twitter and a general web corpus, over both a random sample of usages and a sample of usages from a single user/document. We found strong evidence for Twitter users to use a given word with a single sense, and also that individual first sense preferences differ between users, suggesting that me"
E14-4042,S13-1036,1,0.890686,"Missing"
E14-4042,S07-1006,0,0.0735875,"on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007)"
E14-4042,N09-4006,0,0.0347805,"Missing"
E14-4042,W04-2807,0,0.529119,"ense-tagged lexical sample dataset based on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve"
E14-4042,N10-1021,0,0.110433,"Missing"
E14-4042,N06-2015,0,\N,Missing
E17-2004,P06-1048,0,0.0136187,"pression is based on deep learning methods such as recurrent neural networks (Filippova et al., 2015), we implement a simple parser-based model, due to the lack of large-scale annotated data for training and the fact that a relative lack of precision in the output may ultimately help our method. First, we parse the sentence using the Stanford CoreNLP constituency parser (Chen and Manning, 2014). Next, we model the conditional probability of deleting a sub-tree C with label S given its parent node with label R by p(C|S, R) = p(C,S,R) ΣC p(C,S,R) , trained on the sentence compression corpora of Clarke and Lapata (2006),4 made up of a few hundred labelled instances. Generating Text Noise Our method involves the explicit generation of several kinds of linguistic corruption, to train more robust deep models. The first question is how to generate the linguistic noise, focusing on English for the purposes of this paper. We focus on the generation of two classes of text noise: (1) syntactic noise; and (2) semantic noise.2 Syntactic Noise The first class of linguistic noise is syntactic, focusing on the syntactic strucSemantic Noise The second class of linguistic noise is semantic noise. Semantic noise is more sub"
E17-2004,P14-1062,0,0.00307318,"sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to train models to be more robust, such as injecting noise into the data and hidden layers (Jiang et al., 2009), dropout (Srivastava et al., 2014), and the incorporation of explicit regularization terms into the training objective (Ng, 2004; Li et al., 2016). In this work, we propose a linguisticallymotivated method customised to text applications, based on in"
E17-2004,copestake-flickinger-2000-open,0,0.0656511,"eration of linguistic corruption over the source training instances, to train robust text models. Empirically, we demonstrate the effectiveness of our method over a range of sentiment analysis datasets using a state-of-the-art convolutional neural network model (Kim, 2014). In this, we show that our method is superior to a baseline and dropout (Srivastava et al., 2014) using MAP training.1 2 ture of the input, either through explicit parsing and generation using a deep linguistic parser, or sentence compression. For the deep linguistic parser, we use the LinGO English Resource Grammar (“ERG”: Copestake and Flickinger (2000)) with the ACE parser, based on pyDelphin.3 The ERG supports both parsing and generation, via the semantic formalism of Minimal Recursion Semantics (“MRS”: Copestake et al. (2005)). To generate paraphrases with the ERG, we simply parse a given input, select the preferred parse using a pretrained parse selection model (Oepen et al., 2002), and exhaustively generate from the resultant MRS. We then use uniform random sampling to select from the generator outputs, which potentially numbers in the thousands of variants. To handle unknown words during parsing and generation, we use POS mapping and i"
E17-2004,D14-1181,0,0.00379593,"example sentence and sample corrupted outputs after applying each type of linguistic noise. The ERG seldom changes words, and instead tends to reorder the words based on syntactic alternation. C OMP performs like word dropout in that it tends to remove tokens with low semantic content and to generate complete sentences. WN and CF IT both only modify the text at the word level, based on near-synonyms and words with similar semantic function, respectively. 3 Models and Training We evaluate our methods on several sentence classification tasks, using a convolutional neural network (“CNN”) model (Kim, 2014). Note that our method corrupts the input directly, and is thus easily transferrable to other classes of models (e.g., other deep learning or linear models). We experiment with two approaches to generating the substitution candidates. The first is based on Princeton WordNet (“WN”: Miller et al. (1990)), over all synsets that a given substitutable word occurs in, using the NLTK API (Bird, 2006). The second is based on the “counterfitting” method of Mrkˇsi´c et al. (2016) (“CF IT”), whereby word embeddings from WORD 2 VEC are projected based on a supervised objective function which penalises sim"
E17-2004,N13-1037,0,0.0301969,"Missing"
E17-2004,D15-1042,0,0.0106324,"Missing"
E17-2004,P05-1045,0,0.00440533,"urces/ 4 22 not to impact on the fidelity of the original labels, which can readily occur with full paraphrasing or abstractive summarisation. As such, we focus on lexical substitution of near-synonyms of words in the original text, and experiment with two methods for generating near-synonyms. Our approach to generating semantic noise proceeds as follows. First, we apply filters to identify words which should not be candidates for lexical substitution, namely words which are parts of named entities or function words. As such, we use the Stanford CoreNLP POS tagger and named entity recogniser (Finkel et al., 2005; Chen and Manning, 2014), and identify “substitutable words” as those which are nouns, verbs, adjectives or adverbs, and not part of a named entity. For each substitutable word w, we generate the set of substitution candidates s(w). For each candidate wi ∈ {w} ∪ s(w) we allow the original word to be preserved with p(wi ) = α, and share the remaining 1−α proportional to the language model score based on substituting wi into the original text. For this, we use the pre-trained US English language model from the CMU Sphinx Speech Recognition toolkit.5 Finally, we sample from the probability distr"
E17-2004,D16-1207,1,0.901542,"Missing"
E17-2004,N16-1018,0,0.0151929,"Missing"
E17-2004,P11-1038,1,0.822346,"Missing"
E17-2004,C02-2025,0,0.0401639,"Missing"
E17-2004,P05-1015,0,0.0079222,"o thresholds on the random variable for substitution of each word: low (“lo”; α = 0.5) and high (“hi”; α = 0). Besides the above methods which employ a single type noise, we experiment with a combination (C OMB) of the four different noise types (ERG + C OMP + WNlo + CF ITlo ), by uniformly randomly choosing one of the four methods for noise generation each time we process a training instance. Datasets We experiment on the following datasets: • MR: sentence polarity dataset from movie reviews (Pang and Lee, 2008)8 • CR: customer review dataset (Hu and Liu, 2004)9 • Subj: subjectivity dataset (Pang and Lee, 2005)8 • SST: Stanford Sentiment Treebank, using the 2-class configuration (Socher et al., 2013)10 We evaluate using classification accuracy, based on both in-domain evaluation11 and a crossdomain setting, in which we evaluate a model trained on MR and tested on CR, and vice versa. This last setting characterises a realistic applica7 Using a single application of noise is less effective, but still yields improvements over baseline methods including dropout. 8 https://www.cs.cornell.edu/people/ pabo/movie-review-data/ 9 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 10 http://nlp.stanford."
E17-2004,P16-2024,0,0.00700087,"Missing"
E17-2004,D14-1162,0,0.116561,"Missing"
E17-2004,P05-1044,0,0.0610564,"rk on adversarial training (Goodfellow et al., 2015). This kind of noise is cheap to generate for images and is transferable between different models, but it is less clear how to generate analogous textual noise while preserving the fidelity of the training data, due to text being discrete and sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to train models to be more robust, such as injecting noise into t"
E17-2004,D13-1170,0,0.00311578,"Missing"
E17-2004,P13-2113,0,0.0816002,"an observation that has been harnessed in recent work on adversarial training (Goodfellow et al., 2015). This kind of noise is cheap to generate for images and is transferable between different models, but it is less clear how to generate analogous textual noise while preserving the fidelity of the training data, due to text being discrete and sequential in nature, with latent syntactic structure. Based on the same linguistic intuition, adversarial evaluation for natural language processing models was proposed by Smith (2012). Also, adversarial learning for text, such as perceptron learning (Søgaard, 2013) and unsupervised estimation methods (Smith and Eisner, 2005), have been studied in the language area. Introduction Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Bitvai and Cohn, 2015). However, deep models tend to be overconfident in their predictions over noisy test instances, including adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). A range of methods have been proposed to t"
E17-2004,P06-4018,0,\N,Missing
E17-2004,D14-1082,0,\N,Missing
E17-2004,I13-1041,1,\N,Missing
E17-2019,K15-1017,1,0.899754,"Missing"
E17-2019,N16-1080,1,0.899654,"Missing"
E17-2019,P15-1033,0,0.0145967,"), as well as other nominalisations (e.g., explain 7→ explanation). Nominallemma <s> environmental { d e v a s t a t e } contextR contextL guages such as English where grapheme-phoneme correspondences are opaque. For this reason we consider orthographic rather than phonological representations. In our approach, we test how well models incorporating distributional semantics can capture derivational transformations. Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2015). Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch¨utze, 2016; Cotterell et al., 2016a). is best exemplified by ... </s> output generation ... encoding d e v a s t a t e } } } d e v a s t a t i o n } Figure 1: The encoder–decoder model, showing the stem devastate in context producing the form devastation. Coloured arrows indicate shared parameters isations have varyingly different meanings from their base verbs, and a key focus of this study is the prediction of which form is most approp"
E17-2019,N16-2002,0,0.0171762,"d a key focus of this study is the prediction of which form is most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (20"
E17-2019,W11-0115,0,0.0312045,"6a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The"
E17-2019,N16-1149,1,0.837461,"m, to bias the model to generate a derived form that is morphologically-related to the base 120 baseline biLSTM+BS biLSTM+CTX biLSTM+CTX+BS biLSTM+CTX+BS+POS LSTM+CTX+BS+POS Shared Split 0.63 0.58 0.80 0.83 0.89 0.90 — 0.36 0.45 0.52 0.63 0.66 Table 1: Accuracy for predicted lemmas (bases and derivations) on shared and split lexicons verb. In most cases, the derived form is longer than its stem, and accordingly, when we reach the end of the base form, we continue to input an end-of-word symbol. We provide the model with the context vector o at each decoding step. It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.4 Finally, we use max pooling to enable the model to switch between copying of a stem or producing a new character. 5.3 Settings We used a 3-layer bidirectional LSTM network, with hidden dimensionality h for both context and base-form stem states of 100, and character embedding cj of 100.5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b). During the training of the model, we keep the word embeddings fixed, for greater applicability to unseen test instances. All token"
E17-2019,P16-2090,0,0.0804887,"Missing"
E17-2019,W15-0108,0,0.11611,"Missing"
E17-2019,P13-1149,0,0.0775424,"ther using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al. 119 (2015) and Pad´o et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The third area of research focuses on the analysis of derivationally complex forms, which differs from this study in that we focus on generation. The goal o"
E17-2019,P96-1004,0,0.165816,"ology is the set of processes through which the word form outwardly displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs and ran, all of which convey the concept “moving by foot quickly”, but appear in complementary syntactic contexts. Derivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness. In this work, we deal with the formation of deverbal nouns, i.e., nouns that are formed from verbs. Common examples of this in English include agentives (e.g., explain 7→ explainer), gerunds (e.g., explain 7→ explaining"
E17-2019,C16-1122,0,0.137298,"Missing"
E17-2019,P16-1158,1,0.668374,"most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems. 3 Related Work Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a “derivational” shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch¨utze (2017) represent derivational affixes as vectors and investigate v"
E17-2057,N15-1124,1,0.941061,"slation quality and the actual quality of translated documents. Including such weights in the construction of a gold standard potentially invalidates the human evaluation, and is unfortunately very likely to exaggerate the apparent performance of some systems while under-rewarding others. 357 0.2 0.1 3 0.0 −0.2 0 10 20 30 40 Alternate Human Gold Standard A recent development in human evaluation of MT is direct assessment (“DA”), a human assessment shown to yield highly replicable segment-level scores, by combination of a minimum of 15 repeat human assessments per translation into mean scores (Graham et al., 2015). Human adequacy assessments are collected via a 0–100 rating scale that facilitates reliable quality control of crowd-sourcing. Document-level DA scores are computed by repeat assessment of the individual segments within a given document, computation of the mean score for each segment (micro-average), and finally, combination of the mean segment scores into an overall mean document score (macro-average).2 DA assessments are carried out by comparison of a given MT output segment (rendered in black) with a human-generated reference translation (in gray), and human annotators rate the degree to"
E17-2057,C16-1294,1,0.865252,"hree participating sys5 Post-editing cost estimates are based on 0.06 and 0.12 Euro per source document word converted to USD$. Further details provided by the post-editor in relation to estimates can be found at https://github.com/ygraham/ eacl2017 4 Variance in numbers of repeat assessments per document is due to sentences of all documents being sampled without preference for documents made up of larger numbers of sentences. 359 RTM-FS+PLS-TREE GRAPH-DISC BASE-EMB-GP BASELINE RTM-FS-SVR DA WMT-16 0.38 0.32 0.31 0.26 0.23 0.36 0.26 0.39 0.29 0.29 it in document-level QE evaluation therefore. Graham et al. (2016a) provide an investigation into reference bias in monolingual evaluation of MT and despite the risk of reference bias that DA adequacy could potentially encounter, experiment results show no evidence of reference bias. Human assessors of MT appear to genuinely read and compare the meaning of the reference translation and the MT output, as requested with DA, applying their human intelligence to the task in a reliable way, and are not overly influenced by the generic reference. Although DA fluency could still have its own applications, for the purpose of evaluating MT or MT QE, this additional"
E17-2057,P15-1174,1,0.837171,"ments (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison"
E17-2057,E06-1032,0,0.076271,"m (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiabl"
E17-2057,W07-0718,0,0.0579243,"proving Evaluation of Document-level Machine Translation Quality Estimation Yvette Graham Dublin City University Qingsong Ma Chinese Academy of Sciences Timothy Baldwin University of Melbourne yvette.graham@dcu.ie maqingsong@ict.ac.cn tb@ldwin.net Qun Liu Dublin City University Carla Parra Dublin City University Carolina Scarton University of Sheffield qun.liu@dcu.ie carla.parra@adaptcentre.ie c.scarton@sheffield.ac.uk Abstract subjective, making high IAA difficult to achieve. For example, in past large-scale human evaluations of MT, low IAA levels have been highlighted as a cause of concern (Callison-Burch et al., 2007; Bojar et al., 2016). Such problems cause challenges not only for evaluation of MT systems, but also for MT quality estimation (QE), where the ideal gold standard comprises human assessment. Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weight"
E17-2057,2006.amta-papers.25,0,0.11857,"ely be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiable when more context is available, to produce post-edition 2 (P E2 ). Next, two translation edit rate (TER) scores were computed by: (1) comparing the document-level MT outp"
E17-2057,W11-2107,0,0.0349009,"machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, s"
E17-2057,P10-1063,0,0.0362197,"(IAA) enable the likelihood of replicability to be taken into account, were an evaluation to be repeated with a distinct set of human annotators. One approach to achieving high IAA is through the development of a strict set of annotation guidelines, while for machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system"
E17-2057,D14-1020,1,0.877386,"ct assessment (DA) and original gold standard (WMT-16 QE English to Spanish) tems, while under-rewarding two other systems. Notably, system GRAPH-DISC, which includes discourse features learned from document-level features, achieves a higher correlation when evaluated with DA compared to the original gold standard. Differences in correlations are small, however, and can’t be interpreted as differences in performance without significance testing. Differences in dependent correlations showed no significant difference for all pairs of competing systems according to Williams test (Williams, 1959; Graham and Baldwin, 2014). 3.3 4 Conclusion Methodological concerns were raised with respect to optimization of weights employed in construction of document-level QE gold standards in WMT-16. We demonstrated the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard. Experiments showed with respect to the alternate gold standard we propose, direct assessment (DA), scores for documents are highly reliable, achieving a correlation of above 0.9 in a self-replication experiment. Finally, DA resulted in a substantial estimated cost reduction, with the original post-edi"
E17-2057,W13-2305,1,0.877135,"by qualitycontrolled crowd-sourcing in two separate data collection runs (Runs A and B) on Mechanical Turk, and compare scores for individual documents collected in each run. Quality control is carried out by inclusion of pairs of genuine MT outputs and automatically degraded versions of them (bad references) within 100-translation HITs, before a difference of means significance test is applied to the ratings belonging to a given worker. The resulting p-value is employed as an estimate of the reliability of a given human assessor to accurately distinguish between the quality of translations (Graham et al., 2013; Graham et al., 2014). Table 1 shows numbers of judgments collected in total for each data collection run on Mechanical Turk, including numbers of assessments before and after quality control filtering, where only data belonging to workers with a p-value below 0.05 were retained. Figure 2 shows the correlation between document-level DA scores collected in Run A with scores produced in Run B, where, for Run B, repeat assessments are down-sampled to show the increasing correspondence between scores as ever-increasing numbers of repeat assessments are collected for a given document. Correlation"
E17-2057,E14-1047,1,\N,Missing
E17-2057,W16-2301,1,\N,Missing
E17-2111,N13-1016,1,0.897975,"ikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the optimal modality (text vs. image) for"
E17-2111,P14-2103,1,0.88136,"ethods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most rele"
E17-2111,C16-1091,1,0.294226,"ut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the"
E17-2111,P16-1110,0,0.0188527,"rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to"
E17-2111,W14-3110,0,0.143765,"automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of"
E17-2111,N16-1057,1,0.860234,"Missing"
E17-2111,P16-1217,0,0.0185589,"r a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine si"
E17-2111,P11-1154,1,0.80732,"opic Labelling Ionut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a"
E17-2111,P14-2050,0,0.0115681,"(Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine similarity between the topic and article title embeddings. Finally, top labels are re-ranked in a supervised fashion using various features such as the PageRank score of the article in Wikipedia (Brin and Page, 1998), trigram letter ranking (Kou et al., 2015), topic word overlap, and word length of the label. Aletras and Mittal (2017) use pre-computed dependency-based word embeddings (Levy and Goldberg, 2014) to represent the topics and the caption of the images, as well as image embeddings using the output layer of VGG-net (Simonyan and Zisserman, 2014) pretrained on ImageNet (Deng et al., 2009). A concatenation of these three vectors is the input to a simple deep neural network with four hidden layers and a sigmoid output layer to predict the relevance score. Textual or visual modalities for labelling topics have been studied extensively, although independently from one another. Our work differs from the single-modality methods described above in that it uses a joint model to predict the continu"
E17-2111,P14-2101,0,\N,Missing
hughes-etal-2006-reconsidering,W98-1111,0,\N,Missing
hughes-etal-2006-reconsidering,C96-2110,0,\N,Missing
hughes-etal-2006-reconsidering,A94-1003,0,\N,Missing
I05-1068,J93-2004,0,0.0258624,"art of this process, we identify the R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 779–791, 2005. c Springer-Verlag Berlin Heidelberg 2005  780 P. Ye and T. Baldwin Fig. 1. An example of the preposition semantic roles in Penn Teebank level of complementarity of a dedicated PP semantic role labeller with a conventional holistic semantic role labeller, suggesting PP semantic role labelling as a potential avenue for boosting the performance of existing systems. 2 Preposition Semantic Role Disambiguation in Penn Treebank Signiﬁcant numbers of prepositional phrases (PPs) in the Penn treebank [1] are tagged with their semantic role relative to the governing verb. For example, Figure 1, shows a fragment of the parse tree for the sentence [Japan’s reserves of gold, convertible foreign currencies, and special drawing rights] fell by a hefty $1.82 billion in October to $84.29 billion [the Finance Ministry said], in which the three PPs governed by the verb fell are tagged as, respectively: PP-EXT (“extend”), meaning how much of the reserve fell; PP-TMP (“temporal”), meaning when the reserve fell; and PP-DIR (“direction”), meaning the direction of the fall. According to our analysis, there"
I05-1068,W03-0411,0,0.0247859,"found in the treebank: PP-LOC, PP-LOC-1, PP-LOC-2, PP-LOC-3, PP-LOC-4, PP-LOC-5, PP-LOC-CLR, PPLOC-CLR-2, PP-LOC-CLR-TPC-1. Inspection of the data revealed no systematic semantic diﬀerences between these PP types. Indeed, for most PPs, it was impossible to distinguish the subtypes of a given superclass (e.g. PP-LOC in our example). We therefore decided to collapse the PP semantic roles based on their ﬁrst semantic feature. For example, all semantic roles that start with PP-LOC are collapsed to the single class PP-LOC. Table 1 shows the distribution of the collapsed preposition semantic roles. [2] describe a system1 for disambiguating the semantic roles of prepositions in the Penn treebank according to 7 basic semantic classes. In their system, O’Hara and Weibe used a decision tree classiﬁer, and the following types of features: – POS tags of surrounding tokens: The POS tags of the tokens before and after the target preposition within a predeﬁned window size. In O’Hara and Wiebe’s work, this window size is 2. 1 This system was trained with WEKA’s J48 decision tree implementation. Semantic Role Labelling of Prepositional Phrases 781 Table 1. Penn treebank semantic role distribution (top"
I05-1068,J96-1002,0,0.00609563,"ntic role labeller and which collocation features are more important. 2.1 Results Since some of the preposition semantic roles in the treebank have extremely low frequencies, we decided to build our ﬁrst classiﬁer using only the top 9 semantic roles, as detailed in Table 1. We also noticed that the semantic roles PP-CLR, PP-CD and PP-PUT were excluded from O’Hara’s system which only used PP-BNF, PP-EXT, PP-MNR, PP-TMP, PP-DIR, PP-LOC and PP-PRP, therefore we built a second classiﬁer using only the semantic roles used by O’Hara’s system2 . The two classiﬁers were trained with a maximum entropy [4] learner3. Table 2 shows the results of our classiﬁer under stratiﬁed 10-fold cross validation4 using diﬀerent parameters for the rank-based ﬁlter. We also list the accuracy reported by O’Hara and Wiebe for comparison. The results show that the performance of the classiﬁer increases as we add more collocation features. However, this increase is not linear, and the improvement of performance is only marginal when the number collocation features is greater than 100. It also can be observed that there is a consistent performance diﬀerence between classiﬁers 1 and 2, which may suggest that PP-CLR"
I05-1068,W04-2412,0,0.0229832,"set Having built a classiﬁer which has reasonable performance on the task of treebank preposition semantic role disambiguation, we decided to investigate 2 3 4 PP-BNF with only 47 counts was not used by the second classiﬁer. http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html O’Hara’s system was also evaluated using stratiﬁed 10-fold cross validation. Semantic Role Labelling of Prepositional Phrases 783 whether we could use the same feature set to perform PP semantic role labelling over alternate systems of PP classiﬁcation. We chose the 2004 CoNLL Semantic Role Labelling (SRL) dataset [5] because it contained a wide range of semantic classes of PPs, in part analogous to the Penn treebank data, and also because we wished to couple our method with a holistic SRL system to demonstrate the ability of PP semantic role labelling to enhance overall system performance. Since the focus of the CoNLL data is on SRL relative to a set of predetermined verbs for each sentence input,5 our primary objective is to investigate whether the performance of SRL systems in general can be improved in any way by an independent preposition SRL system. We achieve this by embedding our PP classiﬁcation m"
I05-1068,briscoe-carroll-2002-robust,0,0.0232197,"Missing"
I05-1068,W04-2416,0,0.0187116,"from the holistic SRL system. S2 When a conﬂict is encountered, if the start positions of the semantic role are the same for both SRL systems, then replace the semantic role of the holistic SRL system with that of the preposition SRL system, but keep the holistic SRL system’s boundary end. S3 When a conﬂict is encountered, only use the semantic role information from the preposition SRL system. 3.7 Results To evaluate the performance of our preposition SRL system, we combined its outputs with the 3 top-performing holistic SRL systems from the CoNLL 2004 SRL shared task.7 The three systems are [7], [8] and [9]. Furthermore, in order to establish the upper bound of the improvement of preposition SRL on verb 7 Using the test data outputs of the three systems made available at http://www.lsi.upc.edu/∼ srlconll/st04/st04.html. Semantic Role Labelling of Prepositional Phrases 787 Table 5. Preposition SRL results before merging with the holistic SRL systems, (P = precision, R = recall, F = F-score; above-baseline results in boldface) SRDAUTO SEGNP P R SRDORACLE SEGORACLE F P R SEGNP F P R SEGORACLE F P R F VAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48 VAORACLE 42."
I05-1068,W04-2421,0,0.154919,"the holistic SRL system. S2 When a conﬂict is encountered, if the start positions of the semantic role are the same for both SRL systems, then replace the semantic role of the holistic SRL system with that of the preposition SRL system, but keep the holistic SRL system’s boundary end. S3 When a conﬂict is encountered, only use the semantic role information from the preposition SRL system. 3.7 Results To evaluate the performance of our preposition SRL system, we combined its outputs with the 3 top-performing holistic SRL systems from the CoNLL 2004 SRL shared task.7 The three systems are [7], [8] and [9]. Furthermore, in order to establish the upper bound of the improvement of preposition SRL on verb 7 Using the test data outputs of the three systems made available at http://www.lsi.upc.edu/∼ srlconll/st04/st04.html. Semantic Role Labelling of Prepositional Phrases 787 Table 5. Preposition SRL results before merging with the holistic SRL systems, (P = precision, R = recall, F = F-score; above-baseline results in boldface) SRDAUTO SEGNP P R SRDORACLE SEGORACLE F P R SEGNP F P R SEGORACLE F P R F VAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48 VAORACLE 42.2 6.9"
I05-1068,W04-2415,0,0.0772532,"istic SRL system. S2 When a conﬂict is encountered, if the start positions of the semantic role are the same for both SRL systems, then replace the semantic role of the holistic SRL system with that of the preposition SRL system, but keep the holistic SRL system’s boundary end. S3 When a conﬂict is encountered, only use the semantic role information from the preposition SRL system. 3.7 Results To evaluate the performance of our preposition SRL system, we combined its outputs with the 3 top-performing holistic SRL systems from the CoNLL 2004 SRL shared task.7 The three systems are [7], [8] and [9]. Furthermore, in order to establish the upper bound of the improvement of preposition SRL on verb 7 Using the test data outputs of the three systems made available at http://www.lsi.upc.edu/∼ srlconll/st04/st04.html. Semantic Role Labelling of Prepositional Phrases 787 Table 5. Preposition SRL results before merging with the holistic SRL systems, (P = precision, R = recall, F = F-score; above-baseline results in boldface) SRDAUTO SEGNP P R SRDORACLE SEGORACLE F P R SEGNP F P R SEGORACLE F P R F VAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48 VAORACLE 42.2 6.96 11.95"
I05-1068,W99-0606,0,\N,Missing
I05-1068,A00-2018,0,\N,Missing
I05-1068,E99-1023,0,\N,Missing
I05-1068,W05-0620,0,\N,Missing
I05-1068,W01-0715,0,\N,Missing
I05-1082,C02-1011,0,0.0223591,"modifier and head noun for all NCs in the training data; we experiment with two methods for calculating the combined similarity. The third step is to choose the NC in the training data which is most similar to the test instance, and tag the test instance according to the semantic relation associated with that training instance. Formally, SA is the similarity between NCs (Ni,1 , Ni,2 ) and (Bj,1 , Bj,2 ): SA ((Ni,1 , Ni,2 ), (Bj,1 , Bj,2 )) = ((αS1 + S1) × ((1 − α)S2 + S2)) 2 (1) where S1 is the modifier similarity (i.e. S(Ni,1 , Bj1 )) and S2 is head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. SB is an analogous similarity function, based on the F-score: N11 N12 N21 N22 NN RELATION B11 B12 B21 B22 B31 B32 Relation3 Relation19 Relation3 Bj1 Bj2 Relation_k Ni1 Ni2 Similarity in detail S(Ni1,B11) S(Ni2,B12) S(Ni1,B21) S(Ni2,B22) S(Ni1,Bj1) S(Ni2,Bj2) S(Ni1,Bm1) S(Ni2,Bm2) Nn1 Nn2 Bm1 Bm2 Relation2 Fig. 2. Similarity between the ith NC in the test data and j th NC in the training data Automatic Interpretation of Noun Compounds SB ((Ni,1 , Ni,2 ), B(j,1 , Bj,2 )) = 2 × (S1 + αS1) × (S2 + (1 − α)S2) (S1 + αS1) + (S2 + (1 − αS2)) 951 (2) The semantic relation is det"
I05-1082,W04-0404,1,0.643147,"Missing"
I05-1082,J02-3004,0,0.0686548,"also investigated the relative contribution of the modifier and the head noun in noun compounds of different semantic types. 1 Introduction ¯ made up of two or more nouns, such as golf club or A noun compound (NC) is an N paper submission; we will refer to the rightmost noun as the head noun and the remainder of nouns in the NC as modifiers. The interpretation of noun compounds is a well-researched area in natural language processing, and has been applied in applications such as question answering and machine translation [1,2,3]. Three basic properties make the interpretation of NCs difficult [4]: (1) the compounding process is extremely productive; (2) the semantic relationship between head noun and modifier in the noun compounds is implicit; and (3) the interpretation can be influenced by contextual and pragmatic factors. In this paper, we are interested in recognizing the semantic relationship between the head noun and modifier(s) of noun compounds. We introduce a method based on word similarity between the component nouns in an unseen test instance NC and annotated training instance NCs. Due to its simplicity, our method is able to interpret NCs with significantly reduced cost. We"
I05-1082,C94-2125,0,0.230991,", pp. 945–956, 2005. c Springer-Verlag Berlin Heidelberg 2005  946 S.N. Kim and T. Baldwin distinguish semantic relations from semantic roles. The semantic relation in an NC is the underlying relation between the head noun and its modifier, whereas its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity sys"
I05-1082,W01-0511,0,0.891828,"its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity system which we use to calculate word similarity (Section 3), outline the set of semantic relations used (Section 4), detail how we collected the data (Section 5), introduce the proposed method (Section 6), and describe experimental results (Section 7). 2 M"
I05-1082,W04-2609,0,0.740155,"its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity system which we use to calculate word similarity (Section 3), outline the set of semantic relations used (Section 4), detail how we collected the data (Section 5), introduce the proposed method (Section 6), and describe experimental results (Section 7). 2 M"
I05-1082,P98-1015,0,0.783066,"xperimental results (Section 7). 2 Motivation Most work related to interpreting NCs depends on hand-coded rules [5]. The first attempt at automatic interpretation by [6] showed that it was possible to successfully interpret NCs. However, the system involved costly hand-written rules involving manual intervention. [9] estimated the amount of world knowledge required to interpret NCs and claimed that the high cost of data acquisition offsets the benefits of automatic interpretation of NCs. Recent work [4,7,8] has investigated methods for interpreting NCs automatically with minimal human effort. [10] introduced a semi-automatic method for recognizing noun–modifier relations. [4] examined nominalizations (a proper subset of NCs) in terms of whether the modifier is a subject or object of the verb the head noun is derived from (e.g. language understanding = understand language). [7] assigned hierarchical tags to nouns in medical texts and classified them according to their semantic relations using neural networks. [8] used the word senses of nouns to classify the semantic relations of NCs. However, in all this work, there has been some underlying simplifying assumption, in terms of the domai"
I05-1082,W04-0816,0,0.0240189,"medical texts and classified them according to their semantic relations using neural networks. [8] used the word senses of nouns to classify the semantic relations of NCs. However, in all this work, there has been some underlying simplifying assumption, in terms of the domain or range of interpretations an NC can occur with, leading to questions of scalability and portability to novel domains/NC types. In this paper, we introduce a method which uses word similarity based on WordNet. Word similarity has been used previously in various lexical semantic tasks, including word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a context space can be used to disambiguate word senses. [12] measured the relatedness of concepts using similarity based on WordNet. [13] examined the task of disambiguating noun groupings with respect to word senses using similarity between nouns in NCs. Our research uses similarities between nouns in the training and test data to interpret the semantic relations of novel NCs. Automatic Interpretation of Noun Compounds MATERIAL apple juice s11 947 TIME morning milk s12 s21 s22 chocolate milk Fig. 1. Similarity between test NC chocolate milk and t"
I05-1082,W95-0105,0,0.0274609,"this work, there has been some underlying simplifying assumption, in terms of the domain or range of interpretations an NC can occur with, leading to questions of scalability and portability to novel domains/NC types. In this paper, we introduce a method which uses word similarity based on WordNet. Word similarity has been used previously in various lexical semantic tasks, including word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a context space can be used to disambiguate word senses. [12] measured the relatedness of concepts using similarity based on WordNet. [13] examined the task of disambiguating noun groupings with respect to word senses using similarity between nouns in NCs. Our research uses similarities between nouns in the training and test data to interpret the semantic relations of novel NCs. Automatic Interpretation of Noun Compounds MATERIAL apple juice s11 947 TIME morning milk s12 s21 s22 chocolate milk Fig. 1. Similarity between test NC chocolate milk and training NCs apple juice and morning milk Table 1. WordNet-based similarities for component nouns in the training and test data Training noun Test noun Sij t1 apple chocolate 0.71 t2 ju"
I05-1082,P94-1019,0,0.399544,"chocolate 0.71 t2 juice milk 0.83 t1 morning chocolate 0.27 t2 milk milk 1.00 Figure 1 shows the correspondences between two training NCs, apple juice and morning milk, and a test NC, chocolate milk; Table 1 lists the noun pairings and noun– noun similarities based on WordNet. Each training noun is a component noun from the training data, each test noun is a component noun in the input, and Sij provides a measure of the noun–noun similarity in training and test, where t1 is the modifier and t2 is the head noun in the NC in question. The similarities in Table 1 were computed by the WUP method [14] as implemented in WordNet::Similarity (see Section 3). The simple product of the individual similarities (of each modifier and head noun, respectively) gives the similarity of the NC pairing. For example, the similarity between chocolate milk and apple juice is 0.60, while that between chocolate milk and morning milk is 0.27. Note that although milk in the input NC also occurs in a training exemplar, the semantic relations for the individual NCs differ. That is, while apple juice is juice made from apples (MATERIAL), morning milk is milk served in the morning (TIME). By comparing the similari"
I05-1082,O97-1002,0,\N,Missing
I05-1082,C98-1015,0,\N,Missing
I08-1074,P98-1015,0,0.0283656,"= BENEFICIARY), while student protest conventionally means a student undertaking a protest (SR = AGENT).2 NCs are formed from simplex nouns with high productivity. The huge number of possible NCs and potentially large number of SRs makes NC interpretation a very difficult problem. In the past, much NC interpretation work has been carried out which targets particular NLP applications such as information extraction, question-answering and machine translation. Unfortunately, much of it has not gained 1 The 4th International Workshop on Semantic Evaluation SRs used in the examples are taken from Barker and Szpakowicz (1998). 2 The first step in NC interpretation is to define a set of SRs. Levi (1979), for example, proposed a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation"
I08-1074,P07-1072,0,0.0135112,"le others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (cov"
I08-1074,I05-1082,1,0.775674,"a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in differen"
I08-1074,P06-2064,1,0.677213,"duced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation methods over a common, publicly availab"
I08-1074,W04-2609,0,0.350338,"tation a very difficult problem. In the past, much NC interpretation work has been carried out which targets particular NLP applications such as information extraction, question-answering and machine translation. Unfortunately, much of it has not gained 1 The 4th International Workshop on Semantic Evaluation SRs used in the examples are taken from Barker and Szpakowicz (1998). 2 The first step in NC interpretation is to define a set of SRs. Levi (1979), for example, proposed a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Van"
I08-1074,W07-1108,0,0.0678177,"Missing"
I08-1074,W01-0511,0,0.0345516,"Missing"
I08-1074,C94-2125,0,0.042037,"04). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation me"
I08-1074,P94-1019,0,0.0238202,"Method The SENSE COLLOCATION method of Moldovan et al. (2004) is based on the pair of word senses of NC constituents. The basic idea is that NCs which have the same or similar sense collocation tend to have the same SR. As an example, car factory and auton(r, fij ) n(fij ) (3) where S1 is the modifier similarity (i.e. S(Ni,1 , Bj1 )) and S2 is the head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated across the bag of WordNet senses (without choosing between 570 Method SC OLL SC OLLCT CS IM CS IM +SC OLLCT them) using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. HYBRID CS IMCT Description sense collocation sense collocation + SC OLL co-training constituent similarity constituent similarity + SC OLL co-training SC OLL + CS IM + SC OLLCT constituent similarity + CS IM co-training Table 1: Systems used in our experiments 2.4 Co-Training by Sense Collocation Co-training by sense collocation (SC OLL CO TRAINING )"
I08-1074,J02-3004,0,0.0303976,"end to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation methods over a c"
I08-1074,S07-1003,0,\N,Missing
I08-1074,C98-1015,0,\N,Missing
I08-2108,P01-1004,1,0.808695,"ally expensive. We thus adopt a cheaper scoring mechanism which normalises relative to the length of w and di,j , but ignores the length of substring matches. Namely, we use the Dice coefficient. 4.2 Tokenisation Tokenisation is particularly important in Japanese because it is a non-segmenting language with a logographic orthography (kanji). As such, we can chose to either word tokenise via a word splitter such as ChaSen, or character tokenise. Character and word tokenisation have been compared in the context of Japanese information retrieval (Fujii and Croft, 1993) and translation retrieval (Baldwin, 2001), and in both cases, characters have been found to be the superior representation overall. Orthogonal to the question of whether to tokenise into words or characters, we adopt an n-gram segment representation, in the form of simple unigrams and simple bigrams. In the case of word tokenisation and simple bigrams, e.g., example (1) would be represented as { おとなしい犬 , 犬を , を飼いたい }. 4.3 Extended Glosses The main direction in which Banerjee and Pedersen (2002) successfully extended the Lesk algorithm was in including hierarchically-adjacent glosses (i.e. hyponyms and hypernyms). We take this a step"
I08-2108,J98-1006,0,0.0481197,"is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended the Lesk method for WordNetbased WSD tasks, to include hierarchical data from the WordNet ontology (Fellbaum, 1998). They observed that the hierarchical relations significantly enhance the basic model. Both these methods will be described extensively in Section 3.1, as our approach is based on them. Other notable unsupervised and semi-supervised approaches are those of McCarthy et al. (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al. (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. Parallel corpora have also been used to avoid the need for hand-tagged data, e.g. by Chan and Ng (2005). 3 Background As background to our work, we first describe the basic and extended Lesk algorithms that form the core of our approach. Then we present the Lexeed lexical resource we have used in our experiments, and finally we outline aspects of Japanese relevant for this work. 3.1 Basic and Extended Lesk The original Lesk algorithm (Lesk, 1986) performs WSD by calculating the relative word overlap betw"
I08-2108,P04-1036,0,0.226288,"l definitions. In our experiments, we will make clear when hand-tagged sense information is being used. Unsupervised methods rely on different knowledge sources to build their models. Primarily the following types of lexical resources have been used for WSD: MRDs, lexical ontologies, and untagged corpora (monolingual corpora, second language corpora, and parallel corpora). Although early approaches focused on exploiting a single resource (Lesk, 1986), recent trends show the benefits of combining different knowledge sources, such as hierarchical relations from an ontology and untagged corpora (McCarthy et al., 2004). In this summary, we will focus on a few representative systems that make use of different resources, noting that this is an area of very active research which we cannot do true justice to within the confines of this paper. The Lesk method (Lesk, 1986) is an MRD-based system that relies on counting the overlap between the words in the target context and the dictionary definitions of the senses. In spite of its simplicity, it has been shown to be a hard baseline for unsupervised methods in Senseval, and it is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended th"
I08-2108,W03-2408,0,0.0276544,"o the knowledge sources they use to build their models. A top-level distinction is made between supervised and unsupervised systems. The former rely on training instances that have been hand-tagged, while the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexee"
I08-2108,shirai-2002-construction,0,0.48783,"the POS tag of the target word should match the word class of the word sense, and this provides a coarse-grained filter for discriminating homographs with different word classes. We also experiment with a stop word-based filter which ignores a closed set of 18 lexicographic markers commonly found in definitions (e.g. 略 [ryaku] “an abbreviation for ...”), in line with those used by Nichols et al. (2005) in inducing the ontology. 5 Evaluation We evaluate our various extensions over two datasets: (1) the example sentences in the Lexeed sensebank, and (2) the Senseval-2 Japanese dictionary task (Shirai, 2002). All results below are reported in terms of simple precision, following the conventions of Senseval evaluations. For all experiments, precision and recall are identical as our systems have full coverage. For the two datasets, we use two baselines: a random baseline and the first-sense baseline. Note that the first-sense baseline has been shown to be hard to beat for unsupervised systems (McCarthy et al., 2004), and it is considered supervised when, as in this case, the first-sense is the most frequent sense from hand-tagged corpora. 5.1 Lexeed Example Sentences The goal of these experiments i"
I08-2108,W06-0608,1,0.912613,"hile the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexeed sensebank, we investigate a number of areas of general interest to the WSD community. First, we test extensions of the Lesk algorithm (Lesk, 1986) over Japanese, focusing specifically on th"
I11-1028,P09-1006,0,0.050545,"Missing"
I11-1028,P02-1035,0,0.1593,"Missing"
I11-1028,P05-1041,1,0.936429,"new parse forest, and checking inter-annotator agreement on the overlap, we annotated the remaining sentences. All accuracy figures we report are over the data set of 669 trees complete at the time of experimentation. 3.3 4 Blazing In §2, we reviewed work that uses linguistic information from superficially incompatible formalisms for treebanking or parse selection. Our experiments here use syntactic information from the GTB to partially disambiguate the parse forest produced by the ERG. We do this by disallowing certain candidate ERG trees on the basis of GTBderived information, and we follow Tanaka et al. (2005) in denoting this process “blazing”.1 As detailed below, we can use this partially disambiguated forest: (1) to train parse selection models; and (2) to reduce treebanking effort, abstractly similarly to Tanaka et al. (2005). The goal is not to apply all constraints from the GTB to the ERG parse trees; rather, we want to apply the minimal amount of constraints possible, while still sufficiently restricting the parse forest for our target application. We call the set of trees remaining after blazing silver trees, to represent the fact that they are not gold standard, but are generally of better"
I11-1028,W10-3007,1,0.800767,"scriminants, which are easier to identify. This process happens with all discriminants for a sentence simultaneously, so it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies s"
I11-1028,P94-1034,0,0.142853,"Missing"
I11-1028,W07-2207,1,0.929339,"Missing"
I11-1028,A00-1031,0,0.578191,"it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies some pre-configured transformations to the GTB tree, and examines each discriminant for whether it should be ruled"
I11-1028,W02-1503,0,0.354299,"Missing"
I11-1028,W97-1502,0,0.655547,"Missing"
I11-1028,W11-2927,1,0.695034,"rsing using a pure WeScience model. Other configurations used models trained from the same training sentence parse forest, setting a pseudo-gold tree either randomly, selftrained (best from a WeScience model), or blazing (highest-ranked of the silver trees, other silver trees discarded). The gold WeScience data is also used for training. Significance figures are against “WeSc only”, ( *: p &lt; 0.05; ***:p &lt; 0.001), and “Self-train”, ( ††: p &lt; 0.01; †††: p &lt; 0.001) Experimental Configuration how ‘right’ or ‘wrong’ the top analysis is. To supplement AccN , we use Elementary Dependency Match (EDM: Dridan and Oepen (2011)). This is based on triples extracted from the semantic output of the parser, providing a more granular measure of the quality of the analyses. We use the EDMN A configuration that is arguably the most compatible with other dependency-based parser evaluation, although we make no claims of direct comparability. We create a parse forest by parsing 10747 sentences from a GTB subset not overlapping with the test corpus, using the WeScience model to determine the top 500 parses. The best-performing method we found to create parse selection models from this parse forest was to apply the blazing conf"
I11-1028,W01-0521,0,0.056251,"s stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adj"
I11-1028,hockenmaier-steedman-2002-acquiring,0,0.18136,"n option, however, for finegrained tasks which require an expert understanding of a theory or domain, such as syntactic treebanking or discourse annotation. Two main approaches have been adopted to efficiently create new resources: (1) domain adaptation, where a trained model from one domain is stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, w"
I11-1028,W02-2018,0,0.0442808,"Missing"
I11-1028,P06-1043,0,0.197318,"06); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adjoining Grammar (LTAG) structures. The work of Hockenmaier and Steedman (2002) is roughly parallel, but targets Combina"
I11-1028,W05-0603,0,0.0611187,"Missing"
I11-1028,I05-2038,0,\N,Missing
I11-1062,P10-1010,0,0.0260413,"ns, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-labelled development data from five sources of diverse origin, and use these as the basis of our examination of in-domain, inductive (all-domain) and transductive (cross-domain) learning. We additionally use three independent test data sets to validate the effectiveness of the final methodology. Statistics of all datasets are provided in Table 1. Transfer learning refers to the use of data from external domains to improve task performance on a target domain. Pan and Yang (2010) provide a survey, in which they define transductive transfer le"
I11-1062,D07-1016,0,0.0543467,"where a document is classified according to its overlap with lists for different languages. There has also been work on word and part of speech correlation (Grefenstette, 1995), cross-language tokenisation (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID has been applied in a variety of domains, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-labelled development data from five sources of diverse origin, and use these as the basis of our examination of in-domain, inductive (all-domain) and transductive (cross-domain) learning. We additionally use thr"
I11-1062,N10-1027,1,0.528738,"ansductive transfer learning for language identification. We provide an implementation of the method and show that our system is faster than popular standalone language identification systems, while maintaining competitive accuracy. 1 Introduction Language identification (LangID) is the task of determining the language(s) that a text is written in. It is considered by some researchers to be a solved task, because previous research has reported near-perfect accuracy (Cavnar and Trenkle, 1994). Hughes et al. (2006) elaborated a number of simplifying assumptions that have made this the case, and Baldwin and Lui (2010a) showed that when some of these assumptions are relaxed to make the task closer to the actuality of open-web LangID, it becomes considerably harder. In this paper, we demonstrate that the style of evaluation used by Baldwin and Lui (2010a) performs well in-domain but badly cross-domain, and develop a novel method for preserving high in-domain accuracy, while significantly boosting cross-domain accuracy. Similarly to Baldwin and Lui (2010a), we make the simplifying assumption that all documents are monolingual, despite recent work on multilingual LangID (Baldwin and Lui, 2010b). LangID is usu"
I11-1062,P09-1120,0,0.0245572,"vated models for LangID have also been proposed, such as stop word list overlap (Johnson, 1993), where a document is classified according to its overlap with lists for different languages. There has also been work on word and part of speech correlation (Grefenstette, 1995), cross-language tokenisation (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID has been applied in a variety of domains, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-labelled development data from five sources of diverse origin, and use these as the basis of our examination of in-dom"
I11-1062,P07-1033,0,0.0394099,"Missing"
I11-1062,hughes-etal-2006-reconsidering,1,0.749294,"d that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identification. We provide an implementation of the method and show that our system is faster than popular standalone language identification systems, while maintaining competitive accuracy. 1 Introduction Language identification (LangID) is the task of determining the language(s) that a text is written in. It is considered by some researchers to be a solved task, because previous research has reported near-perfect accuracy (Cavnar and Trenkle, 1994). Hughes et al. (2006) elaborated a number of simplifying assumptions that have made this the case, and Baldwin and Lui (2010a) showed that when some of these assumptions are relaxed to make the task closer to the actuality of open-web LangID, it becomes considerably harder. In this paper, we demonstrate that the style of evaluation used by Baldwin and Lui (2010a) performs well in-domain but badly cross-domain, and develop a novel method for preserving high in-domain accuracy, while significantly boosting cross-domain accuracy. Similarly to Baldwin and Lui (2010a), we make the simplifying assumption that all docume"
I11-1062,C96-2110,0,0.358262,"nel methods have also been applied to the task of LangID (Kruengkrai et al., 2005). Linguistically motivated models for LangID have also been proposed, such as stop word list overlap (Johnson, 1993), where a document is classified according to its overlap with lists for different languages. There has also been work on word and part of speech correlation (Grefenstette, 1995), cross-language tokenisation (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID has been applied in a variety of domains, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-label"
I11-1062,steinberger-etal-2006-jrc,0,0.0614461,"Missing"
I11-1062,W09-0307,0,0.0112013,", 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID has been applied in a variety of domains, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-labelled development data from five sources of diverse origin, and use these as the basis of our examination of in-domain, inductive (all-domain) and transductive (cross-domain) learning. We additionally use three independent test data sets to validate the effectiveness of the final methodology. Statistics of all datasets are provided in Table 1. Transfer learning refers to the use of data from external domains to improve t"
I11-1062,E09-1099,0,0.0402269,"kenisation (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID has been applied in a variety of domains, including USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), and web search queries (Hammarstrom, 2007; Ceylan and Kim, 2009). It has been shown to improve performance of other tasks such as parsing (Alex et al., 2007) and multilingual text retrieval (McNamee and Mayfield, 2004). It has also been used for gathering data for linguistic corpus creation (Ghani et al., 2004; Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009), and is an important area of research for supporting low-density languages (Hughes et al., 2006; Abney and Bird, 2010). 3 Data Sources For this work, we collected language-labelled development data from five sources of diverse origin, and use these as the basis of our examination of in-domain, inductive (all-domain) and transductive (cross-domain) learning. We additionally use three independent test data sets to validate the effectiveness of the final methodology. Statistics of all datasets are provided in Table 1. Transfer learning refers to the use of data from externa"
I11-1062,U10-1003,1,\N,Missing
I11-1102,W07-1106,0,0.0629281,"ng the dependency parse of the sentence. In the conventional word order for Japanese, dependency links are always forwards so the head of a phrase is also its rightmost (final) chunk. From Figure 1, we see that keiko “Keiko” and ude “arm” are the dependents of ageta “raised”, with sakk¯a “soccer” modifying ude “arm”. This gives rise Fazly et al. (2009) observed that idiomatic uses of MWEs tend to occur in one of a small set of canonical forms, and developed an unsupervised method for learning these canonical forms, based on a set of linguistically-motivated features which built on the work of Cook et al. (2007). They applied the learned set of canonical forms to the task of MWE-token identification with remarkable success. Our method similarly uses linguisticallymotivated features to perform MWE-token identification, but using a cross-type supervised model. 1 http://nlp.ist.i.kyoto-u.ac.jp/EN/ ?KNP 2 KNP’s memory usage is high and on some sentences exceeded the available memory in our machine (32GB). Those instances were excluded from our analysis. 3 In fact, KNP delegates the initial segmentation and token feature annotation to the morphological analyser Juman. 913 上げる/あげる 上げ た 。 桂子/けいこ TOPIC 桂子 さん"
I11-1102,W09-2903,0,0.419699,"as well. 3.1 Preprocessing Our feature extraction makes extensive use of the Japanese dependency parser KNP (Kurohashi and Nagao, 1994),1 however features should be reproducible in other languages with a suitable dependency parser, morphological analyser, and electronic thesaurus or ontology. Each instance in the corpus was preprocessed by running it through KNP to extract specific linguistic information.2 To help elucidate both the details of our feature extraction and how it might be replicated for another language, we will look at the information extracted for the sentence in Example (1): Diab and Bhutada (2009) described a novel supervised MWE-token classification system based on a sequence labelling model. Unlike our method, their model identifies the position of the token in the text as part of the process. Like Li and Sporleder (2010), the size of the corpus is small ( 2500 MWE-tokens of 53 MWE-types), and classifiers were trained on collections of MWE-types. Anecdotally, the classifiers were able to pick some MWEs out of running text without even knowing their constituents beforehand, however their performance at this was not tested. A major finding of Diab and Bhutada (2009) was that reducing t"
I11-1102,W09-0213,0,0.0186706,"process. Like Li and Sporleder (2010), the size of the corpus is small ( 2500 MWE-tokens of 53 MWE-types), and classifiers were trained on collections of MWE-types. Anecdotally, the classifiers were able to pick some MWEs out of running text without even knowing their constituents beforehand, however their performance at this was not tested. A major finding of Diab and Bhutada (2009) was that reducing the feature space of context features by replacing word lemmas with their named-entity category had a significant positive effect on classification performance, a finding that is consolidated by Diab and Krishna (2009). (1) 桂子さんは、 サッカーの 腕を ude-o keiko-saN-wa, sakk¯a-no Keiko-TOPIC soccer-GEN arm-OBJ 上げた。 ageta. raised. # “Keiko raised her soccer arm.” (literal) “Keiko improved her skills at soccer.” (idiomatic) Japanese is a non-segmenting language in that it has no clearly marked word boundaries. In Figure 1, Example (1) has been segmented by KNP into tokens3 which are then grouped into chunks. Each chunk has a parent link to a higher chunk describing the dependency parse of the sentence. In the conventional word order for Japanese, dependency links are always forwards so the head of a phrase is also its r"
I11-1102,J09-1005,0,0.559978,"rained MWE-types as a majority class baseline which has knowledge of the MWE-type. 1 Introduction A multiword expression (MWE) is an idiosyncratically interpreted linguistic unit which consists of more than a single word (or “crosses word boundaries”) (Sag et al., 2002; Baldwin and Kim, 2009). The nature of these idiosyncrasies can vary greatly — from traffic light and street light which are remarkable only in that they are not interchangeable — to how do you do?, the meaning of which is non-compositional in modern English. MWEs will typically resist lexico-syntactic variation to some extent (Fazly et al., 2009). For example, the phrase a picture is worth a thousand words does not allow the freedom of lexical substitution and modification that its constituent words would usually enjoy, making otherwise equivalent Relation to Word Sense Disambiguation MWE-token disambiguation can be approached as if it were a word sense disambiguation (WSD) task where the MWE-types correspond to word types and MWE-tokens to word tokens (Hashimoto and Kawahara, 2009). In this conception, the analogue of word senses are the idiomatic and literal classes for an MWE-type. In WSD, supervised methods are by far the most suc"
I11-1102,W02-1006,0,0.401129,". 2 Related Work The OpenMWE corpus was compiled by Hashimoto and Kawahara (2009). To our knowledge it is by far the largest freely available gold-standard corpus of ambiguous MWE-tokens in any language, comprising 146 ambiguous MWE-types with 102,856 annotated MWEtokens in total. Apart from the enormous task of constructing the OpenMWE corpus, Hashimoto and Kawahara (2009) also used it to perform some experiments with supervised MWE-token classification. Noting the similarities between this task and WSD, they employed the most effective WSD features and machine learning algorithm surveyed by Lee and Ng (2002). They also included linguistic features explored by Hashimoto et al. (2006) designed to capture the relative fixedness of Japanese idioms, which we will refer to as idiom features. The machine learning algorithm used was Support Vector Machines and models were trained on the WSD features with various combinations of the idiom features. Type-specialised classifiers were trained for the 90 MWE-types which were deemed to have sufficient idiomatic and literal examples in the corpus. The model trained on WSD features was found to improve greatly on the typespecialised baseline, with some additiona"
I11-1102,C10-2078,0,0.249206,"3 tion by training classifiers which work on MWEtypes on which they have not been trained. To that end, we have extended the features used by Hashimoto and Kawahara (2009) with complementary features and introduced a new class of features designed for crosstype classification. Feature Extraction We extracted features in three main groups: WSD features, idiom (token) features and idiom type features. The first two categories include the features used by Hashimoto and Kawahara (2009) and our own extensions. The third category was introduced by us and is specialised to crosstype classification. Li and Sporleder (2010) conducted a thorough investigation of features used for supervised MWE-token classification. Context features similar to the WSD features of Hashimoto and Kawahara (2009) were used, as were a number of linguistically motivated features. Like us, Li and Sporleder (2010) performed crosstype classification. Unfortunately many of the features were too sparse to have a significant effect and only the context features produced significant results. This may have been due to the relatively small size of the corpus used, which comprised around 4000 MWE-tokens across 13 MWE-types. In our results the co"
I11-1102,J07-4005,0,0.0692383,"Missing"
I11-1102,W09-2403,0,0.0459261,"Missing"
I11-1102,W03-1812,1,\N,Missing
I13-1041,P13-4002,1,0.499896,"ols. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on social media, including lexical varian"
I13-1041,adolphs-etal-2008-fine,0,0.0360756,"Missing"
I13-1041,P12-3005,1,0.204428,"Missing"
I13-1041,N13-1039,0,0.0249366,"respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallow parser and named-entity recogniser. Foster et al. (2011) evaluated standard parsers on social media data, and found them to perform particularly poorly on Twitter, but showed that their performance can be improved through a retraining strategy. Another natural question to ask is how similar the characteristics of social media text are to t"
I13-1041,P11-1040,0,0.0125344,"as Twitter or blogs. A natural question to ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14"
I13-1041,N12-1034,0,0.00934521,"ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from acr"
I13-1041,D10-1124,0,0.0437419,"Missing"
I13-1041,C12-2096,0,0.0361344,"nalysis of these Corpus Pre-processing We first pre-process each dataset using the following standardised methodology.3 In the case that the corpus comes with tokenisation and POS information, we strip this and perform automatic preprocessing to ensure consistency in the quality and composition of the tokens/tags. We first apply langid.py (Lui and Baldwin, 2012) — an off-the-shelf language identifier — to each document to detect its majority language. We then extract all documents identified as English for further processing. We next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specif"
I13-1041,read-etal-2012-wesearch,0,0.0651736,"nalysis of these Corpus Pre-processing We first pre-process each dataset using the following standardised methodology.3 In the case that the corpus comes with tokenisation and POS information, we strip this and perform automatic preprocessing to ensure consistency in the quality and composition of the tokens/tags. We first apply langid.py (Lui and Baldwin, 2012) — an off-the-shelf language identifier — to each document to detect its majority language. We then extract all documents identified as English for further processing. We next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specif"
I13-1041,N13-1037,0,0.200271,"which we compare to a reference corpus of edited English text. We first extract out various descriptive statistics from each data type (including the distribution of languages, average sentence length and proportion of out-ofvocabulary words), and then investigate the proportion of grammatical sentences in each, based on a linguistically-motivated parser. We also investigate the relative similarity between different data types. 1 Introduction 2 Various claims have been made about social media text being “noisy” (Java, 2007; Becker et al., 2009; Yin et al., 2012; Preotiuc-Pietro et al., 2012; Eisenstein, 2013, inter alia). However, there has been little effort to quantify the extent to which social media text is more noisy than conventional, edited text types. Moreover, social media comes in many flavours — including microblogs, blogs, and user-generated comments — and research has tended to focus on a specific data source, such as Twitter or blogs. A natural question to ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to dat"
I13-1041,D11-1141,0,0.0294364,"s of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallow parser and named-entity recogniser. Foster et al. (2011) evaluated standard parsers on social media data, and found them to perform particularly poorly on Twitter, but showed that their performance can be improved through a retraining strategy. Another natural question to ask is how similar the characteristics of social media text are to those of other domains. Mo"
I13-1041,flickinger-etal-2010-wikiwoods,0,0.00721079,"next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specifically, we remove any token tagged as #, @, ˜, U, or E. 358 T WITTER -1 en .406 ja .144 pt .098 es .093 id .031 nl .025 ms .016 ko .015 de .015 it .013 T WITTER -2 en .439 ja .124 es .091 pt .072 id .029 nl .022 ar .019 ko .018 ms .015 fr .015 C OMMENTS en .757 de .034 es .028 fr .023 ru .023 pt .020 pl .012 ar .011 it .011 nl .006 F ORUMS en .914 de .016 es .011 ro .009 it .007 nl .007 fr .006 pl .003 da .002 sv .002 B LOGS en .784 ru .050 fr .025 zh .022 de .019 es .017 ja .010 it .010 pt .009 sv .008 W IKIPEDIA en .998 la .000 de .00"
I13-1041,I11-1100,0,0.0599393,"Missing"
I13-1041,D12-1137,0,0.00547494,"g a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level o"
I13-1041,W11-2210,0,0.00887458,"ard NLP tools cannot be immediately applied. Efforts to address this problem have taken two main approaches: modifying social media data to more closely resemble standard text, and building social media-specific tools. Lexical normalisation is the task of converting non-standard forms such as tlkin and touchdooown to their standard forms (talking and touchdown, respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further de"
I13-1041,P11-1038,1,0.696697,"be immediately applied. Efforts to address this problem have taken two main approaches: modifying social media data to more closely resemble standard text, and building social media-specific tools. Lexical normalisation is the task of converting non-standard forms such as tlkin and touchdooown to their standard forms (talking and touchdown, respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallo"
I13-1041,D12-1039,1,0.81612,"ite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on"
I13-1041,P11-1096,0,0.0308249,"first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per docu"
I13-1041,C12-1064,1,0.362465,"ite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on"
I13-1080,baldwin-awab-2006-open,1,0.568017,"r example, ‘mk’ would capture context features for the membeli and membelikan variants of the stem beli “buy”. 4 We had also manually grouped stems from other word classes: 48 noun stems were grouped into 13 subclasses; and 27 adjective stems were grouped into 5, giving us a total of 100 stems with the 25 verbs, but we only report on the verb experiments. 5 http://dumps.wikimedia.org/idwiki/ 6 http://www.cs.technion.ac.il/˜gabr/ resources/code/wikiprep 7 http://opennlp.apache.org/ Our experiments showed that OpenNLP’s English models performed better than a rule-based Malay sentence tokeniser (Baldwin and Awab, 2006). Window Size (win): This stipulates the context window size, relative to individual occurrences of the target lexeme, and can take a value of 1–5. 688 M ORPHOLOGY V ERB F RAME D ECOMPOSITION Example Type A: acuh “to heed”, terjemah “translate”, mandi “bathe” ME N+V 1 – – ME N+V 1 + KAN <NPa , NPb &gt; DOto ( [NPa ], [V1 TO( [NP] ) ] ) Example Type B: dengar “hear”, kenang “think of” ME N+V 3 <NPa , NPb &gt; HAPPENto ( [NPb ], [ V3 TO([NPa ] ) ] ) ME N+V 3 + KAN <NPa , NPb &gt; DOto ( [NPa ], [ V3 TO( [NPb ] ) ] ) Table 2: Manually generated verb Types (‘–’ = no attested word form in the text; ‘{. . ."
I13-1080,J03-4004,0,0.0240065,"enchmark system, a method also employed by systems such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367"
I13-1080,W11-0910,0,0.0917186,"les Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discover syntactico-semantic Levin-style classes, rather than systems that aim to induce valency or syntactic frame information from corpora. These can be built in a supervised fashion as in Lapata and Brew (2004) or tackled as a clustering task as in Schulte im Walde (2006) or Bonial et al. (2011). Lapata and Brew (2004) develop a semi-supervised system that generates, for a given verb and its syntactic frame, a probability distribution over the Levin verb classes. They then use this system to disambiguate tokens using collocation information. Our system, like Schulte im Walde (2006), uses an unsupervised clustering approach. In her approach, Schulte Im Walde employs hierarchical agglomerative clustering over parse features to discover word classes in German, and evaluates using manually-created goldstandard data. 3 Evaluation Data This section describes how we arrive at the two evalua"
I13-1080,J93-2002,0,0.0976834,"eparating unaccusative from unergative stems, which predicts their morphosynatic behaviour. However the facts of -kan seem more intricate than this characterisation. Even though the causative and benefactive constructions uses of kan are the most commonly cited, its usage is much more varied and nuanced, as shown by Kroeger (2007), which is why we chose this morpho-syntactic construction as our case study. Since the early ’90s, the tools and resources employed in valency acquisition tasks have become increasingly sophisticated and lingusitically-rich. One of the earlier examples of this is by Brent (1993), who employs a system based on deterministic morphological cues to identify predefined syntactic patterns from the Brown Corpus. Manning (1993) employs a shallow parser or chunker in order to acquire subcategorisation frames from the New York Times. Schulte im Walde (2002) induces subcategorisation information for German with the use of a lexicalised probabilistic context free grammar (PCFG), and O’Donovan et al. (2005) employ the richly-annotated Penn Treebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguisti"
I13-1080,J05-3003,0,0.0273919,"Missing"
I13-1080,E09-1013,0,0.0309022,"such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367 .114 .065 L EVIN - NOHDP .057 .111 T YPES - HDP"
I13-1080,schulte-im-walde-2002-subcategorisation,0,0.172537,"s much more varied and nuanced, as shown by Kroeger (2007), which is why we chose this morpho-syntactic construction as our case study. Since the early ’90s, the tools and resources employed in valency acquisition tasks have become increasingly sophisticated and lingusitically-rich. One of the earlier examples of this is by Brent (1993), who employs a system based on deterministic morphological cues to identify predefined syntactic patterns from the Brown Corpus. Manning (1993) employs a shallow parser or chunker in order to acquire subcategorisation frames from the New York Times. Schulte im Walde (2002) induces subcategorisation information for German with the use of a lexicalised probabilistic context free grammar (PCFG), and O’Donovan et al. (2005) employ the richly-annotated Penn Treebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discov"
I13-1080,S10-1080,0,0.0191369,"orted are based on the median of 11 random assignments. We use pairwise precision (pP ), recall (pR), and F-score (pF1 ) to evaluate our generated clusters, relative to the gold-standard word classes, as described by Schulte im Walde (2006). 5 Modelling Distributional Similarity Results We perform two experiments. First, we apply the hierarchical Dirichlet process (HDP) to produce topic probabilities, over which we perform HAC. Second, we perform HAC over the raw unigram features (NoHDP), as our benchmark system, a method also employed by systems such as Schulte im Walde (2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in lea"
I13-1080,J06-2001,0,0.455149,"e specific morpho-syntactic behaviour of the kan-affixed verb is very much determined by the type of stem it attaches to, and its resulting behaviour varies from stem type to stem type (Kroeger, 2007; Vamarasi, 1999; Arka, 1993). The spectrum of variation induced by the affixing of -kan is not observed on all types of stems, and so being able to identify these superordinate types, representing the same morpho-syntactic variation, would assist greatly in accelerating lexicon development. It has been shown that Levin classes can be successfully induced employing unsupervised methods (Schulte im Walde, 2006; Kipper et al., 2006). We investigate the viability of automatiIn this study we investigate how we can learn both: (a) syntactic classes that capture the range of predicate argument structures (PASs) of a word and the syntactic alternations it participates in, but ignore large semantic differences in the component words; and (b) syntactico-semantic classes that capture PAS and alternation properties, but are also semantically coherent (a la Levin classes). We focus on Indonesian as our case study, a language that is spoken by more than 165 million speakers, but is nonetheless relatively under"
I13-1080,kipper-etal-2006-extending,0,0.035871,"rpho-syntactic behaviour of the kan-affixed verb is very much determined by the type of stem it attaches to, and its resulting behaviour varies from stem type to stem type (Kroeger, 2007; Vamarasi, 1999; Arka, 1993). The spectrum of variation induced by the affixing of -kan is not observed on all types of stems, and so being able to identify these superordinate types, representing the same morpho-syntactic variation, would assist greatly in accelerating lexicon development. It has been shown that Levin classes can be successfully induced employing unsupervised methods (Schulte im Walde, 2006; Kipper et al., 2006). We investigate the viability of automatiIn this study we investigate how we can learn both: (a) syntactic classes that capture the range of predicate argument structures (PASs) of a word and the syntactic alternations it participates in, but ignore large semantic differences in the component words; and (b) syntactico-semantic classes that capture PAS and alternation properties, but are also semantically coherent (a la Levin classes). We focus on Indonesian as our case study, a language that is spoken by more than 165 million speakers, but is nonetheless relatively under-resourced in terms of"
I13-1080,J04-1003,0,0.196135,"reebank in achieving this endeavour. In terms of resources, our work most closely resembles Brent (1993), in that we rely mainly on linguistic knowledge based on simple lexical features. However, the way linguistic knowledge is learned and applied is quite different, as we will see in Section 3 In terms of the methodology, the studies that we look to are those systems that are built to disambiguate and/or discover syntactico-semantic Levin-style classes, rather than systems that aim to induce valency or syntactic frame information from corpora. These can be built in a supervised fashion as in Lapata and Brew (2004) or tackled as a clustering task as in Schulte im Walde (2006) or Bonial et al. (2011). Lapata and Brew (2004) develop a semi-supervised system that generates, for a given verb and its syntactic frame, a probability distribution over the Levin verb classes. They then use this system to disambiguate tokens using collocation information. Our system, like Schulte im Walde (2006), uses an unsupervised clustering approach. In her approach, Schulte Im Walde employs hierarchical agglomerative clustering over parse features to discover word classes in German, and evaluates using manually-created golds"
I13-1080,E12-1060,1,0.841477,"(2002) for German and Jurgens and Stevens (2010) for English word sense induction. In both cases, we base our experiments on the 735 lexemes identified as being able to be affixed with -kan, and the unigram features from Section 4.1. Note, however, that evaluation is based on the subset of the 735 lexemes which were manually classified into classes and types in Section 3. We employ a bagging approach (sampling with Distributional semantic models are commonly employed in the induction and disambiguation of word senses (McCarthy and Carroll, 2003; Lapata and Brew, 2004; Brody and Lapata, 2009; Lau et al., 2012), and to a lesser extent, in learning syntactic classes and diathesis alternation behaviour (Parisien and Stevenson, 2011; Bonial et al., 2011). We infer lexical similarity and soft word clusters using topic modelling, based on a hierarchical Dirichlet process (HDP: Teh et al. (2006)), a non-parametric extension of latent Dirichlet allocation (LDA: Blei et al. (2003)). LDA is a Bayesian generative topic model that learns latent topics for a collection of documents 689 System Maj. Rand. ON - ALL ON - VERBS L EVIN - HDP .174 .367 .114 .065 L EVIN - NOHDP .057 .111 T YPES - HDP .281 .261 .271 .14"
I13-1080,P93-1032,0,\N,Missing
I17-1056,N16-1030,0,0.47323,"shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we ob"
I17-1056,P05-1045,0,0.658022,"t.cohn@unimelb.edu.au Abstract linear-chain Conditional Random Fields (CRFs) (Wang et al., 2011; Zhang et al., 2017), framing the task as one of sequence tagging. Although CRFs are adept at capturing local structure, the problem does not naturally suit a linear sequential structure, i.e. , a post may be a reply to either a neighbouring post or one posted far earlier within the same thread. In both cases, contextual dependencies can be long-range, necessitating the ability to capture dependencies between arbitrarily distant items. Identifying this key limitation, Sutton and McCallum (2004) and Finkel et al. (2005) proposed the use of CRFs with skip connections to incorporate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a s"
I17-1056,P10-1081,0,0.0317937,"y said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice inference tractability: our model is capable of incorporating non-local features, and is compatible with exact inference methods. mentation of the model is available at: https: //github.com/liufly/mecrf. The paper is organised as follows: after reviewing previous studies on capturing long range contextual dependencies and related m"
I17-1056,D15-1161,0,0.0641599,"Missing"
I17-1056,Q16-1037,0,0.435365,"porate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a significant locality bias in practice (Lai et al., 2015; Linzen et al., 2016). In this paper, taking inspiration from the work of Weston et al. (2015) on memory networks (M EM N ETs), we propose to extend CRFs by integrating external memory mechanisms, thereby enabling the model to look beyond localised features and have access to the entire sequence. This is achieved with attention over every entry in the memory. Experiments on named entity recognition and forum thread parsing, both of which involve long-range contextual dependencies, demonstrate the effectiveness of the proposed model, achieving state-of-the-art performance on the former, and outperforming a number o"
I17-1056,D14-1162,0,0.0826714,"span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we observe better performance with G LOV E (Pennington et al., 2014), for fair comparison purposes, we adopt the same S ENNA embeddings (Collobert et al., 2011) as are used in the baseline methods.5 Experimental Setup We choose Φ(xt ) to be a lookup function, returning the corresponding embedding xt of the word xt . In addition to the word features, we employ a subset of the lexical features described in Huang et al. (2015), based on whether the word: • starts with a capital letter; • is composed of all capital letters; • is composed of all lower case letters; • contains non initial capital letters; • contains both letters and digits; • contains punctuation. T"
I17-1056,W10-2923,1,0.914292,"Missing"
I17-1056,W09-1119,0,0.0736863,"g the ability of ME-CRF to capture document context, to aid in the identification and disambiguation of NEs. 5.1 Dataset and Task 5.3 The CoNLL 2003 NER shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of"
I17-1056,P06-1141,0,0.0305568,"istent identification and disambiguation. A related example is forum thread discourse analysis. Previous work has largely focused on 555 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 555–565, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP ··· B-ORG O O O Interfax news agency said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice"
I17-1056,D16-1021,0,0.0221529,"so associated with a title. We therefore use two encoders, Φtitle (·) and Φtext (·), to process them separately and then concatenate xt = [Φtitle (xt ); Φtext (xt )]&gt; . Here, Φtitle (·) and Φtext (·) take word embeddings as input, processing each post at the word level, as opposed to the post-level bi-directional GRU in Equations (1) and (2), and the representation of a post xt (either title or text) is obtained by transforming the last and first hidden states of the forward and backward word-level GRU, similar to Equation (3). Note that Φtitle (·) and Φtext (·) do not share parameters. As in Tang et al. (2016), we further restrict mki = cki to curb overfitting. In keeping with Wang (2014), we complement the textual representations with hand-crafted structural features Φs (xt ) ∈ R2 : • initiator: a binary feature indicating whether the author of the current post is the same as the initiator of the thread, • position: the relative position of the current post, as a ratio over the total number of posts in the thread; Also drawing on Wang (2014), we incorporate punctuation-based features Φp (xt ) ∈ R3 : the number of question marks, exclamation marks and URLs in the current post. The resultant feature"
I17-1056,W03-0419,0,0.193167,"Missing"
I17-1056,D11-1002,1,0.920639,"Missing"
I17-1056,N16-1174,0,0.0839786,"rk In this section, we review the different families of models that are relevant to this work, in capturing long-range contextual dependencies in different ways. Recurrent Neural Networks (RNNs). Recently, the broad adoption of deep learning methods in NLP has given rise to the prevalent use of RNNs. Long short-term memories (“LSTMs”: Hochreiter and Schmidhuber (1997)), a particular variant of RNN, have become particularly popular, and been successfully applied to a large number of tasks: speech recognition (Graves et al., 2013), sequence tagging (Huang et al., 2015), document categorisation (Yang et al., 2016), and machine translation (Cho et al., 2014; Bahdanau et al., 2014). However, as pointed out by Lai et al. (2015) and Linzen et al. (2016), RNNs — including LSTMs — are biased towards immediately preceding (or neighbouring, in the case of bidirectional RNNs) items, and perform poorly in contexts which involve long-range contextual dependencies, despite the inclusion of memory cells. This is further evidenced by the work of Cho et al. (2014), who show that the performance of a basic encoder–decoder deteriorates as the length of the input sentence increases. Conditional Random Fields (CRFs). CRF"
I17-1056,W14-4012,0,\N,Missing
J09-2001,J07-4002,0,0.0149345,"used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Mannin"
J09-2001,P98-1013,0,0.0554213,"Missing"
J09-2001,baldwin-etal-2004-road,1,0.714541,"eposition PVs allow limited coordination of PP objects (e.g., refer to the book and to the DVD), and the NP object of the selected preposition can be passivized (e.g., the book they referred to). Even subtler are the syntactic effects observed for determinerless PPs. The singular noun in the PP–D is often strictly countable (e.g., off screen, on break), resulting in syntactic markedness as, without a determiner, the noun does not constitute a saturated NP. This in turn dictates the need for a dedicated analysis in a linguistically motivated grammar in order to be able to avoid parse failures (Baldwin et al. 2004; van der Beek 2005). Additionally, there is considerable variation in the internal modiﬁability of determinerless PPs, with some not permitting any internal modiﬁcation (e.g., of course) and others allowing optional internal modiﬁcation (e.g., at considerable length). There are also, however, cases of obligatory internal modiﬁcation (e.g., at considerable/great expense vs. *at expense) and highly restricted internal modiﬁcation (e.g., at long last vs. *at great/short last). Balancing up these different possibilities in terms of over- and undergeneration in a grammar is far from trivial (Baldw"
J09-2001,W02-2001,1,0.68867,"Missing"
J09-2001,J04-4004,0,0.00886265,"ation. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the availability of a gold-standard parse tree for a given input. They proposed that, instead, PP attachment be evaluated as a means of post-processing over the raw output of an actual parser, and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does remarkably well at PP attachment without a dedicated PP attachment module; but also (b) that post-processing based on a range of methods developed over the RRR data set (Collins and Brooks 1995; Toutanova, Manning, and Ng 2004; Olteanu and Moldovan 2005) generally improves parser accuracy. In addition, they developed a variant of the RRR data set (RRR-sent) which contains full sentential contexts of possible PP attachment ambiguity. Others who have successfully built PP re-attachment models for speciﬁc parsers are Olteanu (2004) and Foth and Menzel (2006). Agirre, Baldwin, ¨ and Martinez"
J09-2001,W07-1604,0,0.0628822,"Missing"
J09-2001,copestake-etal-2002-multiword,1,0.504905,"Missing"
J09-2001,J01-1005,0,0.00769076,"for text classiﬁcation, and demonstrated that dependency tuples incorporating prepositions are a more effective document representation than simple words. In a direct challenge to the prevalent “stop word” perception of prepositions in information retrieval, Hansen (2005) and Lassen (2006) placed emphasis on not only prepositions but preposition semantics in a music retrieval system and ontology-based text search system, respectively. Information extraction is one application where prepositions are uncontroversially crucial to system accuracy, in terms of the role they play in named entities (Cucchiarelli and Velardi 2001; Toral 2005; Kozareva 2006) and in IE patterns, in linking the elements in a text (Appelt et al. 1993; Muslea 1999; Ono et al. 2001; Leroy and Chen 2002). Benamara (2005) used preposition semantics in a cooperative question answering system. In the context of cross-language question answering (CLQA), Hartrumpf, Helbig, and Osswald (2006) used MultiNet to interpret the semantics of German prepositions, and demonstrated that in instances where the answer passage contained a different preposition to that included in the original question, preposition semantics boosted the performance of their CL"
J09-2001,W07-1607,0,0.0580473,"Missing"
J09-2001,J09-1005,0,0.0399341,"Missing"
J09-2001,W07-1603,0,0.0220521,"1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed the aspectual and path properties of goal-marking postpositions in Japanese, and proposed an analysis based on predicate and event restrictions. Boonthum, Toida, and Levinstein (2005, 2006) deﬁned a generalpurpose sense inventory of seven prepositions (but purportedly applicable to all"
J09-2001,P06-2029,0,0.0121475,"repositional MWE types that cause the greatest syntactic problems in English, in terms of their relative frequency and tendency for syntactic variation, are: 1. verb-particle constructions (VPCs), where the verb selects for an intransitive preposition (e.g., chicken out or hand in: Deh´e et al. [2002]); 2. prepositional verbs (PVs), where the verb selects for a transitive preposition (e.g., rely on or refer to: Huddleston and Pullum [2002]); 3. determinerless PPs (PP–Ds), where a PP is made up of a preposition and singular noun without a determiner (e.g., at school, off screen: Baldwin et al. [2006]). All three MWE types undergo limited syntactic variation (Sag et al. 2002). For example, transitive verb particle constructions generally undergo the particle alternation, 125 Computational Linguistics Volume 35, Number 2 whereby the particle may occur either adjacent to the verb (e.g., tear up the letter), or be separated from the verb by the NP complement (e.g., tear the letter up). Some VPCs readily occur with both orders (like tear up), while others have a strong preference for a particular order (e.g., take off —under the interpretation of having the day off—tends to occur in the partic"
J09-2001,I08-1059,0,0.00951014,"Missing"
J09-2001,W99-0614,0,0.022322,"uple is eatv , pizzan1 , with p , chopsticksn2 . The high degree of interest in PP attachment stems from it being a common phenomenon when parsing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazi"
J09-2001,W06-2105,0,0.127742,"Missing"
J09-2001,W07-1608,0,0.045722,"Missing"
J09-2001,W07-1601,0,0.0955897,"tual reality, in the context of interpreting the spatial information of prepositions. For example, Xu and Badler (2000) developed a geometric deﬁnition of the motion trajectories of prepositions, whereas Tokunaga, Koyama, and Saito (2005) use potential functions to estimate the spatial extent of Japanese spatial nouns (which combine with postpositions to have a similar syntactic and semantic proﬁle to English spatial prepositions). Kelleher and van Genabith (2003) proposed a method for interpreting in front of and behind in a virtual reality environment based on different frames of reference. Hying (2007) carried out an analysis of preposition semantics in the HRCR Map Task corpus, and used it to evaluate two models of projective prepositions. Kelleher and Kruijff (2005) developed a model for grounding spatial expressions in visual perception and also for modeling proximity, and Reichelt and Verleih (2005) developed the B3D system for generating a computational representation of prepositions in geospatial applications. Furlan, Baldwin, and Klippel (2007) used preposition occurrence in Web data as a means of classifying landmarks for use in route directions. Finally, Kelleher and Costello (2009"
J09-2001,isahara-etal-2008-development,0,0.0127084,"tions in MT, and automatic error correction of preposition usage in non-native speaker text. Following the lead of Saint-Dizier (2006b), Jørgensen and Lønning (2009), and others, we also hope to see more crosslinguistic and typological research on the lexical semantics of prepositions. Although there has been a steady proliferation of WordNets for different languages, linked variously to English WordNet (e.g., EuroWordNet for several European languages [Vossen 1998], BALKANET for Balkan languages [Stamou et al. 2002], HowNet for Chinese [Dong and Dong 2006], and Japanese WordNet for Japanese [Isahara et al. 2008]), they have tended to follow the lead of English WordNet and focus exclusively on content words. Given the increasing maturity of resources such as the Preposition Project and PrepNet, the time seems right to develop preposition sense inventories for more languages, linked back to English. On the basis of currently available resources and future efforts such as these, we believe there will be a steady lowering of the barrier to including a more systematic handling of prepositions in NLP applications. 137 Computational Linguistics Volume 35, Number 2 The purpose of this article has been to hi"
J09-2001,W04-2604,0,0.162756,"Missing"
J09-2001,P03-1054,0,0.0059965,"en a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in"
J09-2001,W06-2102,0,0.0282577,"Missing"
J09-2001,W02-0802,0,0.135776,"S representation for the directional sense of up (as in up the stairs) is: (4) (toward Loc (nil 2) (UP Loc (nil 2) (∗ Thing 6))) where the numbers indicate the logical arguments of the predicates. This representation indicates that the logical subject of the PP (indexed by “2”; e.g., the piano in move the piano up the stairs) is relocated up in the direction of the logical argument (indexed by “6”; e.g., the stairs in our example), which is in turn a concrete thing. The LCS Lexicon was developed from a theoretical point of view and isn’t directly tied to corpus usage. The Preposition Project (Litkowski 2002; Litkowski and Hargraves 2005, 2006) is an attempt to develop a comprehensive semantic database for English prepositions, intended for NLP applications. The project took the New Oxford Dictionary of English (Pearsall 1998) as its source of preposition sense deﬁnitions, which it then ﬁne-tuned based on cross-comparison with both functionally tagged prepositions in FrameNet (Baker, Fillmore, and Lowe 1998) and the account of preposition semantics in a descriptive grammar of English (Quirk et al. 1985); it also draws partially on Dorr’s LCS deﬁnitions of prepositions. Importantly, the Prepositio"
J09-2001,W06-2106,0,0.216037,"Missing"
J09-2001,S07-1005,0,0.462645,"a decision tree classiﬁer based on a set of contextual features similar to those used in WSD systems. O’Hara and Wiebe (2009) is an updated version of this original research, using a broader range of resources. Ye and Baldwin (2006) also built on the earlier research, in attempting to enhance the accuracy of semantic role labeling with dedicated PP disambiguation. 131 Computational Linguistics Volume 35, Number 2 They demonstrated the potential for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, an"
J09-2001,U05-1008,1,0.776051,"ll work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short and long prepositions in Polish (Tseng 2004), and the"
J09-2001,J93-2004,0,0.0342879,"Missing"
J09-2001,W03-1810,0,0.0601999,"Missing"
J09-2001,E03-1079,0,0.011156,"relative to a baseline parser. As part of this effort, they developed a standardized data set for exploration of the interaction between lexical semantics and parsing/PP attachment accuracy. 3 Because it was automatically extracted, the RRR data set is notoriously noisy. For instance, Pantel and Lin (2000) observed that 133 tuples contain the as either n1 or n2 . 124 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications One signiﬁcant variation on the classic binary PP attachment task which attempts to generate a richer semantic characterisation of the PP is the work of Merlo (2003) and Merlo and Esteve Ferrer (2006), who included classiﬁcation of the PP as an argument or adjunct, making for a four-way classiﬁcation task. In this context, they found that PP attachment resolution for argument PPs is considerably easier than is the case for adjunct PPs. Returning to our observation that PP attachment can occur in multiple syntactic conﬁgurations, Merlo, Crocker, and Berthouzoz (1997) applied backed-off estimation to the problem of multiple PP attachment, in the form of 14 discrete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the bas"
J09-2001,W97-0317,0,0.0285464,"Missing"
J09-2001,J06-3002,0,0.0586485,"Missing"
J09-2001,odijk-2004-reusable,0,0.0197692,"Missing"
J09-2001,W03-0411,0,0.483873,"Missing"
J09-2001,J09-2002,0,0.213341,"Missing"
J09-2001,H05-1035,0,0.0667112,"P parser (Briscoe, Carroll, and Watson 2006) is highly effective at VPC identiﬁcation, and (b) that the incorporation of lexicalized models of selectional preferences can lead to modest improvements in parser accuracy. In terms of English PV extraction, Baldwin (2005b) proposed a method based on a combination of statistical measures and linguistic diagnostics, and demonstrated that the combination of statistics with linguistic diagnostics achieved the best extraction performance. Research on prepositional MWEs in languages other than English includes Krenn and Evert (2001) and Evert and Krenn (2005) on the extraction of German PP–verb collocations (which are similar to verbal idioms/verb–noun combinations in English [Fazly, Cook, and Stevenson 2009]) based on a range of lexical association measures. Pecina (2008) further extended this work using a much broader set of lexical association ¨ measures and classiﬁer combination. Looking at German, Domges et al. (2007) analyzed the productivity of PP–Ds headed by unter, and used their results to motivate a syntactic analysis of the phenomenon. For Dutch, van der Beek (2005) worked on the extraction of PP–Ds from the output of a parser, once ag"
J09-2001,P00-1014,0,0.0776453,"), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the"
J09-2001,S07-1040,0,0.127872,"Missing"
J09-2001,W02-0312,0,0.0166734,"Missing"
J09-2001,P05-1034,0,0.00722033,"Missing"
J09-2001,H94-1048,0,0.238432,"Missing"
J09-2001,W06-2109,0,0.0511325,"Missing"
J09-2001,W93-0307,0,0.114962,"f PP attachment (e.g., cases of n1 being a pronoun [high attachment], or the PP post-modifying n1 in subject position [low attachment]) to derive smoothed estimates of Prhigh ( p|v), Prhigh ( NULL|n) (i.e., the probability of n not being post-modiﬁed by a PP), and Prlow ( p|n), which then form the basis of Prhigh ( p|v, n) and Prlow ( p|v, n), respectively. The proposed method was signiﬁcant in demonstrating the effectiveness of simple co-occurrence probabilities, without explicit semantics or discourse processing, and also in its ability to operate without explicitly annotated training data. Resnik and Hearst (1993) observed that PP attachment preferences are also conditioned on the semantics of the noun object of the preposition in the PP, as can be seen in our earlier example of Kim eats pizza with chopsticks/anchovies where chopsticks leads to verb attachment and anchovies to noun attachment. Although they were unable to come up with a model which was empirically superior to existing methods which did not represent the semantics of the noun object, this paved the way for a new wave of research using the full 4-tuple of v, n1 , p, n2 . 123 Computational Linguistics Volume 35, Number 2 The ﬁrst to suc"
J09-2001,E95-1040,0,0.0174849,", classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics impinges on verb semantics. In other work, Sablayrolles (1995) classiﬁed 199 simple and complex spatial prepositions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to Dorr and Habash (2002) in developing a multilingual sense inventory for Basque postpositions and English and Spanish prepositions. Fort and Guillaume (2007) developed a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their particular interest was in enhancing parsing performance. Old (2003) analyzed Roget’s Thesaurus and arrived at the conclusion that it was not a good source of standalone preposition semantics. Beavers (2003) analyzed"
J09-2001,saint-dizier-2006-prepnet,0,0.389946,"s an attempt to develop a compositional account of preposition semantics which interfaces with the semantics of the predicate (e.g., verb or predicative noun). Similarly to the English LCS Lexicon, it uses LCS as the descriptive language, in conjunction with typed λ-calculus and underspeciﬁed representations. Noteworthy elements of PrepNet are that it attempts to capture selectional constraints, metaphorical sense extension, and complex arguments. PrepNet was originally developed over French prepositions, but has since been applied to the analysis of instrumentals across a range of languages (Saint-Dizier 2006b). VerbNet (Kipper, Dang, and Palmer 2000; Kipper Schuler 2005) contains a shallow hierarchy of 50 spatial prepositions, classiﬁed into ﬁve categories. The hierarchy is 6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics. 130 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications derived from pioneering work by Sp¨arck Jones and Boguraev (1987), which is in turn derived from Wood (1979). The preposition sense inventory was used as the basis for extending VerbNet and led to signiﬁcant redevelopment of the verb class set (Kipper, Snyde"
J09-2001,I08-2106,0,0.148755,"Missing"
J09-2001,W04-2118,0,0.060867,"Missing"
J09-2001,W06-3312,0,0.0198173,"ete syntactic conﬁgurations. Unsurprisingly, they found the task considerably harder than the basic V NP PP case, due to increased ambiguity and data sparseness. Mitchell (2004) similarly performed an extensive analysis of the Penn Treebank to investigate the different contexts PP attachment ambiguities occur in, and the relative ability of different PP attachment methods to disambiguate each. There have also been domain-speciﬁc methods proposed for PP attachment, for example, in the area of biomedicine (Hahn, Romacker, and Schulz 2002; Pustejovsky et al. 2002; Leroy, Chen, and Martinez 2003; Schuman and Bergler 2006). 2.2 The Syntax of Prepositional Multiword Expressions Prepositions are also often found as part of multiword expressions (MWEs), such as verb-particle constructions (break down), prepositional verbs (rely on), determinerless PPs (in hospital), complex prepositions (by means of ) and compound nominals (affairs of state). MWEs are lexical items which are composed of more than one word and are lexically, syntactically, semantically, pragmatically, and/or statistically idiosyncratic in some way (Sag et al. 2002). In this section, we present a brief overview of the syntax of the key prepositional"
J09-2001,2003.mtsummit-papers.44,0,0.0189971,"Missing"
J09-2001,P93-1042,0,0.0400489,"exposure in the NLP literature but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finn"
J09-2001,P98-2201,0,0.0394995,"Missing"
J09-2001,J87-1007,0,0.395324,"Missing"
J09-2001,A00-1034,0,0.133018,"Missing"
J09-2001,W97-0109,0,0.0185413,"elemans, and Veenstra 1997; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in"
J09-2001,W08-1205,0,0.0228208,"Missing"
J09-2001,C08-1109,0,0.0128719,"Missing"
J09-2001,W00-1308,0,0.0182022,"ure but has been a latent feature of all work on part of speech (POS) tagging and parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive prepositions (RP), and unselected intransitive prepositions (RB, along with a range of other adverbials).2 There have been only isolated cases of research where a dedicated approach has been used to distinguish between these three sub-usages, in the interests of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005). Two large areas of research on the syntactic aspects of prepositions are (a) PPattachment and (b) prepositions in multiword expressions, which are discussed in the following sections. Although this article will focus largely on the syntax of prepositions in English, prepositions in other languages naturally have their own challenges. Notable examples which have been the target of research in computational linguistics ¨ are adpositions in Estonian (Muischnek, Mu¨ urisep, and Puolakainen 2005), adpositions in Finnish (Lestrade 2006), short a"
J09-2001,N07-1007,0,0.059034,"Missing"
J09-2001,W06-2103,0,0.0494386,"Missing"
J09-2001,E03-1051,0,0.0846401,"(Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of English–Japanese machine translation improved signiﬁcantly as a result. ¨ More recently, Atterer and Schutze (2007) challenged the real-world utility of methods based on the RRR data set, on the grounds that it is based on the ava"
J09-2001,C02-1004,0,0.0113591,"(Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and demonstrated that, the quality of"
J09-2001,W06-2112,0,0.0587411,"sing languages such as English, and hence a major cause of parser errors (Lin 2003). As such, it has implications for any task requiring full syntactic analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al. 2003). Languages other than English with PP attachment ambiguity which have been the target of research include Dutch (van Herwijnen et al. 2003), French (Gaussier and Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003; Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish (Kokkinakis 2000; Aasa 2004; Volk 2006). PP attachment research has undergone a number of signiﬁcant paradigm shifts over the course of the last three decades, and been the target of interest of theoretical syntax, AI, psycholinguistics, statistical NLP, and statistical parsing. Early research on PP attachment focused on the development of heuristics intended to model human processing strategies, based on analysis of the competing parse trees ¨ independent of lexical or discourse context (Frazier 1979; Schutze 1995). For example, Minimal Attachment was the strategy of choosing the attachment site which “minimizes” the parse tree, a"
J09-2001,P90-1004,0,0.024064,"this would be unable to disambiguate between Examples (2) and (3) as they both contain the same number of nodes. Late Attachment, on the other hand, was the strategy of attaching “low” in the parse tree, corresponding to Example (2). Ford, Bresnan, and Kaplan (1982) proposed an alternative heuristic strategy, based on the existence of p in a subcategorization frame for v. In later research, Pereira (1985) described a method for incorporating Right Association and Minimal Attachment 122 Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications into a shift-reduce parser, and Whittemore and Ferrara (1990) developed a rule-based algorithm to combine various attachment preferences on the basis of empirical evaluation of the predictive power of each. Syntactic preferences were of course a blunt instrument in dealing with PP attachment, and largely ineffectual in predicting the difference in PP attachment between Kim eats pizza with chopsticks (verb attachment) and Kim eats pizza with anchovies (noun attachment), for example. This led to a shift away from syntactic methods in the 1980s towards AI-inspired techniques which used world knowledge to resolve PP attachment ambiguity. In the case of Exam"
J09-2001,S07-1051,1,0.50376,"tial for accurate preposition labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decisi"
J09-2001,P98-2234,0,0.0307679,"97; Zhao and Lin 2004), log-linear models (Franz 1996), maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees (Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, and Moliner 1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer 1999). In addition to the four lexical and class-based features provided by the 4-tuple v, n1 , p, n2 , researchers have used noun deﬁniteness, distributional similarity, noun number, subcategorization categories, word proximity in corpus data, and PP semantic class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics. In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that Japanese translations of English verb and noun attachment involve distinct constructions. This allowed them to automatically identify instances of PP attachment ambiguity in a parallel English–Japanese corpus, complete with attachment information. They used this data to disambiguate PP attachment in English inputs, and d"
J09-2001,S07-1044,0,0.00886363,"position labeling to contribute to largescale improvements in overall semantic role labeling performance. As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical sample WSD task, participants were required to disambiguate token instances of each preposition relative to the provided discrete sense inventory. Three teams participated in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all systems outperforming two baselines over both ﬁne- and coarse-grained sense inventories, through various combinations of lexical, syntactic, and semantic features. The bestperforming system achieved F-scores of 0.818 and 0.861 over ﬁne- and coarse-grained senses, respectively (Ye and Baldwin 2007). In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein (2005, 2006) proposed a semantic collocation-based approach to preposition interpretation, and demonstrated the import of the method in a paraphrase recognition task. Alam (2003, 2004) used decision trees to d"
J09-2001,W97-1016,0,0.0754138,"Missing"
J09-2001,W99-0606,0,\N,Missing
J09-2001,J82-3004,0,\N,Missing
J09-2001,J87-3005,0,\N,Missing
J09-2001,J93-1005,0,\N,Missing
J09-2001,W99-0628,0,\N,Missing
J09-2001,E03-1044,1,\N,Missing
J09-2001,W06-2110,1,\N,Missing
J09-2001,W07-1605,0,\N,Missing
J09-2001,W06-1207,0,\N,Missing
J09-2001,W01-0707,0,\N,Missing
J09-2001,W06-2107,0,\N,Missing
J09-2001,W04-2608,0,\N,Missing
J09-2001,W06-2113,0,\N,Missing
J09-2001,A97-1052,0,\N,Missing
J09-2001,W04-0403,0,\N,Missing
J09-2001,J09-2003,0,\N,Missing
J09-2001,P08-1037,1,\N,Missing
J09-2001,calzolari-etal-2002-towards,0,\N,Missing
J09-2001,C98-1013,0,\N,Missing
J09-2001,C98-2229,0,\N,Missing
J09-2001,P03-2026,0,\N,Missing
J09-2001,P93-1032,0,\N,Missing
J09-2001,C98-2196,0,\N,Missing
J09-2001,W03-1812,1,\N,Missing
J09-2001,J10-4006,0,\N,Missing
J09-2001,P06-4020,0,\N,Missing
J09-2001,J09-2004,0,\N,Missing
J09-2001,J09-2005,0,\N,Missing
J09-2001,E06-3004,0,\N,Missing
J09-2001,P03-1065,0,\N,Missing
J09-2001,W07-1602,1,\N,Missing
J09-2001,W02-0804,0,\N,Missing
J09-2001,P98-2127,0,\N,Missing
J09-2001,C98-2122,0,\N,Missing
J09-2001,P07-1072,0,\N,Missing
J09-2001,W06-2104,0,\N,Missing
J09-4003,C02-1140,1,0.851582,"Missing"
J09-4003,C02-1059,1,0.871134,"Missing"
J09-4003,1997.iwpt-1.16,1,0.629816,"y of Manchester, UK Hozumi Tanaka—or Tanaka-sensei as he was fondly known to his colleagues and students in Japanese—passed away at the age of 67 in the early morning of 27 July 2009. He is survived by his wife Reiko and two sons. Tanaka-sensei’s primary contributions to natural language processing (NLP) are in parsing and semantic analysis. In parsing, he extended the GLR parsing algorithm to incorporate probabilities, multiple connection tables, and simultaneously carry out morphological and syntactic analysis for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tana"
J09-4003,1999.mtsummit-1.1,0,0.0383844,"is for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tanaka 2002), speech recognition (Itou, Hayamizu, and Tanaka 1992; Li, Tanaka, and Tokunaga 1995), dialogue systems (Akiba and Tanaka 1994; Funakoshi, Tokunaga, and Tanaka 2002), and automatic music generation (Suzuki, Tokunaga, and Tanaka 1999). He was the author or editor of a number of popular introductory texts on NLP in Japanese (Tanaka 1989, 1999a). Tanaka-sensei was the technical lead on the Japanese government-funded CICC Machine Translation Project (1987–1995) between East and South-East Asian langua"
J09-4003,1993.iwpt-1.10,0,0.382482,"Missing"
K15-1009,P14-2133,0,0.0119464,"results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequ"
K15-1009,D14-1012,0,0.011555,"rns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that B ROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to four sequence labelling tasks: POS tagging, Chunking, NER and MWE identification. We"
K15-1009,P14-2131,0,0.150071,"Missing"
K15-1009,P11-2125,0,0.0499902,"Missing"
K15-1009,P14-1023,0,0.0950679,"o achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and eval"
K15-1009,S13-1005,0,0.0137913,"Missing"
K15-1009,J92-4003,0,0.179922,"asks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“S KIP - GRAM”) (Mikolov et al., 2013b), and Global vectors (“G LOVE”) (Pennington et al., 2014). Al"
K15-1009,P09-1056,0,0.0429599,"Missing"
K15-1009,N13-1039,1,0.0552629,"Missing"
K15-1009,P08-1068,0,0.114458,"Missing"
K15-1009,D10-1125,0,0.0312081,"Missing"
K15-1009,D14-1162,0,0.127711,"ty in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequence labelling tasks. In addition, we explore the following research questions: RQ1: are word embeddings better than baseline approaches of one-hot unigram2 features and Brown clusters? RQ2: do word embeddings require less training data (i.e., generalise better) than one-hot unigram features? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrica"
K15-1009,P93-1024,0,0.70936,"act of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“S KIP - GRAM”) (Mikolov et al., 2013b), and Gl"
K15-1009,Q15-1016,0,0.069416,"Missing"
K15-1009,P09-1116,0,0.00595057,"s? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”"
K15-1009,Q14-1016,1,0.26664,"e relationship between w and its local contexts of use, either by predicting w based on its local context, or using w to predict the context words. Other than B ROWN , which utilises a cluster-based representation, all the other methods employ a distributed representation. The starting point for CBOW and S KIP - GRAM is to employ softmax to predict word occurrence: ! Tv exp(vw ctx(w) ) J(w, ctx(w)) = − log P T j∈V exp(vj vctx(w) ) 1 MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2 Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = |V |), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. 3 The word embedding approach proposed in Collobert et al. (2011) is not considered because it was found to be inferior to our four target word embedding approaches in previous work. 84 Data set UMBC (Han et al., 2013) One Billion (Chelba et al., 2013) English Wikipedia where vctx(w) denotes the distributed representation of th"
K15-1009,schneider-etal-2014-comprehensive,1,0.242578,"e relationship between w and its local contexts of use, either by predicting w based on its local context, or using w to predict the context words. Other than B ROWN , which utilises a cluster-based representation, all the other methods employ a distributed representation. The starting point for CBOW and S KIP - GRAM is to employ softmax to predict word occurrence: ! Tv exp(vw ctx(w) ) J(w, ctx(w)) = − log P T j∈V exp(vj vctx(w) ) 1 MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2 Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = |V |), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. 3 The word embedding approach proposed in Collobert et al. (2011) is not considered because it was found to be inferior to our four target word embedding approaches in previous work. 84 Data set UMBC (Han et al., 2013) One Billion (Chelba et al., 2013) English Wikipedia where vctx(w) denotes the distributed representation of th"
K15-1009,J93-2004,0,0.0551578,"wo predictions, because we want to evaluate all word representations with the same type of model — a first-order graph transformer. In training the distributed word representations, we consider two settings: (1) the word representations are fixed during sequence model training; and (2) the graph transformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English W"
K15-1009,N03-1028,0,0.0860405,"Missing"
K15-1009,D11-1118,0,0.00833748,"Missing"
K15-1009,W00-0726,0,0.138209,"sformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English Web Treebank5 (“EWT”) For all tasks other than MWE,6 we additionally have an out-of-domain test set, in order to evaluate the out-of-domain robustness of the different word representations, with and without updating. These datasets are as follows: POS tagging: the English Web Treebank with Penn POS tags ("
K15-1009,N03-1033,0,0.0101606,"Missing"
K15-1009,P10-1040,0,0.526824,"nately, it has been shown that they can be “pre-trained” from unlabelled text data using various algorithms to model the distributional hypothesis (i.e., that words which occur in similar contexts tend to be semantically similar). Pre-training methods have been refined considerably in recent years, and scaled up to increasingly large corpora. As with other machine learning methods, it is well known that the quality of the pre-trained word embeddings depends heavily on factors including parameter optimisation, the size of the training data, and the fit with the target application. For example, Turian et al. (2010) showed that the optimal dimensionality for word embeddings is taskspecific. One factor which has received relatively little attention in NLP is the effect of “updating” the pre-trained word embeddings as part of the task-specific training, based on self-taught learning (Raina et al., 2007). Updating leads to word representations that are task-specific, but often at the cost of over-fitting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tas"
K15-1009,D11-1116,0,\N,Missing
K15-1009,D14-1082,0,\N,Missing
K17-1022,W13-0102,0,0.679191,"network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment. Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014). We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels. Our primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic- and document-level topic model evaluation; (2) we demonstrate that previ"
K17-1022,D11-1024,0,0.355379,"Missing"
K17-1022,N10-1012,1,0.918028,"t basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness. 1 Introduction Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model o"
K17-1022,N16-1057,1,0.647912,"cs. Topics highlighted in pink (yellow) are those incorrectly selected by the system (humans) as intruder topics. 8 annotators and the automated method in intruder topic selection. To further understand how the topics relate to the documents in different topic models, we present documents with the corresponding topics for different topic models in Table 8. In the human annotation task, we use the top-10 most probable words to represent a topic. We use 10 words as it is the standard approach to visualising topics, but this is an important hyper-parameter which needs to be investigated further (Lau and Baldwin, 2016), which we leave to future work. Conclusion We demonstrate empirically that there can be large discrepancies between topic coherence and document–topic associations. By way of designing an artificial topic model, we showed that a topic model can simultaneously produce topics that are coherent but be largely undescriptive of the document collection. We propose a method to automatically predict document-level topic quality and found encouraging correlation with manual evaluation, suggesting that it can be used as an alternative approach for extrinsic topic model evaluation. 213 Document lda Topi"
K17-1022,E14-1056,1,0.963902,"lei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data perplexity is the obvious solution, but it has been shown to correlate poorly with direct human assessment of topic model quality (Chang et al., 2009), motivating the need for automatic topic model evaluation methods which emulate human assessment. Research in this vein has focused primarily on evaluating the quality of individual topics (Newman et al., 2010b; Mimno et al., 2011; Aletras and Stevenson, 2013; Lau et al., 2014; Fang et al., 2016) and largely ignored evaluation of topic allocations to individual documents, and it has become widely accepted that topic-level evaluation is a reliable indicator of the intrinsic quality of the overall topic model (Lau et al., 2014). We challenge this assumption, and demonstrate that topic model evaluation should operate at both the topic and document levels. Our primary contributions are as follows: (1) we empirically demonstrate that there can be large discrepancies between topic- and document-level topic model evaluation; (2) we demonstrate that previously-proposed doc"
K17-1022,P14-5010,0,0.0056709,"Missing"
K17-1022,Q17-1001,0,0.017275,"ysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness. 1 Introduction Topic models such as latent Dirichlet allocation (Blei et al., 2003) jointly learn latent topics (in the form of multinomial distributions over words) and topic allocations to individual documents (in the form of multinomial distributions over topics), and provide a powerful means of document collection navigation and visualisation (Newman et al., 2010a; Chaney and Blei, 2012; Smith et al., 2017). One property of LDA-style topic models that has contributed to their popularity is that they are highly configurable, and can be structured to capture a myriad of statistical dependencies, such as between topics (Blei and Lafferty, 2006), between documents associated with the same individual (Rosen-Zvi et al., 2004), or between documents associated with individuals in different network relations (Wang and Blei, 2011). This has led to a wealth of topic models of different types, and the need for methods to evaluate different styles of topic model over the same document collections. Test data"
L16-1042,E09-1013,0,0.0301409,"BNC. Here we too consider known-similarity corpora from the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are"
L16-1042,N09-1041,0,0.0423929,"er known-similarity corpora from the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency"
L16-1042,D10-1028,0,0.0316568,"the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency in one corpus compared"
L16-1042,P14-1025,1,0.843838,"ly construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency in one corpus compared to another — compu"
L16-1042,C10-1078,0,0.0670489,"Missing"
L16-1042,D14-1162,0,0.0831919,"Missing"
L16-1042,read-etal-2012-wesearch,0,0.0241819,"ere the number of KSC varies between 9 and 11 (as shown in Table 1). 3.2.1. K ILGARRIFF KSC We evaluated each of our measures on a subset of the KSC used in Kilgarriff (2001), referred to as K ILGARRIFF, comprising the text type pairs indicated in Table 1. Within each KSC set, the number of words in the corpora varies between 111k and 114k. The three letter codes refer to subsets of the BNC, and are described in Kilgarriff (2001). 3.2.2. WDC KSC The WeSearch Data Collection (“WDC”) is a collection of user-generated text designed to capture differences in both subject matter and writing style (Read et al., 2012). It contains text on the separate topics of NLP and the Linux operating system taken from blogs, Wikipedia, software reviews and forums. Using Linux as a fixed topic and varying the writing style by varying the source through blogs, reviews and forums, we created three KSC with differences in genre. Then, using forums as a fixed source, we created additional KSC by mixing the topics of NLP and Linux. Table 2 shows details of each WDC KSC we constructed. 3.2.3. G IGAWORD Corpus KSC The Gigaword corpus (Parker et al., 2009, “G IGAWORD”) is a collection of date-stamped newswire text. We used the"
L16-1042,baroni-bernardini-2004-bootcat,0,\N,Missing
N09-2018,baldwin-awab-2006-open,1,0.916247,"al. (1994)). 2.2 Malay Data Little work has been done on NLP for Malay, however, a stemmer (Adriani et al., 2007) and a probabilistic parser for Indonesian (Gusmita and Manurung, 2008) have been developed. The mutually intelligibility suggests that Malay resources could presumably be extended from these. In our experiments, we make use of a Malay– English translation dictionary, KAMI (Quah et al., 2001), which annotates about 19K nominal lexical entries for count classifiers. To limit very low frequency entries, we cross-reference these with a corpus of 1.2M tokens of Malay text, described in Baldwin and Awab (2006). We further exclude the two non-sortal count classifiers that are attested as default classifiers in the lexicon, as their distribution is heavily skewed and not lexicalised. In all, 2764 simplex common nouns are attested at least once in the corpus data. We observe 2984 unique noun–to–default classifier assignments. Polysemy leads to an average of 1.08 count classifiers assigned to a given wordform. The most difficult exemplars to classify, and consequently the most interesting ones, correspond to the dispreferred count classifiers of the multi-class wordforms: direct assignment and frequenc"
N09-2018,Y07-1001,1,0.269887,"frequency and machine learning models. We expect that this is a fruitful extension for Web–as–corpus approaches to lexicons in languages other than English, but further research is required in other South-East and East Asian languages. 1 2 2.1 Introduction The objective of this paper is to extend a Malay lexicon with count classifier information for nominal types. This is done under the umbrella of deep lexical acquisition: the process of automatically or semi-automatically learning linguistic structures for use in linguistically rich language resources such as precision grammars or wordnets (Baldwin, 2007). One might call Malay a “medium-density” language: some NLP resources exist, but substantially fewer than those for English, and they tend to be of low complexity. Resources like the Web seem promising for bootstrapping further resources, aided in part by simple syntax and a Romanised orthographic system. The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). We examine using a similar “first gloss” strategy to Lapata and Keller (akin to “first sense” in WSD, in this case, identifying the most basic surface form that a s"
N09-2018,C00-1014,0,0.0216542,"with ekor “animal” in 2 ekor raja “2 kingfishers”. An unintended classifier Proceedings of NAACL HLT 2009: Short Papers, pages 69–72, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics can lead to highly marked or infelicitious readings, such as #2 biji raja “2 (chess) kings”. Most research on count classifiers tends to discuss generating a hierarchy or taxonomy of the classifiers available in a given language (e.g. Bond and Paik (1997) for Japanese and Korean, or Shirai et al. (2008) cross-linguistically) or using languagespecific knowledge to predict tokens (e.g. Bond and Paik (2000)) or both (e.g. Sornlertlamvanich et al. (1994)). 2.2 Malay Data Little work has been done on NLP for Malay, however, a stemmer (Adriani et al., 2007) and a probabilistic parser for Indonesian (Gusmita and Manurung, 2008) have been developed. The mutually intelligibility suggests that Malay resources could presumably be extended from these. In our experiments, we make use of a Malay– English translation dictionary, KAMI (Quah et al., 2001), which annotates about 19K nominal lexical entries for count classifiers. To limit very low frequency entries, we cross-reference these with a corpus of 1.2"
N09-2018,N04-1016,0,0.0339955,"on: the process of automatically or semi-automatically learning linguistic structures for use in linguistically rich language resources such as precision grammars or wordnets (Baldwin, 2007). One might call Malay a “medium-density” language: some NLP resources exist, but substantially fewer than those for English, and they tend to be of low complexity. Resources like the Web seem promising for bootstrapping further resources, aided in part by simple syntax and a Romanised orthographic system. The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). We examine using a similar “first gloss” strategy to Lapata and Keller (akin to “first sense” in WSD, in this case, identifying the most basic surface form that a speaker would use to disambiguate between possible classes), where the Web is used a corpus to query a set of candidate surface forms, and the frequencies are used to disambiguate the lexical property. Due to the heterogeneity of the Web, we expect 69 Background Count Classifiers A count classifier (CL) is a noun that occurs in a specifier phrase with one of a set of (usually numeric) specifiers; the specifier phrase typically occu"
N09-2018,U08-1015,1,0.683444,"Missing"
N09-2018,I08-1052,0,0.0425678,"Missing"
N09-2018,C94-1091,0,0.0982712,"kingfishers”. An unintended classifier Proceedings of NAACL HLT 2009: Short Papers, pages 69–72, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics can lead to highly marked or infelicitious readings, such as #2 biji raja “2 (chess) kings”. Most research on count classifiers tends to discuss generating a hierarchy or taxonomy of the classifiers available in a given language (e.g. Bond and Paik (1997) for Japanese and Korean, or Shirai et al. (2008) cross-linguistically) or using languagespecific knowledge to predict tokens (e.g. Bond and Paik (2000)) or both (e.g. Sornlertlamvanich et al. (1994)). 2.2 Malay Data Little work has been done on NLP for Malay, however, a stemmer (Adriani et al., 2007) and a probabilistic parser for Indonesian (Gusmita and Manurung, 2008) have been developed. The mutually intelligibility suggests that Malay resources could presumably be extended from these. In our experiments, we make use of a Malay– English translation dictionary, KAMI (Quah et al., 2001), which annotates about 19K nominal lexical entries for count classifiers. To limit very low frequency entries, we cross-reference these with a corpus of 1.2M tokens of Malay text, described in Baldwin an"
N09-2065,W04-2412,0,0.0197543,"Missing"
N09-2065,P03-1002,0,0.0195741,"mplified in the word-order variants provided with (1). There are no discernible meaning differences between the provided variants, but there are various soft constraints on free word order, as discussed by Kroeger (1993) and Sells (2000). This paper describes research on parsing Tagalog text for predicate–argument structure (PAS). We first outline the linguistic phenomenon and corpus annotation process, then detail a series of PAS parsing experiments. 1 (1) Nagbigay ng Introduction Predicate–argument structure (PAS) has been shown to be highly valuable in tasks such as information extraction (Surdeanu et al., 2003; Miyao et al., 2009). In this research, we develop a resource for analysing the predicate–argument structure of Tagalog, a free word order language native to the Philippines, and carry out preliminary empirical investigation of PAS parsing methods over Tagalog. The motivation for this research is the investigation of the interaction between information structure and word order in Tagalog. That is, we wish to determine the utility of discourse-based contextual information in predicting word order in Tagalog, in a natural language generation context. We see PAS as the natural representation for"
N10-1002,I05-1015,0,0.0145244,"000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carroll and Oepen (2005) and Zhang et al. (2007b) have proposed efficient k-best unpacking algorithms that can selectively extract the most probable readings from the packed parse forest according to a discriminative parse disambiguation model, by minimising the number of potential unifications. The algorithm can be applied to unpack any passive edges. Because of the dynamic programming used in the algorithm and the hierarchical structure of the edges, the cost of the unpacking routine is empirically linear in the number of desired readings, and O(1) when invoked more than once on the same edge. The other challenge c"
N10-1002,W09-2609,0,0.0339964,"Missing"
N10-1002,P99-1069,0,0.0532038,"ired readings, and O(1) when invoked more than once on the same edge. The other challenge concerns the selection of informative and representative pieces of knowledge from the massive sea of partial analyses in the parsing chart. How to effectively extract the indicative features for a specific language phenomenon is a very task-specific question, as we will show in the context of the VPC extraction task in Section 3.2. However, general strategies can be applied to generate parse ranking scores on each passive edge. The most widely used parse ranking model is the loglinear model (Abney, 1997; Johnson et al., 1999; Toutanova et al., 2002). When the model does not use non-local features, the accumulated score on a sub-tree under a certain (unpacked) passive edge can be used to approximate the probability of the partial analysis conditioned on the sub-string within that span.3 3.2 The Application: Acquiring Features for VPC Extraction As stated above, the target task we use to illustrate the capabilities of our chart mining method is VPC extraction. The grammar we apply our chart mining method to in this paper is the English Resource Grammar (ERG, Flickinger (2002)), a large-scale precision HPSG for Engl"
N10-1002,P99-1061,0,0.00941937,"merican Chapter of the ACL, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be"
N10-1002,W09-1410,1,0.834536,"tures mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with precision grammars less"
N10-1002,W02-2018,0,0.0128608,"candidate VPC v - le:4, v np le:3, v p le:1, v p-np le:2 LE:M AX C ONS LE:M AX CR ANK PARTICLE off Table 1: Chart mining features used for VPC extraction S3−subjh(.875) S1−subjh(.125) S2−subjh(.925) VP5−hcomp VP1−hadj VP2−hadj(.325) VP3−hcomp v_−_le v_np_le v_p_le VP4−hcomp PP−hcomp v_p−np_le PRTL PREP NP1 NP2 DUMMY−V shows the boy 0 off 2 3 his new toys 4 7 Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG category classification: non-VPC, transitive VPC, or intransitive VPC. For the parameter estimation of the ME model, we use the TADM open source toolkit (Malouf, 2002). The token-level predictions are then combined with a simple majority voting to derive the type-level prediction for the VPC candidate. In the case of a tie, the method backs off to the na¨ıve baseline model described in Section 4.2, which relies on the combined probability of the verb and particle forming a VPC. We have also experimented with other ways of deriving type-level predictions from token-level classification results. For instance, we trained a separate classifier that takes the token-level prediction as input in order to determine the type-level VPC predic14 tion. Our results indi"
N10-1002,A00-2022,0,0.0296662,"que. First, there is potentially a huge number of parsing edges in the chart. For instance, when parsing with a large precision grammar like the HPSG English Resource Grammar (ERG, Flickinger (2002)), it is not unusual for a 20-word sentence to receive over 10,000 passive edges. In order to achieve high efficiency in parsing (as well as generation), ambiguity packing is usually used to reduce the number of productive passive edges on the parsing chart (Tomita, 1985). For constraint-based grammar frameworks like LFG and HPSG, subsumption-based packing is used to achieve a higher packing ratio (Oepen and Carroll, 2000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carr"
N10-1002,2004.tmi-1.2,0,0.0187101,"Missing"
N10-1002,J93-1007,0,0.167573,"ston and Pullum, 2002; Baldwin and Kim, 2009); for the purposes of our dataset, we assume that all particles are prepositional—by far the most common and productive of the three types—and further restrict our attention to single-particle VPCs (i.e., we ignore VPCs such as get along together). 11 One aspect of VPCs that makes them a particularly challenging target for lexical acquisition is that the verb and particle can be non-contiguous (for instance, hand the paper in and battle right on). This sets them apart from conventional collocations and terminology (cf., Manning and Sch¨utze (1999), Smadja (1993) and McKeown and Radev (2000)) in that they cannot be captured effectively using ngrams, due to their variability in the number and type of words potentially interceding between the verb and the particle. Also, while conventional collocations generally take the form of compound nouns or adjective–noun combinations with relatively simple syntactic structure, VPCs occur with a range of valences. Furthermore, VPCs are highly productive in English and vary in use across domains, making them a prime target for lexical acquisition (Deh´e, 2002; Baldwin, 2005; Baldwin and Kim, 2009). In the VPC datas"
N10-1002,P04-1057,0,0.0524824,"Missing"
N10-1002,zhang-kordoni-2006-automated,1,0.860652,"Determine whether each verb–preposition combination is a VPC or not, and further predict its valence(s) (i.e. unknown if VPC, and unknown valence(s)) VPC Determine whether each verb–preposition combination is a VPC or not ignoring valence (i.e. unknown if VPC, and don’t care about valence) Table 2: Definitions of the three DLA tasks 2001). We use a slightly modified version of the ERG in our experiments, based on the nov-06 release. The modifications include 4 newly-added dummy lexical entries for the verb DUMMY- V and the corresponding inflectional rules, and a lexical type prediction model (Zhang and Kordoni, 2006) trained on the LOGON Treebank (Oepen et al., 2004) for unknown word handling. The parse disambiguation model we use is also trained on the LOGON Treebank. Since the parser has no access to any of the verbs under investigation (due to the DUMMYV substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model’s performance. The chart mining feature extraction process took over 10 CPU days, and collected a total of 44K events for 4,090 candidate VPC triples.4 5-fold cross validation is used to train/test the model. As stated above (Section 2), the VPC triples at"
N10-1002,zhang-kordoni-2008-robust,1,0.826589,"es over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with"
N10-1002,W07-1217,1,0.887544,"Missing"
N10-1002,W07-2207,1,0.950451,"L, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be extracted (see Section"
N10-1002,A00-2018,0,\N,Missing
N10-1002,J97-4005,0,\N,Missing
N10-1002,P03-1059,1,\N,Missing
N10-1012,N09-1003,0,0.207692,"Missing"
N10-1012,E09-1013,0,0.0054854,"oduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned t"
N10-1012,N09-1067,0,0.0165068,"Missing"
N10-1012,N09-1041,0,0.0913028,"rest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpretability an"
N10-1012,O97-1002,0,0.036404,"Missing"
N10-1012,J98-1006,0,0.00953055,"is the better combination method. 102 We also experimented with the median, and trialled filtering the set of senses in a variety of ways, e.g. using only the first sense (the sense with the highest prior) for a given word, or using only the word senses associated with the POS with the highest prior. In all cases, the overall trend was for the correlation with the human scores to drop relative to the mean, so we only present the numbers for the mean in this paper. count of nodes includes the beginning and ending word nodes. Leacock-Chodorow (LC H) The measure of semantic similarity devised by Leacock et al. (1998) finds the shortest path between two WordNet synsets (sp(c1 , c2 )) using hypernym and synonym relationships. This path length is then scaled by the maximum depth of WordNet (D), and the log likelihood taken: simlch (c1 , c2 ) = − log sp(c1 , c2 ) 2·D Wu-Palmer (W U P) Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2 ) by the depth of their LCS (depth(lcsc1 ,c2 )): simwup (c1 , c2 ) = 2 · depth(lcsc1 ,c2 ) depthc1 + depthc2 + 2 · depth(lcsc1 ,c2 ) The scaling means that specific terms (deeper in the hierarchy) that are close together are more semant"
N10-1012,P98-2127,0,0.0272583,"Missing"
N10-1012,W04-1013,0,0.0367688,"me possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than 101 perplexity for evaluating topic models. In earlier work, we carried out preliminary experimentation using pointwise mutual information and Google results to evaluate topic coherence over the same set of topics as used in this research (Newman et al., 2009). Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al., 2002) and automatic summarisation (Lin, 2004). Here, the development of automated methods with high correlation with human subjects has opened the door to large-scale automated evaluation of system outputs, revolutionising the respective fields. While our aspirations are more modest, the basic aim is the same: to develop a fully-automated method for evaluating a well-grounded task, which achieves near-human correlation. 3 Topic Modelling In order to evaluate topic modelling, we require a topic model and set of topics for a given document collection. While the evaluation methodology we describe generalises to any method which generates se"
N10-1012,W08-2106,0,0.0201307,"ile statistical evaluation of topic models is reasonably well understood, there has been much less work on evaluating the intrinsic semantic quality of topics learned by topic models, which could have a far greater impact on the overall value of topic modeling for end-user applications. Some researchers have started to address this problem, including Mei et al. (2007) who presented approaches for automatic labeling of topics (which is core to the question of coherence and semantic interpretability), and Griffiths and Steyvers (2006) who applied topic models to word sense discrimination tasks. Misra et al. (2008) used topic modelling to identify semantically incoherent documents within a document collection (vs. coherent topics, as targeted in this research). Chang et al. (2009) presented the first human-evaluation of topic models by creating a task where humans were asked to identify which word in a list of five topic words had been randomly switched with a word from another topic. This work showed some possibly counter-intuitive results, where in some cases humans preferred models with higher perplexity. This type of result shows the need for further exploring measures other than 101 perplexity for"
N10-1012,P02-1040,0,0.105865,"Missing"
N10-1012,J98-1004,0,0.0199885,"Missing"
N10-1012,P08-2068,0,0.0128048,"pture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human. After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpretability and association with a single over-arching semantic concept. We then propose models to p"
N10-1012,widdows-ferraro-2008-semantic,0,0.0587849,"inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best. 1 Introduction There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis (Deerwester et al., 1990), probabilistic latent semantic analysis (Hofmann, 2001), random projection (Widdows and Ferraro, 2008), and more recently, latent Dirichlet allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al., 2008). In each This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics. That is, we develop me"
N10-1012,P94-1019,0,\N,Missing
N10-1012,J06-1003,0,\N,Missing
N10-1012,C98-2122,0,\N,Missing
N10-1027,D07-1016,0,0.0934962,"particular resources for each language to be identified (e.g. POS taggers, or the existence of precompiled stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: Corpus E URO GOV TCL W IKIPEDIA Documents 1500 3174 4963 Languages 10 60 67 Encodings 1 12 1 Document Length (bytes) 17460.5±39353.4 2623.2±3751.9 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents"
N10-1027,hughes-etal-2006-reconsidering,1,0.496442,"used to render a large number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language. Given this, the need for automatic means to determine the source language of web doc1 http://dev.opera.com/articles/view/ mama-head-structure/ uments is crucial for web aggregators of various types. There is widespread misconception of language identification being a “solved task”, generally as a result of isolated experiments over homogeneous datasets with small numbers of languages (Hughes et al., 2006; Xia et al., 2009). Part of the motivation for this paper is to draw attention to the fact that, as a field, we are still a long way off perfect language identification of web documents, as evaluated under realistic conditions. In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance. We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and inve"
N10-1027,W09-0307,0,0.0738341,"led stop word lists) cannot be used. Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004). It can also be used to “word spot” foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al., 2009; Xia and Lewis, 2009). 3 Datasets In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research: Corpus E URO GOV TCL W IKIPEDIA Documents 1500 3174 4963 Languages 10 60 67 Encodings 1 12 1 Document Length (bytes) 17460.5±39353.4 2623.2±3751.9 1480.8±4063.9 Table 1: Summary of the three language identification datasets Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents in that language) the web. All three corpora are available on request. We outline the characteristics of"
N10-1027,E09-1099,0,0.485195,"e number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language. Given this, the need for automatic means to determine the source language of web doc1 http://dev.opera.com/articles/view/ mama-head-structure/ uments is crucial for web aggregators of various types. There is widespread misconception of language identification being a “solved task”, generally as a result of isolated experiments over homogeneous datasets with small numbers of languages (Hughes et al., 2006; Xia et al., 2009). Part of the motivation for this paper is to draw attention to the fact that, as a field, we are still a long way off perfect language identification of web documents, as evaluated under realistic conditions. In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance. We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and investigate the interac"
N12-1040,adolphs-2008-acquiring,0,0.0145551,"morphotactics causes, amongst other phenomena, the final consonant of a morpheme to assimilate the manner of the initial consonant of the following morpheme (as in -villi), or to be dropped (as in natsiviniq-). Consequently, morphemes are not readily accessible from the realised surface form, thereby motivating the use of a morphological analyser. 2.2 Morphological analysis For many languages with a less rich morphology than Inuktitut, an inflectional lexicon is often adequate for morphological analysis (for example, CELEX for English (Burnage, 1990), Lefff for French (Sagot et al., 2006) or Adolphs (2008) for German). Another typical approach is to perform morphological analysis at the same time as POS tagging (as in Hajiˇc and Hladk´a (1998) for the fusional morphology in Czech), as it is often the case that 372 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372–376, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics determining the part-of-speech and choosing the appropriate inflectional paradigm are closely linked. For highly inflecting languages more generally, morphological"
N12-1040,W02-0603,0,0.0515774,"for the fusional morphology in Czech), as it is often the case that 372 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372–376, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics determining the part-of-speech and choosing the appropriate inflectional paradigm are closely linked. For highly inflecting languages more generally, morphological analysis is often treated as a segmentand-normalise problem, amenable to analysis by weighted finite state transducer (wFST), for example, Creutz and Lagus (2002) for Finnish. 3 Resources 3.1 A morphological analyser for Inuktitut The main resource that we are evaluating in this work is a morphological analyser of Inuktitut called Uqa·Ila·Ut.1 It is a rule-based system based on regular morphological variations of about 3200 head, 350 lexical, and 1500 grammatical morphemes, with heuristics for ranking the various readings. The head and lexical morphemes are collated with glosses in both English and French. 3.2 Word alignment The training corpus we use in our experiments is a sentence-aligned segment of the Nunavut Hansards (Martin et al., 2003). The co"
N12-1040,P07-1003,0,0.0273032,"alignments, 1 http://inuktitutcomputing.ca/Uqailaut/ en/IMA.html 373 and 1679 probable), which we use to evaluate word alignments. Our treatment of the alignment problem is most similar to Schafer and Dr´abek (2005) who examine four systems: GIZA++ models (Och and Ney, 2000) for each source-target direction, another where the Inuktitut input has been syllabised, and a wFST model. They observe that aggregating these results through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target direction) over five iterations,2 with so-called competitive thresholding in the decoding step; these are more fully described in DeNero and Klein (2007) and Liang et al. (2006). Our approach examines morphological preprocessing of the Inuktitut training and test sets, with the idea of leveraging the morphological information into a corpus which is more"
N12-1040,P98-1080,0,0.0426799,"Missing"
N12-1040,W05-0810,0,0.0198757,"ich is more in line with the particularities of the task. 1 (1) natsiq- -viniq- -tuq- -lauq- -simaseal meat eat before ever -vit -li INT-2s but “But have you ever eaten seal meat before?” Introduction In this work, we evaluate a morphological analyser of Inuktitut, whose polysynthetic morphosyntax can cause particular problems for natural language processing; but our observations are also relevant to other languages with rich morphological systems. The existing NLP task for Inuktitut is that of word alignment (Martin et al., 2005), where Inuktitut tokens align to entire English clauses. While Langlais et al. (2005) theorises that a morphological analyser could aid in this task, we observed little to no improvement over a baseline model by making use of its segmentation. Nonetheless, morphological analysis does provide a great deal of information, but the task structure tends to disprefer its contribution. 2 2.1 Background Inuktitut Inuktitut is a macrolanguage of many more-or-less mutually intelligible dialects (Gordon, 2005). The Lowe (1996) analyses the morphology as a fourplace relationship: one head morpheme, zero or more lexical morphemes, one or more grammatical morphemes, and an optional enclitic"
N12-1040,N06-1014,0,0.0426572,"s through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target direction) over five iterations,2 with so-called competitive thresholding in the decoding step; these are more fully described in DeNero and Klein (2007) and Liang et al. (2006). Our approach examines morphological preprocessing of the Inuktitut training and test sets, with the idea of leveraging the morphological information into a corpus which is more amenable to alignment. The raw corpus appears to be undersegmented, where data sparseness from the many singletons would prevent reliable alignments. Segmentation might aid in this process by making sublexical units with semantic overlap transparent to the alignment system, so that types appear to have a greater frequency through the data. Through this, we attempt to examine the hypothesis that one-toone alignments be"
N12-1040,W03-0320,0,0.2207,"Missing"
N12-1040,W05-0809,0,0.126928,"that the richer approaches provide little as compared to simply finding the head, which is more in line with the particularities of the task. 1 (1) natsiq- -viniq- -tuq- -lauq- -simaseal meat eat before ever -vit -li INT-2s but “But have you ever eaten seal meat before?” Introduction In this work, we evaluate a morphological analyser of Inuktitut, whose polysynthetic morphosyntax can cause particular problems for natural language processing; but our observations are also relevant to other languages with rich morphological systems. The existing NLP task for Inuktitut is that of word alignment (Martin et al., 2005), where Inuktitut tokens align to entire English clauses. While Langlais et al. (2005) theorises that a morphological analyser could aid in this task, we observed little to no improvement over a baseline model by making use of its segmentation. Nonetheless, morphological analysis does provide a great deal of information, but the task structure tends to disprefer its contribution. 2 2.1 Background Inuktitut Inuktitut is a macrolanguage of many more-or-less mutually intelligible dialects (Gordon, 2005). The Lowe (1996) analyses the morphology as a fourplace relationship: one head morpheme, zero"
N12-1040,P00-1056,0,0.102354,"nments, where phrasal alignments of one token in both the source and target were (generally) called sure alignments, and one-to-many or many-to-many mappings were extended to their cartesian product, and called probable. The test set was composed of 75 of these sentences (about 2K English tokens, 800 Inuktitut tokens, 293 gold-standard sure alignments, 1 http://inuktitutcomputing.ca/Uqailaut/ en/IMA.html 373 and 1679 probable), which we use to evaluate word alignments. Our treatment of the alignment problem is most similar to Schafer and Dr´abek (2005) who examine four systems: GIZA++ models (Och and Ney, 2000) for each source-target direction, another where the Inuktitut input has been syllabised, and a wFST model. They observe that aggregating these results through voting can create a very competitive system for Inuktitut word alignment. 4 Experimental approach We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. The default implementation of the system involves two jointlytrained HMMs (one for each source-target directio"
N12-1040,W05-0811,0,0.0423458,"Missing"
N12-1040,C98-1077,0,\N,Missing
N12-1040,sagot-etal-2006-lefff,0,\N,Missing
N15-1099,W03-1812,1,0.8105,"h to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three models, we first greedily pre-tokenise the corpus to represent each MWE as a single token, similarly to Baldwin et al. (2003). In this, we apply the constraint that no language-specific pre-processing can be applied to the training corpus, in order to make the method maximally language independent. As such, we cannot perform any form of lemmatisation, and MWE identification takes the form of simple string match for concatenated instances of the component words, naively assuming that all occurrences of that word combination are MWEs. We detail each of the distributional similarity methods below. 978 Count-Based Distributional Similarity Our first method for building vectors is that of Salehi et al. (2014b): the top 5"
N15-1099,P14-2131,0,0.0131551,"mple approaches to composition, single word embeddings are empirically slightly superior to multi-prototype word embeddings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of la"
N15-1099,P14-1023,0,0.0657342,"Missing"
N15-1099,W11-1304,0,0.016365,"g pool); (2) English verb particle constructions (“EVPCs”, e.g. stand up and give away); and (3) German noun compounds (“GNCs”, e.g. ahornblatt “maple leaf” and eidechse “lizard”). The ENC dataset consists of 90 binary English noun compounds, and is annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun (Reddy et al., 2011). The state-of-the-art method for this dataset (Salehi et al., 2014b) is a supervised support vector regression 3 We also considered using the dataset from the DisCo shared task (Biemann and Giesbrecht, 2011), but ultimately excluded it because it includes different types of MWEs without indication of the syntactic type of a given MWE, preventing us from carrying out construction-specific parameter tuning. 979 model, trained over the distributional method from Section 3.1 as applied to both English and 51 target languages (under word and MWE translation). The EVPC dataset consists of 160 English verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle (Bannard, 2006). In order to translate the dataset into a regression tas"
N15-1099,C14-1071,0,0.0244373,"ocher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three"
N15-1099,P14-1129,0,0.0601437,"Missing"
N15-1099,J09-1005,1,0.804368,"ks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WOR"
N15-1099,D14-1113,0,0.0815431,"ogle.com/p/word2vec/ (“C-S KIP”), whereby a given word in context is projected onto a projection layer, and used to predict its immediate context (preceding and following words). WORD 2 VEC generates a vector of fixed dimensionality d for each pre-tokenised word/MWE type with frequency above a certain threshold in the training corpus. We again use comp 1 and comp 2 to estimate compositionality from these vectors. 3.3 Multi-Sense Skip-gram Model One potential shortcoming of WORD 2 VEC is that it generates a single word embedding for each word, irrespective of the relative polysemy of the word. Neelakantan et al. (2014) proposed a method motivated by WORD 2 VEC, which efficiently learns multiple embeddings per word/MWE. We refer to this approach as the multi-sense skip-gram (MSSG) model. We once again compose the resultant vectors with comp 1 and comp 2 , but modify the formulation slightly to handle the variable number of vectors for each word/MWE, by searching over the cross-product of vectors in each sim calculation and taking the maximum in each case. We initially set the number of embeddings to 2 in our MSSG experiments — in keeping with the findings in Neelakantan et al. (2014) — but come back to exami"
N15-1099,W14-1609,0,0.0130291,"dings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form"
N15-1099,I11-1024,0,0.847727,"rch questions here are: Introduction Multiword expressions (MWEs) are word combinations that display some form of idiomaticity (Baldwin and Kim, 2009), including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower). Recent NLP work on semantic idiomaticity has focused on the task of “compositionality prediction”, in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2014b). Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of “word embeddings” (Collobert and Weston, 2008; Mikolov et al., RQ1: Are word embeddings superior to conventional count-based models of distributional similarity? RQ2: How sensitive to parameter optimisation are different word embedding approaches? RQ3: Are multi-prototype word embeddings empirically superior to single-prototype word embeddings? We explore these questions relative to three compositionality pr"
N15-1099,S13-1039,1,0.827666,"cale for each of the head verb and particle (Bannard, 2006). In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC. The state-of-the-art method for this dataset (Salehi et al., 2014b) is a linear combination of: (1) the distributional method from Section 3.1; (2) the same method applied to 10 target languages (under word and MWE translation, selecting the languages using supervised learning); and (3) the string similarity method of Salehi and Cook (2013). The GNC dataset consists of 246 German noun compounds, and is annotated on a continuous [1, 7] scale (von der Heide and Borgwaldt, 2009; Schulte im Walde et al., 2013). The state-of-the-art method for this dataset is a distributional similarity method applied to part-of-speech tagged and lemmatised data (Schulte im Walde et al., 2013). 5 Experiments For all experiments, we train our models over raw text Wikipedia corpora for either English or German, depending on the language of the dataset. The raw English and German corpora were preprocessed using the WP2TXT toolbox4 to eliminate XML and H"
N15-1099,D14-1189,1,0.891505,"Missing"
N15-1099,E14-1050,1,0.506823,"essions (MWEs) are word combinations that display some form of idiomaticity (Baldwin and Kim, 2009), including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower). Recent NLP work on semantic idiomaticity has focused on the task of “compositionality prediction”, in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2014b). Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of “word embeddings” (Collobert and Weston, 2008; Mikolov et al., RQ1: Are word embeddings superior to conventional count-based models of distributional similarity? RQ2: How sensitive to parameter optimisation are different word embedding approaches? RQ3: Are multi-prototype word embeddings empirically superior to single-prototype word embeddings? We explore these questions relative to three compositionality prediction datasets spanning two MWE construction type"
N15-1099,Q14-1016,0,0.0499337,"on (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three models, we first greedily pre-tokenise the cor"
N15-1099,S13-1038,0,0.198581,"Missing"
N15-1099,D12-1110,0,0.0704611,"Missing"
N15-1099,D13-1170,0,0.00176361,"d embeddings are empirically slightly superior to multi-prototype word embeddings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014"
N15-1099,D13-1141,0,0.0333351,"The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predictin"
N15-1124,W07-0718,0,0.144209,"ood that an individual metric’s correlation with human judgment is not equal to zero. In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also be taken into account by significance tests. 3 Segment-Level Human Evaluation Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge (Callison-Burch et al., 2007; CallisonBurch et al., 2008; Callison-Burch et al., 2009). An alternative approach is to take into account the fact that different judges may genuinely disagree, and allow assessments provided by individuals to each contribute to an overall estimate of the quality of a given translation. In an ideal world in which we had access to assessments provided by the entire population of qualified human assessors, for example, the mean of those assessments would provide a statistic that, in theory at least, would provide a meaningful segment-level human score for translations. If it were possible to c"
N15-1124,W08-0309,0,0.101169,"Missing"
N15-1124,W11-2107,0,0.0326716,"evaluation data. Figure 5 displays the outcome of the Williams significance test as applied to each pairing of competing metrics. Since the power of Williams test increases with the strength of correlation between a pair of metrics, it is important not to conclude the best system by the number of other metrics it outperforms. Instead, the best choice of metric for that language pair is any metric that is not signicifantly outperformed by any other metric. Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENT BLEU- MOSES (sBLEU-moses). 1189 0.05 0.1 Figure 5: Evaluation of significance of increase in correlation with human judgment between every pair of segment-level metrics competing in the Spanish-toEnglish W MT-13 metrics task. A colored cell (i,j) indicates that system named in row i significantly outperforms system named in column j at p &lt; 0.1, and green cells at p &lt; 0.05. Metric METEOR NLEPOR SENT BLEU- MOSES SIMP BLEU P SIMP BLEU R LEPOR r 0.441 0.416 0.422 0.418 0.404 0.326 Table 3: Pearson’s correlation between each W MT-13 segment-level metric and huma"
N15-1124,D14-1020,1,0.427255,"nor indeed did the evaluation ever directly compare translations for different source language inputs (as relative preference judgments were always relative to other translations for the same input). Pearson’s correlation, on the other hand, compares scores across the entire test set. 4.1 Significance Testing of Segment-level Metrics With the move to Pearson’s correlation, we can also test statistical significance in differences between metrics, based on the Williams test (Williams, 1959),3 which evaluates significance in a difference in dependent correlations (Steiger, 1980). As suggested by Graham and Baldwin (2014), the test is appropriate for evaluation of document-level MT metrics since the data is not independent, and for similar reasons, the test can also be used for evaluation of segment-level metrics. 4.2 Spanish-to-English Segment-level Metrics We first carry out tests for Spanish-to-English segment-level metrics from W MT-13. In our experiments in Section 3.1, we used only a sub-sample 3 Also sometimes referred to as the Hotelling–Williams test. 1188 METEOR NLEPOR SENT BLEU- MOSES DEP - REF - EX DEP - REF - A SIMP BLEU P SIMP BLEU R LEPOR UMEANT MEANT TERRORCAT r τ 0.484 0.483 0.465 0.453 0.453"
N15-1124,W13-2305,1,0.776539,"ents must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13, where each point repre"
N15-1124,E14-1047,1,0.149696,"tion: how many assessments must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13,"
N15-1124,W14-3333,1,0.814581,"tion: how many assessments must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13,"
N15-1124,W13-2253,0,0.0188496,"the outcome of the Williams significance test as applied to each pairing of competing metrics. Since the power of Williams test increases with the strength of correlation between a pair of metrics, it is important not to conclude the best system by the number of other metrics it outperforms. Instead, the best choice of metric for that language pair is any metric that is not signicifantly outperformed by any other metric. Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENT BLEU- MOSES (sBLEU-moses). 1189 0.05 0.1 Figure 5: Evaluation of significance of increase in correlation with human judgment between every pair of segment-level metrics competing in the Spanish-toEnglish W MT-13 metrics task. A colored cell (i,j) indicates that system named in row i significantly outperforms system named in column j at p &lt; 0.1, and green cells at p &lt; 0.05. Metric METEOR NLEPOR SENT BLEU- MOSES SIMP BLEU P SIMP BLEU R LEPOR r 0.441 0.416 0.422 0.418 0.404 0.326 Table 3: Pearson’s correlation between each W MT-13 segment-level metric and human assessment for the combin"
N15-1124,W04-3250,0,0.28857,"Missing"
N15-1124,W14-3336,0,0.0917597,"Missing"
N15-1124,W09-0401,0,\N,Missing
N15-1124,aziz-etal-2012-pet,0,\N,Missing
N15-1124,2012.eamt-1.31,0,\N,Missing
N15-1124,W14-3302,0,\N,Missing
N15-1124,W13-2201,0,\N,Missing
N15-1124,2012.tc-1.5,0,\N,Missing
N15-1153,D10-1124,0,0.666289,"Missing"
N15-1153,C12-1064,1,0.82669,"ber of users with reliable location information, there has been significant interest in the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking differ"
N15-1153,D12-1137,0,0.819755,"ignificant interest in the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking different languages (Han et al., 2014), but is also reflected in regional"
N15-1153,D14-1039,0,0.505918,"n the task 2 Related Work Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first prepartitioning the regions into discrete sub-regions using either known city locations (Han et al., 2012; Rout et al., 2013) or a k-d tree partitioning (Roller et al., 2012; Wing and Baldridge, 2014). In the k-d tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy. 1362 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1362–1367, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2.1 Text-based Geolocation Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking different languages (Han et al., 2014), but is also reflected in regional dialects and the use of reg"
N16-1057,W13-0102,0,0.759356,"clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009), N = 5, whereas for Newman et al. (2010), Aletras and Stevenson (2013) and Lau"
N16-1057,E14-1056,1,0.633524,"r document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arbitrarily: for Chang et al. (2009), N = 5, whereas for Newman et al. (2010), Aletras and Stevenson (2013) and Lau et al. (2014), N = 10. 483 Proceedings of NAACL-HLT 2016, pages 483–487, c San Diego, California, June 12-17, 2016. 2016 A"
N16-1057,D11-1024,0,0.5959,"lity.1 1 Introduction Latent Dirichlet Allocation (“LDA”: Blei et al. (2003)) is an approach to document clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of topic words, or its cardinality. These methodologies evaluate coherence over the top-N topic words, where N is selected arb"
N16-1057,N10-1012,1,0.782452,"substantially more stable and robust evaluation. We release the code and the datasets used in this research, for reproducibility.1 1 Introduction Latent Dirichlet Allocation (“LDA”: Blei et al. (2003)) is an approach to document clustering, in which “topics” (multinomial distributions over terms) and topic allocations (multinomial distributions over topics per document) are jointly learned. When the topic model output is to be presented 1 https://github.com/jhlau/ topic-coherence-sensitivity Since then, several methodologies have been introduced to automate the evaluation of topic coherence. Newman et al. (2010) found that aggregate pairwise PMI scores over the top-N topic words correlated well with human ratings. Mimno et al. (2011) proposed replacing PMI with conditional probability based on co-document frequency. Aletras and Stevenson (2013) showed that coherence can be measured by a classical distributional similarity approach. More recently, Lau et al. (2014) proposed a methodology to automate the word intrusion task directly. Their results also reveal the differences between these methodologies in their assessment of topic coherence. A hyper-parameter in all these methodologies is the number of"
N18-1178,D16-1250,0,0.0182365,"esentations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are built using pre-trained FastText monolingual embeddings (Bojanowski et al., 2017) and a dictionary D constructed by translating 5000 frequent English words using Google Translate. Given a pair of embedding matrices E (English) and O (Other), we use singular value decomposition of OT DE (which is U ΣV T ) to get the projection matrix (W ∗ =U V T ), since it also enforces monolingual invariance (Artetxe et al., 2016; Smith et al., 2017). Finally, we obtain the aligned embedding matrix, We , as OW ∗ . We use a bi-LSTM to derive a vector representation of each word in context. The bi-LSTM traverses the sentence si in both the forward and backward directions, and the encoded representation for a given word wit ∈ si , is defined by con→ − catenating its forward ( h it ) and backward hidden   ← − states ( h it ), t ∈ 1, T . Sentence model: Similarly, we use a bi-LSTM to generate a sentence embedding from the wordlevel bi-LSTM, where each input sentence si is represented using the last hidden state of both t"
N18-1178,Q17-1010,0,0.0101676,"65 Proposed Approach with positions on fine-grained policy issues (57 classes). The task here is to learn a model that can: (a) classify sentences according to policy issue classes; and (b) score the overall document on the policy-based left–right spectrum (RILE), in an inter-dependent fashion. Word encoder: We initialize word vector representations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are built using pre-trained FastText monolingual embeddings (Bojanowski et al., 2017) and a dictionary D constructed by translating 5000 frequent English words using Google Translate. Given a pair of embedding matrices E (English) and O (Other), we use singular value decomposition of OT DE (which is U ΣV T ) to get the projection matrix (W ∗ =U V T ), since it also enforces monolingual invariance (Artetxe et al., 2016; Smith et al., 2017). Finally, we obtain the aligned embedding matrix, We , as OW ∗ . We use a bi-LSTM to derive a vector representation of each word in context. The bi-LSTM traverses the sentence si in both the forward and backward directions, and the encoded re"
N18-1178,S15-1012,1,0.60701,"he number of times two parties have been in a coalition in the past (to get a value between 0 and 1), for both RegCoalition and EUCoalition. We also construct rules based on transitivity for both the relational features, i.e., parties which have had common coalition partners, even if they were not allies themselves, are likely to have similar policy positions. Manifesto similarity: Manifestos that are similar in content are expected to have similar RILE scores (and associated sentence6 http://www.europarl.europa.eu 1968 level label distributions), similar to the modeling intuition captured by Burford et al. (2015) in the context of congressional debate vote prediction. For a pair of recent manifestos (Recent) we use the cosine similarity (Similarity) between their respective document vectors Vd (Figure 1). Right–left ratio: For a given manifesto, we compute the ratio of sentences categorized under RIGHT to OTHERS RIGHT ( # RIGHT+##LEFT +# NEUTRAL ), where the categorization for sentences is obtained using the joint-structured model (Equation (4)). We also encode the location of sentence ls in a document, by weighing the count of sentences for each class C by its location value P log(l s + 1) (referred"
N18-1178,W17-2906,0,0.500358,"Missing"
N18-1178,E17-2109,0,0.0877985,"Missing"
N18-1178,P82-1020,0,0.822503,"Missing"
N18-1178,P17-1069,0,0.054273,"Missing"
N18-1178,P15-1107,0,0.083391,"Missing"
N18-1178,P07-1055,0,0.0286267,"cture of the text and use a much simpler model architecture: averages of word embeddings, versus our bi-LSTM encodings; and they do not leverage domain information and temporal regularities that can influence policy positions (Greene, 2016). This work will act as a baseline in our experiments in Section 5. Policy-specific position classification can be seen as related to target-specific stance classification (Mohammad et al., 2017), except that the target is not explicitly mentioned in most cases. Secondly, manifestos have both fine- and coarsegrained positions, similar to sentiment analysis (McDonald et al., 2007). Finally, manifesto text is well structured within and across documents (based on coalition), has temporal dependencies, and is multilingual in nature. 2 In this section, we detail the first step of our two-stage approach. We use a hierarchical bidirectional long short-term memory (“biLSTM”) model (Hochreiter and Schmidhuber, 1997; Graves et al., 2013; Li et al., 2015) with a multi-task objective for the sentence classification and document-level regression tasks. A post-hoc calibration of coarse-grained manifesto position is given in Section 4. Let D be the set of manifestos, where a manifes"
N18-1178,D17-1318,0,0.17367,"entences, and a sentence si has T words: wi1 , wi2 , ...wiT . The set Ds ⊂ D is annotated at the sentence-level Related Work Analysing manifesto text is a relatively new application at the intersection of political science and NLP. One line of work in this space has been on sentence-level classification, including classifying each sentence according to its major political theme (1-of-7 categories) (Zirn et al., 2016; Glavaˇs et al., 2017a), its position on various policy themes (Verberne et al., 2014; Biessmann, 2016; Subramanian et al., 2017), or its relative disagreement with other parties (Menini et al., 2017). Recent approaches (Glavaˇs et al., 2017a; Subrama3 1965 Proposed Approach with positions on fine-grained policy issues (57 classes). The task here is to learn a model that can: (a) classify sentences according to policy issue classes; and (b) score the overall document on the policy-based left–right spectrum (RILE), in an inter-dependent fashion. Word encoder: We initialize word vector representations using a multilingual word embedding matrix, We . We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion. Bilingual projection matrices are bu"
N18-1178,W14-2715,0,0.048388,"over the sentence-level labeled set Ds ⊂ D, which is denoted as LSP . The explicit structured sentence-document loss is given as: Lstruc 3 |D| 1 X = |D| d=1 1 X (piright − pileft ) − rd Ld i∈d !2 (3) Strictly speaking, for these documents even, sentence annotation was used to derive the RILE score, but the sentencelevel labels were never made available. 1966 Vd y1 ???? Concatenate y2 ??1 ℎ1 ℎ2 ℎ1 ℎ2 s2 s1 ℎ21 ℎ21 ℎ22 Ban Saturday w21 w22 ℎ22 been used for many tasks including political framing analysis on Twitter (Johnson et al., 2017) and user stance classification on socio-political issues (Sridhar et al., 2014). These models can be specified using Probabilistic Soft Logic (“PSL”) (Bach et al., 2015), a weighted first order logical template language. An example of a PSL rule is Average Pooling ℎ23 ℎ23 night w 2323 yL ??2 ℎ?? special … λ : P(a) ∧ Q(a, b) → R(b) sL ℎ2?? w24 Sentences ℎ?? … ℎ24 ℎ24 ???? ℎ2?? Words handgun w2T Figure 1: Hierarchical bi-LSTM for joint sentence– document analysis (yi denotes the predicted 57-class distribution of sentence si ; pi denotes the distribution over LEFT (in red), RIGHT (in blue) and NEUTRAL (in yellow); rd denotes the RILE score of d). where piright and pileft a"
N18-1178,U17-1003,1,0.545142,"ntries, from elections dating back to 1945. In CMP, a subset of the manifestos has been manually annotated at the sentence-level with one of 57 political themes, divided into 7 major categories.1 Such categories capture party positions (FAVORABLE, UNFAVORABLE or NEITHER) 1 https://manifesto-project.wzb.eu/ coding_schemes/mp_v5 {t.cohn,tbaldwin}@unimelb.edu.au Such manual annotations are labor-intensive and prone to annotation inconsistencies (Mikhaylov et al., 2012). In order to overcome these challenges, supervised sentence classification approaches have been proposed (Verberne et al., 2014; Subramanian et al., 2017). Other than the sentence-level labels, the manifesto text also has a document-level score that quantifies its position on the left–right spectrum. Different approaches have been proposed to derive this score, based on alternate definitions of “left–right” (Slapin and Proksch, 2008; Benoit and Laver, 2007; Lo et al., 2013; D¨aubler and Benoit, 2017). Among these, the RILE index is the most widely adopted (Merz et al., 2016; Jou and Dalton, 2017), and has been shown to correlate highly with other popular scores (Lowe et al., 2011). RILE is defined as the difference between RIGHT and LEFT positi"
N18-2045,C16-1251,0,0.0412767,"n of the task is presented in Section 2. Evaluation. We benchmark against baseline systems presented in the works of Saeidi et al. (2016) and Ma et al. (2018): (1) LR: a logistic regression classifier with n-gram and POS tag features; (2) LSTM-Final: a biLSTM taking the final states as representations; (3) LSTM-Loc: a biLSTM taking the states at the location where target t is mentioned as representations; (4) LSTM+TA+SA: a biLSTM equipped with complex target and sentence-level attention mechanisms; (5) SenticLSTM: an improved version of (4) incorporating the SenticNet external knowledge base (Cambria et al., 2016). We additionally implement a bi-directional EntNet with the same hyper-parameter settings and GloVe embeddings as our model (Henaff et al., 2017). In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentime"
N18-2045,D14-1162,0,0.0836611,"best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentiment classification. Following Ma et al. (2018), we also report strict accuracy for aspect detection, as the fraction of sentences where all aspects are detected correctly. Model configuration. We initialise our model with GloVe (300-D, trained on 42B tokens, 1.9M vocab, not updated during training: Pennington et al. (2014)) 4 and pre-process the corpus with tokenisation using NLTK (Bird et al., 2009) and case folding. Training is carried out over 800 epochs with the FTRL optimiser (McMahan et al., 2013) and a batch size of 128 and learning rate of 0.05. We use the following hyper-parameters for weight matrices in both directions: R ∈ R300×3 , H, U, V, W are all matrices of size R300×300 , v ∈ R300 , and hidden size of the GRU in Equation (4) is 300. Dropout is applied to the output of φ in the final classifier (Equation (8)) with a rate of 0.2. Moreover, we employ the technique introduced by Gal and Ghahramani"
N18-2045,D17-1047,0,0.0420584,"rack of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the tw"
N18-2045,C16-1146,0,0.218549,"ohn Timothy Baldwin School of Computing and Information Systems The University of Melbourne Victoria, Australia fliu3@student.unimelb.edu.au t.cohn@unimelb.edu.au tb@ldwin.net Abstract The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-specific word and sentence representations (Tang et al., 2016a). Despite these successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed fo"
N18-2045,C16-1311,0,0.0845094,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,D16-1021,0,0.0933057,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,S14-2036,0,0.234655,"Missing"
N18-2045,S14-2076,0,0.172009,"Missing"
N18-2076,N10-1027,1,0.810339,"nimum entropy classification distribution. This is based on an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are common with the C OND model as described in §2.1, including the use of private and shared representations, their u"
N18-2076,D14-1181,0,0.00402637,"a is how best to use the domain. Basic methods might learn several separate models, or simply ignore the domain and learn a single model. Neither method is ideal: the former fails to share statistics between the models to capture the general concept, while the latter discards information that can aid classification, e.g., domain-specific vocabulary or class skew. To address these issues, we propose two architectures as illustrated in Figure 1 (a and b), parameterised as a convolutional network (CNN) over the input instance, chosen based on the success of CNNs for text categorisation problems (Kim, 2014); note, however, that our method is general and can be applied with other network types. Both representations are based on the idea of twin representations of each instance,2 denoted shared and private representations, which are trained to capture domain-general versus domain-specific concepts, respectively. This is achieved using various loss functions, most notably an adversarial loss to discourage learning of domain-specific concepts in the shared representations. The two architectures differ in whether the domain is provided as an input (C OND) or an output (G EN). Below, we elaborate on t"
N18-2076,P15-2030,1,0.862185,"constructed from different sources, featuring different topics, register, writing style, etc. An important, yet elusive, goal is to produce NLP tools that are capable of handling all types of texts, such that we can have, e.g., text classifiers that work well on texts from newswire to wikis to micro-blogs. A key roadblock is application to new domains, unseen in training. Accordingly, training needs to be robust to domain variation, such that domain-general concepts are learned in preference to domain-specific phenomena, which will not transfer well to out-of-domain evaluation. To illustrate, Bitvai and Cohn (2015) report learning formatting quirks of specific reviewers in a review text regression task, which are unlikely to prove useful on other texts. This classic problem in NLP has been tackled under the guise of “domain adaptation”, also known as unsupervised transfer learning, using feature-based methods to support knowledge 1 Code, data and evaluation scripts available at https://github.com/lrank/Domain_Robust_ Text_Representation.git 474 Proceedings of NAACL-HLT 2018, pages 474–479 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics methods outperform the st"
N18-2076,C16-1038,0,0.132272,"Missing"
N18-2076,P07-1056,0,0.885378,"is classic problem in NLP has been tackled under the guise of “domain adaptation”, also known as unsupervised transfer learning, using feature-based methods to support knowledge 1 Code, data and evaluation scripts available at https://github.com/lrank/Domain_Robust_ Text_Representation.git 474 Proceedings of NAACL-HLT 2018, pages 474–479 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics methods outperform the state-of-the-art (Lui and Baldwin, 2012) in terms of out-of-domain accuracy. As a secondary evaluation, we use the MultiDomain Sentiment Dataset (Blitzer et al., 2007), where we once again observe a clear advantage for our approaches, illustrating the potential of our technique more broadly in NLP. 2 Ds (θd ) CNNs (θs ) xi hpi θc yi (a) Domain-conditional (C OND) model. Ds (θd ) Multi-domain Learning CNNs (θs ) A primary consideration when formulating models of multi-domain data is how best to use the domain. Basic methods might learn several separate models, or simply ignore the domain and learn a single model. Neither method is ideal: the former fails to share statistics between the models to capture the general concept, while the latter discards informat"
N18-2076,2005.mtsummit-papers.11,0,0.0588508,"an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are common with the C OND model as described in §2.1, including the use of private and shared representations, their use in the classification output, and the adversar"
N18-2076,I11-1062,1,0.872615,"each domain in the test set as belonging to one of the training domains, and then select the domain with the minimum entropy classification distribution. This is based on an assumption that a closely matching domain should be able to make confident predictions.3 2.2 min θc ,θs ,θp ,θg Experiments 3.1 Language Identification To evaluate our approach, we first consider the language identification task. Data We follow the settings of Lui and Baldwin (2012), involving 5 training sets from 5 different domains with 97 languages in total: Debian, JRC-Acquis, Wikipedia, ClueWeb and RCV2, derived from Lui and Baldwin (2011).5 We evaluate accuracy on seven holdout benchmarks: EuroGov, TCL, Wikipedia26 (all from Baldwin and Lui (2010)), EMEA (Tiedemann, 2009), EuroPARL (Koehn, 2005), T-BE (Tromp and Pechenizkiy, 2011), and T-SC (Carter et al., 2013). Documents are tokenized as a byte sequence (consistent with Lui and Baldwin (2012)), and truncated or padded to a length of 1k bytes.7 Domain-Generative Model (G EN) The second model is based on generation of, rather than conditioning on, the domain, which allows the model to learn domain signals that transfer across some, but not all, domains. Most components are com"
N18-2076,P12-3005,1,0.874271,"ses the feature augmentation method of Daum´e III (2007) to convolutional neural networks, as part of a larger deep learning architecture. Additionally, we use adversarial training such that the shared representation is explicitly discouraged from learning domain identifying information (Ganin and Lempitsky, 2015). We present two architectures which differ in whether domain is conditioned on or generated, and in terms of parameter sharing in forming private representations. We primarily evaluate on the task of language identification (“LangID”: Cavnar and Trenkle (1994)), using the corpora of Lui and Baldwin (2012), which combine large training sets over a diverse range of text domains. Domain adaptation is an important problem for this task (Lui and Baldwin, 2014; Jurgens et al., 2017), where text resources are collected from numerous sources, and exhibit a wide variety of language use. We show that while domain adversarial training overall improves over baselines, gains are modest. The same applies to twin shared/private architectures, but when the two methods are combined, we observe substantial improvements. Overall, our Most real world language problems require learning from heterogenous corpora, r"
N18-2076,P07-1033,0,0.442683,"Missing"
N18-2076,W14-1303,1,0.87424,"we use adversarial training such that the shared representation is explicitly discouraged from learning domain identifying information (Ganin and Lempitsky, 2015). We present two architectures which differ in whether domain is conditioned on or generated, and in terms of parameter sharing in forming private representations. We primarily evaluate on the task of language identification (“LangID”: Cavnar and Trenkle (1994)), using the corpora of Lui and Baldwin (2012), which combine large training sets over a diverse range of text domains. Domain adaptation is an important problem for this task (Lui and Baldwin, 2014; Jurgens et al., 2017), where text resources are collected from numerous sources, and exhibit a wide variety of language use. We show that while domain adversarial training overall improves over baselines, gains are modest. The same applies to twin shared/private architectures, but when the two methods are combined, we observe substantial improvements. Overall, our Most real world language problems require learning from heterogenous corpora, raising the problem of learning robust models which generalise well to both similar (in domain) and dissimilar (out of domain) instances to those seen in"
N18-2076,W13-4068,0,0.041982,"Missing"
N18-2076,D12-1119,0,0.0882485,"Missing"
N19-1203,P17-1183,0,0.0301289,"vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attention from Aharoni and Goldberg (2017). The model was one of the top performers in the 2016 SIGMORPHON shared task (Cotterell et al., 2016); it achieved particularly high accuracy in the low-resource setting. Hard attention is motivated by the observation that alignment between the input and output sequences is often monotonic in inflection tasks. In the model, the input lemma is treated as a sequence of characters, and encoded using a bidirectional LSTM (Graves and Schmidhuber, 2005), to produce vectors xj for each character position j. Next the word wi = c = c1 · · · c|wi |is generated in a decoder character-by-character: p(cj |"
N19-1203,W05-0909,0,0.0318394,"trumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mapping a meaning representation to text, w"
N19-1203,P17-1080,0,0.0256269,"version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of this paper is a novel str"
N19-1203,W11-2832,0,0.0158478,"presentation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally, as a morphological i"
N19-1203,Q17-1010,0,0.0234037,"or decoding we use a greedy strategy where we first decode the CRF, that is, we solve the problem m? = argmaxm log p(m |`), using the Viterbi (1967) algorithm. We then use this decoded m? to generate forms from the inflector. Note that finding the one-best string under our neural inflector is intractable, and for this reason we use greedy search. 3 Experiments Dataset. We use the Universal Dependencies v1.2 dataset (Nivre et al., 2016) for our experiments. We include all the languages with information on their lemmata and fine-grained grammar tag annotation that also have fasttext embeddings (Bojanowski et al., 2017), which are used for word embedding initialization.5 Evaluation. We evaluate our model’s ability to predict: (i) the correct morphological tags from the lemma context, and (ii) the correct inflected forms. As our evaluation metric, we report 1-best accuracy for both tags and word form prediction. Configuration. We use a word and character embedding dimensionality of 300 and 100, respectively. The hidden state dimensionality is set to 200. All models are trained with Adam (Kingma and Ba, 2014), with a learning rate of 0.001 for 20 epochs. Baselines. We use two baseline systems: (1) the CoNLL–SI"
N19-1203,K18-3001,1,0.790791,"on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguisticallymotivated latent variables into NLP models. 1 Introduction NLP systems are often required to generate grammatical text, e.g., in machine translation, summarization, dialogue, and grammar correction. One component of grammaticality is the use of contextually appropriate closed-class morphemes. In this work, we study contextual inflection, which has been recently introduced in the CoNLLSIGMORPHON 2018 shared task (Cotterell et al., 2018) to directly investigate context-dependent morphology in NLP. There, a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the p"
N19-1203,K17-2001,1,0.93262,"a system must inflect partially lemmatized tokens in sentential context. For example, in English, the system must reconstruct the correct word sequence two cats are sitting from partially lemmatized sequence two _cat_ are sitting. Among other things, this requires: (1) identifying cat as a noun in this context, (2) recognizing that cat should be inflected as plural to agree with the nearby verb and numeral, and (3) realizing this inflection as the suffix s. Most past work in supervised computational morphology—including the previous CoNLL-SIGMORPHON shared tasks on morphological reinflection (Cotterell et al., 2017)—has focused mainly on step (3) above. As the task has been introduced into the literature only recently, we provide some background. Contextual inflection amounts to a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. Th"
N19-1203,N16-1030,0,0.0407715,"tured neural model 1 Y p(m |`) = ψ (mi , mi−1 , `) Z(`) (2) i=1 2 Although wi can sometimes be computed by concatenating `i with mi -specific affixes, it can also be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both ar"
N19-1203,D15-1176,0,0.0216304,"lso be irregular. 3 In case of partially lemmatized sequence we still train the model to predict the tags over the entire sequence, but evaluate it only for lemmatized slots. `1 where ψ(·, ·, ·) ≥ 0 is an arbitrary potential, Z(`) normalizes the distribution, and m0 is a distinguished start-of-sequence symbol. 2019 In this work, we opt for a recurrent neural potential—specifically, we adopt a parameterization similar to the one given by Lample et al. (2016). Our potential ψ is computed as follows. First, the sequence ` is encoded into a sequence of word vectors using the strategy described by Ling et al. (2015): word vectors are passed to a bidirectional LSTM (Graves et al., 2005), where the corresponding hidden states are concatenated at each time step. We simply refer to the hidden state hi ∈ Rd as the result of said concatenation at the i-th step. Using hi , we can define the potential function as ψ (mi , mi−1 ) =  where Ami ,mi−1 is a exp Ami ,mi−1 + o> h , i mi transition weight matrix and omi ∈ Rd is a morphological tag embedding; both are learned. 2.3 The Morphological Inflector The conditional distribution p(wi |`i , mi ) is parameterized by a neural encoder–decoder model with hard attentio"
N19-1203,Q16-1037,0,0.481413,"a highly constrained version of language modeling. Language modeling predicts all words of a sentence from scratch, so the usual training and evaluation metric— perplexity—is dominated by the language model’s ability to predict content, which is where most of the uncertainty lies. Our task focuses on just the ability to reconstruct certain missing parts of the sentence—inflectional morphemes and their orthographic realization. This refocuses the modeling effort from semantic coherence to morphosyntactic coherence, an aspect of language that may take a back seat in current language models (see Linzen et al., 2016; Belinkov et al., 2017). Contextual inflection does not perfectly separate grammaticality modeling from content modeling: as illustrated in Tab. 1, mapping two cats _be_ sitting to the fully-inflected two cats were sitting does not require full knowledge of English grammar—the system does not have to predict the required word order nor the required auxiliary verb be, as these are supplied in the input. Conversely, this example does still require predicting some content—the semantic choice of past tense is not given by the input and must be guessed by the system.1 The primary contribution of t"
N19-1203,L16-1262,0,0.0579627,"Missing"
N19-1203,P02-1040,0,0.103705,"shifting positions (such as the instrumental), as well as those not associated with specific prepositions, are less well predicted. In addition, we evaluated the model’s performance when all forms are replaced by their corresponding lemmata (as in two cat be sit). For freer word order languages such as Polish or Latin, we observe a substantial drop in performance because most information on inter-word relations and their roles (expressed by means of case system) is lost. 5 Related Work The primary evaluation for most contemporary language and translation modeling research is perplexity, BLEU (Papineni et al., 2002), or METEOR (Banerjee and Lavie, 2005). Undoubtedly, such metrics are necessary for extrinsic evaluation and comparison. However, relatively few studies have focused on intrinsic evaluation of the model’s mastery of grammaticality. Recently, Linzen et al. (2016) investigated the ability of an LSTM language model to capture sentential structure, by evaluating subject–verb agreement with respect to number, and showed that under strong supervision, the LSTM is able to approximate dependencies. Taking it from the other perspective, a truer measure of grammatical competence would be a task of mappi"
N19-1203,petrov-etal-2012-universal,0,0.0315183,"Missing"
N19-1203,P17-2002,0,0.014134,"where the meaning representation specifies all necessary semantic content—content lemmata, dependency relations, and “inherent” closed-class morphemes (semantic features such as noun number, noun definiteness, and verb tense)—and the system is to realize this content according to the morphosyntactic conventions of a language, which means choosing word order, agreement morphemes, function words, and the surface forms of all words. Such tasks have been investigated to some extent—generating text from tectogrammatical trees (Hajic et al., 2002; Ptáˇcek and Žabokrtský, 2006) or from an AMR graph (Song et al., 2017). Belz et al. (2011) organized a related surface realization shared task on mapping unordered and uninflected dependency trees to properly ordered inflected sentences. The generated sentences were afterwards assessed by human annotators, making the task less scalable and more time consuming. Although our task is not perfectly matched to grammaticality modeling, the upside is that it is a “lightweight” task that works directly on text. No meaning representation is required. Thus, training and test data in any language can be prepared simply by lemmatizing a naturally occurring corpus. Finally,"
nicholson-etal-2008-evaluating,copestake-flickinger-2000-open,0,\N,Missing
nicholson-etal-2008-evaluating,zhang-kordoni-2006-automated,1,\N,Missing
nicholson-etal-2008-evaluating,W07-1220,1,\N,Missing
nicholson-etal-2008-evaluating,W02-1210,0,\N,Missing
nicholson-etal-2008-evaluating,W06-1206,1,\N,Missing
nicholson-etal-2008-evaluating,P04-1057,0,\N,Missing
nicholson-etal-2008-evaluating,W05-1008,1,\N,Missing
nicholson-etal-2008-evaluating,baldwin-etal-2004-road,1,\N,Missing
P01-1004,C00-1006,1,0.394486,"her hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the ﬁnal translation. A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the ﬁnal retrieval accuracy will become. Naturally, any appreciation in retrieval complexity comes at a price in terms of computational overhead. We thus follow the lead of Baldwin and Tanaka (2000) in asking the question: what is the empirical eﬀect on retrieval performance of diﬀerent match approaches? Here, retrieval performance is deﬁned as the combination of retrieval speed and accuracy, with the ideal method oﬀering fast response times at high accuracy. In this paper, we choose to focus on retrieval performance within a Japanese–English TR context. One key area of interest with Japanese is the eﬀect that segmentation has on retrieval performance. As Japanese is a non-segmenting language (does not explicitly delimit words orthographically), we can take the brute-force approach in tr"
P01-1004,1993.tmi-1.4,0,0.330798,"tion), which the TM system accesses in suggesting a list of target language (L2) translation candidates for a given source language (L1) input (Trujillo, 1999; Planas, 1998). Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input. Typically in example-based machine translation, either a single TRec is retrieved from the TM based on a match with the overall L1 input, or the input is partitioned into coherent segments, and individual translations retrieved for each (Sato and Nagao, 1990; Nirenburg et al., 1993); this is the ﬁrst step toward generating a customised translation for the input. With stand-alone TM systems, on the other hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the ﬁnal translation. A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the ﬁnal retrieval accuracy will become. Naturally, a"
P01-1004,C90-3044,0,0.0507933,"arget language translation), which the TM system accesses in suggesting a list of target language (L2) translation candidates for a given source language (L1) input (Trujillo, 1999; Planas, 1998). Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input. Typically in example-based machine translation, either a single TRec is retrieved from the TM based on a match with the overall L1 input, or the input is partitioned into coherent segments, and individual translations retrieved for each (Sato and Nagao, 1990; Nirenburg et al., 1993); this is the ﬁrst step toward generating a customised translation for the input. With stand-alone TM systems, on the other hand, the system selects an arbitrary number of translation candidates falling within a certain empirical corridor of similarity with the overall input string, and simply outputs these for manual manipulation by the user in fashioning the ﬁnal translation. A key assumption surrounding the bulk of past TR research has been that the greater the match stringency/linguistic awareness of the retrieval mechanism, the greater the ﬁnal retrieval accuracy"
P01-1004,C92-4203,0,0.582923,"gment order, and segment contiguity. 2.1 Segmentation Despite non-segmenting languages such as Japanese not making use of segment delimiters, it is possible to artiﬁcially partition oﬀ a given string into constituent morphemes through the process of segmentation. We will collectively term the resultant segments as words for the remainder of this paper. Looking to past research on string comparison methods for TM systems, almost all systems involving Japanese as the source language rely on segmentation (Nakamura, 1989; Sumita and Tsutsumi, 1991; Kitamura and Yamamoto, 1996; Tanaka, 1997), with Sato (1992) and Sato and Kawase (1994) providing rare instances of character-based systems. This is despite Fujii and Croft (1993) providing evidence from Japanese information retrieval that character-based indexing performs comparably to word-based indexing. In analogous research, Baldwin and Tanaka (2000) compared characterand word-based indexing within a Japanese– English TR context and found character-based indexing to hold a slight empirical advantage. The most obvious advantage of character-based indexing over word-based indexing is that there is no pre-processing overhead. Other arguments for char"
P03-1059,W03-1010,1,0.830295,"Missing"
P03-1059,C02-1052,1,0.742646,"Missing"
P03-1059,C94-1002,1,0.810442,"Missing"
P03-1059,briscoe-carroll-2002-robust,0,0.00981866,"yle regular expression over Penn POS tags: (PDT)* DT (RB|JJ[RS]?|NNS?)* NNS? [ˆN]. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks. For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision. As the full parser, we used RASP (Briscoe and Carroll, 2002), a robust tag sequence grammarbased parser. RASP’s grammatical relation output function provides the phrase structure in the form of lemmatised dependency tuples, from which it is possible to read off the feature information. RASP has the advantage that recall is high, although precision is potentially lower than chunking or tagging as the parser is forced into resolving phrase attachment ambiguities and committing to a single phrase structure analysis. Although all three systems map onto an identical feature space, the feature vectors generated for a given target noun diverge in content due"
P03-1059,1991.mtsummit-papers.16,0,0.0175845,"and Japanese, do not mark countability, which means that the choice of countability will be largely the responsibility of the generation component (Bond, 2001). In addition, knowledge of countability obtained from examples of use is an important resource for dictionary construction. In this paper, we learn the countability preferences of English nouns from unannotated corpora. We first annotate them automatically, and then train classifiers using a set of gold standard data, taken from COMLEX (Grishman et al., 1998) and the transfer dictionaries used by the machine translation system ALT-J/E (Ikehara et al., 1991). The classifiers and their training are described in more detail in Baldwin and Bond (2003). These are then run over the corpus to extract nouns as members of four classes — countable: dog; uncountable: furniture; bipartite: [pair of] scissors and plural only: clothes. We first discuss countability in more detail (§ 2). Then we present the lexical resources used in our experiment (§ 3). Next, we describe the learning process (§ 4). We then present our results and evaluation (§ 5). Finally, we discuss the theoretical and practical implications (§ 6). 2 Background Grammatical countability is mo"
P03-1059,P96-1004,0,0.418037,"Missing"
P03-1059,N01-1006,0,0.0164125,"P and PP boundaries, determining subject–verb agreement and deconstructing NPs in order to recover conjuncts and noun-modifier data. We adopt three approaches. First, we use part-of-speech (POS) tagged data and POS-based templates to extract out the necessary information. Second, we use chunk data to determine NP and PP boundaries, and mediumrecall chunk adjacency templates to recover interphrasal dependency. Third, we fully parse the data and simply read off all necessary data from the dependency output. With the POS extraction method, we first Penntagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brown and WSJ corpora with some spelling, number and hyphenation normalisation. We then lemmatised this data using a version of morph (Minnen et al., 2001) customised to the Penn POS tagset. Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data. For example, NPs are in many cases recoverable with the following Perl-style regular expression over Penn POS tags: (PDT)* DT (RB|JJ[RS]?|NNS?)* NNS? [ˆN]. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjon"
P03-1059,J00-4004,0,0.0305492,"Missing"
P03-1059,W00-0726,0,0.0249334,"ning over the Brown and WSJ corpora with some spelling, number and hyphenation normalisation. We then lemmatised this data using a version of morph (Minnen et al., 2001) customised to the Penn POS tagset. Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data. For example, NPs are in many cases recoverable with the following Perl-style regular expression over Penn POS tags: (PDT)* DT (RB|JJ[RS]?|NNS?)* NNS? [ˆN]. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks. For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision. As the full parser, we used RASP (Briscoe and Carroll, 2002), a robust tag sequence grammarbased parser. RASP’s grammatical relation output function provides the phrase structure in the form of lemmatised dependency tuples, from which it is poss"
P06-2064,briscoe-carroll-2002-robust,0,0.0184929,"we can identify the matching relation forms of semantic relations to decide the semantic relation for NCs. 3 Noun Compound Get sentences with H,M S({have, own, possess} V , M SUBJ , H OBJ ) (5) S({belong to} V , H SUBJ , M OBJ ) RASP parser Collect Subj, Obj, PP, PPN, V, T 4 Method Figure 1 outlines the system architecture of our approach. We used three corpora: the Brown corpus (as contained in the Penn Treebank), the Wall Street Journal corpus (also taken from the Penn treebank), and the written component of the British National Corpus (BNC). We ﬁrst parsed each of these corpora using RASP (Briscoe and Carroll, 2002), and identiﬁed for each verb token the voice, head nouns of the subject and object, and also, for each PP attached to that verb, the head preposition and head noun of the Semantic Relations in Compound Nouns While there has been wide recognition of the need for a system of semantic relations with which to classify NCs, there is still active debate as to what the composition of that set should be, or indeed 493 As mentioned earlier, we built our supervised classiﬁer using TiMBL. NP (hereafter, PPN). Next, for our test NCs, we identiﬁed all verbs for which the modiﬁer and head noun co-occur as"
P06-2064,C02-1011,0,0.0108296,"Missing"
P06-2064,I05-1082,1,0.383251,"t performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructions involving the modiﬁer and head noun and a set of seed verbs for each semantic relation: e.g. (the) family owns (a) car is taken as evidence for family car being an instance of the POSSESSOR relation. We then attempt to map all instances of the modiﬁer and head noun as the heads of NPs in a transitive sen"
P06-2064,J02-3004,0,0.0730099,"tware Engineering University of Melbourne, Victoria 3010 Australia and ‡ NICTA Victoria Research Lab University of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method"
P06-2064,W04-2609,0,0.676526,"e automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructions involving the modiﬁer and head noun and a set of seed verbs for each semantic relation: e.g. (the) family owns (a) c"
P06-2064,W04-0404,1,0.883662,"Missing"
P06-2064,W01-0511,0,0.884588,"ity of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructio"
P06-2064,C94-2125,0,0.19583,"b−Mapping map verbs onto seed verbs Modified Sentences Match modified sentences wrt relation forms WordNet::Similarity Moby’s Thesaurus Classifier Final Sentences Semantic Relation Classifier:Timbl Figure 1: System Architecture whether it is reasonable to expect that all NCs should be interpretable with a ﬁxed set of semantic relations. Based on the pioneering work on Levi (1979) and Finin (1980), there have been efforts in computational linguistics to arrive at largely taskspeciﬁc sets of semantic relations, driven by the annotation of a representative sample of NCs from a given corpus type (Vanderwende, 1994; Barker and Szpakowicz, 1998; Rosario and Marti, 2001; Moldovan et al., 2004). In this paper, we use the set of 20 semantic relations deﬁned by Barker and Szpakowicz (1998), rather than deﬁning a new set of relations. The main reasons we chose this set are: (a) that it clearly distinguishes between the head noun and modiﬁers, and (b) there is clear documentation of each relation, which is vital for NC annotation effort. The one change we make to the original set of 20 semantic relations is to exclude the PROPERTY relation since it is too general and a more general form of several other relati"
P06-2064,P98-1015,0,0.854863,"in Noun Compounds via Verb Semantics Su Nam Kim† and Timothy Baldwin†‡ † Computer Science and Software Engineering University of Melbourne, Victoria 3010 Australia and ‡ NICTA Victoria Research Lab University of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NC"
P06-2064,P94-1019,0,0.0538751,"loyed in our method. As our parser, we used RASP, generating a dependency representation for the most probable parse for each sentence. Note that RASP also lemmatises all words in a POS-sensitive manner. To map actual verbs onto seed verbs, we experimented with two resources: WordNet::Similarity and Moby’s thesaurus. WordNet::Similarity2 is an open source software package that allows the user to measure the semantic similarity or relatedness between two words (Patwardhan et al., 2003). Of the many methods implemented in WordNet::Similarity, we report on results for one path-based method (WUP, Wu and Palmer (1994)), one content-information based method (JCN, Jiang and Conrath (1998)) and two semantic relatedness methods (LESK, Banerjee and Pedersen (2003), and VECTOR, (Patwardhan, 2003)). We also used a random similarity-generating method as a baseline (RANDOM). The second semantic resource we use for verbmapping method is Moby’s thesaurus. Moby’s thesaurus is based on Roget’s thesaurus, and contains 30K root words, and 2.5M synonyms and related words. Since the direct synonyms of seed verbs have limited coverage over the set of sentences used in our experiment, we also experimented with using second-l"
P06-2064,O97-1002,0,\N,Missing
P06-2064,C98-1015,0,\N,Missing
P08-1037,J07-4002,0,0.269959,"Missing"
P08-1037,W00-1320,0,0.706722,"line Bikel parser thus represents an advancement in state-of-the-art performance. That we speciﬁcally present results for PP attachment in a parsing context is a combination of us supporting the new research direction for PP attachment established by Atterer and Sch¨utze, and us wishing to reinforce the ﬁndings of Stetina and Nagao that word sense information signiﬁcantly enhances PP attachment performance in this new setting. Lexical semantics in parsing There have been a number of attempts to incorporate word sense information into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with"
P08-1037,P05-1022,0,0.0092543,"evel of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks rela"
P08-1037,C02-1126,0,0.036289,"Missing"
P08-1037,J05-1003,0,0.0177864,"that the appropriate level of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing"
P08-1037,P96-1025,0,0.0195782,"information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to ha"
P08-1037,J03-4003,0,0.060359,"of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences. In order to"
P08-1037,P94-1016,0,0.178508,"and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to conside"
P08-1037,W07-1204,0,0.305836,". The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection"
P08-1037,W01-0521,0,0.036849,"Missing"
P08-1037,1997.iwpt-1.15,0,0.634222,"produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to consider in this process:"
P08-1037,N06-2015,0,0.00853735,"Missing"
P08-1037,J98-2002,0,0.0122187,"A selection of SFs is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 co"
P08-1037,P98-2127,0,0.0468725,"ote that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet development, and also changes in WordNet from the original version used for the SemCor tagging. S YSTEM Baseline SF SFn SFv word + SF"
P08-1037,P95-1037,0,0.0185297,"that word sense information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and th"
P08-1037,J93-2004,0,0.0412602,"Below, we outline these tasks. Parsing As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000). While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the"
P08-1037,J03-4004,0,0.0329229,"is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 combinations of representation"
P08-1037,P04-1036,0,0.0610971,"1: A selection of WordNet SFs 2. First Sense (1 ST): All token instances of a given word are tagged with their most frequent sense in WordNet.4 Note that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet d"
P08-1037,N06-1020,0,0.00748381,"rs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks relative to the baseline par"
P08-1037,H94-1048,0,0.517562,"eters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the verb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the"
P08-1037,W97-0109,0,0.0957769,"rb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2), where the attachment decision is either v or n1. The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classiﬁer. Their work is particularly inspiring in that it signiﬁcantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts. In a recent paper, Atterer and Sch¨utze (2007) criticised the RRR dataset because it assumes that an oracle parser provides the two hypothesised structures to choose between. This is needed to derive the fact that there are two possible attachment sites, as well as information about the lex"
P08-1037,I05-1007,0,0.0689819,"ation into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over"
P08-1037,A00-2018,0,\N,Missing
P08-1037,J04-4004,0,\N,Missing
P08-1037,C98-2122,0,\N,Missing
P09-2041,P02-1053,0,\N,Missing
P09-2041,P05-1045,0,\N,Missing
P11-1038,P06-2005,0,0.952961,"e a normalised version of examples as a gloss in double quotes. The message normalisation task is challenging. It has similarities with spell checking (Peterson, 1980), but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 “before”, which tend to be considered beyond the remit of spell checking (Aw et al., 2006). The free writing style of text messages makes the task even more complex, e.g. with word lengthening such as goooood being commonplace for emphasis. In addition, the detection of ill-formed words is difficult due to noisy context. Our objective is to restore ill-formed words to their canonical lexical forms in standard English. Through a pilot study, we compared OOV words in Twitter and SMS data with other domain corpora, 368 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368–378, c Portland, Oregon, June 19-24, 2011. 2011 Association for Compu"
P11-1038,N10-1027,1,0.248104,"sed on a random sampling of 549 English tweets. The English tweets were annotated by three independent annotators. All OOV words were pre-identified, and the annotators were requested to determine: (a) whether each OOV word was ill-formed or not; and (b) what the standard form was for ill-formed words, subject to the task definition outlined in Section 3.1. The total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.7 The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cros"
P11-1038,P10-1079,0,0.489752,"It is labor intensive to construct an annotated corpus to sufficiently cover ill-formed words and context-appropriate corrections. Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech recognition problem. For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. Beaufort et al. (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. Kaufmann and Kalita (2010) exploit a machine translation approach with a preprocessor for syntactic (rather than lexical) normalisation. Predominantly, however, these methods require large-scale annotated training data, limiting their adaptability to new domains or languages. In contrast, our proposed method doesn’t require annotated data. It builds on the work on SMS text normalisation, and adapts it to Twitter data, exploiting multiple data sources for normalisation. Fi"
P11-1038,P00-1037,0,0.134138,"e a novel normalisation approach that exploits dictionary lookup, word similarity and word context, without requiring annotated data; and (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P (S|T ) by computing arg max P (T |S)P (S), in which P (S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov 369 model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampl"
P11-1038,W09-2010,0,0.928219,"P (S|T ) by computing arg max P (T |S)P (S), in which P (S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov 369 model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P (T |S), which encodes the underlying error production process, is hard to approximate accurately. Additionally, these methods make the strong assumption that a token ti ∈ T only depends on si ∈ S, ignoring the context around the token, which could be utilised to help in resolving ambiguity. Statistical machine translation (SMT) has been proposed as a means of context-sensitive text norm"
P11-1038,de-marneffe-etal-2006-generating,0,0.0253551,"Missing"
P11-1038,P03-2026,0,0.024506,"Missing"
P11-1038,2008.jeptalnrecital-long.13,0,0.0914896,"S normalisation method with bootstrapped phrase alignments. SMT approaches tend to suffer from a critical lack of training data, however. It is labor intensive to construct an annotated corpus to sufficiently cover ill-formed words and context-appropriate corrections. Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. Some researchers have also formulated text normalisation as a speech recognition problem. For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. Beaufort et al. (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. Kaufmann and Kalita (2010) exploit a machine translation approach with a preprocessor for syntactic (rather than lexical) normalisation. Predominantly, however, these methods require large-scale annotated training data, limiting their adaptability to new domains or languages. In contrast, our proposed method doesn’t require annotated"
P11-1038,P07-2045,0,0.0102106,"e total number of ill-formed words contained in the SMS and Twitter datasets were 3849 and 1184, respectively.7 The language filtering of Twitter to automatically identify English tweets was based on the language identification method of Baldwin and Lui (2010), using the EuroGOV dataset as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7 The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. 374 In addition to comparing our method with competitor methods, we also study the contribution of different feature groups. We separately compare dictionary lookup over our Internet sl"
P11-1038,P02-1040,0,0.102,"set as training data, a mixed unigram/bigram/trigram byte feature representation, and a skew divergence nearest prototype classifier. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. (2006) as benchmark methods. We implement the SMT approach in Moses (Koehn et al., 2007), with synthetic training and tuning data of 90,000 and 1000 sentence pairs, respectively. This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus. The 10-fold cross-validated BLEU score (Papineni et al., 2002) over this data is 0.81. 7 The Twitter dataset is available at http://www. csse.unimelb.edu.au/research/lt/resources/ lexnorm/. 374 In addition to comparing our method with competitor methods, we also study the contribution of different feature groups. We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three. 5.2 Evaluation metrics The evaluation of lexical normalisation consists of two stages (Hirst and Budanitsky, 2005): (1) illformed word detection, and (2) candidat"
P11-1038,N10-1020,0,0.025795,"ataset based on Twitter. 1 Introduction Twitter and other micro-blogging services are highly attractive for information extraction and text mining purposes, as they offer large volumes of real-time data, with around 65 millions tweets posted on Twitter per day in June 2010 (Twitter, 2010). The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings. Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools (Sproat et al., 2001; Ritter et al., 2010). For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (“You must be talking about the paper but I was thinking movies”),1 the Stanford parser (Klein and 1 Throughout the paper, we will provide a normalised version of examples as a gloss in double quotes. The message normalisation task is challenging. It has similarities with spell checking (Peterson, 1980), but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre."
P11-1038,P02-1019,0,0.0125249,"d (4) we demonstrate that our method achieves state-of-the-art accuracy over both SMS and Twitter data. 2 Related work The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. Suppose the ill-formed text is T and its corresponding standard form is S, the approach aims to find arg max P (S|T ) by computing arg max P (T |S)P (S), in which P (S) is usually a language model and P (T |S) is an error model. Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov 369 model (HMM) state transitions and emissions, respectively (Rabiner, 1989). Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sampled error distribution. While the noisy channel model is appropriate for text normalisation, P (T |S), which encodes the underlying error pro"
P11-1038,C00-2137,0,0.12992,"Missing"
P11-1151,P08-1034,0,0.0118503,"positives by the SVM. Use of ψi as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc a"
P11-1151,C08-2004,0,0.315566,"st the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph capacities based on simple heuristics. Two of the three approaches they trial appear to offer performance improvements: The SetTo heuristic: This heuristic works through E in order and tries to force Vi and Vj into different classes for every “different class” (dij < 0) citation classifier output where i < j. It does this by altering the four relevant content-only preferences, φi (y), φi (n), φj (y), and φj (n)."
P11-1151,P09-2041,1,0.435288,"r incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains w"
P11-1151,U08-1003,0,0.0416228,"only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond the scope of this paper but nevertheless an important area for future research. A similar technique is used in a variation on the Thomas et al. experiment where additional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional"
P11-1151,E09-1020,0,0.015401,"ed via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. Majority Content only Minimum-cut Minimum-cut (SetTo(.6)) Minimum-cut (SetTo(.8)) Minimum-cut (SetTo(1)) Minimum-cut (IncBy(.05)) Minimum-cut (IncBy(.15)) Minimum-cut (IncBy(.25)) Iterative-classifier (citation cou"
P11-1151,W09-3305,0,0.0251591,"classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effecti"
P11-1151,P08-2065,0,0.0155245,"as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph"
P11-1151,P05-1015,0,0.0877281,"expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond"
P11-1151,W02-1011,0,0.0212671,"local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedia"
P11-1151,D08-1016,0,0.016352,"ional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. Majority Content only Minimum-cut Minimum-cut (SetTo(.6)) Minimum-cut (SetTo(.8)) Minimum-cut (SetTo(1)) Minimum-cut (IncBy(.05)) Minimum-cut (IncBy(.15)) Minimum-cut (I"
P11-1151,P09-1026,0,0.0325159,"Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores"
P11-1151,D09-1018,0,0.0128398,"this. They note that the cost of incorrectly classifying a given instance can be magnified in collective classification, because errors are propagated throughout the network. The extent to which this happens may depend on the random interaction between base classification accuracy and network structure. There is scope for further work to more fully explain this phenomenon. From these statistical and theoretical factors we infer that more reliable conclusions can be drawn from collective classification experiments that use cross-validation instead of a single, fixed data split. 5 Related work Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (20"
P11-1151,W06-1639,0,0.72925,"ider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effective novel feature models for iterative local classification of debate transcript data. In the next section (Section 2) we provide a formal definition of collective classification and describe the ConVote corpus that is the basis for our experimental evaluation. Sub"
P11-1154,E09-1013,0,0.0186437,"fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic terms, based on p(wi |tj ). which are clearly associat"
P11-1154,N10-1125,0,0.0206364,"than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic terms, based on p(wi |tj ). which are clearly associated with the domain of stock market trading. The aim of this research is to automatically generate topic labels which explicitly identify"
P11-1154,N09-1041,0,0.102661,"ion of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method. 1 Introduction Topic modelling is an increasingly popular framework for simultaneously soft-clustering terms and documents into a fixed number of “topics”, which take the form of a multinomial distribution over terms in the document collection (Blei et al., 2003). It has been demonstrated to be highly effective in a wide range of tasks, including multidocument summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). One standard way of interpreting a topic is to use the marginal probabilities p(wi |tj ) associated with each term wi in a given topic tj to extract out the 10 terms with highest marginal probability. This results in term lists such as:1 stock market investor fund trading investment firm exchange companies share 1 Here and throughout the paper, we will represent a topic tj via its ranking of top-10 topic te"
P11-1154,C10-2069,1,0.620712,"Missing"
P11-1154,N10-1012,1,0.447059,"ng et al. (2009) were one of the first to propose human labelling of topic models, in the form of synthetic intruder word and topic detection tasks. In the intruder word task, they include a term w with low marginal probability p(w|t) for topic t into the topN topic terms, and evaluate how well both humans and their model are able to detect the intruder. The potential applications for automatic labelling of topics are many and varied. In document collection visualisation, e.g., the topic model can be used as the basis for generating a two-dimensional representation of the document collection (Newman et al., 2010a). Regions where documents have a high marginal probability p(di |tj ) of being associated with a given topic can be explicitly labelled with the learned label, rather than just presented as an unlabelled region, or presented with a dense “term cloud” from the original topic. In topic modelbased selectional preference learning (Ritter et al., ` S´eaghdha, 2010), the learned topics can 2010; O be translated into semantic class labels (e.g. DAYS OF THE WEEK ), and argument positions for individual predicates can be annotated with those labels for greater interpretability/portability. In dynamic"
P11-1154,P10-1045,0,0.0159018,"Missing"
P11-1154,N04-1041,0,0.00580004,"We reimplement their method and present an empirical comparison in Section 5.3. In other work, Magatti et al. (2009) proposed a method for labelling topics induced by a hierarchical topic model. Their label candidate set is the Google Directory (gDir) hierarchy, and label selection takes the form of ontological alignment with gDir. The experiments presented in the paper are highly preliminary, although the results certainly show promise. However, the method is only applicable to a hierarchical topic model and crucially relies on a pre-existing ontology and the class labels contained therein. Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. When presented with semantically homogeneous, fine-grained nearsynonym clusters, the method appears to work well. With topic modelling, however, the top-ranking topic terms tended to be associated and not lexically similar to one another. It is thus highly questionable whether their method could be applied to topic models, but it would certainly be interesting to investigate whether our model could conversely be applied to the labelling of sets of near"
P11-1154,P10-1044,0,0.0135116,"Missing"
P11-1154,D08-1027,0,0.136267,"Missing"
P11-2046,W10-0907,0,0.0252375,"Missing"
P11-2046,J07-4004,1,0.788933,"The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1 , t2 , t3 , t4 , t5 ), where the set of candidate terms correspond to the middle tokens (t3 ) and the patterns are formed from the surrounding tokens (t1 , t2 , t4 , t5 ). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1 , t5 ) and the patterns are formed from the intervening tokens (t2 , t3 , t4 ). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams. The final relation dataset (5gm + DC) includes dependency chains up to length 5"
P11-2046,W06-2703,0,0.012205,"coma melanoma neuroblastoma osteosarcoma CELL CLNE DISE DRUG FUNC MUTN PROT SIGN TUMR Table 2: The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1 , t2 , t3 , t4 , t5 ), where the set of candidate terms correspond to the middle tokens (t3 ) and the patterns are formed from the surrounding tokens (t1 , t2 , t4 , t5 ). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1 , t5 ) and the patterns are formed from the intervening tokens (t2 , t3 , t4 ). The second relation dataset (5gm + 4gm), also includes length 2 patterns form"
P11-2046,U08-1013,1,0.959359,"ries. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsu"
P11-2046,P09-1045,1,0.875441,"endency chains up to length 5 as the patterns between terms (Greenwood et al., 2005). These chains are formed using the Stanford dependencies generated by the Rimell and Clark (2009) parser. All candidates occurring less than 10 times were filtered. The sizes of the resulting datasets are shown in Table 1. WMEB +negative intra-RGB +negative inter-RGB +negative mixed-RGB +negative 1-500 76.1 86.9 75.7 87.4 80.5 87.7 74.7 87.9 501-1000 56.4 68.7 62.7 72.4 69.9 76.4 69.9 73.5 1-1000 66.3 77.8 69.2 79.9 75.1 82.0 72.3 80.7 INTER - RGB 5gm +negative 5gm + 4gm +negative 5gm + DC +negative We follow McIntosh and Curran (2009) in using the 10 biomedical semantic categories and their hand-picked seeds in Table 2, and manually crafted negative categories: amino acid, animal, body part and organism. Our evaluation process involved manually judging each extracted term and we calculate the average precision of the top-1000 terms over the 10 target categories. We do not calculate recall, due to the open-ended nature of the categories. Results and Discussion Table 3 compares the performance of WMEB and RGB , with and without the negative categories. For RGB , we compare intra-, inter- and mixed relation types, and use the"
P11-2046,D10-1035,1,0.76419,"ift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh (2010) made some progress towards automatically learning useful negative categories during bootstrapping. In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL, Carlson et al. (2010)). In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the target relation’s arguments. Semantic constraints on categories and relations are manually crafted in CPL . For example, a candidate of the relation I S C EO O F will only be extracted if its arguments can be extracted into the ceo and company lex"
P11-2046,W97-0313,0,0.051701,"fted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative cate"
P11-2046,P10-1013,0,0.0996479,"Missing"
P11-2046,C02-1154,0,0.0389977,"anually generating negative categories. 1 Timothy Baldwin ♠ Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with 266 The University of Sydney the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of h"
P12-3005,N10-1027,1,0.771019,"corpus contains documents in a total of 97 languages. The data is drawn from 5 different domains: government documents, software documentation, newswire, online encyclopedia and an internet crawl, though no domain covers the full set of languages by itself, and some languages are present only in a single domain. More details about this corpus are given in Lui and Baldwin (2011). 2 We do not perform explicit encoding detection, but we do not assume that all the data is in the same encoding. Previous research has shown that explicit encoding detection is not needed for language identification (Baldwin and Lui, 2010). Our training data consists mostly of UTF8-encoded documents, but some of our evaluation datasets contain a mixture of encodings. 5 Evaluation In order to benchmark langid.py, we carried out an empirical evaluation using a number of languagelabelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incorporate a pre-trained model, and are thus usable offthe-shelf for language identification. These tools are listed in Table 3. 5.1 TextCat is an implementation of the method of Cavnar and Trenkle (1994) by Gertj"
P12-3005,P09-1120,0,0.0511671,"accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. A recent application where language identification is an open issue is over the rapidly-increasing volume of data being generated by social media. Microblog services such as Twitter4 allow users to post short text messages. Tw"
P12-3005,2005.mtsummit-papers.11,0,0.0399219,"ance for langid.py, and relative accuracy and slowdown for the other systems. For this experiment, we used a machine with 2 Intel Xeon E5540 processors and 24GB of RAM. We only utilized a single core, as none of the language identification tools tested are inherently multicore. 5.2 Comparison on standard datasets We compared the four systems on datasets used in previous language identification research (Baldwin and Lui, 2010) (E URO GOV, TCL, W IKIPEDIA), as well as an extract from a biomedical parallel corpus (Tiedemann, 2009) (EMEA) and a corpus of samples from the Europarl Parallel Corpus (Koehn, 2005) (E URO PARL). The sample of E URO PARL we use was originally prepared by Shuyo Nakatani (author of LangDetect) as a validation set. langid.py compares very favorably with other language identification tools. It outperforms TextCat in terms of speed and accuracy on all of the datasets considered. langid.py is generally 28 orders of magnitude faster than TextCat, but this advantage is reduced on larger documents. This is primarily due to the design of TextCat, which requires that the supplied models be read from file for each document classified. langid.py generally outperforms LangDetect, exce"
P12-3005,I11-1062,1,0.922587,"f. Since langid.py implements a supervised classifier, this presents two primary challenges: (1) a pretrained model must be distributed with the classifier, and (2) the model must generalize to data from different domains, meaning that in its default configuration, it must have good accuracy over inputs as diverse as web pages, newspaper articles and microblog messages. (1) is mostly a practical consideration, and so we will address it in Section 3. In order to address (2), we integrate information about the language identification task from a variety of domains by using LD feature selection (Lui and Baldwin, 2011). Lui and Baldwin (2011) showed that it is relatively easy to attain high accuracy for language iden25 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25–30, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Dataset Documents E URO GOV TCL W IKIPEDIA EMEA E URO PARL T-BE T-SC 1500 3174 4963 19988 20828 9659 5000 Langs 10 60 67 22 22 6 5 Doc Length (bytes) 1.7×104 2.6×103 1.5×103 2.9×105 1.7×102 1.0×102 8.8×101 ±3.9×104 ±3.8×103 ±4.1×103 ±7.9×105 ±1.6×102 ±3.2×101 ±3.9×101 Table 1: Summary of the LangID datas"
P12-3005,vatanen-etal-2010-language,0,0.0359285,"put documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. A recent application where language identification is an open issue is over the rapidly-increasing volume of data being generated by social media. Microblog services such as Twitter4 allow users to post short text messages. Twitter has a worldwide user base, evidenced by the large array of languages present on Twitter (Carter et al., to appear). It is estimated that half the messages on Twitter are not in English. 5 This new domain presents a significant challenge for auto"
P13-4002,D10-1124,0,0.508203,"Missing"
P13-4002,C12-1064,1,0.528301,"er data for experiments and exemplification in this study. 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–12, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sized cell representation is that it introduces class imbalance: urban areas tend to contain far more tweets than rural areas. Based on this observation, Roller et al. (2012) introduced an adaptive grid representation in which cells contain approximately the same number of users, based on a KDtree partition. Given that most tweets are from urban areas, Han et al. (2012) consider a citybased class division, and explore different feature selection methods to extract “location indicative words”, which they show to improve prediction accuracy. Additionally, time zone information has been incorporated in a coarse-to-fine hierarchical model by first determining the time zone, and then disambiguating locations within it (Mahmud et al., 2012). Topic models have also been applied to the task, in capturing regional linguistic differences (Eisenstein et al., 2010; Yin et al., 2011; Hong et al., 2012). When designing a practical geolocation system, simple models such as"
P13-4002,P12-3005,1,0.810339,"90 Acc@161 .277 .262 .487 .492 .525 .171 .665 Median 793 913 181 170 92 1330 9 Table 1: Results over WORLD radius of the home location (Cheng et al., 2010), to capture near-misses (e.g., Edinburgh UK being predicted as Glasgow, UK). • Median : The median distance from the predicted city to the home location (Eisenstein et al., 2010). 4.2 Comparison with Benchmarks We base our evaluation on the publicly-available WORLD dataset of Han et al. (2012). The dataset contains 1.4M users whose tweets are primarily identified as English based on the output of the langid.py language identification tool (Lui and Baldwin, 2012), and who have posted at least 10 geotagged tweets. The city-level home location for a geotagged user is determined as follows. First, each of a user’s geotagged tweets is mapped to its nearest city (based on the same set of 3,709 cities used for the city-based location representation). Then, the most frequent city for a user is selected as the home location. To benchmark our method, we reimplement two recently-published state-of-the-art methods: (1) the KL-divergence nearest prototype method of Roller et al. (2012) based on KD-tree partitioned grid cells, which we denote as KL; and (2) the mu"
P13-4002,D12-1137,0,0.318331,"andall et al., 2009) and Wikipedia editors (Lieberman and Lin, 2009). Recently, a considerable amount of work has been devoted to extending geolocation prediction for Twitter 1 We only use public Twitter data for experiments and exemplification in this study. 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–12, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sized cell representation is that it introduces class imbalance: urban areas tend to contain far more tweets than rural areas. Based on this observation, Roller et al. (2012) introduced an adaptive grid representation in which cells contain approximately the same number of users, based on a KDtree partition. Given that most tweets are from urban areas, Han et al. (2012) consider a citybased class division, and explore different feature selection methods to extract “location indicative words”, which they show to improve prediction accuracy. Additionally, time zone information has been incorporated in a coarse-to-fine hierarchical model by first determining the time zone, and then disambiguating locations within it (Mahmud et al., 2012). Topic models have also been"
P13-4002,P11-1096,0,0.165646,"erms which are associated with particular locations (e.g. ferry for Seattle or Sydney). Beyond identifying geographical references using off-the-shelf tools, more sophisticated methods have been introduced in the social media realm. Cheng et al. (2010) built a simple generative model based on tweet words, and further added words which are local to particular regions and applied smoothing to under-represented locations. Kinsella et al. (2011) applied different similarity measures to the task, and investigated the relative difficulty of geolocation prediction at city, state, and country levels. Wing and Baldridge (2011) introduced a grid-based representation for geolocation modeling and inference based on fixed latitude and longitude values, and aggregated all tweets in a single cell. Their approach was then based on lexical similarity using KL-divergence. One drawback to the uniform3 Methodology In this study, we adopt the same city-based representation and multinomial naive Bayes learner as Han et al. (2012). The city-based representation consists of 3,709 cities throughout the world, and is obtained by aggregating smaller cities with the largest nearby city. Han et al. (2012) found that using feature sele"
P14-1025,P13-1141,0,0.0173254,"ve of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W04-3204,0,0.195409,"Missing"
P14-1025,S07-1002,0,0.0143677,"nto a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in"
P14-1025,P06-1012,0,0.0155071,"Missing"
P14-1025,E09-1005,0,0.0121583,"he flexibility and robustness of our methodology. Future work could pursue a more sophisticated methodology, using non-linear combinations of sim(si , tj ) for computing the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. where f (tj ) is the frequency of topic tj in the corpus. The intuition behind novelty is that a target lemma with a novel sense should have a (somewhat-)frequent topic that has low association with any sense. That we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense. For each of our t"
P14-1025,cook-stevenson-2010-automatically,1,0.840918,"Missing"
P14-1025,I13-1041,1,0.812251,"one dataset), but HDP-WSI is better at inducing the overall sense distribution. It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained"
P14-1025,N06-1017,0,0.0586004,"A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability dist"
P14-1025,S07-1060,0,0.466214,"Missing"
P14-1025,D07-1109,0,0.017322,"tems, The University of Melbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable se"
P14-1025,E14-4042,1,0.869137,"Missing"
P14-1025,E09-1013,0,0.0180149,"let and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata a"
P14-1025,W11-2508,0,0.125836,"Missing"
P14-1025,D07-1108,0,0.0240323,"k@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition"
P14-1025,S13-2039,1,0.835591,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,I08-1073,1,0.825054,"d in Table 3. HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution. Testing for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FINANCE is significant (p &lt; 0.05) but the results for the other two datasets are not (p &gt; 0.1 in each case). 8 McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9 The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dict"
P14-1025,O97-1002,0,0.0490006,"n for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date ("
P14-1025,P10-1116,0,0.0142022,"lbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further"
P14-1025,N09-2059,1,0.960189,"senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguat"
P14-1025,P98-2127,0,0.0433121,"ing predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automa"
P14-1025,P12-3005,1,0.65009,"ary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Twitter, and the POS tags provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as “Other” in instances where the usage was not captured by any of the Macmillan senses. After quality control over the annotators/annotations (see 5.1 Learning Sense Distributions As in Section 4, we evaluate in terms of WSD accuracy (Table 4) and JS divergence over the gold-standard sen"
P14-1025,S13-2049,0,0.0136682,"the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignm"
P14-1025,S10-1011,0,0.00982955,"ds, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution"
P14-1025,C04-1177,1,0.882843,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,H05-1053,1,0.623299,"r of usages assigned to topic tj ), and T is the number of topics. The intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s). 4 WordNet annotations. The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. For the remainder of the paper, we denote their system as MKWC. To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of Koeling et al. (2005). For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the WordNet senses (Equation (1)), and rank the senses based on the prevalence scores (Equation (2)). In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) AccUB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation);7 and (2) ERR, the error rate reduction between the accuracy for a given system (Acc) and the upper bound (AccUB ), calculated a"
P14-1025,P04-1036,1,0.84242,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,J04-1003,0,0.0384531,"ta, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to"
P14-1025,J07-4005,1,0.874451,"e learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy e"
P14-1025,E12-1060,1,0.881316,"ity of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method f"
P14-1025,H93-1061,0,0.648686,"Missing"
P14-1025,S13-2051,1,0.840589,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,E06-1016,0,0.357917,"Missing"
P14-1025,S13-2035,0,0.0261491,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W11-1102,0,0.0307236,"Missing"
P14-1025,P10-4014,0,0.10309,"Missing"
P14-1025,S07-1006,0,0.0131677,"(Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl ov"
P14-1025,W04-2807,0,0.0704841,"move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of T"
P14-1025,N13-1079,0,0.0120313,"eyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distri"
P14-1025,W09-0214,0,0.0845474,"Missing"
P14-1025,S07-1053,0,\N,Missing
P14-1025,N06-2015,0,\N,Missing
P14-1025,C98-2122,0,\N,Missing
P14-2016,C10-3010,1,0.898211,"Missing"
P14-2016,W01-1413,0,0.0501707,"site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lanMotivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Bere"
P14-2016,J03-3002,0,0.0847608,". 1 2 Related Work This research seeks to identify documents of a particular type on the web, namely multilingual dictionaries. Related work broadly falls into four categories: (1) mining of parallel corpora; (2) automatic construction of bilingual dictionaries/thesauri; (3) automatic detection of multilingual documents; and (4) classification of document genre. Parallel corpus construction is the task of automatically detecting document sets that contain the same content in different languages, commonly based on a combination of site-structural and content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora"
P14-2016,2009.mtsummit-papers.15,0,0.0171765,"ilarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lanMotivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009). While there are syndicated efforts to produce multilingual dictionaries for different pairings of the world’s languages such as freedict.org, more commonly, multilingual dictionaries are developed in isolation for a specific set of languages, with ad hoc formatting, great variability in lexical coverage, and no central indexing of the content or existence of that dictionary (Baldwin et al., 2010). Projects such as panlex.org aspire to aggregate these dictionaries into a single lexical database, but are hampered by the need to identify individual multilingual dictionaries, especially for lang"
P14-2016,kamholz-etal-2014-panlex,0,0.0246456,"Missing"
P14-2016,I05-3027,0,0.0119983,"open web experiments, we evaluate our method based on mean average precision (MAP), that is the mean of the average precision scores for each query which returns a non-empty result set. To train our method, we use 52 bilingual Freedict (Freedict, 2011) dictionaries and Wikipedia1 documents for each of our target languages. As there are no bilingual dictionaries in Freedict for Chinese and Japanese, the training of Score values is based on the Wikipedia documents only. Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al., 2005), respectively. See Table 1 for details of the number of Wikipedia articles and dictionaries for each language. Below, we detail the construction of the synthetic dataset. 4.1 Table 2: Language proportions in ClueWeb09. million documents for the 8 languages targeted in this research (the original 10 ClueWeb09 languages minus Korean and Portuguese). We then sourced a random set of 246 multilingual dictionaries that were used in the construction of panlex.org, and injected them into the document collection. Each of these dictionaries contains at least one of our 8 target languages, with the seco"
P14-2016,P12-1102,0,0.0314345,"odology for query construction based on these elements in greater detail. The only assumption on the method is that we have access to a selection of dictionaries D (mono- or multilingual) and a corpus of conventional (non-dictionary) documents C, and knowledge of the language(s) contained in each dictionary and document. Given a set of dictionaries Dl for a language l and the complement set Dl = D Dl , we first construct the lexicon Ll for that language as follows:  Ll = wi |wi ∈ Dl ∩ wi ∈ / Dl (1) guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exha"
P14-2016,Q14-1003,1,0.637037,"ed on these elements in greater detail. The only assumption on the method is that we have access to a selection of dictionaries D (mono- or multilingual) and a corpus of conventional (non-dictionary) documents C, and knowledge of the language(s) contained in each dictionary and document. Given a set of dictionaries Dl for a language l and the complement set Dl = D Dl , we first construct the lexicon Ll for that language as follows:  Ll = wi |wi ∈ Dl ∩ wi ∈ / Dl (1) guage identification, and specifically the detection of multilingual documents (Prager, 1999; Yamaguchi and Tanaka-Ishii, 2012; Lui et al., 2014). Here, multi-label document classification methods have been adapted to identify what mix of languages is present in a given document, which could be used as a pre-filter to locate documents containing a given mixture of languages, although there is, of course, no guarantee that a multilingual document is a dictionary. Finally, document genre classification is relevant in that it is theoretically possible to develop a document categorisation method which classifies documents as multilingual dictionaries or not, with the obvious downside that it would need to be applied exhaustively to all doc"
P14-2016,2009.mtsummit-posters.26,0,0.0373255,"content-based features (Chen and Nie, 2000; Resnik and Smith, 2003). Such methods could potentially identify parallel word lists from which to construct a bilingual dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lanMotivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language l"
P14-2016,1996.amta-1.13,0,0.224941,"l dictionary, although more realistically, bilingual dictionaries exist as single documents and are not well suited to this style of analysis. Methods have also been proposed to automatically construct bilingual dictionaries or thesauri, e.g. based on crosslingual glossing in predictable patterns such as a technical term being immediately proceeded by that term in a lingua franca source language such as English (Nagata et al., 2001; Yu and Tsujii, 2009). Alternatively, comparable or parallel corpora can be used to extract bilingual dictionaries based on crosslingual distributional similarity (Melamed, 1996; Fung, 1998). While the precision of these methods is generally relatively high, the recall is often very low, as there is a strong bias towards novel technical terms being glossed but more conventional terms not. Also relevant to this work is research on lanMotivation Translation dictionaries and other multilingual lexical resources are valuable in a myriad of contexts, from language preservation (Thieberger and Berez, 2012) to language learning (Laufer and Hadar, 1997), cross-language information retrieval (Nie, 2010) and machine translation (Munteanu and Marcu, 2005; Soderland et al., 2009"
P14-2016,J05-4003,0,\N,Missing
P15-2104,D10-1124,0,0.503589,"Missing"
P15-2104,W06-3808,0,0.029268,"Missing"
P15-2104,C12-1064,1,0.842564,", and temporal data. Text-based methods model the geographical bias of language use in social media, and use it to geolocate non-geotagged users. Gazetted expressions (Leidner and Lieberman, 2011) and geographical names (Quercini et 630 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 630–636, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics stein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). In each dataset, users are represented by a single meta-document, generated by concatenating their tweets. The datasets are pre-partitioned into training, development and test sets, and rebuilt from the original version to include mention information. The first two datasets were constructed to contain mostly English messages. G EOT EXT consists of tweets from 9.5K users: 1895 users are held out for each of development and test data. The primary location of each user is set to the coordinates of their first tweet. T WITTER -US consists of 449K users, of which 10K users are held out for each o"
P15-2104,N15-1153,1,0.615367,"Information Systems The University of Melbourne arahimi@student.unimelb.edu.au {t.cohn,tbaldwin}@unimelb.edu.au Abstract methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015). Methods which combine the two, however, are rare. In this paper, we present our work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline network-based geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013; Rahimi et al., 2015); (2) we demonstrate that removing “celebrity” nodes (nodes with high in-degrees) from the network increases geolocation accuracy and dramatically decreases network edge size; and (3) we in"
P15-2104,D12-1137,0,0.837304,"eolocation Using a Unified Text and Network Prediction Model Afshin Rahimi, Trevor Cohn, and Timothy Baldwin Department of Computing and Information Systems The University of Melbourne arahimi@student.unimelb.edu.au {t.cohn,tbaldwin}@unimelb.edu.au Abstract methods. Orthogonally, the geolocation task can be viewed as a regression task over real-valued geographical coordinates, or a classification task over discretised region-based locations. Most previous research on user geolocation has focused either on text-based classification approaches (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or, to a lesser extent, network-based regression approaches (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015). Methods which combine the two, however, are rare. In this paper, we present our work on Twitter user geolocation using both text and network information. Our contributions are as follows: (1) we propose the use of Modified Adsorption (Talukdar and Crammer, 2009) as a baseline network-based geolocation model, and show that it outperforms previous network-based approaches (Jurgens, 2013; Rahimi et al., 2015); (2) we demonstrate that removing “celebrity” node"
P15-2104,P11-1096,0,0.623803,"Missing"
P15-2104,D14-1039,0,0.550208,"development and test data. The primary location of each user is, once again, set to the coordinates of their first tweet. T WITTER -W ORLD consists of 1.3M users, of which 10000 each are held out for development and test. Unlike the other two datasets, the primary location of users is mapped to the geographic centre of the city where the majority of their tweets were posted. al., 2010) were used as feature in early work, but were shown to be sparse in coverage. Han et al. (2014) used information-theoretic methods to automatically extract location-indicative words for location classification. Wing and Baldridge (2014) reported that discriminative approaches (based on hierarchical classification over adaptive grids), when optimised properly, are superior to explicit feature selection. Cha et al. (2015) showed that sparse coding can be used to effectively learn a latent representation of tweet text to use in user geolocation. Eisenstein et al. (2010) and Ahmed et al. (2013) proposed topic modelbased approaches to geolocation, based on the assumption that words are generated from hidden topics and geographical regions. Similarly, Yuan et al. (2013) used graphical models to jointly learn spatio-temporal topics"
P16-1143,W04-3204,0,0.117847,"Missing"
P16-1143,N15-1132,0,0.200608,"the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of s"
P16-1143,P11-2087,0,0.0263279,"Missing"
P16-1143,S07-1060,0,0.165989,"nt sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on"
P16-1143,D07-1109,0,0.185536,"o the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been ma"
P16-1143,P06-1013,0,0.127078,"heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 word corpus that has been manually tagged with W"
P16-1143,W10-0701,0,0.0326124,"our English Wikipedia corpus. The remaining lemmas were then split into 5 groups of approximately equal size based on S EM C OR frequency. The S EM C OR frequencies contained in each group are summarised in Table 1. From each of the S EM C OR frequency groups, we randomly sampled 10 lemmas, giving 50 lemmas in total. Then for each lemma, we randomly sampled 100 usages to be annotated from English Wikipedia. This was done in the same way as the sampling of lemma usages for L EX S EM TM (see Section 6). We obtained crowdsourced sense annotations for each lemma using Amazon Mechanical Turk (AMT: Callison-Burch and Dredze (2010)). The sentences for each lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions we"
P16-1143,P06-1012,0,0.19513,", 2014), semi-automatic dictionary construction (Cook et al., 2013), lexical simplification (Biran et al., 2011), and textual entailment (Shnarch et al., 2011). Automatically acquired sense distributions themselves are also used to improve unsupervised WSD, for example by providing a most frequent sense heuristic (McCarthy et al., 2004b; Jin et al., 2009) or by improving unsupervised usage sampling strategies (Agirre and Martinez, 2004). Furthermore, the improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency"
P16-1143,C14-1035,0,0.0254343,"others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 20"
P16-1143,D14-1110,0,0.186671,"e improvement due to the most frequent sense heuristic has been particularly strong when used with domain-specific data (Koeling et al., 2005; Chan and Ng, 2006; Lau et al., 2014). Introduction Word sense disambiguation (WSD), as well as more general problems involving word senses, have been of great interest to the NLP community for many years (for a detailed overview, see Agirre and Edmonds (2007) and Navigli (2009)). In particular, there has recently been a lot of work on unsupervised techniques for these problems. This includes unsupervised methods for performing WSD (Postma et al., 2015; Chen et al., 2014; Boyd-Graber et al., 2007; Brody et al., 2006), In addition, there is great scope to use these techniques to improve existing sense frequency resources, which are currently limited by the bottleneck of requiring manual sense annotation. The most prominent example of such a resource is W ORD N ET (Fellbaum, 1998), where the sense 1513 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1513–1524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics frequency data is based on S EM C OR (Miller et al., 1993), a 220,000 wo"
P16-1143,C00-1027,0,0.202938,"Missing"
P16-1143,N13-1132,0,0.0174577,"ch lemma were split into 4 batches (25 sentences per batch). In addition, two control sentences18 were created for each lemma, and added to each corresponding batch. Each batch of 27 items was annotated separately by 10 annotators. For each item to be annotated, annotators were provided with the sentence containing the lemma, the gloss for each sense as listed in W ORD N ET 3.019 and a list of hypernyms and synonyms for each sense. Annotators were asked to assign each item to exactly one sense. From these crowdsourced annotations, our gold-standard sense distributions were created using MACE (Hovy et al., 2013), which is a generalpurpose tool for inferring item labels from multiannotator, multi-item tasks. It provides a Bayesian framework for modelling item annotations, modelling the individual biases of each annotator, and 17 We chose to restrict our scope in this evaluation to nouns because much of the prior work has also focussed on nouns, and these are the words we would expect others to care the most about disambiguating, since they are more often context bearing. Also, introducing other POS would require a greater quantity of expensive annotated data. 18 These were created manually, to be as c"
P16-1143,N09-2059,1,0.917589,"oyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense distribution learning — is in principle very similar to identifying the MFS. Indeed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan a"
P16-1143,H05-1053,1,0.832069,"ew/527 8 For simplicity we use HCA to refer to both the topic modelling algorithm implemented by Buntine and Mishra (2014) as well as the corresponding software suite, whereas elsewhere HCA often only refers to the software. 1516 HCA Topic Model Convergence 104 log2 (perplexity) Topic Model Training Time (s) Training Time for HDP-WSI vs. HCA-WSI 103 102 HDP-WSI HCA-WSI 0 20 40 Number of Lemma Usages (1000’s) 14 12 10 0 Figure 1: Comparison of the time taken to train the topic models of HDP-WSI and HCA-WSI for each lemma in the BNC dataset. For each method, one data point is plotted per lemma. Koeling et al. (2005),9 which was also used by Lau et al. (2014). This dataset consists of 40 English lemmas, and for each lemma it contains a set of usages of varying size from the BNC and a gold-standard sense distribution that was created by hand-annotating a subset of the usages with W ORD N ET 1.7 senses. Using this dataset, we can calculate the quality of a candidate sense distribution by calculating its Jensen Shannon divergence (JSD) with respect to the corresponding gold-standard distribution. JSD is a measure of dissimilarity between two probability distributions, so a lower JSD score means the distribut"
P16-1143,J04-1003,0,0.0262866,"iments is available via: https://github.com/awbennett/LexSemTm 2 Background and Related Work Given the difficulty and expense of obtaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar appro"
P16-1143,N07-1044,0,0.0165171,"lly similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository containing sense glosses. The task we are interested in — namely, sense di"
P16-1143,P04-1036,1,0.835389,"posed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapt"
P16-1143,J07-4005,1,0.0500043,"btaining large-scale and robust annotated data, unsupervised approaches to problems involving word learning and recognising word senses have long been studied in NLP. Perhaps the most famous such problem is word sense disambiguation (WSD), for which many unsupervised solutions have been proposed. Some methods are very complex, performing WSD separately for each word usage using information such as word embeddings of surrounding words (Chen et al., 2014) or POStags (Lapata and Brew, 2004). On the other hand, most approaches make use of the difficult-to-beat most frequent sense (MFS) heuristic (McCarthy et al., 2007), which assigns each usage of a given word-type to its most frequent sense. Given the popularity of the MFS heuristic, much of the past work on unsupervised techniques has focused on identifying the most frequent sense. The original method of this kind was proposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method"
P16-1143,H93-1061,0,0.858704,"Missing"
P16-1143,E06-1016,0,0.0192742,"roposed by McCarthy et al. (2004b), which relied on finding distributionally similar words to the target word, and comparing these to the candidate senses. Most subsequent approaches have followed a similar approach, based on the words appearing nearby the target word across its token usages. Boyd-Graber and Blei (2007) formalise the method of McCarthy et al. (2004b) with a probabilistic model, while others take different approaches, such as adapting existing sense frequency data to specific domains (Chan and Ng, 2005; Chan and Ng, 2006), using coarse grained thesaurus-like sense inventories (Mohammad and Hirst, 2006), adapting information retrieval-based methods (Lapata and Keller, 2007), using ensemble learning (Brody et al., 2006), utilising the network structure of W ORD N ET (Boyd-Graber et al., 2007), or making use of word embeddings (Bhingardive et al., 2015). Alternatively, Jin et al. (2009) focus on how best to use the MFS heuristic, by identifying when best to apply it, based on sense distribution entropy. Perhaps the most promising approach is that of Lau et al. (2014), due to its state-of-the art performance, and the fact that it can easily by applied to any language and any sense repository co"
P16-1143,D14-1113,0,0.02152,"kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural n"
P16-1143,E12-1060,1,0.603835,"eed, of these methods for identifying the MFS, some of them are 1514 explicitly described in terms of sense distribution learning (Chan and Ng, 2005; Chan and Ng, 2006; Lau et al., 2014), while the others implicitly learn sense distributions by calculating some kind of scores used to rank senses. The state-of-the-art technique of Lau et al. (2014) that we are building upon involves performing unsupervised word sense induction (WSI), which itself is implemented using nonparametric HDP (Teh et al., 2006) topic models, as detailed in Section 3. The WSI component, HDP-WSI, is based on the work of Lau et al. (2012), which at the time was state-of-the-art. Since then, however, other competitive WSI approaches have been developed, involving complex structures such as multi-layer topic models (Chang et al., 2014), or complex word embedding based approaches (Neelakantan et al., 2014). We have not used these approaches in this work on account of their complexity and likely computational cost, however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Re"
P16-1143,S15-2058,0,0.0349866,"Missing"
P16-1143,P14-1025,1,0.218445,"f languages, which could be extremely computationally expensive. To make things worse, domain differences could require learning numerous distributions per word. Despite this, though, we would not want to make these techniques scalable at the expense of sense distribution quality. Therefore, we would like to understand the tradeoff between the accuracy and computation time of these techniques, and optimise this tradeoff. This could be particularly critical in applying them in an industrial setting. The current state-of-the-art technique for unsupervised sense distribution learning is HDP-WSI (Lau et al., 2014). In order to address the above concerns, we provide a series of investigations exploring how to best optimise HDP-WSI for largescale application. We then use our optimised technique to produce L EX S EM TM,1 a semantic and sense frequency dataset of unprecedented size, spanning the entire vocabulary of English. Finally, we use crowdsourced data to produce a new set of gold-standard sense distributions to accompany L EX S EM TM. We use these to investigate the quality of the sense frequency data in L EX S EM TM with respect to S EM C OR. 1 L EX S EM TM, as well as code for accessing L EX S EM"
P16-1143,P11-2098,0,0.0658532,"Missing"
P16-1143,N15-1074,0,0.0151679,"however we believe they are worth future exploration. On the other hand, because HDP-WSI is implemented using topic models, it can be customised by replacing HDP with newer, more efficient topic modelling algorithms. Recent work has produced more advanced topic modelling approaches, some of which are extensions of existing approaches using more advanced learning algorithms or expanded models (Buntine and Mishra, 2014), while others are more novel, involving variations such as neural networks (Larochelle and Murray, 2011; Cao et al., 2015), or incorporating distributional similarity of words (Xie et al., 2015). Of these approaches, we chose to experiment with that of Buntine and Mishra (2014) because a working implementation was readily available, it has previously shown very strong performance in terms of accuracy and speed, and it is similar to HDP and thus easy to incorporate into our work. 3 HDP-WSI Sense Learning HDP-WSI (Lau et al., 2014) is a state-of-theart unsupervised method for learning sense distributions, given a sense repository with per-sense glosses. It takes as input a collection of example usages of the target lemma2 and the glosses 2 Except where stated otherwise, a lemma usage i"
P16-1158,N07-4013,0,0.01262,"Missing"
P16-1158,S07-1003,0,0.010422,"Missing"
P16-1158,W11-2501,0,0.0573442,"ccurring in all embedding sets. There is some overlap between our relations and those included in the analogy task of Mikolov et al. (2013c), but we include a much wider range of lexical semantic relations, especially those standardly evaluated in the relation classification literature. We manually filtered the data to remove duplicates (e.g., as part of merging the two sources of L EX S EMHyper intances), and normalise directionality. The final dataset consists of 12,458 triples hrelation, word1 , word2 i, comprising 15 relation types, extracted from SemEval’12 (Jurgens et al., 2012), BLESS (Baroni and Lenci, 2011), the MSR analogy dataset (Mikolov et al., 2013c), the light verb dataset of Tan et al. (2006a), Princeton WordNet (Fellbaum, 1998), Wiktionary,5 and a web lexicon of collective nouns,6 as listed in Table 2.7 4 Clustering Assuming D IFF V ECs are capable of capturing all lexical relations equally, we would expect clustering to be able to identify sets of word pairs with high relational similarity, or equivalently clusters of similar offset vectors. Under the additional assumption that a given word pair corresponds to a unique lexical relation (in line with our definition of the lexical relatio"
P16-1158,S10-1006,0,0.0326996,"Missing"
P16-1158,E12-1004,0,0.0620553,"map each word pair (wi , wj ) as follows: (a) (wi , wj ) 7→ rk ∈ R, i.e. the “closed-world” setting, where we assume that all word pairs can be uniquely classified according to a relation in R; or (b) (wi , wj ) 7→ rk ∈ R ∪ {φ} where φ signifies the fact that none of the relations in R apply to the word pair in question, i.e. the “open-world” setting. Our starting point for lexical relation learning is the assumption that important information about various types of relations is implicitly embedded in the offset vectors. While a range of methods have been proposed for composing word vectors (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014), in this research we focus exclusively on D IFF V EC (i.e. w2 − w1 ). A second assumption is that there exist dimensions, or directions, in the embedding vector spaces responsible for a particular lexical relation. Such dimensions could be identified and exploited as part of a clustering or classification method, in the context of identifying relations between word pairs or classes of D IFF V ECs. In order to test the generalisability of the D IFF V EC method, we require: (1) word embeddings, and (2) a set of lexical relations to evaluate against. As"
P16-1158,N15-1184,0,0.00539531,"a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gen"
P16-1158,S12-1047,0,0.193923,"set. In the Open Information Extraction paradigm (Banko et al., 2007; Weikum and Theobald, 2010), also known as unsupervised relation extraction, the relations themselves are also learned from the text (e.g. in the form of text labels). On the other hand, relational similarity prediction involves assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This"
P16-1158,D13-1169,0,0.0654934,"Missing"
P16-1158,W15-0105,0,0.0442251,"Missing"
P16-1158,S12-1012,0,0.0133615,"es assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based clas"
P16-1158,W14-1618,0,0.0602254,"chine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010). Recently, attention has been focused on identifying lexical relations using word embeddings, which are dense, low-dimensional vectors obtained either from a “predict-based” neural network trained to predict word contexts, or a “countbased” traditional distributional similarity method combined with dimensionality reduction. The skipgram model of Mikolov et al. (2013a) and other similar language models have been shown to perform well on an analogy completion task (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014a), in the space of relational simThe key operation in these models is vector difference, or vector offset. For example, the paris − france vector appears to encode CAPITAL - OF , presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a). The success of the simple offset method on analogy completion suggests that the difference vectors (“D IFF V EC” hereafter) must themselves be meaningful: their direction and/or magnitude encodes a lexical relation. Previous analogy completion tasks used w"
P16-1158,P14-1113,0,0.0554104,"d the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help exp"
P16-1158,P05-1014,0,0.0294569,"er hand, relational similarity prediction involves assessing the degree to which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine"
P16-1158,Q15-1016,0,0.185434,"move to an open-world setting including random word pairs — many of which do not correspond to any lexical relation in the training data — the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesising negative instances. We find that this improves the model performance substantially. We also find that hyper-parameter optimised count-based methods are competitive with predictbased methods under both clustering and supervised relation classification, in line with the findings of Levy et al. (2015a). 2 Background and Related Work A lexical relation is a binary relation r holding between a word pair (wi , wj ); for example, the pair (cart, wheel) stands in the WHOLE - PART relation. Relation learning in NLP includes relation extraction, relation classification, and relational similarity prediction. In relation extraction, related word pairs in a corpus and the relevant relation are identified. Given a word pair, the relation classification task involves assigning a word pair to the correct relation from a pre-defined set. In the Open Information Extraction paradigm (Banko et al., 2007;"
P16-1158,N15-1098,0,0.135987,"move to an open-world setting including random word pairs — many of which do not correspond to any lexical relation in the training data — the results are poor. We then investigate methods for better attuning the learned class representation to the lexical relations, focusing on methods for automatically synthesising negative instances. We find that this improves the model performance substantially. We also find that hyper-parameter optimised count-based methods are competitive with predictbased methods under both clustering and supervised relation classification, in line with the findings of Levy et al. (2015a). 2 Background and Related Work A lexical relation is a binary relation r holding between a word pair (wi , wj ); for example, the pair (cart, wheel) stands in the WHOLE - PART relation. Relation learning in NLP includes relation extraction, relation classification, and relational similarity prediction. In relation extraction, related word pairs in a corpus and the relevant relation are identified. Given a word pair, the relation classification task involves assigning a word pair to the correct relation from a pre-defined set. In the Open Information Extraction paradigm (Banko et al., 2007;"
P16-1158,W13-3207,0,0.0166618,"al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gender, and distance, finding that for about two-thirds of antonym classes, D IFF V ECs are significantly more correlated than random. Necs¸ulescu et al. (2015) train a classifier on word pairs, using word embeddings to predict coordinates, hypernyms, and meronyms. Roller and Erk (2016) analyse the performance of vector concatenation and difference on the task of predicting lexical entailment and show that vector concatenation overwhelmingly learns to detect Hearst patterns (e.g., including, such as). K¨oper et al. (2015) under"
P16-1158,S10-1011,0,0.0280231,"Missing"
P16-1158,N13-1090,0,0.722758,"lexical relations is a fundamental task in natural language processing (“NLP”), and can contribute to many NLP applications including paraphrasing and generation, machine translation, and ontology building (Banko et al., 2007; Hendrickx et al., 2010). Recently, attention has been focused on identifying lexical relations using word embeddings, which are dense, low-dimensional vectors obtained either from a “predict-based” neural network trained to predict word contexts, or a “countbased” traditional distributional similarity method combined with dimensionality reduction. The skipgram model of Mikolov et al. (2013a) and other similar language models have been shown to perform well on an analogy completion task (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014a), in the space of relational simThe key operation in these models is vector difference, or vector offset. For example, the paris − france vector appears to encode CAPITAL - OF , presumably by cancelling out the features of paris that are France-specific, and retaining the features that distinguish a capital city (Levy and Goldberg, 2014a). The success of the simple offset method on analogy completion suggests that the differe"
P16-1158,S15-1021,0,0.444375,"Missing"
P16-1158,C04-1111,0,0.00884956,"f the frequency profile of our corpus. We train 9 binary RBF-kernel SVM classifiers on the training partition, and evaluate on our randomly augmented test set. Fully annotating our random word pairs is prohibitively expensive, so instead, we manually annotated only the word pairs which were positively classified by one of our models. The results of our experiments are presented in the left half of Table 5, in which we report on results over the combination of the original test data from §5.1 and the random word pairs, noting that recall (R) for O PEN -W ORLD takes the form of relative recall (Pantel et al., 2004) over the positively-classified word pairs. The results are much lower than for the closed-word setting (Table 4), most notably in terms of precision (P). For instance, the random pairs (have, works), (turn, took), and (works, started) were incorrectly classified as V ERB3 , V ERBPast and V ERB3Past , respectively. That is, the model captures syntax, but lacks the ability to capture lexical paradigms, and tends to overgenerate. 5.3 O PEN -W ORLD Training with Negative Sampling To address the problem of incorrectly classifying random word pairs as valid relations, we retrain the classifier on a"
P16-1158,D14-1162,0,0.0773416,"Missing"
P16-1158,E14-1054,1,0.235198,"(A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013)"
P16-1158,D16-1234,0,0.0367941,"berg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such as quality, time, gender, and distance, finding that for about two-thirds of antonym classes, D IFF V ECs are significantly more correlated than random. Necs¸ulescu et al. (2015) train a classifier on word pairs, using word embeddings to predict coordinates, hypernyms, and meronyms. Roller and Erk (2016) analyse the performance of vector concatenation and difference on the task of predicting lexical entailment and show that vector concatenation overwhelmingly learns to detect Hearst patterns (e.g., including, such as). K¨oper et al. (2015) undertake a systematic study of morphosyntactic and semantic relations on word embeddings produced with word2vec (“w2v” hereafter; see §3.1) for English and German. They test a variety of relations including word similarity, antonyms, synonyms, hypernyms, and meronyms, in a novel analogy task. Although the set of relations tested by 1672 Name w2v GloVe SENN"
P16-1158,C14-1097,0,0.128351,"Missing"
P16-1158,D07-1043,0,0.0108822,"Missing"
P16-1158,E14-4008,0,0.0133383,"in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings t"
P16-1158,W06-2407,0,0.0232598,"EFIX N OUNColl Description hypernym meronym characteristic quality, action cause, purpose, or goal location or time association expression or representation object’s action plural form of a noun first to third person verb present-tense form present-tense to past-tense verb form third person present-tense to past-tense verb form light verb construction nominalisation of a verb prefixing with re morpheme collective noun Pairs 1173 2825 71 249 235 187 3583 100 99 100 100 58 3303 118 257 Source SemEval’12 + BLESS SemEval’12 + BLESS SemEval’12 SemEval’12 SemEval’12 SemEval’12 BLESS MSR MSR MSR MSR Tan et al. (2006b) WordNet Wiktionary Web source Example (animal, dog) (airplane, cockpit) (cloud, rain) (cook, eat) (aquarium, fish) (song, emotion) (zip, coat) (year, years) (accept, accepts) (know, knew) (creates, created) (give, approval) (approve, approval) (vote, revote) (army, ants) Table 2: Description of the 15 lexical relations. to train the model. We use the focus word vectors, W = {wk }Vk=1 , normalised such that each kwk k = 1. The GloVe model (Pennington et al., 2014) is based on a similar bilinear formulation, framed as a low-rank decomposition of the matrix of corpus co-occurrence frequencies:"
P16-1158,P10-1040,0,0.00287074,"caling matrix, and b∗ is a bias term. The final model, SENNA (Collobert and Weston, 2008), was initially proposed for multi-task training of several language processing tasks, from language modelling through to semantic role labelling. Here we focus on the statistical language modelling component, which has a pairwise ranking objective to maximise the relative score of each word in its local context: J= where the last c − 1 words are used as context, and f (x) is a non-linear function of the input, defined as a multi-layer perceptron. For HLBL and SENNA, we use the pre-trained embeddings from Turian et al. (2010), trained on the Reuters English newswire corpus. In both cases, the embeddings were scaled by the global standard deviation over the word-embedding matrix, W Wscaled = 0.1 × σ(W ). For w2vwiki , GloVewiki and SVDwiki we used English Wikipedia. We followed the same preprocessing procedure described in Levy et al. (2015a),3 i.e., lower-cased all words and removed non-textual elements. During the training phase, for each model we set a word frequency threshold of 5. For the SVD model, we followed the recommendations of Levy et al. (2015a) in setting the context window size to 2, negative samplin"
P16-1158,J06-3003,0,0.0353682,"Missing"
P16-1158,C14-1212,0,0.467066,"o which a word pair (A, B) stands in the same relation as another pair (C, D), or to complete an analogy A:B :: C: –?–. Relation learning is an important and long-standing task in NLP and has been the focus of a number of shared tasks (Girju et al., 2007; Hendrickx et al., 2010; Jurgens et al., 2012). Recently, attention has turned to using vector space models of words for relation classification and relational similarity prediction. Distributional word vectors have been used for detection of relations such as hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de M"
P16-1158,D09-1097,0,0.0273496,"Missing"
P16-1158,P14-2089,0,0.00548246,"(2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2015; Fried and Duh, 2015). Another strand of work responding to the vector difference approach has analysed the structure of predict-based embedding models in order to help explain their success on the analogy and other tasks (Levy and Goldberg, 2014a; Levy and Goldberg, 2014b; Arora et al., 2015). However, there has been no systematic investigation of the range of relations for which the vector difference method is most effective, although there have been some smallerscale investigations in this direction. Makrai et al. (2013) divide antonym pairs into semantic classes such"
P16-1158,N13-1120,0,0.0317494,"hypernymy (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Santus et al., 2014) and qualia structure (Yamada et al., 2009). An exciting development, and the inspiration for this paper, has been the demonstration that vector difference over word embeddings (Mikolov et al., 2013c) can be used to model word analogy tasks. This has given rise to a series of papers exploring the D IFF V EC idea in different contexts. The original analogy dataset has been used to evaluate predict-based language models by Mnih and Kavukcuoglu (2013) and also Zhila et al. (2013), who combine a neural language model with a pattern-based classifier. Kim and de Marneffe (2013) use word embeddings to derive representations of adjective scales, e.g. hot—warm—cool— cold. Fu et al. (2014) similarly use embeddings to predict hypernym relations, in this case clustering words by topic to show that hypernym D IFF V ECs can be broken down into more fine-grained relations. Neural networks have also been developed for joint learning of lexical and relational similarity, making use of the WordNet relation hierarchy (Bordes et al., 2013; Socher et al., 2013; Xu et al., 2014; Yu and"
P16-2056,I13-1171,0,0.0517045,"Missing"
P16-2056,W15-0705,1,0.89095,"advantage of the properties of fiction texts, in particular the repetition of names, to build a high-performing 3-class NER system which distinguishes people and locations from other capitalized words and phrases. Notably, we do this without any hand-labelled 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–350, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tion also involves clustering multiple aliases of the same character, and discarding person names that don’t correspond to characters. Vala et al. (2015) identify some of the failures of off-the-shelf NER with regards to character identification, and attempt to fix them; their efforts are focused, however, on characters that are referred to by description rather than names or aliases. 3 3.2 The next step is to induce Brown clusters (Brown et al., 1992) over the pre-segmented corpus (including potential names), using the tool of Liang (2005). Briefly, Brown clusters are formed using an agglomerative hierarchical cluster of terms based on their immediate context, placing terms into categories to maximize the probability of consecutive terms over"
P16-2056,N04-1043,0,0.166354,"Missing"
P16-2056,J92-4003,0,0.700724,"Missing"
P16-2056,J93-2004,0,\N,Missing
P16-2056,N09-1019,0,\N,Missing
P16-2056,W09-1119,0,\N,Missing
P16-2056,P14-1035,0,\N,Missing
P16-2056,W02-1028,0,\N,Missing
P16-2056,D11-1141,0,\N,Missing
P16-2056,H92-1045,0,\N,Missing
P16-2056,2015.lilt-12.4,0,\N,Missing
P16-2056,E14-4042,1,\N,Missing
P16-2056,P05-1045,0,\N,Missing
P16-2056,R15-1016,0,\N,Missing
P16-4022,N16-1122,0,0.184973,"Missing"
P16-4022,D10-1124,0,0.170675,"Missing"
P16-4022,C12-1064,1,0.957211,"n language use is most evident for countries with different languages (e.g. Germany versus China), but also exists for countries which share the same languages (e.g. in the spelling of centre vs. center in British vs. American English). The linguistic geographical bias is not limited to these obvious cases, however, and includes the use of toponyms, names of people, sport teams, and dialectal terms. These differences in use of language can be captured in text-based geolocation models. Previous work have used topic models (Eisenstein et al., 2010) and supervised flat (Wing and Baldridge, 2011; Han et al., 2012; Han et al., 2013; Han et al., 2014; Rahimi et al., 2015b) and hierarchical (Wing and Baldridge, 2014) classification models. The main idea is to learn the geographical distribution of a given word across different locations from training data, and use it to predict a location for a new user. Social ties have also been used for social media user geolocation. Backstrom et al. (2010) showed that Facebook users tend to interact more with nearby people (“location homophily”), and used this property to geolocate users based on the location of their friends, hence popularising networkbased geolocat"
P16-4022,P15-2104,1,0.807258,"e the geographical barrier for users to communicate, the majority of user interactions are still local (Backstrom et al., 2010). This geographical bias can be utilised to geolocate a user by analysing their social interactions. Based on the assumption that social interactions are more likely to be local, a user should be geographically close to their connections. The simplest approach to geolocation is to use the median location of a user’s friends. Recent studies have shown that using both network and text information can improve the coverage and keep the predictions accurate simultaneously (Rahimi et al., 2015b). Despite the widespread use of geolocation, most services are proprietary, overly-simplistic, or complicated to use. Supervised classification models often require huge amounts of geotagged data and large amounts of computing power to be trained. The performance is also heavily dependent on hyperparameter tuning, making the training procedure more challenging for end-users. In this paper we introduce pigeo, a Python geolocation tool that has the following characteristics: (1) it comes with a pre-trained textbased model; (2) it is easy to use; (3) it has been tuned, benchmarked and proven to"
P16-4022,P13-4002,1,0.766633,"Missing"
P16-4022,N15-1153,1,0.664296,"e the geographical barrier for users to communicate, the majority of user interactions are still local (Backstrom et al., 2010). This geographical bias can be utilised to geolocate a user by analysing their social interactions. Based on the assumption that social interactions are more likely to be local, a user should be geographically close to their connections. The simplest approach to geolocation is to use the median location of a user’s friends. Recent studies have shown that using both network and text information can improve the coverage and keep the predictions accurate simultaneously (Rahimi et al., 2015b). Despite the widespread use of geolocation, most services are proprietary, overly-simplistic, or complicated to use. Supervised classification models often require huge amounts of geotagged data and large amounts of computing power to be trained. The performance is also heavily dependent on hyperparameter tuning, making the training procedure more challenging for end-users. In this paper we introduce pigeo, a Python geolocation tool that has the following characteristics: (1) it comes with a pre-trained textbased model; (2) it is easy to use; (3) it has been tuned, benchmarked and proven to"
P16-4022,P11-1096,0,0.109648,"model. Geographical bias in language use is most evident for countries with different languages (e.g. Germany versus China), but also exists for countries which share the same languages (e.g. in the spelling of centre vs. center in British vs. American English). The linguistic geographical bias is not limited to these obvious cases, however, and includes the use of toponyms, names of people, sport teams, and dialectal terms. These differences in use of language can be captured in text-based geolocation models. Previous work have used topic models (Eisenstein et al., 2010) and supervised flat (Wing and Baldridge, 2011; Han et al., 2012; Han et al., 2013; Han et al., 2014; Rahimi et al., 2015b) and hierarchical (Wing and Baldridge, 2014) classification models. The main idea is to learn the geographical distribution of a given word across different locations from training data, and use it to predict a location for a new user. Social ties have also been used for social media user geolocation. Backstrom et al. (2010) showed that Facebook users tend to interact more with nearby people (“location homophily”), and used this property to geolocate users based on the location of their friends, hence popularising net"
P16-4022,D14-1039,0,0.676885,"Missing"
P16-4022,W14-4204,0,0.0597922,"Missing"
P17-1033,W13-0102,0,0.018532,"topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model,"
P17-1033,P03-1054,0,0.0199484,"g, we have additional minibatches for the documents. We start the document classification training after the topic and language models have completed training in each epoch. We use 20 NEWS in this experiment, which is a popular dataset for text classification. 20 NEWS is a collection of forum-like messages from 20 newsgroups categories. We use the “bydate” version of the dataset, where the train and test partition is separated by a specific date. We sample 2K documents from the training set to create the development set. For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words. As with previous experiments (Section 4) we additionally filter low/high frequency word types and stopwords. Preprocessed dataset statistics are presented in Table 5. For comparison, we use the same two topic ntm: ntm is a neural topic model proposed by Cao et al. (2015). The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions. Model hyper-parameters are tuned using development loss. Topic model performance is presented in Table 4. There are two models of tdlm (tdlm-small and tdlm-large), which spec"
P17-1033,W14-4012,0,0.190012,"Missing"
P17-1033,N16-1057,1,0.799751,"does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4). We use the same hyper-parameter settings for tdlm and do not tune them. Based on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words. To aggregate topic coherence scores for a model, we calculate the mean coherence over topics. For comparison, we use the following topic models: lda: We use a LDA model as a baseline topic model. We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4). Following Lau et al. (2014), we compute topic coherence using normalised PMI (“NPMI”) scores. Given the top-n words of a topic, coherence is computed based on the sum of pair13 We use this toolkit to compute topic coherence: https://github.com/"
P17-1033,E14-1056,1,0.793704,"distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics. Let Bt denote the topic output vector for the t-th topic. To generate the multinomial distribution over word types for the t-th topic, we replace s with Bt before computing the softmax over the vocabulary. wise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13 Topic models are traditionally evaluated using model perplexity. There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al. (2009) show that perplexity does not correlate with the coherence of the generated topics. Newman et al. (2010b); Mimno et al. (2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al. (2014) summarises these methods to understand their differences. We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm. In terms of datasets, we use the same"
P17-1033,P11-1015,0,0.226548,"Missing"
P17-1033,D08-1038,0,0.0526451,"datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al."
P17-1033,D11-1024,0,0.126281,"Missing"
P17-1033,N10-1012,1,0.90555,"e that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics. 1 Introduction Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006). A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009). Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text). The primary purpose of a language model is to predict the probability of a 2 Related Work Griffiths et al. (2004) propose a mod"
P17-1033,N16-1036,0,0.0174909,"ci i In the case where we use a filters, we have d ∈ Ra , and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network. The topic vectors are stored in two lookup tables A ∈ Rk×a (input vector) and B ∈ Rk×b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors. To align the document vector d with the topics, we compute an attention vector which is used to 2 The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016). We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best. 1 A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm. 357 We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model: ument collections from 3 sources: APNEWS, IMDB and BNC. APNEWS is a collection of Associated Press5 news articles from 2009 to 2016. IMDB is a set of m"
P17-1033,P16-1125,0,0.0617449,"Missing"
P17-2033,C12-1064,1,0.8646,"from geotagged documents (Eisenstein et al., 2010; Ahmed et al., 2013; Cook et al., 2014; Eisenstein, 2015). The main idea is to find lexical variables that are disproportionately distributed in different locations either via model-based or statistical methods (Monroe et al., 2008). There is a research gap in evaluating the geolocation models in terms of their usability in retrieving dialect terms given a geographic region. 3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). These datasets have been used widely for training and evaluation of geolocation models. They are all prepartitioned into training, development and test sets. Each user is represented by the concatenation of their tweets, and labeled with the latitude/longitude of the first collected geotagged tweet in the case of G EOT EXT and T WITTER -US, and the centre of the closest city in the case of T WITTER -W ORLD.1 G EOT EXT and T WITTER -US cover the continental US, and T WITTER -W ORLD covers the whole world, with 9k, 449k and 1.3m users, respectively as shown in Figure 1.2 DAREDS is a dialect-te"
P17-2033,E14-1011,0,0.148399,"Missing"
P17-2033,D10-1124,0,0.789721,"Missing"
P17-2033,W15-1527,0,0.421041,"inates into equalsized grids (Serdyukov et al., 2009), administrative regions (Cheng et al., 2010; Hecht et al., 2011; Kinsella et al., 2011; Han et al., 2012, 2014), or flat (Wing and Baldridge, 2011) or hierarchical k-d tree clusters (Wing and Baldridge, 2014). Network-based methods also use either real-valued coordinates (Jurgens et al., 2015) or discretised regions (Rahimi et al., 2015a) as labels, and use label propagation over the interaction graph (e.g. @-mentions). More recent methods have focused on representation learning by using sparse coding (Cha et al., 2015) or neural networks (Liu and Inkpen, 2015), utilising both text and network information (Rahimi et al., 2015a). Dialect is a variety of language shared by a group of speakers (Wolfram and Schilling, 2015). Our focus here is on geographical dialects which are spoken (and written in social media) by people from particular areas. The traditional approach to dialectology is to find the geographical distribution of known lexical alternatives (e.g. you, yall and yinz: (Labov et al., 2005; Nerbonne et al., 2008; Gonc¸alves and S´anchez, 2014; Doyle, 2014; Huang et al., 2015; Nguyen and Eisenstein, 2016)), the shortcoming of which is that the"
P17-2033,P15-2104,1,0.426469,"term detection methods. 1 Introduction Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, a"
P17-2033,N15-1153,1,0.731334,"term detection methods. 1 Introduction Many services such as web search (Leung et al., 2010), recommender systems (Ho et al., 2012), targeted advertising (Lim and Datta, 2013), and rapid disaster response (Ashktorab et al., 2014) rely on the location of users to personalise information and extract actionable knowledge. Explicit user geolocation metadata (e.g. GPS tags, WiFi footprint, IP address) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, a"
P17-2033,D12-1137,0,0.547879,"ress) is not usually available to third-party consumers, giving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015). Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regio"
P17-2033,P11-1096,0,0.720681,"Missing"
P17-2033,D15-1256,0,0.0516024,"Missing"
P17-2033,D14-1039,0,0.661441,"iving rise to the need for geolocation based on profile data, text content, friendship graphs (Jurgens et al., 2015) or some combination of these (Rahimi et al., 2015b,a). The strong geographical bias, most obviously at the language level (e.g. Finland vs. Japan), and more subtly at the dialect level (e.g. in English used in north-west England vs. north-east USA vs. Texas, USA), clearly reflected in language use in social media services such as Twitter, has been used extensively either for geolocation of users (Eisenstein et al., 2010; Roller et al., 2012; Rout et al., 2013; Han et al., 2014; Wing and Baldridge, 2014) or dialectology (Cook et al., 2014; Eisenstein, 2015). In these methods, a user is often represented by the concatenation of their tweets, and the geolocation model is trained on a very small percentage of explicitly geotagged tweets, noting the potential biases implicit in geotagged tweets (Pavalanathan and Eisenstein, 2015). Lexical dialectology is (in part) the converse of user geolocation (Eisenstein, 2015): given text associated with a variety of regions, the task is to identify terms that are distinctive of particular regions. The complexity of the task is two-fold: (1) localised named"
P18-1181,W15-0705,1,0.794367,"typical rhyming scheme being ABAB CDCD EFEF GG. There are a number of variants, however, mostly seen in the quatrains; e.g. AABB or ABBA are also common. We build our sonnet dataset from the latest image of Project Gutenberg.4 We first create a 3 There are other forms of sonnets, but the Shakespearean sonnet is the dominant one. Hereinafter “sonnet” is used to specifically mean Shakespearean sonnets. 4 https://www.gutenberg.org/. Partition #Sonnets #Words Train Dev Test 2685 335 335 367K 46K 46K Table 1: SONNET dataset statistics. (generic) poetry document collection using the GutenTag tool (Brooke et al., 2015), based on its inbuilt poetry classifier and rule-based structural tagging of individual poems. Given the poems, we use word and character statistics derived from Shakespeare’s 154 sonnets to filter out all non-sonnet poems (to form the “BACKGROUND” dataset), leaving the sonnet corpus (“SONNET”).5 Based on a small-scale manual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high precision. BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets. Statis"
P18-1181,W14-4012,0,0.155437,"Missing"
P18-1181,D10-1051,0,0.80635,"rains with stress and rhyme patterns that are indistinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Asso"
P18-1181,P17-1016,0,0.423851,"istinguishable from human-written poems and rated highly by an expert; • a vanilla language model trained over our sonnet corpus, surprisingly, captures meter implicitly at human-level performance; • while crowd workers rate the poems generated by our best model as nearly indistinguishable from published poems by humans, an expert annotator found the machine-generated poems to lack readability and emotion, and our best model to be only comparable to a vanilla language model on these dimensions; • most work on poetry generation focuses on meter (Greene et al., 2010; Ghazvininejad et al., 2016; Hopkins and Kiela, 2017); our results suggest that future research should look beyond meter and focus on improving readability. In this, we develop a new annotation framework for the evaluation of machine-generated poems, and release both a novel data of sonnets and the full source code associated with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), p"
P18-1181,D14-1181,0,0.00252216,"s and preceding context improves performance substantially, reducing perplexity by almost 10 points from LM to LM∗∗ . The inferior performance of LM∗∗ -C compared to LM∗∗ demonstrates that our approach of processing context with recurrent networks with selective encoding is more effective than convolutional networks. The full model LM∗∗ +PM+RM, which learns stress 20 In Zhang and Lapata (2014), the authors use a series of convolutional networks with a width of 2 words to convert 5/7 poetry lines into a fixed size vector; here we use a standard convolutional network with max-pooling operation (Kim, 2014) to process the context. Pentameter Model To assess the pentameter model, we use the attention weights to predict stress patterns for words in the test data, and compare them against stress patterns in the CMU pronunciation dictionary.21 Words that have no coverage or have nonalternating patterns given by the dictionary are discarded. We use accuracy as the metric, and a predicted stress pattern is judged to be correct if it matches any of the dictionary stress patterns. To extract a stress pattern for a word from the model, we iterate through the pentameter (10 time steps), and append the app"
P18-1181,W09-2005,0,0.105737,"d with this research.2 Noting that there are many notable divergences from this in the work of particular poets (e.g. Walt Whitman) and poetry types (such as free verse or haiku). 2 https://github.com/jhlau/deepspeare 1948 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunc"
P18-1181,E17-2025,0,0.0475363,"), and optimise the model with standard categorical cross-entropy loss. We use dropout as regularisation (Srivastava et al., 2014), and apply it to the encoder/decoder LSTM outputs and word embedding lookup. The same regularisation method is used for the pentameter and rhyme models. As our sonnet data is relatively small for training a neural language model (367K words; see Table 1), we pre-train word embeddings and reduce parameters further by introducing weight-sharing between output matrix Wout and embedding matrix Wwrd via a projection matrix Wprj (Inan et al., 2016; Paulus et al., 2017; Press and Wolf, 2017): the pentameter model learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.11 As punctuation is not pronounced, we preprocess each sonnet line to remove all punctuation, leaving only spaces and letters. Like the language model, the pentameter model is fashioned as an encoder–decoder network. In the encoder, we embed the characters using the shared embedding matrix Wchr and feed them to the shared bidirectional character-level LSTM (Equation (1)) to produce the character encodings for the sentence: uj = [~uj ; u~j ]. In the decoder, it attends to"
P18-1181,P11-2014,0,0.504514,"ncluded in the dictionary are discarded. Rhyme is determined by extracting the final stressed phoneme for the paired words, and testing if their phoneme patterns match. We predict rhyme for a word pair by feeding them to the rhyme model and computing cosine similarity; if a word pair is assigned a score > 0.8,23 it is considered to rhyme. As a baseline (Rhyme-BL), we first extract for each word the last vowel and all following consonants, and predict a word pair as rhyming if their extracted sequences match. The extracted sequence can be interpreted as a proxy for the last syllable of a word. Reddy and Knight (2011) propose an unsupervised model for learning rhyme schemes in poems via EM. There are two latent variables: φ specifies the distribution of rhyme schemes, and θ defines 23 0.8 is empirically found to be the best threshold based on development data. the pairwise rhyme strength between two words. The model’s objective is to maximise poem likelihood over all possible rhyme scheme assignments under the latent variables φ and θ. We train this model (Rhyme-EM) on our data24 and use the learnt θ to decide whether two words rhyme.25 Table 2 details the rhyming results. The rhyme model performs very str"
P18-1181,P17-1099,0,0.0158761,"that it can simply ignore u∗t to predict the alternating stresses based on gt . For this reason we use only u∗t to compute the stress probability: P (S − ) = σ(We u∗t + be ) P which gives the loss Lent = t − log P (St? ) for the whole sequence, where St? is the target stress at time step t. We find the decoder still has the tendency to attend to the same characters, despite the incorporation of position information. To regularise the model further, we introduce two loss penalties: repeat and coverage loss. The repeat loss penalises the model when it attends to previously attended characters (See et al., 2017), and is computed as follows: Lrep = XX t min(fjt , j t−1 X fjt ) t=1 By keeping a sum of attention weights over all previous time steps, we penalise the model when it focuses on characters that have non-zero history weights. The repeat loss discourages the model from focussing on the same characters, but does not assure that the appropriate characters receive attention. Observing that stresses are aligned with the vowels of a syllable, we therefore penalise the model when vowels are ignored: Lcov = X j∈V ReLU(C − 10 X fjt ) t=1 where V is a set of positions containing vowel characters, and C"
P18-1181,D14-1074,0,0.589919,"Association for Computational Linguistics (Long Papers), pages 1948–1958 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Early poetry generation systems were generally rule-based, and based on rhyming/TTS dictionaries and syllable counting (Gerv´as, 2000; Wu et al., 2009; Netzer et al., 2009; Colton et al., 2012; Toivanen et al., 2013). The earliest attempt at using statistical modelling for poetry generation was Greene et al. (2010), based on a language model paired with a stress model. Neural networks have dominated recent research. Zhang and Lapata (2014) use a combination of convolutional and recurrent networks for modelling Chinese poetry, which Wang et al. (2016) later simplified by incorporating an attention mechanism and training at the character level. For English poetry, Ghazvininejad et al. (2016) introduced a finite-state acceptor to explicitly model rhythm in conjunction with a recurrent neural language model for generation. Hopkins and Kiela (2017) improve rhythm modelling with a cascade of weighted state transducers, and demonstrate the use of character-level language model for English poetry. A critical difference over our work is"
P18-1181,P17-1101,0,0.0192447,"a number of variations in addition to the standard pattern (Greene et al., 2010), but our model uses only the standard pattern as it is the dominant one. 1949 (a) Language model (b) Pentameter model (c) Rhyme model Figure 2: Architecture of the language, pentameter and rhyme models. Colours denote shared weights. ter model to sample meter-conforming sentences and the rhyme model to enforce rhyme. The architecture of the joint model is illustrated in Figure 2. We train all the components together by treating each component as a sub-task in a multitask learning setting.8 a selective mechanism (Zhou et al., 2017) to each hi . By defining the representation of the whole context h = [~hC ; h~1 ] (where C is the number of words in the context), the selective mechanism filters the hidden states hi using h as follows: 4.1 where denotes element-wise product. Hereinafter W, U and b are used to refer to model parameters. The intuition behind this procedure is to selectively filter less useful elements from the context words. In the decoder, we embed words xt in the current line using the encoder-shared embedding matrix (Wwrd ) to produce wt . In addition to the word embeddings, we also embed the characters of"
P18-1187,D10-1124,0,0.571092,"Missing"
P18-1187,C12-1064,1,0.905405,"256, and 930 for G EOT EXT, T WITTER -US, and T WITTER -W ORLD, respectively. 3.2 Constructing the Views We build matrix Aˆ as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user. The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples. 3.3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller et al., 2012), and (3) T WITTER -W ORLD (Han et al., 2012). These datasets have been used widely for training and evaluation of geolocation models. They are all pre-partitioned into training, development and test Model Selection For GCN, we use highway layers to control the amount of neighbourhood information passed to a node. We use 3 layers in GCN with size 300, 600, 900 for G EOT EXT, T WITTER -US and T WITTER W ORLD respectively. Note that the final softmax layer is also graph convolutional, which sets the radius of the averaging neighbourhood to 4. The 2011 k-d tree bucket size hyperparameter which controls the maximum number of users in each cl"
P18-1187,W16-3929,0,0.129149,"Missing"
P18-1187,P17-1116,0,0.819088,"ge that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge (2014) Cha et al. (2015) 38 — — 844 — 581 389 — 425 54 48 — 554 686 — 120 191 — 34 31 — 1456 1669 — 415 509 — Net"
P18-1187,D17-1016,1,0.81063,"adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (induc"
P18-1187,P15-2104,1,0.825044,"f GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge"
P18-1187,P17-2033,1,0.801368,"adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (induc"
P18-1187,N15-1153,1,0.864948,"f GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Median↓ Text (inductive) Rahimi et al. (2017b) Wing and Baldridge"
P18-1187,W03-0108,0,0.109296,"Missing"
P18-1187,P11-1096,0,0.674526,"Missing"
P18-1187,D14-1039,0,0.592556,"re 5 when no gates are used. We see that by adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without much change. The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6. 4.4 Performance The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1. The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017). MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important. MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on G EO - 2013 G EOT EXT T WITTER -US T WITTER -W ORLD Acc@161↑ Mean↓ Median↓ Acc@161↑ Mean↓ Med"
P18-1187,W15-3821,0,0.279426,"Missing"
P18-1187,D12-1137,0,0.669846,"nput to another neural network (right), which is trained supervisedly to predict locations. sets. Each user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet in the case of G EOT EXT and T WITTER -US, and the centre of the closest city in the case of T WITTER -W ORLD. G EOT EXT and T WITTER -US cover the continental US, and T WITTER -W ORLD covers the whole world, with 9k, 449k and 1.3m users, respectively. The labels are the discretised geographical coordinates of the training points using a k-d tree following Roller et al. (2012), with the number of labels equal to 129, 256, and 930 for G EOT EXT, T WITTER -US, and T WITTER -W ORLD, respectively. 3.2 Constructing the Views We build matrix Aˆ as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user. The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples. 3.3 Data We use three existing Twitter user geolocation datasets: (1) G EOT EXT (Eisenstein et al., 2010), (2) T WITTER -US (Roller"
P18-2005,P07-1056,0,0.0893091,"Cohn School of Computing and Information Systems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au {tbaldwin,tcohn}@unimelb.edu.au Abstract is rarely fundamental to the task of modelling language, and is better considered as a confounding influence. These auxiliary learning signals can mean the models do not adequately capture the core linguistic problem. In such situations, removing these confounds should give better generalisation, especially for out-of-domain evaluation, a similar motivation to research in domain adaptation based on selection biases over text domains (Blitzer et al., 2007; Daum´e III, 2007). Another related problem is privacy: texts convey information about their author, often inadvertently, and many individuals may wish to keep this information private. Consider the case of the AOL search data leak, in which AOL released detailed search logs of many of their users in August 2006 (Pass et al., 2006). Although they deidentified users in the data, the log itself contained sufficient personally identifiable information that allowed many of these individuals to be identifed (Jones et al., 2007). Other sources of user text, such as emails, SMS messages and social m"
P18-2005,P07-1033,0,0.33862,"Missing"
P18-2005,P15-1073,0,0.071204,"heir background, and personal attributes such as gender, age, education and nationality. This variation can have a substantial effect on NLP models learned from text (Hovy et al., 2015), leading to significant variation in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however,"
P18-2005,E17-1101,0,0.0234177,"on in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however, especially if an adversary has preliminary knowledge of the training model, in which case they can readily reverse engineer the input, for example, by a GAN attack algorithm (Hitaj et al., 2017). This is true even when differentia"
P18-2005,P15-2079,0,0.129384,"ate, training is based on standard gradient backpropagation for learning the main task, but for the auxiliary task, we start with standard loss backpropagation, however gradients are reversed in sign when they reach h. Consequently the linear output components are trained to be good predictors, but h is trained to be maximally good for the main task and maximally poor for the auxiliary task. Furthermore, Equation 1 can be expanded to scenarios with several (N ) protected attributes, θˆ = min max θM {θ i }N D i=1 X (ˆ y(x; θM ), y) − N  X Data We use the TrustPilot English POS tagged dataset (Hovy and Søgaard, 2015), which consists of 600 sentences, each labelled with both the sex and age of the author, and manually POS tagged based on the Google Universal POS tagset (Petrov et al., 2012). For the purposes of this paper, we follow Hovy and Søgaard’s setup, categorising SEX into female (F) and male (M), and AGE into over-45 (O45) and under-35 (U35). We train the taggers both with and without the adversarial loss, denoted ADV and BASELINE, respectively. For evaluation, we perform a 10-fold cross validation, with a train:dev:test split using ratios of 8:1:1. We also follow the evaluation method in Hovy and"
P18-2005,D17-1323,0,0.0903437,"t al., 2015), leading to significant variation in inferences across different types of corpora, such as the author’s native language, gender and age. Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population (Hovy, 2015). Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (Zhao et al., 2017), or translation choices (Rabinovich et al., 2017). This signal, however, 25 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 25–30 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics processing, such as grammar correction or translation. The transfered representations may still contain sensitive information, however, especially if an adversary has preliminary knowledge of the training model, in which case they can readily reverse engineer the input, for example, by a GAN attack algorithm (Hitaj"
P18-2005,N16-1130,0,0.045199,"ovy and Søgaard (2015), by reporting the tagging accuracy for sentences over different slices of the data based on SEX and AGE, and the absolute difference between the two settings. Considering the tiny quantity of text in the TrustPilot corpus, we use the Web English Treebank (WebEng: Bies et al. (2012)), as a means of pre-training the tagging model. WebEng was chosen to be as similar as possible in domain to the TrustPilot data, in that the corpus includes unedited user generated internet content. As a second evaluation set, we use a corpus of African-American Vernacular English (AAVE) from Jørgensen et al. (2016), which is used purely for held-out evaluation. AAVE consists of three very heterogeneous domains: LYRICS, SUBTI TLES and TWEETS . Considering the substantial (2)  λi · X (ˆb(x; θDi ), bi ) . i=1 3 Experiments In this section, we report experimental results for our methods with two very different language tasks. 3.1 POS-tagging This first task is part-of-speech (POS) tagging, framed as a sequence tagging problem. Recent demographic studies have found that the author’s gender, age and race can influence tagger performance (Hovy and Søgaard, 2015; Jørgensen et al., 2016). Therefore, we use the"
P18-2005,D14-1181,0,0.0109704,"Missing"
P18-2005,P12-3005,1,0.75166,"s demonstrate that our model can learn relatively gender and age debiased representations, while simultaneously improving the predictive performance, both for indomain and out-of-domain evaluation scenarios. 3.2 LYRICS Data We again use the TrustPilot dataset derived from Hovy et al. (2015), however now we consider the RATING score as the target variable, not POS-tag. Each review is associated with three further attributes: gender (SEX), age (AGE), and location (LOC). To ensure that LOC cannot be trivially predicted based on the script, we discard all non-English reviews based on LANGID . PY (Lui and Baldwin, 2012), by retaining only reviews classified as English with a confidence greater than 0.9. We then subsample 10k reviews for each location to balance the five location classes (US, UK, Germany, Denmark, and France), which were highly skewed in the original dataset. We use the same binary representation of SEX and AGE as the POS task, following the setup in Hovy et al. (2015). To evaluate the different models, we perform 10-fold cross validation and report test performance in terms of the F1 score for the RATING task, and the accuracy of each discriminator. Note that the discriminator can be applied"
P18-2005,petrov-etal-2012-universal,0,0.0253308,"are reversed in sign when they reach h. Consequently the linear output components are trained to be good predictors, but h is trained to be maximally good for the main task and maximally poor for the auxiliary task. Furthermore, Equation 1 can be expanded to scenarios with several (N ) protected attributes, θˆ = min max θM {θ i }N D i=1 X (ˆ y(x; θM ), y) − N  X Data We use the TrustPilot English POS tagged dataset (Hovy and Søgaard, 2015), which consists of 600 sentences, each labelled with both the sex and age of the author, and manually POS tagged based on the Google Universal POS tagset (Petrov et al., 2012). For the purposes of this paper, we follow Hovy and Søgaard’s setup, categorising SEX into female (F) and male (M), and AGE into over-45 (O45) and under-35 (U35). We train the taggers both with and without the adversarial loss, denoted ADV and BASELINE, respectively. For evaluation, we perform a 10-fold cross validation, with a train:dev:test split using ratios of 8:1:1. We also follow the evaluation method in Hovy and Søgaard (2015), by reporting the tagging accuracy for sentences over different slices of the data based on SEX and AGE, and the absolute difference between the two settings. Co"
P18-2030,P15-2030,1,0.933608,"representatives directly. Our objective is to predict the popularity of a petition at the end of its lifetime, solely based on the petition text. Elnoshokaty et al. (2016) is the closest work to this paper, whereby they target Change.org petitions and perform correlation analysis of popularity with the petition’s category, target goal set,2 and the distribution of words in General Inquirer categories (Stone et al., 1962). In our case, we are interested in the task of automatically predicting the number of signatures. We build on the convolutional neural network (CNN) text regression model of Bitvai and Cohn (2015) to infer deep latent features. In addition, we evaluate the effect of an auxiliary ordinal regression objective, which can discriminate petitions that attract different scales of popularOnline petitions are a cost-effective way for citizens to collectively engage with policy-makers in a democracy. Predicting the popularity of a petition — commonly measured by its signature count — based on its textual content has utility for policymakers as well as those posting the petition. In this work, we model this task using CNN regression with an auxiliary ordinal regression objective. We demonstrate t"
P18-2030,D14-1162,0,0.0857336,"signature count. An outline of the model is provided in Figure 1. A petition has three parts: (1) title, (2) main content, and (3) (optionally) additional details.3 We concatenate all three parts to form a single document for each petition. We have n petitions as input training examples of the form {ai , yi }, where ai and yi denote the text and signature count of petition i, respectively. Note that we log-transform the signature count, consistent with previous work (Elnoshokaty et al., 2016; Proskurnia et al., 2017). We represent each token in the document via its pretrained GloVe embedding (Pennington et al., 2014), which we update during learning. We then apply multiple convolution filters with width one, two and three to the dense input document matrix, and apply a ReLU to each. They are then passed through a max-pooling layer with a tanh activation function, and finally a multi-layer perceptron via the exponential linear unit activation, ( x, if x > 0 f (x) = α (exp(x) − 1) otherwise , that achieve different scale of signatures. The intuition behind this is that there are pre-determined thresholds on signatures which trigger different events, with the most important of these being 10k (to guarantee a"
P18-2030,D17-1070,0,0.020889,"General Inquirer lexicon. • Ratio of biased words (B IAS) from the bias lexicon (Recasens et al., 2013). • Syntactic features: number of nouns (NNC), verbs (VBC), adjectives (ADC) and adverbs (RBC). • Number of named entities (NEC), based on the NLTK NER model (Bird et al., 2009). • Freshness (F RE): cosine similarity with all previous petitions, inverse weighted by the difference in start date of petitions (in weeks). • Action score of title (ACT): probability of title conveying the action requested. Predictions are obtained using an one-class SVM model built on the universal representation (Conneau et al., 2017) of titles of rejected petitions,5 as they don’t contain any action request. These rejected petitions are not part of our evaluation dataset. • Policy category popularity score (C SC): commonality of the petition’s policy issue (Subra20 15 10 0 5 6 7 8 9 10 11 12 Log Signature Count 13 Figure 3: US Petitions Signature Distribution manian et al., 2017), based on the recent UK/US election manifesto promises. • Political bias and polarity: relative lean#left+#right ing/polarity based on: (a) #left+#right+#neutral (P BIAS) (b) #left−#right #left+#right (L–R). Sentence-level left, right and neutral"
P18-2030,P13-1162,0,0.0257489,"brief description of the features below: • Additional Information (A DD): binary flag indicating whether the petition has additional details or not. • Ratio of indefinite (I ND) and definite (D EF) articles. • Ratio of first-person singular pronouns (F SP), first-person plural pronouns (F PP), secondperson pronouns (S PP), third-person singular pronouns (T SP), and third-person plural pronouns (T PP). • Ratio of subjective words (S UBJ) and difference between count of positive and negative words (P OL), based on General Inquirer lexicon. • Ratio of biased words (B IAS) from the bias lexicon (Recasens et al., 2013). • Syntactic features: number of nouns (NNC), verbs (VBC), adjectives (ADC) and adverbs (RBC). • Number of named entities (NEC), based on the NLTK NER model (Bird et al., 2009). • Freshness (F RE): cosine similarity with all previous petitions, inverse weighted by the difference in start date of petitions (in weeks). • Action score of title (ACT): probability of title conveying the action requested. Predictions are obtained using an one-class SVM model built on the universal representation (Conneau et al., 2017) of titles of rejected petitions,5 as they don’t contain any action request. These"
P18-2030,U17-1003,1,0.891779,"Missing"
P18-2030,P14-1017,0,0.148678,"1 The code and data from this paper are available from http://github.com/shivashankarrs/ Petitions 2 See http://bit.ly/2BXd0Sl. 182 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 182–188 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Convolution filters with ReLu activation ity (e.g., 10 signatures, the minimum count needed to not be closed vs. 10k signatures, the minimum count to receive a response from UK government). Finally, motivated by text-based message propagation analysis work (Tan et al., 2014; Piotrkowicz et al., 2017), we hand-engineer features which capture wording effects on petition popularity, and measure the ability of the deep model to automatically infer those features. Word embeddings Hold a referendum on Brexit deal ?? . . Max-pooling 2 Fully Connected Layers > ??1 > ??2 > ???? Custom Features Proposed Approach Figure 1: CNN-Regression Model. y denotes signature count. > rk is the auxiliary task that denotes p(petition attracting > rk signatures). Inspired by the successes of CNN for text categorization (Kim, 2014) and text regression (Bitvai and Cohn, 2015), we propose"
P18-2030,D14-1181,0,0.00392836,"d by text-based message propagation analysis work (Tan et al., 2014; Piotrkowicz et al., 2017), we hand-engineer features which capture wording effects on petition popularity, and measure the ability of the deep model to automatically infer those features. Word embeddings Hold a referendum on Brexit deal ?? . . Max-pooling 2 Fully Connected Layers > ??1 > ??2 > ???? Custom Features Proposed Approach Figure 1: CNN-Regression Model. y denotes signature count. > rk is the auxiliary task that denotes p(petition attracting > rk signatures). Inspired by the successes of CNN for text categorization (Kim, 2014) and text regression (Bitvai and Cohn, 2015), we propose a CNN-based model for predicting the signature count. An outline of the model is provided in Figure 1. A petition has three parts: (1) title, (2) main content, and (3) (optionally) additional details.3 We concatenate all three parts to form a single document for each petition. We have n petitions as input training examples of the form {ai , yi }, where ai and yi denote the text and signature count of petition i, respectively. Note that we log-transform the signature count, consistent with previous work (Elnoshokaty et al., 2016; Proskurn"
P18-2045,W17-0908,0,0.0287999,"Missing"
P18-2045,P14-5010,0,0.00454304,"ect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respective"
P18-2045,D17-1168,0,0.413973,"erent Ending: Sam was happy. Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art. 1 1 Figure 1: Story Cloze Test example. The current state-of-the-art approach of Chaturvedi et al. (2017) is based on understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial in"
P18-2045,D14-1082,0,0.00766782,"e lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respectively, y is the gold label f"
P18-2045,W17-0913,0,0.06733,"he necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehensio"
P18-2045,N10-1138,0,0.0149388,"upervise the memory update gates of these chains, we design three sequences of binary labels: lj = {l1j , l2j , . . . , lTj } for j ∈ [1, 3] representing event, sentiment, and topic, and lij ∈ {0, 1}. The label at time i for the j-th aspect is only assigned a value of 1 if the word is a trigger for that particular aspect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss o"
P18-2045,N16-1098,0,0.31157,"he classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpor"
P18-2045,W17-0906,0,0.06849,"ves, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehension, we introduce a novel multi-task learning objec"
P18-2045,P16-1028,0,0.0303386,"Missing"
P18-2045,K17-1004,0,0.339321,"e (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstr"
P18-2045,H89-1033,0,0.701499,"understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al.,"
P19-1186,N10-1027,1,0.726506,"aining, a single epoch of training took about 25min for the CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and o"
P19-1186,P07-1056,0,0.876772,"a subset of training inputs. We show that our model leads to substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning. 1 Introduction Text corpora are often collated from several different sources, such as news, literature, microblogs, and web crawls, raising the problem of learning NLP systems from heterogenous data, and how well such models transfer to testing settings. Learning from these corpora requires models which can generalise to different domains, a problem known as transfer learning or domain adaptation (Blitzer et al., 2007; Daum´e III, 2007; Joshi et al., 2012; Kim et al., 2016). In most stateof-the-art frameworks, the model has full knowledge of the domain of instances in the training data, and the domain is treated as a discrete indicator variable. However, in reality, data is often messy, with domain labels not always available, or providing limited information about the style and genre of text. For example, web-crawled corpora are comprised of all manner of text, such as news, marketing, blogs, novels, and recipes, however the type of each document is typically not explicitly specified. Moreover, even corpo"
P19-1186,K16-1002,0,0.0229715,"thods have been employed to learn robust domain-generalised representations (Liu et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on"
P19-1186,P07-1033,0,0.273361,"Missing"
P19-1186,D12-1119,0,0.0520117,"Missing"
P19-1186,D14-1181,0,0.0057556,"Missing"
P19-1186,2005.mtsummit-papers.11,0,0.0532348,"CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and one state-of-theart off-the-shelf LangID tool: LANG"
P19-1186,N18-2076,1,0.460956,"Missing"
P19-1186,D16-1012,0,0.0330065,"Missing"
P19-1186,I11-1062,1,0.806149,"ntial sentiment, and some domain knowledge, as observed in Figure 3. In terms of the time required for training, a single epoch of training took about 25min for the CSDA method, using the default settings, and a similar time for DSDA and M - CNN. The runtime increases sub-linearly with increasing latent size k. 3.3 Language Identification To further demonstrate our approaches, we then evaluate our models with the second task, language identification (LangID: Jauhiainen et al. (2018)). For data processing, we use 5 training sets from 5 different domains with 97 language, following the setup of Lui and Baldwin (2011). We evaluate accuracy over 7 holdout benchmarks: E URO G OV, TCL, W IKIPEDIA from Baldwin and Lui (2010), EMEA (Tiedemann, 2009), E URO PARL (Koehn, 2005), TBE (Tromp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks"
P19-1186,P12-3005,1,0.786528,"omp and Pechenizkiy, 2011) and TSC (Carter et al., 2013). Differently from sentiment tasks, here, we evaluate our methods using the full dataset, but with two configurations: (1) domain unsupervised, where all instance have only labels but no domain (denoted Y); and (2) domain supervised learning, where all instances have labels and domain (F). 3.3.1 Results Table 4 shows the performance of different models over 7 holdout benchmarks and the averaged scores. We also report the results of GEN, the best model from Li et al. (2018a), and one state-of-theart off-the-shelf LangID tool: LANGID . PY (Lui and Baldwin, 2012). Note that, both S - CNN and M - CNN are domain unsupervised methods. In terms of results, overall, both of our CSDA models consistently outperform all other baseline models. Comparing the different CSDA variants, Beta vs. Dirichlet, both perform closely across the LangID tasks. Furthermore, CSDA out-performs the state-of-the-art in terms of average scores. Interestingly the two training configurations show that domain knowledge F provides a small performance boost for CSDA, but not does help for DSDA . Above all, the LangID results confirm the effectiveness of our proposed approaches. 4 Rela"
P19-1186,C16-1038,0,0.416007,"substantial performance improvements over competitive benchmark domain adaptation methods, including methods using adversarial learning. 1 Introduction Text corpora are often collated from several different sources, such as news, literature, microblogs, and web crawls, raising the problem of learning NLP systems from heterogenous data, and how well such models transfer to testing settings. Learning from these corpora requires models which can generalise to different domains, a problem known as transfer learning or domain adaptation (Blitzer et al., 2007; Daum´e III, 2007; Joshi et al., 2012; Kim et al., 2016). In most stateof-the-art frameworks, the model has full knowledge of the domain of instances in the training data, and the domain is treated as a discrete indicator variable. However, in reality, data is often messy, with domain labels not always available, or providing limited information about the style and genre of text. For example, web-crawled corpora are comprised of all manner of text, such as news, marketing, blogs, novels, and recipes, however the type of each document is typically not explicitly specified. Moreover, even corpora that are labelled with a specific domain might themsel"
P19-1186,D16-1050,0,0.029098,"et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on the variational autoencoder. In empirical evaluation over a multi-domain sentimen"
P19-1186,P17-1029,0,0.0180063,"d representations (Liu et al., 2016). Li et al. (2018a) considered the case of the model having no access to the target domain, and using adversarial learning to generate domaingeneration representations by cross-comparison between source domains. The other important component of this work is Variational Inference (“VI”), a method from machine learning that approximates probability densities through optimisation (Blei et al., 2017; Kucukelbir et al., 2017). The idea of a variational auto-encoder has been applied to language generation (Bowman et al., 2016; Kim et al., 2018; Miao et al., 2017; Zhou and Neubig, 2017; Zhang et al., 2016) and machine translation (Shah and Barber, 2018; Eikema and Aziz, 2018), but not in the context of semi-supervised domain adaptation. 5 Conclusion In this paper, we have proposed two models— DSDA and CSDA —for multi-domain learning, which use a graphical model with a latent variable to represent the domain. We propose models with a discrete latent variable, and a continuous vectorvalued latent variable, which we model with Beta or Dirichlet priors. For training, we adopt a variational inference technique based on the variational autoencoder. In empirical evaluation over a"
P19-1269,W05-0909,0,0.51463,"neni et al., 2002) and TER (Snover et al., 2006) use n-gram matching or more explicit word alignment to match the system output with the reference translation. Character-level variants such as BEER, CHR F and C HARAC T ER overcome the problem of harshly penalising morphological variants, and perform surprisingly well despite their simplicity (Stanojevic and Sima’an, 2014; Popovi´c, 2015; Wang et al., 2016). In order to allow for variation in word choice and sentence structure, other metrics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More r"
P19-1269,W15-3001,0,0.0549887,"f WMT 2016, which is entirely crowdsourced. The sentence-level-metric evaluation data consists of accurate scores for 560 translations each for 6 to-English language pairs and English-to-Russian (we call this the “TrainS” dataset). The dataset also includes mostly singly-annotated2 DA scores for around 125 thousand translations from six source languages into English, and 12.5 thousand translations from English-to-Russian (“TrainL” dataset), that were collected to obtain human scores for MT systems. For the validation set, we use the sentencelevel DA judgements collected for the WMT 2015 data (Bojar et al., 2015): 500 translation-reference pairs each of four to-English language pairs, and English-to-Russian. For more details on implementation and training of our models, see Appendix A. We test our metrics on all language pairs from the WMT 2017(Bojar et al., 2017b) news task in both the sentence and system level setting, and evaluate using Pearson’s correlation between our metrics’ predictions and the Human DA scores. For the sentence level evaluation, insufficient DA annotations were collected for five fromEnglish language pairs, and these were converted to preference judgements. If two MT system tra"
P19-1269,W07-0738,0,0.0858882,"Missing"
P19-1269,D14-1020,1,0.882017,"h ρ Baselines S ENT-BLEU CHR F BEER MEANT 2.0- NOSRL MEANT 2.0 0.274 0.376 0.398 0.395 – 0.269 0.336 0.336 0.324 – 0.446 0.503 0.557 0.565 – 0.259 0.420 0.420 0.425 – 0.468 0.605 0.569 0.636 – 0.377 0.466 0.490 0.482 – 0.642 0.608 0.622 0.705 0.727 T P Table 1: Pearson’s r on the WMT 2017 sentence-level evaluation data. P: Unsupervised metric that relies on pretrained embeddings; TrainS: trained on accurate 3360 instances; TrainL: trained on noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William’s test; Graham and Baldwin, 2014) BERT R ESIM 0.390 0.338 0.365 0.362 0.564 0.523 0.417 0.350 0.630 0.700 0.457 0.506 0.803 0.699 Table 2: Pearson’s r and Kendall’s τ on the WMT 2017 from-English system-level evaluation data. The first section represents existing metrics, both trained and untrained. We then present results of our unsupervised metric, followed by our supervised metric trained in the TrainL setting: noisy 125k instances. Correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold (William’s test (Graham and Baldwin, 2014) for Pearson’s r and Bootstrap (Efro"
P19-1269,N15-1124,1,0.879144,"led and max-pooled hidden states of this BiLSTM. To compute the final predicted score, we apply a feedforward regressor over the concatenation of the two sentence representations. p = [vr,avg ; vr,max ; vt,avg ; vt,max ] (12) 0 (13) | | y = w ReLU(W p + b) + b For all models, the predicted score of an MT system is the average predicted score of all its translations in the testset. 4 Experimental Setup We use human evaluation data from the Conference on Machine Translation (WMT) to train and evaluate our models (Bojar et al., 2016, 2017a), which is based on the Direct Assessment (“DA”) method (Graham et al., 2015, 2017). Here, system translations are evaluated by humans in comparison to a human reference translation, using a continuous scale (Graham et al., 2015, 2017). Each annotator assesses a set of 100 items, of which 30 items are for quality control, which is used to filter out annotators who are unskilled or careless. Individual worker scores are first standardised, and then the final score of an MT system is computed as the average score across all translations in the test set. Manual MT evaluation is subjective and difficult, and it is not possible even for a diligent human to be entirely cons"
P19-1269,D15-1124,0,0.248337,"Missing"
P19-1269,P14-1065,0,0.0512274,"Missing"
P19-1269,W17-4755,0,0.274311,"s mostly singly-annotated2 DA scores for around 125 thousand translations from six source languages into English, and 12.5 thousand translations from English-to-Russian (“TrainL” dataset), that were collected to obtain human scores for MT systems. For the validation set, we use the sentencelevel DA judgements collected for the WMT 2015 data (Bojar et al., 2015): 500 translation-reference pairs each of four to-English language pairs, and English-to-Russian. For more details on implementation and training of our models, see Appendix A. We test our metrics on all language pairs from the WMT 2017(Bojar et al., 2017b) news task in both the sentence and system level setting, and evaluate using Pearson’s correlation between our metrics’ predictions and the Human DA scores. For the sentence level evaluation, insufficient DA annotations were collected for five fromEnglish language pairs, and these were converted to preference judgements. If two MT system translations of a source sentence were evaluated by at least two reliable annotators, and the average score for System A is reasonably greater than the average score of System B, then this is interpreted as a Relative Ranking (DA RR) judgement where Sys A is"
P19-1269,W10-1754,0,0.0340102,") use n-gram matching or more explicit word alignment to match the system output with the reference translation. Character-level variants such as BEER, CHR F and C HARAC T ER overcome the problem of harshly penalising morphological variants, and perform surprisingly well despite their simplicity (Stanojevic and Sima’an, 2014; Popovi´c, 2015; Wang et al., 2016). In order to allow for variation in word choice and sentence structure, other metrics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo,"
P19-1269,W12-3103,0,0.0254921,"ics use information from shallow linguistic tools such as POStaggers, lemmatizers and synonym dictionaries (Banerjee and Lavie, 2005; Snover et al., 2006; Liu et al., 2010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo, 2017) have adopted word embeddings (Mikolov et al., 2013) to capture the semantics of individual words. However, classic word embeddings are independent of word context, and context is captured instead using hand-crafted features or heuristics. Neural metrics such as ReVal and RUSE solve this problem by directly learning embeddings of the entire translation and reference sentences. ReVal (Gupta et al., 2015) learns sentence representations of the MT o"
P19-1269,W17-4767,0,0.130085,"010), or deeper linguistic informa1 code is available at https://github.com/nitikam/mtevalin-context 2799 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2799–2808 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion such as semantic roles, dependency relationships, syntactic constituents, and discourse roles (Gim´enez and M`arquez, 2007; Castillo and Estrella, 2012; Guzm´an et al., 2014). On the flip side, it is likely that these are too permissive of mistakes. More recently, metrics such as MEANT 2.0 (Lo, 2017) have adopted word embeddings (Mikolov et al., 2013) to capture the semantics of individual words. However, classic word embeddings are independent of word context, and context is captured instead using hand-crafted features or heuristics. Neural metrics such as ReVal and RUSE solve this problem by directly learning embeddings of the entire translation and reference sentences. ReVal (Gupta et al., 2015) learns sentence representations of the MT output and reference translation as a Tree-LSTM, and then models their interactions using the element-wise difference and angle between the two. RUSE ("
P19-1269,P17-1152,0,0.223068,"rd embeddings (Peters et al., 2018; Devlin et al., 2019), a technique which captures rich and portable representations of words in context, which have been shown to provide important signal to many other NLP tasks (Rajpurkar et al., 2018). We propose a simple untrained model that uses off-the{tbaldwin,tcohn}@unimelb.edu.au shelf contextual embeddings to compute approximate recall, when comparing a reference to an automatic translation, as well as trained models, including: a recurrent model over reference and translation sequences, incorporating attention; and the adaptation of an NLI method (Chen et al., 2017) to MT evaluation. These approaches, though simple in formulation, are highly effective, and rival or surpass the best approaches from WMT 2017. Moreover, we show further improvements in performance when our trained models are learned using noisy crowd-sourced data, i.e., having single annotations for more instances is better than collecting and aggregating multiple annotations for single instances. The net result is an approach that is more data efficient than existing methods, while producing substantially better human correlations.1 2 Related work MT metrics attempt to automatically predict"
Q14-1003,P10-1010,0,0.0108201,"nd estimates the proportion of the document that is written in each language. Detecting multilingual documents has a variety of applications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the"
Q14-1003,D07-1016,0,0.0480532,"Missing"
Q14-1003,N10-1027,1,0.505424,") we show that our method is able to identify multilingual documents in real-world data. 2 Background Most language identification research focuses on language identification for monolingual documents (Hughes et al., 2006). In monolingual LangID, the task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters (Cavnar and Trenkle, 1994; Kikui, 1996), often approximated by the corresponding stream of bytes (Kruengkrai et al., 2005; Baldwin and Lui, 2010a) for robustness over variable character encodings. In this work, we follow Baldwin and Lui (2010a) in training a single model for languages that naturally use multiple encodings 28 (e.g. UTF8, Big5 and GB encodings for Chinese), as issues of encoding are"
Q14-1003,W12-2108,0,0.145694,"including measures based on rank order statistics (Cavnar and Trenkle, 1994), information theory (Baldwin and Lui, 2010a), string kernels (Kruengkrai et al., 2005) and vector space models (Prager, 1999a; McNamee, 2005). Language identification has been applied in domains such as USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), web search queries (Ceylan and Kim, 2009; Bosca and Dini, 2010), mining the web for bilingual text (Resnik, 1999; Nie et al., 1999), building minority language corpora (Ghani et al., 2004; Scannell, 2007; Bergsma et al., 2012) as well as a largescale database of Interlinear Glossed Text (Xia et al., 2010), and the construction of a large-scale multilingual web crawl (Callan and Hoy, 2009). 2.1 Multilingual Documents Language identification over documents that contain text from more than one language has been identified as an open research question (Hughes et al., 2006). Common examples of multilingual documents are web pages that contain excerpts from another language, and documents from multilingual organizations such as the European Union. character byte English the French pour Italian di German auf Dutch voo Jap"
Q14-1003,P09-1120,0,0.016262,"nstette, 1995; Lui and Baldwin, 2011; Tiedemann and Ljubeˇsi´c, 2012), and compressive models (Teahan, 2000). The nearest-prototype methods vary primarily in the distance measure used, including measures based on rank order statistics (Cavnar and Trenkle, 1994), information theory (Baldwin and Lui, 2010a), string kernels (Kruengkrai et al., 2005) and vector space models (Prager, 1999a; McNamee, 2005). Language identification has been applied in domains such as USENET messages (Cavnar and Trenkle, 1994), web pages (Kikui, 1996; Martins and Silva, 2005; Liu and Liang, 2008), web search queries (Ceylan and Kim, 2009; Bosca and Dini, 2010), mining the web for bilingual text (Resnik, 1999; Nie et al., 1999), building minority language corpora (Ghani et al., 2004; Scannell, 2007; Bergsma et al., 2012) as well as a largescale database of Interlinear Glossed Text (Xia et al., 2010), and the construction of a large-scale multilingual web crawl (Callan and Hoy, 2009). 2.1 Multilingual Documents Language identification over documents that contain text from more than one language has been identified as an open research question (Hughes et al., 2006). Common examples of multilingual documents are web pages that co"
Q14-1003,U12-1014,1,0.72294,"move this monolingual assumption, and address the problem of language identification in documents that may contain text from more than one language from the candidate set. We propose a method that concurrently detects that a document is multilingual, and estimates the proportion of the document that is written in each language. Detecting multilingual documents has a variety of applications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such a"
Q14-1003,hughes-etal-2006-reconsidering,1,0.689156,"nt a method for identifying multilingual documents, the languages contained therein and the relative proportion of the document in each language; (2) we show that our method outperforms state-of-the-art methods for language identification in multilingual documents; (3) we show that our method is able to estimate the proportion of the document in each language to a high degree of accuracy; and (4) we show that our method is able to identify multilingual documents in real-world data. 2 Background Most language identification research focuses on language identification for monolingual documents (Hughes et al., 2006). In monolingual LangID, the task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters ("
Q14-1003,C96-2110,0,0.21726,"he task is to assign each document D a unique language Li ∈ L. Some work has reported near-perfect accuracy for language identification of large documents in a small number of languages (Cavnar and Trenkle, 1994; McNamee, 2005). However, in order to attain such accuracy, a large number of simplifying assumptions have to be made (Hughes et al., 2006; Baldwin and Lui, 2010a). In this work, we tackle the assumption that each document is monolingual, i.e. it contains text from a single language. In language identification, documents are modeled as a stream of characters (Cavnar and Trenkle, 1994; Kikui, 1996), often approximated by the corresponding stream of bytes (Kruengkrai et al., 2005; Baldwin and Lui, 2010a) for robustness over variable character encodings. In this work, we follow Baldwin and Lui (2010a) in training a single model for languages that naturally use multiple encodings 28 (e.g. UTF8, Big5 and GB encodings for Chinese), as issues of encoding are not the focus of this research. The document representation used for language identification generally involves estimating the relative distributions of particular byte sequences, selected such that their distributions differ between lang"
Q14-1003,N13-1131,0,0.474412,"ts can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that each document is genera"
Q14-1003,P13-1018,0,0.0260113,"ge processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the pro"
Q14-1003,I11-1062,1,0.71337,"h as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that each document is generated as samples from an unknown mixture of languages from the training set. We introduce a Gibbs sampler to map samples to languages for any given set of languages, and use this to select the set of languages that maximizes the posterior probability of the document. 27 Transactions of the Association for Computational Linguistics, 2 (2014) 27–40. Action Editor: Kristina Toutanova. c Submitted 1/2013; Revised 7/2013; Published 2/2014. 2014 Association for Computational Linguistics. Our method is able to learn a language identifier for multilingual d"
Q14-1003,D09-1026,0,0.0510119,"uish between languages. The exact set of features is selected from the training data using Information Gain (IG), an information-theoretic metric developed as a splitting criterion for decision trees (Quinlan, 1993). IGbased feature selection combined with a naive Bayes classifier has been shown to be particularly effective for language identification (Lui and Baldwin, 2011). 3.2 Generative Mixture Models Generative mixture models are popular for text modeling tasks where a mixture of influences governs the content of a document, such as in multi-label document classification (McCallum, 1999; Ramage et al., 2009), and topic modeling (Blei et al., 2003). Such models normally assume full exchangeability between tokens (i.e. the bag-of-words assumption), and label each token with a single discrete label. Multi-label text classification, topic modeling and our model for language identification in multilingual documents share the same fundamental representation of the latent structure of a document. Each label is modeled with a probability distribution over tokens, and each document is modeled as a probabilistic mixture of labels. As presented in Griffiths and Steyvers (2004), the probability of the ith to"
Q14-1003,P99-1068,0,0.27484,"pplications. Most natural language processing techniques presuppose monolingual input data, so inclusion of data in foreign languages introduces noise, and can degrade the performance of NLP systems (Alex et al., 2007; Cook and Lui, 2012). Automatic detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each langua"
Q14-1003,C12-1160,0,0.189363,"Missing"
Q14-1003,P12-1102,0,0.696165,"detection of multilingual documents can be used as a pre-filtering step to improve the quality of input data. Detecting multilingual documents is also important for acquiring linguistic data from the web (Scannell, 2007; Abney and Bird, 2010), and has applications in mining bilingual texts for statistical machine translation from online resources (Resnik, 1999; Nie et al., 1999; Ling et al., 2013). There has been particular interest in extracting text resources for low-density languages from multilingual web pages containing both the low-density language and another language such as English (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013). King and Abney (2013, p1118) specifically mention the need for an automatic method “to examine a multilingual document, and with high accuracy, list the languages that are present in the document”. We introduce a method that is able to detect multilingual documents, and simultaneously identify each language present as well as estimate the proportion of the document written in that language. We achieve this with a probabilistic mixture model, using a document representation developed for monolingual language identification (Lui and Baldwin, 2011). The model posits that"
Q14-1003,C00-2137,0,0.031814,"rt both the documentlevel micro-average, as well as the language-level macro-average. For consistency with Baldwin and Lui (2010a), the macro-averaged F-score we report is the average of the per-class F-scores, rather than the harmonic mean of the macro-averaged precision and recall; as such, it is possible for the F-score to not fall between the precision and recall values. As is common practice, we compute the F-score for β = 1, giving equal importance to precision and recall.2 We tested the difference in performance for statistical significance using an approximate randomization procedure (Yeh, 2000) with 10000 iterations. Within each table of results (Tables 2, 3 and 2 Intuitively, it may seem that the maximal precision and recall should be achieved when precision and recall are balanced. However, because of the multi-label nature of the task and variable number of labels assigned to a given document by our models, it is theoretically possible and indeed common in our results for the maximal macro-averaged F-score to be achieved when macro-averaged precision and recall are not balanced. 33 4), all differences between systems are statistically significant at a p &lt; 0.05 level. To evaluate"
Q14-1003,U10-1003,1,\N,Missing
Q17-1032,W13-2408,0,0.0630567,"Missing"
Q17-1032,W11-0822,0,0.0571938,"Missing"
Q17-1032,C14-1071,1,0.831411,"exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold. The algorithm of da Silva and Lopes (1999) is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (Ramisch et al., 2012). Brooke et al. (2014; 2015) developed a heuristic method intended for general FS extraction in larger corpora, first using conditional probabilities to do an initial (single pass) coarse-grained segmentation of the corpus, followed by a pass through the resulting vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. The work of Newman et al. (2012) is an example of an unsupervised approach which does not use association measures: it extends the Bayesian word segmentation approach of Goldwater et al. (2009) to multiword tokenization, applying a generat"
Q17-1032,W15-0915,1,0.0565729,"ample PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones. Mutual expectation (Dias et al., 1999) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of Brooke et al. (2015) is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (Riedl and Biemann, 2015) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure a"
Q17-1032,J90-1003,0,0.392984,"semantics are accessible by analogy (e.g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequen"
Q17-1032,P16-1016,0,0.0130188,"014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen and Widdows, 2009), as can parsing (Constant and Nivre, 2016) and topic models (Lau et al., 2013). One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency threshold. Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., u"
Q17-1032,J93-1003,0,0.165092,"e by analogy (e.g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log l"
Q17-1032,J09-1005,0,0.0191603,"vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. The work of Newman et al. (2012) is an example of an unsupervised approach which does not use association measures: it extends the Bayesian word segmentation approach of Goldwater et al. (2009) to multiword tokenization, applying a generative Dirichlet Process model which jointly constructs a segmentation of the corpus and a corresponding multiword vocabulary. Other research in MWEs has tended to be rather focused on particular syntactic patterns such as verbnoun combinations (Fazly et al., 2009). The system of Schneider et al. (2014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen a"
Q17-1032,D08-1104,0,0.0313431,"rge, unlabeled corpus of text. Since our primary association measure is an adaption of LPR, our approach in this section mostly follows Brooke et al. (2015) up until the last stage. An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following Brooke et al. (2015).3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be “separated”, including verb particle constructions in English (Deh´e, 2002), and noun-verb idioms in Japanese (Hashimoto and Kawahara, 2008). Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (Wasow, 2002), which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details). This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall. We also exclude punctuation and lemmatize the corpus, and enforce an n-gram count threshold. As long as the count threshold is substantially above 1, efficient extraction of all n-grams can be done iteratively: in iteration i, i-gr"
Q17-1032,W11-0818,0,0.0164636,"ion of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones. Mutual expectation (Dias"
Q17-1032,W14-0405,0,0.0270618,"Missing"
Q17-1032,C12-1127,1,0.547831,"word nonetheless stubbornly remains the de facto standard processing unit for most research in modern NLP. The potential of multiword knowledge to improve both the automatic processing of language as well as offer new understanding of human acquisition and usage of language is the primary motivator of this work. Here, we present an effective, expandable, and tractable new approach to comprehensive multiword lexicon acquisition. Our aim is to find a middle ground between standard MWE acquisition approaches based on association measures (Ramisch, 2014) and more sophisticated statistical models (Newman et al., 2012) that do not scale to large corpora, the main source of the distributional information in modern NLP systems. A central challenge in building comprehensive multiword lexicons is paring down the huge space of possibilities without imposing restrictions which disregard a major portion of the multiword vocabulary of a language: allowing for diversity creates significant redundancy among statistically promising candidates. The lattice model proposed here addresses this primarily by having the candidates— contiguous and non-contiguous n-gram types— compete with each other based on subsumption and o"
Q17-1032,W12-3301,0,0.0640475,"Missing"
Q17-1032,D15-1290,0,0.020927,"quent larger ones. Mutual expectation (Dias et al., 1999) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of Brooke et al. (2015) is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (Riedl and Biemann, 2015) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold. The algorithm of da Silva and Lopes (1999) is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (Ramisch et al.,"
Q17-1032,W16-1816,0,0.0189816,"ructs a segmentation of the corpus and a corresponding multiword vocabulary. Other research in MWEs has tended to be rather focused on particular syntactic patterns such as verbnoun combinations (Fazly et al., 2009). The system of Schneider et al. (2014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen and Widdows, 2009), as can parsing (Constant and Nivre, 2016) and topic models (Lau et al., 2013). One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency threshold. Like FS, lexical b"
Q17-1032,Q16-1013,0,0.0288207,"in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures). Lexical bundles have been applied, in particular, to learner language: for example, Chen and Baker (2010) show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while Granger and Bestgen (2014) investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners. Sakaguchi et al. (2016) demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; Brooke et al. (2015) explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency. 3 Method Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type. The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual node"
Q17-1032,Q14-1016,0,0.252037,"computational linguistics, the most common term used to describe multiword lexical units is multiword expression (“MWE”: Sag et al. (2002), Baldwin and Kim (2010)), but here we wish to make a principled distinction between at least somewhat non-compositional, strongly lexicalized MWEs and FS, a near superset which includes many MWEs but also compositional linguistic formulas. This distinction is not a new one; it exists, for example, in the original paper of Sag et al. (2002) in the distinction between lexicalized and institutionalized phrases, and also to some extent in the MWE annotation of Schneider et al. (2014b), who distinguish between weak (collocational)2 and strong (non-compositional) MWEs. It is our contention, however, that separate, precise terminology is useful for research targeted at either class: we need not strain the concept of MWE to include items which do not require special semantics, nor are we inclined to disregard the larger formulaticity of language simply because it is not the dominant focus of MWE 1 Though by this definition individuals or small groups may have their own FS, here we are only interested in FS that are shared by a recognizable language community. 2 Here we avoid"
Q17-1032,schneider-etal-2014-comprehensive,0,0.241709,"computational linguistics, the most common term used to describe multiword lexical units is multiword expression (“MWE”: Sag et al. (2002), Baldwin and Kim (2010)), but here we wish to make a principled distinction between at least somewhat non-compositional, strongly lexicalized MWEs and FS, a near superset which includes many MWEs but also compositional linguistic formulas. This distinction is not a new one; it exists, for example, in the original paper of Sag et al. (2002) in the distinction between lexicalized and institutionalized phrases, and also to some extent in the MWE annotation of Schneider et al. (2014b), who distinguish between weak (collocational)2 and strong (non-compositional) MWEs. It is our contention, however, that separate, precise terminology is useful for research targeted at either class: we need not strain the concept of MWE to include items which do not require special semantics, nor are we inclined to disregard the larger formulaticity of language simply because it is not the dominant focus of MWE 1 Though by this definition individuals or small groups may have their own FS, here we are only interested in FS that are shared by a recognizable language community. 2 Here we avoid"
Q17-1032,W01-0513,0,0.126288,".g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while p"
Q17-1032,shinzato-etal-2008-large,0,0.0171967,"probing the challenges associated with using an n-gram approach to FS identification in such languages. For Croatian, we used ˇ the 1.2-billion-token fhrWaC corpus (Snajder et al., 2013), a filtered version of the Croatian web corpus hrWaC (Ljubeˇsi´c and Klubiˇcka, 2014), which is POS-tagged and lemmatized using the tools of Agi´c et al. (2013). Similar to English, the POS regex for Croatian includes simple nouns, adjectives and pronouns, but also other elements that regularly appear inside FS, including both adverbs and copulas. For Japanese, we used a subset of the 100M-page web corpus of Shinzato et al. (2008), which was roughly the same token length as the English corpus. We segmented and POS-tagged the corpus with MeCab (Kudo, 2008) using the UNIDIC morphological dictionary (Den et al., 2007). The POS regex for Japanese covers the same basic nominal structures as English, but also includes case markers and adverbials. Though our processing of Japanese includes basic lemmatization related to superficial elements like the choice of writing script and politeness markers, many elements (such as case marking) which are removed by lemmatization in Croatian are segmented into independent morphological u"
Q17-1032,P13-2137,1,0.886577,"Missing"
Q17-1032,C00-2137,0,0.0294829,"0.43 0.46 0.47 0.49 0.49 0.63 0.53 0.78 0.61 0.49 0.60 0.48 0.54 0.55 0.56 0.59 Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al. (2015); “−cl” = no clearing; “−ovr” = no penalization of overlaps; “P” = Precision; “R” = Recall; and “F” = F-score. Bold is best in a given column. The performance difference of the Lattice model relative to the best baseline for all test sets considered together is significant at p < 0.01 (based on the permutation test: Yeh (2000)). P R F PMIrank minLPRrank LPR-seg LocalMaxs DP-seg 0.20 0.34 0.42 0.56 0.35 0.29 0.45 0.45 0.39 0.71 0.23 0.39 0.43 0.46 0.47 Lattice 0.47 0.63 0.54 Table 3: Results of FS identification in contiguous BNC test set; LocalMaxs = method of da Silva and Lopes (1999); DP-seg = method of Newman et al. (2012) ble 3), we found that both the LocalMaxs algorithm and the DP-seg method of Newman et al. (2012) were able to beat our other baseline methods with roughly similar F-scores, though both are well below our Lattice method. Some of the difference seems attributable to fairly severe precision/recal"
S01-1013,P01-1004,1,0.766483,"in IN;,.., and le~ (IN;,..) is the character bigram length of IN;,... 2 B1gram frequency is weighted according to character type: a bigram made up entirely of hiragana charact~rs (gener_ally used in functional words/ particles) is given a weight of 0.2 and all other bigrams a weight of 1. Note that Dice&apos;s Coefficient ignores segment order, and that each string is thus treated as a ""bag of character bigrams"". Our choice of the combination of Dice&apos;s Coefficient, character-based indexing and character higrams (rather than any other n-gram order or mixed n-gram model) is based on the findings of Baldwin (2001b; 2001a), who compared character- and wordbased indexing in combination with both segment order-sensitive and bag-of-words similarity measures and with various n-gram models. As a result of extensive evaluation, Baldwin found the combination of character bigram-based indexing and a bagof-words method (in the form of either the vector space model or Dice&apos;s Coefficient) to be optimal. Our choice of Dice&apos;s Coefficient over the vector space m?del is due to the vector space model tending to bhthely prefer shorter strings in cases of low-level character overlap, and the ability of Dice&apos;s Coefficien"
S01-1013,H92-1045,0,0.0170499,"by the analysis of punctuation. These clause-level instances served as the inputs for the str·uctur·al method. We 2 freqTR;(e) 56 and len(TRi) are defined similarly. inputs with translation records is undesirable as high levels of spurious matches can be expected outside the scope of the original translation record expression. Inter-comparison of full inputs, on the other hand, provides a primitive model of domain similarity. Assuming that high similarity correlates with a high level of domain correspondence, we can apply a cross-lingual corollary of the ""one sense per discourse"" observation (Gale et al., 1992) in stipulating that a given word will be translated consistently within a given domain. By ascertaining that a given input closely resembles a second input, we can use the combined translation retrieval results for the two inputs to hone in on the optimal translation for the two. We term this procedure domain-based similarity consolidation. The overall retrieval process thus involves: (1) carrying out standard translation retrieval based on the abbreviated input, (2) using the original test set to determine the full input string most similar to the current input, and (3) performing translatio"
S07-1049,P98-1015,0,0.116023,"ven semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the C ONTENTC ONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-deﬁned inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are M AKE,1 , as exempliﬁed in apple pie where the pie is made from apple(s), and P OSSES SOR , as exempliﬁed in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: C AUSE -E FFECT, I NSTRUMENT-AGENCY, P RODUCT-P RODUCER, O RIGIN -E NTITY, T HEME T OOL, PART-W HOLE and C ONTENT-C ONTAINER. Our approach to the task was to: (1) naively trea"
S07-1049,I05-1082,1,0.800034,"of general-purpose SR interpretation over the nominal classiﬁcation task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The ﬁrst method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of automobile and car, and factory 232 in the two instances, are identical. As a result, the two NCs have the semantic relati"
S07-1049,W04-2609,0,0.121186,"(and word sense) is a strong determinant of the SR in practice. Our aim in this paper is to demonstrate the effectiveness of general-purpose SR interpretation over the nominal classiﬁcation task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The ﬁrst method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of"
S07-1049,C94-2125,0,0.0109686,"mpatibility of a given semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the C ONTENTC ONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-deﬁned inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are M AKE,1 , as exempliﬁed in apple pie where the pie is made from apple(s), and P OSSES SOR , as exempliﬁed in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: C AUSE -E FFECT, I NSTRUMENT-AGENCY, P RODUCT-P RODUCER, O RIGIN -E NTITY, T HEME T OOL, PART-W HOLE and C ONTENT-C ONTAINER. Our approach to the"
S07-1049,P94-1019,0,0.0074081,"s .71 and .27, and .83 and 1.00 respectively. We would then add these up to derive the overall similarity for a given NC and ﬁnd that apple juice is a better match. From this, we would assign the SR of M AKE from apple juice to chocolate milk. Formally, SA is the similarity between NCs (Ni,1 , Ni,2 ) and (Bj,1 , Bj,2 ): SA ((Ni,1 , Ni,2 ), (Bj,1 , Bj,2 )) = ((αS1 + S1) × ((1 − α)S2 + S2)) (3) 2 where S1 is the modiﬁer similarity (i.e. S(Ni,1 , Bj1 )) and S2 is head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of each of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. The ﬁnal classiﬁcation is derived from the training instance which has the highest lexical similarity with the test instance in question. 3 Co-Training As with many semantic annotation tasks, SR tagging is a time-consuming and expensive process. At the same time, due to the inherent complexity of the SR interpretation task, we require large a"
S07-1049,C98-1015,0,\N,Missing
S07-1050,agirre-de-lacalle-2004-publicly,0,0.0691583,"Missing"
S07-1050,P98-2127,0,0.246172,"Missing"
S07-1050,U06-1008,1,0.898149,"Missing"
S07-1050,C98-2122,0,\N,Missing
S07-1051,P05-1022,0,0.0190025,"mbines features developed in the context of preposition sense disambiguation for semantic role labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and our parameter tuning method (Section 4). We then discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling (Pradhan et al., 2004). ASSERT version 1.4 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed t"
S07-1051,W06-1670,0,0.0807154,"nd Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and our parameter tuning method (Section 4). We then discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling (Pradhan et al., 2004). ASSERT version 1.4 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect to the different senses of the target preposition. Details of the features in thi"
S07-1051,P05-1002,0,0.0196829,"ole labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and our parameter tuning method (Section 4). We then discuss and analyse the results of our method (Section 5) and conclude the paper (Section 6). Parsing Charniak’s re-ranking parser, version August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling (Pradhan et al., 2004). ASSERT version 1.4 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect"
S07-1051,gimenez-marquez-2004-svmtool,0,0.02943,"Missing"
S07-1051,H93-1103,0,0.311944,"Missing"
S07-1051,N01-1006,0,0.0137499,"Missing"
S07-1051,W03-0411,0,0.432314,"Missing"
S07-1051,P95-1026,0,0.0334825,"rsion August, 2006 (Charniak and Johnson, 2005). Named entity extraction A statistical NER system described in Cohn et al. (2005). Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). Semantic role labeling (Pradhan et al., 2004). ASSERT version 1.4 3 Features The disambiguation features used by our system can be divided into three categories: collocation features, syntactic features and semantic-role based features. We discuss each in turn below. 3.1 Collocation Features The collocation features were inspired by the one-sense-per-collocation heuristic proposed by Yarowsky (1995). These features were designed to capture open class words that exhibit strong collocation properties with respect to the different senses of the target preposition. Details of the features in this category are listed below. 1 This chunker is not exactly the same as Ngai and Florian’s system, however it does use the default transformation templates supplied by fnTBL. 241 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241–244, c Prague, June 2007. 2007 Association for Computational Linguistics Bag of WordNet synsets The WordNet (Miller, 1993) synonym"
S07-1051,U06-1020,1,0.922532,"an, 2001), and trained on the British National Corpus (BNC).1 1 Introduction Prepositional phrases (PPs) are both common and semantically varied in open English text. While the conventional view on prepositions from the computational linguistics community has been that they are semantically transient at best, and semanticallyvacuous at worst, a robust account of the semantics of prepositions and disambiguation method can be helpful in a range of NLP tasks including machine translation, parsing (prepositional phrase attachment) and semantic role labelling (Durand, 1993; O’Hara and Wiebe, 2003; Ye and Baldwin, 2006a). The SemEval 2007 preposition sense disambiguation task provides a common test bed for the evaluation of preposition sense disambiguation systems. Our proposed method is maximum entropy based, and combines features developed in the context of preposition sense disambiguation for semantic role labelling (Ye and Baldwin, 2006a), and verb sense disambiguation (Ye and Baldwin, 2006b). The remainder of this paper is structured as follows. We first discuss the pre-processing steps used in our system (Section 2), and outline the features our preposition disambiguation method uses (Section 3) and o"
S07-1051,H92-1116,0,\N,Missing
S07-1076,P06-1013,0,0.0199303,"the integration of this method into a supervised system by different means. Thus, this paper describes both the unsupervised system (UBC-UMB-1), and the combined supervised system (UBC-UMB-2) submitted to the all-words task. Our motivation in building unsupervised systems comes from the difﬁculty of creating hand-tagged data for all words and all languages, which is colloquially known as the knowledge acquisition bottleneck. There have also been promising results in recent work on the combination of unsupervised approaches that suggest the gap with respect to supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation"
S07-1076,W06-1663,0,0.012481,"o supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation of topic signatures built from a variety of knowledge sources. In this work we built those coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 200"
S07-1076,U06-1008,1,0.677397,"se coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 2007 Association for Computational Linguistics 2.2 Relatives in Context (RIC) This is an unsupervised method presented in Martinez et al. (2006). This algorithm makes use of the WordNet relatives of the target word for disambiguation. The process is carried out in these steps: (i) obtain a set of close relatives from WordNet for each sense (the relatives can be polysemous); (ii) for each test instance deﬁne all possible word sequences that include the target word; (iii) for each word sequence, substitute the target word with each relative and query a web search engine; (iv) rank queries according to the following factors: length of the query, distance of the relative to the target word, and number of hits; and (v) select the sense ass"
S07-1076,J01-3001,0,0.0361181,"nstraints. 2.5 Combination of systems We explored two approaches to combine the standalone systems. The ﬁrst consisted simply of adding up the normalized weights that each system would give to each sense. We tested this voting approach both for the unsupervised and supervised settings. The second method could only be applied in combination with the supervised kNN system. The idea was to include the unsupervised predictions as weighted features for the supervised system. We refer to this method as “stacking”, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). 3 Development experiments We tested the single algorithms and their combination over both Semcor and the training distribution of the SemEval-2007 lexical-sample subtask of task 17 (S07LS for short). The goal of these experiments was to obtain an estimate of the expected performance, and submit the most promising conﬁguration. We present ﬁrst the tests on the unsupervised setting, and then the supervised setting. It is important to note that the hand-tagged corpora was not used to ﬁne-tune the parameters of the unsupervised algorithms. 3.1 Unsupervised systems For the ﬁrst evaluation of our"
S07-1076,agirre-de-lacalle-2004-publicly,1,\N,Missing
S10-1004,W03-1028,0,0.955241,"g, National University of Singapore, Singapore sunamkim@gmail.com, medelyan@gmail.com, kanmy@comp.nus.edu.sg, tb@ldwin.net Abstract have showcased the potential benefits of keyphrase extraction to downstream NLP applications. In light of these developments, we felt that this was an appropriate time to conduct a shared task for keyphrase extraction, to provide a standard assessment to benchmark current approaches. A second goal of the task was to contribute an additional public dataset to spur future research in the area. Currently, there are several publicly available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 d"
S10-1004,N04-4005,0,0.0188339,"tions such as summarization, information retrieval and question-answering. In summarization, keyphrases can be used as a form of semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Worksho"
S10-1004,C08-1122,0,0.704332,"available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 documents taken from DUC 2001 which contain up to 10 manually-assigned keyphrases per document. Several databases, including the ACM Digital Library, IEEE Xplore, Inspec and PubMed provide articles with authorassigned keyphrases and, occasionally, readerassigned ones. Medelyan (2009) automatically generated a dataset using tags assigned by the users of the collaborative citation platform CiteULike. This dataset additionally records how many people have assigned the same keyword to the same publication. In total, 180 full-text publications were annotated by over 300"
S10-1004,R09-1086,0,0.312048,"tual F-scores 2.2 Evaluation Method and Baseline Traditionally, automatic keyphrase extraction systems have been assessed using the proportion of top-N candidates that exactly match the goldstandard keyphrases (Frank et al., 1999; Witten et al., 1999; Turney, 1999). In some cases, inexact matches, or near-misses, have also been considered. Some have suggested treating semanticallysimilar keyphrases as correct based on similarities computed over a large corpus (Jarmasz and Barriere, 2004; Mihalcea and Tarau, 2004), or using semantic relations defined in a thesaurus (Medelyan and Witten, 2006). Zesch and Gurevych (2009) compute near-misses using an ngram based approach relative to the gold standard. For our shared task, we follow the traditional exact match evaluation metric. That is, we match the keyphrases in the answer set with those the systems provide, and calculate micro-averaged precision, recall and F-score (β = 1). In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by each system. We rank the participating systems by F-score over the top 15 candidates. Participants were required to extract existing phrases from the documents. Since it is theoretically possible"
S10-1004,P09-2046,0,0.0298245,"1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21–26, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics search areas in each case. Note that the trial d"
S10-1004,W04-3252,0,\N,Missing
S10-1004,W97-0703,0,\N,Missing
S10-1004,D09-1027,0,\N,Missing
S10-1004,C02-1142,0,\N,Missing
S12-1017,W09-2903,0,0.0444712,"nd Kawahara, 2009) is a gold-standard corpus of over 100, 000 Japanese MWE-tokens covering 146 types. It is the largest resource we are aware of which has handannotated instances of MWEs which are ambiguous between a literal and idiomatic interpretation, and has been used by Hashimoto and Kawahara (2009) and Fothergill and Baldwin (2011) for supervised classification of MWE-tokens using features capturing lexico-syntactic variation and traditional semantic features borrowed from word sense disambiguation (WSD) . Similar work in other languages has been performed by Li and Sporleder (2010) and Diab and Bhutada (2009). We build on this work in exploring the use of MWE-type-level features drawn from an idiom dictionary for MWE identification. 100 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 100–104, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Hashimoto and Kawahara (2009) developed a variety of features capturing lexico-syntactic variation but only one — a Boolean feature for “internal modification”, which fired only when a non-constituent word appeared between constituent words in an MWE-token — had an appreciable impact on classificati"
S12-1017,J09-1005,0,0.103592,"owever, they found that this effect was far overshadowed by semantic context features inspired by WSD. That is, treating each MWE-type as a word with two senses and performing sense disambiguation was far more successful than any features based on lexico-syntactic characteristics of idioms. Intuitively, we would expect that if we had access to a rich inventory of expression-specific type-level features encoding the ability of the expression to participate in different syntactic alternations, we should be better equipped to disambiguate token occurrences of that expression. Indeed, the work of Fazly et al. (2009) would appear to support this hypothesis, in that the authors used unsupervised methods to learn type-level preferences for a range of MWE types, and demonstrated that these could be successfully applied to a token-level disambiguation task. Hashimoto and Kawahara (2009) trained individual classifiers for each MWE-type in their corpus and tested them only on instances of the type they were trained on. In contrast to this typespecialised classification, Fothergill and Baldwin (2011) trained classifiers on a subset of MWE-types and tested on instances of the remaining held-out MWE-types. The mot"
S12-1017,I11-1102,1,0.555862,"butional — such as hot and cold being more common than cold and hot — but in this paper we study MWEs with idiosyncratic semantics. Specifically we are concerned with expressions such as jump the gun which are ambiguous between a literal interpretation of “to leap over a firearm”, and an idiomatic interpretation of “to act prematurely”. While MWEs are increasingly entering the mainstream of NLP, the accurate identification of MWEs remains elusive for current methods, particularly in the absence of MWE type-specialised training data. This paper builds on the work of Hashimoto et al. (2006) and Fothergill and Baldwin (2011) in exploring whether type-level MWE properties sourced from an idiom dictionary can boost the accuracy of crosstype MWE-token classification. That is, we attempt to determine whether token occurrences of ambiguous expressions such as Kim jumped the gun on this issue are idiomatic or literal, based on: (a) annotated instances for MWEs other than jump the gun (e.g. we may only have token-level annotations for kick the bucket and throw in the towel), and (b) dictionary-based information on the syntactic properties of the idiom in question. We find that constituent modifiability judgments extract"
S12-1017,C10-2078,0,0.0152245,"OpenMWE corpus (Hashimoto and Kawahara, 2009) is a gold-standard corpus of over 100, 000 Japanese MWE-tokens covering 146 types. It is the largest resource we are aware of which has handannotated instances of MWEs which are ambiguous between a literal and idiomatic interpretation, and has been used by Hashimoto and Kawahara (2009) and Fothergill and Baldwin (2011) for supervised classification of MWE-tokens using features capturing lexico-syntactic variation and traditional semantic features borrowed from word sense disambiguation (WSD) . Similar work in other languages has been performed by Li and Sporleder (2010) and Diab and Bhutada (2009). We build on this work in exploring the use of MWE-type-level features drawn from an idiom dictionary for MWE identification. 100 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 100–104, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Hashimoto and Kawahara (2009) developed a variety of features capturing lexico-syntactic variation but only one — a Boolean feature for “internal modification”, which fired only when a non-constituent word appeared between constituent words in an MWE-token — had an apprec"
S12-1017,P11-1017,0,0.0154361,"the lack of a common semantics between MWE-types. However, this turned out not to be the case, with by far the 101 most successful results arising again from use of WSD features. This surprising result raises the possibility of distributional similarity between the contexts of idiomatic MWE-tokens of different MWEtypes, however the result was not explained or explored further. In this paper we offer new insights into the distributional similarity hypothesis. The recently-published JDMWE (Japanese Dictionary of Multiword Expressions) encodes typelevel information on thousands of Japanese MWEs (Shudo et al., 2011). A subset of the dictionary has been released, and overlaps to some extent with the MWE-types in the OpenMWE corpus. JDMWE encodes information about lexico-syntactic variations allowed by each MWE-type it contains. For example, the expression hana wo motaseru — literally “to have [someone] hold flowers” but figuratively “to let [someone] take the credit” — has the syntactic form entry [N wo] *V30. The asterix indicates modifiability, telling us that the head [V]erb motaseru “cause to hold” allows modification by nonconstituent dependents – such as adverbs – but the dependent [N]oun hana “flow"
S12-1031,P08-1037,1,0.945589,"l candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers to make more efficient use of training data, and indeed, there has been some prior work investigating this possibility. Agirre et al. (2008) applied two state-of-the-art treebank parsers to the sensetagged subset of the Brown corpus version of the Penn Treebank (Marcus et al., 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PPattachment. The annotations they used were oracle sense annotations, automatic sense recognition and the first sense heuristic, and it was this last method which was the best performer in general. The sense annotations were either the WordNet synset ID or the coarse semantic file, which we explain in more detail below, and replaced the ori"
S12-1031,P11-2123,0,0.472393,"Missing"
S12-1031,A00-1031,0,0.0177658,"open class token is labelled with multiple synsets, starting with the assigned leaf synset and travelling as high as possible up the hierarchy, with no distinction made between the different levels in the hierarchy. Configurations using this are designated HP, for ‘hypernym path’. 3.4 Word Sense Annotations We return now to the question of determination of the synset for a given token. One frequentlyused and robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense"
S12-1031,briscoe-carroll-2002-robust,0,0.0322519,"n, instead leaving it the MaxEnt parse ranker to pick those labels from the hierarchy which are most useful. Each 1 This could be useful for verbs since senses interact strongly subcategorisation frames, but that is not our focus here. 231 3.4.2 Disambiguating senses 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. root_frag Adding Word Sense to Parse Selection Models np_frg_c We noted above that parse selection using the methodology established by Velldal (2007) uses hdn_bnp_c human-annotated incorrect and correct derivation aj-hdn_norm_c trees to train a maximum entropy parse selection model. More specifically, the model is trained using legal_a1 n_pl_olr features extracted from the candidate HPSG deriva&quot;legal&quot; issue_n1 tion trees, using the labels of each node (which are the rule names from the grammar) and those of a &quot;issues&quot; lim"
S12-1031,W96-0209,0,0.0869887,"g. In all configurations, there were instances of F-score decreases, sometimes substantial. It is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above. One possible explanation is that the model training parameters were suboptimal for this data set since the characteristics of the data are somewhat different than without sense annotations. The failure to improve some235 what mirrors the results of Clark (2001), who was attempting to improve the parse ranking performance of the unification-based based probabilistic parser of Carroll and Briscoe (1996). Clark (2001) used dependencies to rank parses, and WordNet-based techniques to generalise this model and learn selectional preferences, but failed to improve performance over the structural (i.e. non-dependency) ranking in the original parser. Additionally, perhaps the changes we applied in this work to the parse ranking could possibly have been more effective with features based on semantic dependences as used by Fujita et al. (2007), although we outlined reasons why we wished to avoid this approach. This work is preliminary and there is room for more exploration in this space. There is sco"
S12-1031,A00-2018,0,0.0387452,"performance. 1 (1) I saw a tree with my telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the"
S12-1031,W06-1670,0,0.155466,"d robust strategy is to lemmatise and POStag each token, and assign it the first-listed sense from WordNet (which may or may not be based on actual frequency counts). We POS-tag using TnT (Brants, 2000) and lemmatise using WordNet’s native lemmatiser. This yields a leaf-level synset, making it suitable as a source of annotations for both SS and HP. We denote this ‘WNF’ for ‘WordNet First’ (shown in parentheses after SS or HP). Secondly, to evaluate whether a more informed approach to sense-tagging helps beyond the naive WNF method, in the ‘SST’ method, we use the outputs of SuperSense Tagger (Ciaramita and Altun, 2006), which is optimised for assigning the supersenses described above, and can outperform a WNFstyle baseline on at least some datasets. Since this only gives us coarse supersense labels, it can only provide SS annotations, as we do not get the leaf synsets needed for HP. The input we feed in is POStagged with TnT as above, for comparability with the WNF method, and to ensure that it is compatible with the configuration in which the corpora were parsed – specifically, the unknown-word handling uses a version of the sentences tagged with TnT. We ignore multi-token named entity outputs from SuperSe"
S12-1031,P04-1014,0,0.0471892,"1) I saw a tree with my telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An interm"
S12-1031,P97-1003,0,0.0706991,"telescope (2) I saw a tree with no leaves The most obvious interpretation in each case has the prepositional phrase headed by with attaching in different places: to the verb phrase in the first example, and to the noun tree in the second. Such distinctions are difficult for a parser to make when the training data is sparse, but imagine we had seen examples such as the following in the training corpus: (3) Kim saw a eucalypt with his binoculars (4) Sandy observed a willow with plentiful foliage Introduction Most start-of-the-art natural language parsers (Charniak, 2000; Clark and Curran, 2004; Collins, 1997) use lexicalised features for parse ranking. These are important to achieve optimal parsing accuracy, and yet these are also the features which by their nature suffer from data-sparseness problems in the training data. In the absence of reliable fine-grained statistics for a given token, various strategies are possible. There will often be statistics available for coarser categories, such as the POS of the particular token. However, it is possible that these coarser representations discard too much, missing out information which could be valuable to the parse ranking. An intermediate level of"
S12-1031,W11-2927,1,0.833287,"ra are summarised in Table 1. With these corpora, we are able to investigate indomain and cross-domain effects, by testing on a 230 different corpus to the training corpus, so we can examine whether sense-tagging alleviates the crossdomain performance penalty noted in MacKinlay et al. (2011). We can also use a subset of each training corpus to simulate the common situation of sparse training data, so we can investigate whether sensetagging enables the learner to make better use of a limited quantity of training data. 3.2 Evaluation Our primary evaluation metric is Elementary Dependency Match (Dridan and Oepen, 2011). This converts the semantic output of the ERG into a set of dependency-like triples, and scores these triples using precision, recall and F-score as is conventional for other dependency evaluation. Following MacKinlay et al. (2011), we use the EDMNA mode of evaluation, which provides a good level of comparability while still reflecting most the semantically salient information from the grammar. Other work on the ERG and related grammars has tended to focus on exact tree match, but the granular EDM metric is a better fit for our needs here – among other reasons, it is more sensitive in terms o"
S12-1031,W07-1204,0,0.430351,"impact on parse selection of semantic annotations such as coarse sense labels or synonyms from a distributional theTotal Sentences Parseable Sentences Validated Sentences Train/Test Sentences Tokens/sentence Training Tokens W E S CIENCE 9632 9249 7631 6149/1482 15.0 92.5k LOGON 9410 8799 8550 6823/1727 13.6 92.8k Table 1: Corpora used in our experiments, with total sentences, how many of those can be parsed, how many of the parseable sentences have a single gold parse (and are used in these experiments), and average sentence length saurus. Our work here differs from the aforementioned work of Fujita et al. (2007) in a number of ways. Firstly, we use purely syntactic parse selection features based on the derivation tree of the sentence (see Section 3.4.3), rather than ranking using dependency triples, meaning that our method is in principle able to be integrated into a parser more easily, where the final set of dependencies would not be known in advance. Secondly, we do not use humancreated sense annotations, instead relying on heuristics or trained sense-taggers, which is closer to the reality of real-world parsing tasks. 3.1 Corpora Following MacKinlay et al. (2011), we use two primary training corpo"
S12-1031,P98-2127,0,0.131146,"only to PP attachment, but may help in a range of other syntactic phenomena, such as distinguishing between complements and modifiers of verbs. 228 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228–236, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics The synonyms or hypernyms could take the form of any grouping which relates word forms with semantic or syntactic commonality – such as a label from the WordNet (Miller, 1995) hierarchy, a subcategorisation frame (for verbs) or closely related terms from a distributional thesaurus (Lin, 1998). We present work here on using various levels of semantic generalisation as an attempt to improve parse selection accuracy with the English Resource Grammar (ERG: Flickinger (2000)), a precision HPSG-based grammar of English. 2 Related Work 2.1 Parse Selection for Precision Grammars The focus of this work is on parsing using handcrafted precision HPSG-based grammars, and in particular the ERG. While these grammars are carefully crafted to avoid overgeneration, the ambiguity of natural languages means that there will unavoidably be multiple candidate parses licensed by the grammar for any non-"
S12-1031,W02-2018,0,0.0515672,"the ERG, the number of parses postulated for a given sentence can be anywhere from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a"
S12-1031,J93-2004,0,0.0394835,"Missing"
S12-1031,P04-1036,1,0.851121,"making assumptions about which level of the hierarchy will be most useful for parse disambiguation, instead leaving it the MaxEnt parse ranker to pick those labels from the hierarchy which are most useful. Each 1 This could be useful for verbs since senses interact strongly subcategorisation frames, but that is not our focus here. 231 3.4.2 Disambiguating senses 3.4.3 A distributional thesaurus method A final configuration attempts to avoid the need for curated resources such as WordNet, instead using an automatically-constructed distributional thesaurus (Lin, 1998). We use the thesaurus from McCarthy et al. (2004), constructed along these lines using the grammatical relations from RASP (Briscoe and Carroll, 2002) applied to 90 millions words of text from the British National Corpus. root_frag Adding Word Sense to Parse Selection Models np_frg_c We noted above that parse selection using the methodology established by Velldal (2007) uses hdn_bnp_c human-annotated incorrect and correct derivation aj-hdn_norm_c trees to train a maximum entropy parse selection model. More specifically, the model is trained using legal_a1 n_pl_olr features extracted from the candidate HPSG deriva&quot;legal&quot; issue_n1 tion trees,"
S12-1031,W02-1210,0,0.0694168,"Missing"
S12-1031,W07-2207,0,0.0129779,"here from zero to tens of thousands. It is the job of the parse selection model to select the best parse from all of these candidates as accurately as possible, for some definition of ‘best’, as we discuss in Section 3.2. Parse selection is usually performed by training discriminative parse selection models, which ‘discriminate’ between the set of all candidate parses. A widely-used method to achieve this is outlined in Velldal (2007). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. This method is used by Zhang et al. (2007) and MacKinlay et al. (2011) inter alia. One important implementation detail is that rather than exhaustively ranking all candidates out of possibly many thousands of trees, Zhang et al. (2007) showed that it was possible to use ‘selective unpacking’, which means that the exhaustive parse forest can be represented compactly as a ‘packed forest’, and the top-ranked trees can be successively reconstructed, enabling faster parsing using less memory. 229 2.2 Semantic Generalisation for parse ranking Above, we outlined a number of reasons why semantic generalisation of lexemes could enable parsers"
S12-1031,C98-2122,0,\N,Missing
S13-1030,J01-3001,0,0.0357172,"d petition, and case and verdict are also similar. One straightforward way of estimating semantic similarity of two texts is by using approaches based on the similarity of the surface forms of the words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel l"
S13-1030,S12-1051,0,0.15264,"e words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Se"
S13-1030,S13-1004,0,0.0474373,"Missing"
S13-1030,S12-1059,0,0.0950475,"Missing"
S13-1030,S12-1097,0,0.0219499,"y (SS) features. LEV2, SW, and NAW have not been previously considered for STS. 2.2 Topic Modelling Similarity Measures (TM) The topic modelling features (TM) are based on Latent Dirichlet Allocation (LDA), a generative probabilistic model in which each document is modeled as a distribution over a finite set of topics, and each topic is represented as a distribution over words (Blei et al., 2003). We build a topic model on a background corpus, and then for each target text we create a topic vector based on the topic allocations of its content words, based on the method developed by Lui et al. (2012) for predicting word usage similarity. The choice of the number of topics, T , can have a big impact on the performance of this method. Choosing a small T might give overlybroad topics, while a large T might lead to uninterpretable topics (Steyvers and Griffiths, 2007). Moreover smaller numbers of topics have been shown to perform poorly on both sentence similarity (Guo and Diab, 2012) and word usage similarity tasks (Lui et al., 2012). We therefore build topic models for 33 values of T in the range 2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350. The background corpus used for generating the t"
S13-1030,P06-4018,0,0.0677132,"res contains various string similarity measures (SS), which compare the target texts in terms of the words they contain and the order of the words (Islam and Inkpen, 2008). In the SemEval 2012 STS task (Agirre et al., 2012) such features were used by several participants (Biggins et al., 2012; B¨ar et al., 2012; Heilman and Madnani, 2012), including the first-ranked team (B¨ar et al., 2012) who considered string similarity measures alongside a wide range of other features. For our string similarity features, the texts were lemmatized using the implementation of Lancaster Stemming in NLTK 2.0 (Bird, 2006), and all punctuation was removed. Limited stopword removal was carried out by eliminating the words a, and, and the. The output of each string similarity measure is normalized to the range of [0, 1], where 0 indicates that the texts are completely different, while 1 means they are identical. The normalization method for each feature is described in Salehi and Cook (to 208 appear), wherein the authors applied string similarity measures successfully to the task of predicting the compositionality of multiword expressions. Identical Unigrams (IU): This feature measures the number of words shared"
S13-1030,D10-1113,0,0.032236,", a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent variable models of similarity to capture wo"
S13-1030,S12-1086,0,0.11977,"t text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent variable models of"
S13-1030,S12-1076,0,0.0175684,"n previous research, rather than combine these features with the myriad of features that have been proposed by others for the task. 2 Text Similarity Measures In this section we describe the various features used in our system. 2.1 String Similarity Measures (SS) Our first set of features contains various string similarity measures (SS), which compare the target texts in terms of the words they contain and the order of the words (Islam and Inkpen, 2008). In the SemEval 2012 STS task (Agirre et al., 2012) such features were used by several participants (Biggins et al., 2012; B¨ar et al., 2012; Heilman and Madnani, 2012), including the first-ranked team (B¨ar et al., 2012) who considered string similarity measures alongside a wide range of other features. For our string similarity features, the texts were lemmatized using the implementation of Lancaster Stemming in NLTK 2.0 (Bird, 2006), and all punctuation was removed. Limited stopword removal was carried out by eliminating the words a, and, and the. The output of each string similarity measure is normalized to the range of [0, 1], where 0 indicates that the texts are completely different, while 1 means they are identical. The normalization method for each f"
S13-1030,U12-1006,1,0.90757,"re used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent"
S13-1030,U12-1019,1,0.837479,"nce is drawn from. For test data, the entire vector consists of 0s. The second approach we considered is based on metalearning, and we will refer to it as domain stacking. In domain stacking, we train a regressor for each domain (the level 0 regressors (Wolpert, 1992)). Each of these regressors is then applied to a test instance to produce a predicted value (the level 0 prediction). These predictions are then combined using a second regressor (the level 1 regressor), to produce a final prediction for each instance (the level 1 prediction). This approach is closely related to feature stacking (Lui, 2012) and stacked generalization (Wolpert, 1992). A general principle of metalearning is to combine multiple weaker (“less accurate”) predictors — termed level 0 predictors — to produce a stronger (“more accurate”) predictor — the level 1 predictor. In stacked generalization, the level 0 predictors are different learning algorithms. In feature stacking, they are the same algorithm trained on different subsets of features, in this work corresponding to different methods for es210 timating STS (Section 2). In domain stacking, the level 0 predictions are obtained from subsets of the training data, whe"
S13-1030,J92-1001,0,0.356307,"s) of plea and petition, and case and verdict are also similar. One straightforward way of estimating semantic similarity of two texts is by using approaches based on the similarity of the surface forms of the words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarit"
S13-1030,S13-1039,1,0.771798,"Missing"
S13-2024,C10-3010,1,0.864322,"Missing"
S13-2024,S13-1030,1,0.621199,"Missing"
S13-2024,2005.mtsummit-papers.11,0,0.0241262,"directional, forward, backward, no entailment. We take the compositional approach and separately train a forward, as well as a backward binary classifier. Each classifier is run separately on the set of text fragment pairs to produce two binary labels for forward and backward entailment. The two sets of labels are logically combined to produce a final classification for each test pair of forward, backward, bidirectional or no entailment. 3 Word Alignment Features The test set of topically-related text fragments, T1 (German) and T2 (English) were added to Europarl German–English parallel text (Koehn, 2005) and Giza++ was used for automatic word alignment in both language directions. Moses (Koehn et al., 2007) was then used for symmetrization with the grow diag final and algorithm. This produces a many-to-many alignment between the words of the 133 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 133–137, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics The number of words in T2 (English) that are not aligned with anything in T1 (German) should provide a"
S13-2024,N10-1045,0,0.0617989,"orward and backward before a compositional combination into the final four classes, as well as experiments with additional string similarity features. 1 2 Introduction Cross-lingual Textual Entailment (CLTE) (Negri et al., 2012) proposes the task of automatically identifying the kind of relation that exists between pairs of semantically-related text fragments written in two distinct languages, a variant of the traditional Recognizing Textual Entailment (RTE) task (Bentivogli et al., 2009; Bentivogli et al., 2010). The task targets the cross-lingual content synchronization scenario proposed in Mehdad et al. (2010, 2011). Compositional classification can be used by training two distinct binary classifiers for forward and backward entailment classification, before combining labels into the four final entailment categories that now include bidirectional and no entailment labels. The most similar previous work to this work is the crosslingual approach of the FBK system (Mehdad et al., 2012) from Semeval 2012 (Negri et al., 2012), in which the entailment classification is obtained Compositional Classification Given a pair of topically related fragments, T1 (German) and T2 (English), we automatically annota"
S13-2024,P11-1134,0,0.127608,"Missing"
S13-2024,S12-1105,0,0.0246471,"Missing"
S13-2024,D11-1062,0,0.159966,"Missing"
S13-2024,S12-1053,0,0.16135,"Missing"
S13-2024,W99-0604,0,0.290489,"Missing"
S13-2024,S13-1039,1,0.868654,"Missing"
S13-2024,P07-2045,0,\N,Missing
S13-2024,W07-1401,0,\N,Missing
S13-2039,S07-1002,0,0.060288,"model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of"
S13-2039,E12-1060,1,0.867931,"iven data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detection. The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and th"
S13-2039,S13-2051,1,0.837682,"Missing"
S13-2039,S10-1011,0,0.0389883,"rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detectio"
S13-2039,S13-2035,0,0.122581,"induce the ranking based on the sense probabilities assigned to the senses, such that snippets that have the highest probability of the induced sense are ranked highest, and snippets with lower sense probabilities 3 Our implementation can be accessed via https:// github.com/jhlau/hdp-wsi. 219 are ranked lower. Two classes of evaluation are used in the shared task: 1. cluster quality measures: Jaccard Index (JI), RandIndex (RI), Adjusted RandIndex (ARI) and F1; 2. diversification of search results: Subtopic Recall@K and Subtopic Precision@r. Details of the evaluation measures are described in Navigli and Vannella (2013). The idea behind the second form of evaluation (i.e. diversification of search results) is that search engine results should cluster the results based on senses (of the query term in the documents) given an ambiguous query. For example, if a user searches for apple, the search engine may return results related to both the computer brand sense and the fruit sense of apple. Given this assumption, the best WSI/WSD system is the one that can correctly identify the diversity of senses in the snippets. Figure 1: Subtopic Recall@K for all participating systems. Cluster quality, subtopic recall@K and"
S13-2051,S12-1027,0,0.0225917,"he weights of the induced senses and that of the gold senses. For clustering comparison, fuzzy normalised mutual information (FNMI) and fuzzy b-cubed (FBC) are used. Note that the WSD systems participating in this shared task are not evaluated with clustering comparison metrics, as they do not induce senses/clusters in the same manner as WSI systems. WSI systems produce senses that are different to the gold standard sense inventory (WordNet 3.1), and the induced senses are mapped to the gold standard senses using the 80/20 validation setting. Details of this mapping procedure are described in Jurgens (2012). Results for all test instances are presented in Table 3. Note that many baselines are used, only some of which we present in this paper, namely: (1) R AN DOM — label instances with one of three random induced senses; (2) S EMCOR MFS — label instances with the most frequently occurring sense in Semcor; (3) T EST MFS — label instances with the most frequently occurring sense in the test dataset. To benchmark our method, we present one or two of the best 309 systems from each team. Looking at Table 3, our system performs encouragingly well. Although not the best system, we achieve results close"
S13-2051,E12-1060,1,0.721911,"ed to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings. The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model. Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task. 1 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012), and also applied to SemEval-2013 Task 11 on WSI for web snippet clustering (Lau et al., to appear). The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and the association of topics with documents is represented by a multinomial distribution of topics, a distribution for each document. The generative process of LDA for drawing"
S13-2051,S13-2039,1,0.837606,"Missing"
S15-1012,P10-2047,0,0.0135158,"t into a collective classification algorithm. Dual Classifier Approach The dual classifier approach is made up of three steps, as depicted in Figure 1: 1. Base classification: Produce base classifications using (1) a content-only classifier; and (2) a relationship classifier. The contentonly classifier makes a binary prediction: F OR 109 5.1.1 Base classification For our content-only base classifier, we use the same bag-of-words SVM with binary (existencebased) unigram features as (Thomas et al., 2006). This classifier has been shown to be the best bagof-words model for B ITTERLEMONS (Beigman Klebanov et al., 2010). As our relationship base classifier, we use the cosine similarity scores described above, calculated using n-grams of several different lengths. 5.1.2 Normalisation We use probabilistic SVM normalisation to convert the signed decision-plane distance output by the content-only classifier into the probability that the instance is in the positive class (Platt, 1999). For the relationship classifier, the technique used to convert the cosine similarity score into a classification preference needs to fit complex criteria. Preliminary experiments suggested that while the very highest similarity sco"
S15-1012,P11-1151,1,0.936546,"within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accu"
S15-1012,W03-1022,0,0.0604172,"en proposed, although in natural language processing at least, these have focused largely on conditional dependence, in the form of models such as 1 In some tasks, it can also indicate heterophily, i.e. the tendency for connected instances to have contrasting properties, as we shall see for one of our two dataset. hidden Markov models (Rabiner and Juang, 1986) and conditional random fields (Lafferty et al., 2001), where independent properties of words, e.g., are combined with conditional dependencies based on their context of use to jointly predict the senses of all words in a given sentence (Ciaramita and Johnson, 2003; Johannsen et al., 2014). This paper explores the utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002),"
S15-1012,P11-1016,0,0.0417604,"emantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accuracy. The intuition underlying this work is that some types of documents have features which are either absent or ambiguous in training data, but which have 106 Proceedings of the Fourth Joint"
S15-1012,S14-1001,0,0.0313871,"ral language processing at least, these have focused largely on conditional dependence, in the form of models such as 1 In some tasks, it can also indicate heterophily, i.e. the tendency for connected instances to have contrasting properties, as we shall see for one of our two dataset. hidden Markov models (Rabiner and Juang, 1986) and conditional random fields (Lafferty et al., 2001), where independent properties of words, e.g., are combined with conditional dependencies based on their context of use to jointly predict the senses of all words in a given sentence (Ciaramita and Johnson, 2003; Johannsen et al., 2014). This paper explores the utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references i"
S15-1012,W06-2915,0,0.039672,"to allow for a more statistically robust evaluation; and (3) we discard the manually annotated inter-document relationships based on references to speaker names, because implicit relationships are the focus of this work. Table 1 gives statistics for our rendering of C ON VOTE. The identical figures for the average number of speeches and speakers per debate reflect the fact that each speaker now contributes only one unified speech. cused on the content of the contributions rather than stylistic or biographical features that may identify one editor or the other. 3.2 B ITTERLEMONS B ITTERLEMONS (Lin et al., 2006) is a collection of articles on the Israeli–Arab conflict harvested from the Bitterlemons website.2 In each weekly issue, the editors contribute an article giving their perspectives on some aspect of the conflict, and two guest authors contribute articles, one from an Israeli perspective and the other from a Palestinian perspective. Sometimes these guest contributions take the form of an interview, in which case we remove the questions (from the editors) and retain only the answers. The statistics in Table 2 give a picture of the size and structure of B ITTERLEMONS. In accordance with Lin et a"
S15-1012,P05-1015,0,0.214269,"Missing"
S15-1012,W09-3210,0,0.0221556,"ifier approach has generally been found to perform best (Thomas et al., 2006; Burfoot et al., 2011). While the work presented here is conceptually quite simple, the findings are significant and potentially open the door to accuracy improvements on a range of document-level semantic tasks. 2 Related Work Previous work has dealt with the question of collective document classification using implicit interdocument relationships in two basic ways: 1. proximity: use a spatial or temporal dimension of the domain to relate documents (Agrawal et al., 2003; Goldberg et al., 2007; McDowell et al., 2009; Somasundaran et al., 2009). 2. similarity: relate documents via some notion of their content-based similarity (Blum and Chawla, 2001; Joachims, 2003; Takamura et al., 2007; Sindhwani and Melville, 2008; Jurgens, 2013) The work using similarity-based links is the closest to ours but is also strongly differentiated because 107 it focuses on transductive semi-supervised classification. That task begins with the premise that only a small amount of labelled training data is available, so content-only classification is likely to be inaccurate. By contrast, the supervised techniques in this paper deal with large amounts of la"
S15-1012,N12-1013,0,0.0935086,"or document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accuracy. The intuition underlyi"
S15-1012,N07-1037,0,0.0826646,"Missing"
S15-1012,W06-1639,0,0.903224,"utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves docume"
S15-2092,P11-1015,0,0.0451942,"tanford Sentiment Treebank dataset 3.1.1 Labelled Datasets SemEval-2015 Dataset: the official SemEval2015 Task 10, subtask B dataset, comprised of tweets which have been hand-labelled for sentiment at the message-level (in terms of POSITIVE, NEGA TIVE and NEUTRAL sentiment). The dataset is partitioned into three components, as detailed in Table 1:4 (1) training set, (2) development set, and (3) test set. Stanford Sentiment Treebank Dataset: a collection of movie review documents from www. rottentomatoes.com, which have been sentence tokenised and annotated for sentiment at the sentence level (Maas et al., 2011) and prepartitioned into training and test data, as detailed in Table 2. Socher et al. (2013) additionally annotated the data at the phrase and lexical levels, but we use only the sentence-level annotations in this paper. 3.1.2 Unlabelled Datasets Twitter Dataset: a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec, 2014 using the Twitter Trending API. This is used as additional data to pre-train the message-level embeddings for the SemEval2015 Dataset. IMDB Dataset: a 100K sentence movie review dataset from www.imdb.com, collected by Maas 4 As the"
S15-2092,S13-2053,0,0.126003,"lligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMer"
S15-2092,S13-2052,0,0.0353029,"g posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEU TRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014)"
S15-2092,S14-2009,0,0.0181682,"ion randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets. 1 Introduction The rise of social media such as blogs and microblogs (e.g., Twitter) has fueled interest in sentiment analysis (Liu, 2012; Pang and Lee, 2008). One of the most popular settings for carrying out sentiment analysis is at the sentence level or over individual micro-blog posts, using the simple threelabel class set of POSITIVE, NEGATIVE and NEU TRAL (Liu, 2012; Pang and Lee, 2008; Rosenthal et al., 2014). Sentiment classification has been shown to have utility in various business intelligence applications, including product marketing, identifying new business opportunities, and managing a company’s reputation (Liu, 2012; Pang and Lee, 2008). Learning effective features plays an important role in building sentiment classification systems (Liu, 2012; Pang and Lee, 2008). For example, the winning system in the SemEval-2013 message polarity classification task (Nakov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, pun"
S15-2092,D13-1170,0,0.00458167,"al SemEval2015 Task 10, subtask B dataset, comprised of tweets which have been hand-labelled for sentiment at the message-level (in terms of POSITIVE, NEGA TIVE and NEUTRAL sentiment). The dataset is partitioned into three components, as detailed in Table 1:4 (1) training set, (2) development set, and (3) test set. Stanford Sentiment Treebank Dataset: a collection of movie review documents from www. rottentomatoes.com, which have been sentence tokenised and annotated for sentiment at the sentence level (Maas et al., 2011) and prepartitioned into training and test data, as detailed in Table 2. Socher et al. (2013) additionally annotated the data at the phrase and lexical levels, but we use only the sentence-level annotations in this paper. 3.1.2 Unlabelled Datasets Twitter Dataset: a random sample of 10M English tweets from a 5.3TB Twitter dataset crawled from 18 June to 4 Dec, 2014 using the Twitter Trending API. This is used as additional data to pre-train the message-level embeddings for the SemEval2015 Dataset. IMDB Dataset: a 100K sentence movie review dataset from www.imdb.com, collected by Maas 4 As the labels have not been released for the progress test set, we omit this from the table. et al."
S15-2092,S14-2033,0,0.0192202,"akov et al., 2013) was based on a rich set of hand-tuned features such as word-sentiment association lexicon features, word n-grams, punctuation, and emoticons, which were combined using a simple SVM-based classifier (Mohammad et al., 2013). Recently, there has been a surge of interest in representation learning — automatically learning word and document representations, often in the form of continuous-valued vectors or “embeddings” — using auto-encoders or neural network language models (Mikolov et al., 2013; Le and Mikolov, 2014). Of particular relevance to message-level sentiment analysis, Tang et al. (2014) proposed a deep learning approach to learn sentiment-specific word representation features, and Le and Mikolov (2014) proposed a neural network auto-encoder to learn message-level vectors. In this paper, we detail RoseMerry, a (strong) baseline sentiment analysis method that combines hand-crafted features with message-level1 embeddings generated by doc2vec (Le and Mikolov, 2014), using a linear-kernel SVM. 2 The Proposed Method The proposed method combines a set of handcrafted features with automatically-generated message-level representation features. The features are concatenated into a com"
S16-1027,D14-1181,0,0.0331224,"Missing"
S16-1027,S16-1001,0,0.119226,"Missing"
S16-1027,S15-2079,0,0.0683248,"Missing"
S16-1027,P15-2008,0,0.0624019,"Missing"
S16-1131,W05-0909,0,0.152687,"ranslation evaluation measures as extra features before applying the final classifier. Machine translation evaluation measures are designed to detect whether two sentences have a similar meaning or not. They have query questions archived questions Training Set 267 2,669 Development Set 50 500 Test Set 70 700 Table 1: The number of query and archived question pairs in the SemEval-2016 dataset been shown to be useful features for paraphrase identification (Madnani et al., 2012), a task very similar to ours. The measures we used were BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), Ter (Snover et al., 2006), Ter-Plus (Snover et al., 2009), and MaxSim (Chan and Ng, 2008). After adding these additional features to the sentence embedding similarity features, we fed our feature vectors into a Multiple Layer Perceptron (MLP) classifier to get a final similarity score. 2.3.3 Training the model In this section we will explain how we trained our model. Our CNN consisted of three convolutional layers, with a MLP classifier at the top. The MLP combined three fully-connected hidden layers, which contained 512 nodes each and ReLU as its activation function, with a softmax layer on"
S16-1131,P06-4018,0,0.0104741,"on). In this section we will describe the details of the three classifiers. 2.1 String Similarity Features (SS1) Our first set of features consisted of string similarity measures, which we selected based on our recent success in applying these features to measure the compositionality of multiword expressions (Salehi and Cook, 2013), to estimate semantic textual similarity (Gella et al., 2013), and to detect cross-lingual textual entailment (Graham et al., 2013). To measure the string similarity between two questions, the titles and the descriptions of the questions were lemmatized using NLTK (Bird, 2006). 851 Proceedings of SemEval-2016, pages 851–856, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics We used two string similarity measures in this study: longest common substring and the Smith-Waterman algorithm.2 The output of each measure was normalized to the range of [0, 1], where 0 indicated that the questions were completely different, while 1 showed that they were identical. More details on the string similarity measures and how we normalise the scores are described in Salehi and Cook (2013). Our primary experiments showed that measuring string si"
S16-1131,P08-1007,0,0.0256217,"nslation evaluation measures are designed to detect whether two sentences have a similar meaning or not. They have query questions archived questions Training Set 267 2,669 Development Set 50 500 Test Set 70 700 Table 1: The number of query and archived question pairs in the SemEval-2016 dataset been shown to be useful features for paraphrase identification (Madnani et al., 2012), a task very similar to ours. The measures we used were BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), Ter (Snover et al., 2006), Ter-Plus (Snover et al., 2009), and MaxSim (Chan and Ng, 2008). After adding these additional features to the sentence embedding similarity features, we fed our feature vectors into a Multiple Layer Perceptron (MLP) classifier to get a final similarity score. 2.3.3 Training the model In this section we will explain how we trained our model. Our CNN consisted of three convolutional layers, with a MLP classifier at the top. The MLP combined three fully-connected hidden layers, which contained 512 nodes each and ReLU as its activation function, with a softmax layer on the top. For the network training, we used AdaDelta (Zeiler, 2012) to update the weights o"
S16-1131,S13-1030,1,0.857439,"ry question, it was considered to be relevant. The candidate questions were then ranked according to this judgement, with the relevant ones on top (in any order, since this was not taken into account in the official evaluation). In this section we will describe the details of the three classifiers. 2.1 String Similarity Features (SS1) Our first set of features consisted of string similarity measures, which we selected based on our recent success in applying these features to measure the compositionality of multiword expressions (Salehi and Cook, 2013), to estimate semantic textual similarity (Gella et al., 2013), and to detect cross-lingual textual entailment (Graham et al., 2013). To measure the string similarity between two questions, the titles and the descriptions of the questions were lemmatized using NLTK (Bird, 2006). 851 Proceedings of SemEval-2016, pages 851–856, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics We used two string similarity measures in this study: longest common substring and the Smith-Waterman algorithm.2 The output of each measure was normalized to the range of [0, 1], where 0 indicated that the questions were completely different,"
S16-1131,S13-2024,1,0.839261,"s were then ranked according to this judgement, with the relevant ones on top (in any order, since this was not taken into account in the official evaluation). In this section we will describe the details of the three classifiers. 2.1 String Similarity Features (SS1) Our first set of features consisted of string similarity measures, which we selected based on our recent success in applying these features to measure the compositionality of multiword expressions (Salehi and Cook, 2013), to estimate semantic textual similarity (Gella et al., 2013), and to detect cross-lingual textual entailment (Graham et al., 2013). To measure the string similarity between two questions, the titles and the descriptions of the questions were lemmatized using NLTK (Bird, 2006). 851 Proceedings of SemEval-2016, pages 851–856, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics We used two string similarity measures in this study: longest common substring and the Smith-Waterman algorithm.2 The output of each measure was normalized to the range of [0, 1], where 0 indicated that the questions were completely different, while 1 showed that they were identical. More details on the string si"
S16-1131,P14-1062,0,0.0741916,"the question. In contrast to our first classifier, no stemming or lemmatization was applied because it was found not to make any difference. The classifier we used was naive Bayes.4 2.3 Convolutional Neural Network (CNN) The third classifier we used was a convolutional neural network (“CNN”). CNN structures have been shown to be very successful in speech recognition and computer vision tasks (Graves et al., 2013; Krizhevsky et al., 2012). Recently, they have also been applied to natural language tasks, and again, have achieved good results (Collobert and Weston, 2008; Yin and Sch¨utze, 2015). Kalchbrenner et al. (2014) developed a CNNbased model that can be used for sentence modelling 2 We also experimented with Levenshtein and modified Levenshtein, but we did not observe any improvement when using these features. 3 http://scikit-learn.org 4 We used the version implemented in the Weka toolkit: http://www.cs.waikato.ac.nz/ml/weka/, using the default settings. 852 problems. With several combinations of convolutional filters and dynamic k-max pooling filters, the model is very good at capturing features on both the local word level and the global sentence level. The word-level features are combined in several"
S16-1131,N12-1019,0,0.0176771,"that formed part of the input to our final classifier. Apart from the sentence embedding similarity features, we also used several machine translation evaluation measures as extra features before applying the final classifier. Machine translation evaluation measures are designed to detect whether two sentences have a similar meaning or not. They have query questions archived questions Training Set 267 2,669 Development Set 50 500 Test Set 70 700 Table 1: The number of query and archived question pairs in the SemEval-2016 dataset been shown to be useful features for paraphrase identification (Madnani et al., 2012), a task very similar to ours. The measures we used were BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), Ter (Snover et al., 2006), Ter-Plus (Snover et al., 2009), and MaxSim (Chan and Ng, 2008). After adding these additional features to the sentence embedding similarity features, we fed our feature vectors into a Multiple Layer Perceptron (MLP) classifier to get a final similarity score. 2.3.3 Training the model In this section we will explain how we trained our model. Our CNN consisted of three convolutional layers, with a MLP classifier at the top."
S16-1131,S16-1083,0,0.0648801,"Missing"
S16-1131,P02-1040,0,0.0948213,"dding similarity features, we also used several machine translation evaluation measures as extra features before applying the final classifier. Machine translation evaluation measures are designed to detect whether two sentences have a similar meaning or not. They have query questions archived questions Training Set 267 2,669 Development Set 50 500 Test Set 70 700 Table 1: The number of query and archived question pairs in the SemEval-2016 dataset been shown to be useful features for paraphrase identification (Madnani et al., 2012), a task very similar to ours. The measures we used were BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), Ter (Snover et al., 2006), Ter-Plus (Snover et al., 2009), and MaxSim (Chan and Ng, 2008). After adding these additional features to the sentence embedding similarity features, we fed our feature vectors into a Multiple Layer Perceptron (MLP) classifier to get a final similarity score. 2.3.3 Training the model In this section we will explain how we trained our model. Our CNN consisted of three convolutional layers, with a MLP classifier at the top. The MLP combined three fully-connected hidden layers, which contained 512 nodes each"
S16-1131,S13-1039,1,0.842093,"ree classifiers considered a candidate question relevant to a query question, it was considered to be relevant. The candidate questions were then ranked according to this judgement, with the relevant ones on top (in any order, since this was not taken into account in the official evaluation). In this section we will describe the details of the three classifiers. 2.1 String Similarity Features (SS1) Our first set of features consisted of string similarity measures, which we selected based on our recent success in applying these features to measure the compositionality of multiword expressions (Salehi and Cook, 2013), to estimate semantic textual similarity (Gella et al., 2013), and to detect cross-lingual textual entailment (Graham et al., 2013). To measure the string similarity between two questions, the titles and the descriptions of the questions were lemmatized using NLTK (Bird, 2006). 851 Proceedings of SemEval-2016, pages 851–856, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics We used two string similarity measures in this study: longest common substring and the Smith-Waterman algorithm.2 The output of each measure was normalized to the range of [0, 1], wh"
S16-1131,2006.amta-papers.25,0,0.0336878,"s extra features before applying the final classifier. Machine translation evaluation measures are designed to detect whether two sentences have a similar meaning or not. They have query questions archived questions Training Set 267 2,669 Development Set 50 500 Test Set 70 700 Table 1: The number of query and archived question pairs in the SemEval-2016 dataset been shown to be useful features for paraphrase identification (Madnani et al., 2012), a task very similar to ours. The measures we used were BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), Ter (Snover et al., 2006), Ter-Plus (Snover et al., 2009), and MaxSim (Chan and Ng, 2008). After adding these additional features to the sentence embedding similarity features, we fed our feature vectors into a Multiple Layer Perceptron (MLP) classifier to get a final similarity score. 2.3.3 Training the model In this section we will explain how we trained our model. Our CNN consisted of three convolutional layers, with a MLP classifier at the top. The MLP combined three fully-connected hidden layers, which contained 512 nodes each and ReLU as its activation function, with a softmax layer on the top. For the network t"
S16-1131,N15-1091,0,0.0313005,"Missing"
S16-1145,S14-2082,0,0.0716102,"Missing"
S16-1145,N13-1090,0,0.0935626,"Missing"
S16-1145,K15-1009,1,0.887877,"Missing"
S16-1145,N15-1099,1,0.882823,"Missing"
S16-1145,P12-2050,0,0.277402,"Missing"
S16-1145,E09-1087,0,\N,Missing
S16-1145,P16-1158,1,\N,Missing
S16-1150,C14-1205,1,0.896232,"who study academic English, or operate in an academic environment, have a different vocabulary exposure to those who specialize in hospitality English. Therefore, there is value in not only trying to predict some prior difficulty of a word, but also trying to generalize across readers in a similar cohort. Task 11 of the 2016 SemEval competition (Complex Word Identification) is aimed at addressing this challenge. This paper describes our system for the task. We commenced with previous work in word readability scoring (Brooke et al., 2012) and stylistic lexicon creation (Brooke and Hirst, 2013; Brooke and Hirst, 2014). For features, we drew on a diverse set of corpus-based and human-derived metrics, and built a random forest-based classifier. While a mistake related to the proper distribution of complex versus simple words prevented us from scoring amongst the top teams in either of the evaluations metrics used in this task, we show that by appropriate class weighting with the same classifier and features, we can obtain results on either metric that are competitive or better than the best teams. 2 Background The motivation for SemEval 2016 Task 11 was the need to automatically identify complex 975 Proceedi"
S16-1150,C10-2011,1,0.717443,"lexicon built from the ICWSM corpus in Brooke and Hirst (2014). We believe the information in it overlaps considerably with the 6-style lexicon. Readability rank The readability rank of words as given by the model from Brooke et al. (2012). Complexity lexicon Using the words from the training set, we attempted to build a complexity dictionary using the method of Brooke and Hirst (2014). The results were not competitive when the six-style lexicon was included. Latinate affixes A boolean feature which indicates the presence or absence of a Latinate affix, which can indicate increased formality (Brooke et al., 2010). Number of Senses The number of senses of the lemma of the word in WordNet (Fellbaum, 1998). Hyphen fix For hyphenated words, derive all other statistics using the first word in the hyphenation, instead of the whole word. Bigram style lexicon The features we use do not distinguish between the word type in different contexts, so we cannot distinguish between word senses. We saw examples of this in the data, such as the word tried in the legal sense in was tried for murder. We made an initial attempt at integrating this information by building a bigram style lexicon, using the same method as th"
S16-1150,W12-2205,1,0.924131,"may be entirely idiosyncratic to the reader in question. For example, those who study academic English, or operate in an academic environment, have a different vocabulary exposure to those who specialize in hospitality English. Therefore, there is value in not only trying to predict some prior difficulty of a word, but also trying to generalize across readers in a similar cohort. Task 11 of the 2016 SemEval competition (Complex Word Identification) is aimed at addressing this challenge. This paper describes our system for the task. We commenced with previous work in word readability scoring (Brooke et al., 2012) and stylistic lexicon creation (Brooke and Hirst, 2013; Brooke and Hirst, 2014). For features, we drew on a diverse set of corpus-based and human-derived metrics, and built a random forest-based classifier. While a mistake related to the proper distribution of complex versus simple words prevented us from scoring amongst the top teams in either of the evaluations metrics used in this task, we show that by appropriate class weighting with the same classifier and features, we can obtain results on either metric that are competitive or better than the best teams. 2 Background The motivation for"
S16-1150,W15-0705,1,0.834364,"e are too many features (and too many combinations) to offer up individual numerical analysis of what worked and what didn’t. Our features were selected by optimizing G-score (see Section 6) with a 20-fold cross-validation of the training set. 4.1 Major Features Term frequency statistics We collected term frequency statistics from four large corpora: the British National Corpus (“BNC”: Burnard (2000)), the Gigaword corpus (Graff and Cieri, 2003), the International Conference on Web and Social Media (ICWSM) blog corpus (Burton et al., 2009), and Project Gutenberg (read using the GutenTag tool (Brooke et al., 2015)). We consciously chose corpora that had significant variety with respect to their genre, with the intent of allowing the classifier to focus in on particular kinds of words that certain groups might have trouble with. Note that we typically used the count for the specific word type, but where it didn’t exist in the corpus we substituted the lemma count, rather than giving a count of zero. All of the corpora were of benefit to the final model. Six style lexicons For the ICWSM and the Project Gutenberg corpora, we built lexicons for six lexical styles using the co-occurrence information in thes"
S16-1150,N04-1025,0,0.138828,"task of automated text simplification (Shardlow, 2014). However, the modelling of word complexity and text complexity has a long history, much of it using the term readability, and intended for finding reading material of an appropriate level of difficulty for children and language learners (Chall and Dale, 1995). The measurement of word readability, or lexical complexity, is a fundamental component for a range of techniques and their applications beyond automated text simplification: text readability measurement is used as a basis for automatically recommending reading to language learners (Collins-Thompson and Callan, 2004a); lexical complexity measurement can also allow the automatic glossing of reading material presented electronically (Walmsley, 2011), or the display of text comprising a mixture of two languages (Uitdenbogerd, 2014). Early work on readability resulted in a large number of measures being developed, typically based on tests given to school-aged native speakers of English (Klare, 1974). The majority of these measures had two recognizable components: grammatical difficulty and word difficulty. The word difficulty component of the measures had the following varieties: inclusion in a list of gener"
S16-1150,E09-3003,0,0.0636844,"Missing"
S16-1150,P13-3015,0,0.153463,"a mistake related to the proper distribution of complex versus simple words prevented us from scoring amongst the top teams in either of the evaluations metrics used in this task, we show that by appropriate class weighting with the same classifier and features, we can obtain results on either metric that are competitive or better than the best teams. 2 Background The motivation for SemEval 2016 Task 11 was the need to automatically identify complex 975 Proceedings of SemEval-2016, pages 975–981, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics words (Shardlow, 2013) for the task of automated text simplification (Shardlow, 2014). However, the modelling of word complexity and text complexity has a long history, much of it using the term readability, and intended for finding reading material of an appropriate level of difficulty for children and language learners (Chall and Dale, 1995). The measurement of word readability, or lexical complexity, is a fundamental component for a range of techniques and their applications beyond automated text simplification: text readability measurement is used as a basis for automatically recommending reading to language le"
S16-1150,I13-1010,1,\N,Missing
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
S19-1030,P15-1162,0,0.0499894,"Missing"
S19-1030,J96-2004,0,0.691381,"Missing"
S19-1030,D09-1130,0,0.0313251,"ed to significant advances in the field of computational social science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). With the increasing availability of datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). One task of particular importance is the analysis of the functional 274 pensate for the sparsity of labeled data is semisupervised learning, making use of auxiliary unlabeled data, as done previously for speech act classification in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framew"
S19-1030,P16-1165,0,0.0238446,"eases in the lead-up to a federal election, where we expect there to be rich discourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on bag-of-words models (Qadir and Riloff, 2011; Vosoughi and Roy, 2016), although recent approaches have used deep-learning models through data augmentation (Joty and Hoque, 2016) and learning word representations for the target domain (Joty and Mohiuddin, 2018), outperforming traditional bag-ofwords approaches. Another technique that has been applied to comical text is relatively new. Most speech act analyses in the political domain have relied exclusively on manual annotation, and no labeled data has been made available for training classifiers. As it is expensive to obtain large-scale annotations, in addition to developing a novel annotated dataset, we also experiment with a semi-supervised approach by utilizing unlabeled text, which is easy to obtain. The contribut"
S19-1030,J18-4012,0,0.0795045,"scourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on bag-of-words models (Qadir and Riloff, 2011; Vosoughi and Roy, 2016), although recent approaches have used deep-learning models through data augmentation (Joty and Hoque, 2016) and learning word representations for the target domain (Joty and Mohiuddin, 2018), outperforming traditional bag-ofwords approaches. Another technique that has been applied to comical text is relatively new. Most speech act analyses in the political domain have relied exclusively on manual annotation, and no labeled data has been made available for training classifiers. As it is expensive to obtain large-scale annotations, in addition to developing a novel annotated dataset, we also experiment with a semi-supervised approach by utilizing unlabeled text, which is easy to obtain. The contributions of this paper are as follows: (1) we introduce the novel task of target based"
S19-1030,I11-1068,0,0.484793,"its to a future course of action. Following the work of Artés (2011) and Naurin (2011), we distinguish between action and outcome commissives. Action commissives (commissive-action) are those in which an action is to be taken, while outcome commissives (commissive-outcome) can be defined as a description of reality or goals. Secondly, similar to Naurin (2014) we also classify action commissives into vague (commissive-action-vague) and specific (commissive-action-specific), according to their specificity. This distinction is also related to text specificity analysis work addressed in the news (Louis and Nenkova, 2011) and classroom discussion (Lugini and Litman, 2017) domains. A directive occurs when the speaker expects the listener to take action in response. In an expressive, the speaker expresses their psychological state, while a past-action denotes a retrospective action of the target party, and a verdictive refers to an assessment on prospective or retrospective actions. Examples of the eight speech act classes are Further, the following (from the Labor party) has segments comparing L ABOR and L IBERAL: (3) Our party is united – the Liberals are not united. 4 Election Campaign Dataset We collected me"
S19-1030,W17-5006,0,0.159028,"k of Artés (2011) and Naurin (2011), we distinguish between action and outcome commissives. Action commissives (commissive-action) are those in which an action is to be taken, while outcome commissives (commissive-outcome) can be defined as a description of reality or goals. Secondly, similar to Naurin (2014) we also classify action commissives into vague (commissive-action-vague) and specific (commissive-action-specific), according to their specificity. This distinction is also related to text specificity analysis work addressed in the news (Louis and Nenkova, 2011) and classroom discussion (Lugini and Litman, 2017) domains. A directive occurs when the speaker expects the listener to take action in response. In an expressive, the speaker expresses their psychological state, while a past-action denotes a retrospective action of the target party, and a verdictive refers to an assessment on prospective or retrospective actions. Examples of the eight speech act classes are Further, the following (from the Labor party) has segments comparing L ABOR and L IBERAL: (3) Our party is united – the Liberals are not united. 4 Election Campaign Dataset We collected media releases and speeches from the two major Austra"
S19-1030,W18-5214,0,0.0158292,"et party classes. “Speaker” denotes the party making the utterance. intent of utterances in political text. Though it has received notable attention from many political scientists (see Section 1), the primary focus of almost all work has been to derive insights from manual annotations, and not to study computational approaches to automate the task. Another related task in the political communication domain is reputation defense, in terms of party credibility. Recently, Duthie and Budzynska (2018) proposed an approach to mine ethos support/attack statements from UK parliamentary debates, while Naderi and Hirst (2018) focused on classifying sentences from Question Time in the Canadian parliament as defensive or not. In this work, our source data is speeches and press releases in the lead-up to a federal election, where we expect there to be rich discourse and interplay between political parties. Speech act theory is fundamental to study such discourse and pragmatics (Austin, 1962; Searle, 1976). A speech act is an illocutionary act of conversation and reflects shallow discourse structures of language. Due to its predominantly small-data setting, speech act classification approaches have generally relied on"
S19-1030,W12-0603,0,0.0251888,"ances in the field of computational social science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). With the increasing availability of datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). One task of particular importance is the analysis of the functional 274 pensate for the sparsity of labeled data is semisupervised learning, making use of auxiliary unlabeled data, as done previously for speech act classification in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framework (Clark et al., 20"
S19-1030,I17-1071,0,0.150769,"al party referred to in the text — in order to determine the discourse structure. Here, we study the effect of jointly modeling the speech act and target referent of each utterance, in order to exploit the task dependencies. That is, this paper is an application of discourse analysis to the pragmaticsrich domain of political science, to determine the intent of every utterance made by politicians, and in part, automatically extract pledges at varying levels of specificity from campaign speeches and press releases. We assume that each utterance is associated with a unique speech act (similar to Zhao and Kawahara (2017)) and target party,1 meaning that a sentence with multiple speech acts and/or targets must be segmented into component utterances. Take the following example, from the Labor Party: We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domainspecific speech acts, such as commissiveaction, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classific"
S19-1030,D14-1162,0,0.0831537,"Missing"
S19-1030,N18-1202,0,0.188242,"ication in e-mail and forum text (Jeong et al., 2009). Zhang et al. (2012) also used semisupervised methods for speech act classification over Twitter data. They used transductive SVM and graph-based label propagation approaches to annotate unlabeled data using a small seed training set. Joty and Mohiuddin (2018) leveraged outof-domain labeled data based on a domain adversarial learning approach. In this work, we focus on target based speech act analysis (with a custom class-set) for political campaign text and use a deep-learning approach by incorporating contextualized word representations (Peters et al., 2018) and a cross-view training framework (Clark et al., 2018) to leverage in-domain unlabeled text. 3 # Doc # Sent # Utt Avg Utterance Length 258 6609 7641 19.3 Table 2: Dataset Statistics: number of documents, number of sentences, number of utterances, and average utterance length given in Table 1, along with the target party (L ABOR, L IBERAL, or N ONE), indicating which party the speech act is directed towards, and the “speaker” party making the utterance (information which is provided for every utterance). 3.1 Utterance Segmentation Sentences are segmented both in the context of speech act and"
S19-2231,P82-1020,0,0.819572,"Missing"
S19-2231,D14-1162,0,0.0833269,"-Attn Table part Rule-based table detection We use ELMo (Peters et al., 2018) word representations in this paper, which are learned from the internal states of a deep bidirectional language model (biLM), pre-trained on a large text corpus. ELMo representations are purely characterbased, allowing the network to use morphological clues to form robust representations for outof-vocabulary tokens unseen in training. They are also robust to syntactic disfluencies caused by the fine-grainedness of word segmentation. For the purposes of empirical comparison, we also report on experiments using GloVe (Pennington et al., 2014) embeddings. LSTM-Attn Non-table part Input Figure 1: Toponym detection workflow 2.3 terms of syntactic structure. As such, training a single text embedding model over both the main body of text and tables will likely lead to suboptimal representations, leading us to train separate models for: (1) tables, and (2) the remainder of the text content of the paper. To extract tables from the plain text dump provided by the shared task organisers, we use a rule-based table detection method. We first tokenize the entire article, as part of which we treat all punctuation as a separator. In the process"
S19-2231,N18-1202,0,0.285452,"10). This paper describes our system entry to the Toponym resolution in scientific paper task of SemEval 2019 (Weissenbacher et al., 2019). The task consists of three subtasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution. For the toponym detection task, we extract tables from the full text and train separate BiLSTMATTN models for each. For tables, the model captures the horizontal row-wise structure of the table. For non-table content, the model can capture syntactic and semantic features. In both cases, we use a deep contextualized word representation — ELMo (Peters et al., 2018) — to represent each • we show that contextualized word representation boosts performance; • we show that auxiliary organization name recognition model is helpful for toponym detection, and better than training a single named entity recognizer (NER). 2 Toponym Detection Figure 1 shows our workflow on the toponym detection task, which consist of 4 parts: (a) preprocessing, which contains tokenization, table extraction and sentence segmentation; (b) training and inference for the toponym detection model; (c) post-processing, to combine detected words into toponyms; and (d) refinement of the resu"
S19-2231,P15-2104,1,0.859015,"F1 score. For toponym disambiguation and end-to-end resolution, we officially ranked 2nd and 3rd, respectively. 1 • we show that training separate models for table and non-table portions of the paper is better than simply training one model over the full text; Introduction Toponym resolution (TR) refers to the task of automatically assigning geographic references to place names in text, which has applications in question answering and information retrieval tasks (Leidner, 2008; Daoud and Huang, 2013; Vasardani et al., 2013), user geolocation prediction (Roller et al., 2012; Han et al., 2014; Rahimi et al., 2015), and historical research (Grover et al., 2010). This paper describes our system entry to the Toponym resolution in scientific paper task of SemEval 2019 (Weissenbacher et al., 2019). The task consists of three subtasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution. For the toponym detection task, we extract tables from the full text and train separate BiLSTMATTN models for each. For tables, the model captures the horizontal row-wise structure of the table. For non-table content, the model can capture syntactic and semantic features. In both cases, we use a deep"
S19-2231,D12-1137,0,0.0376515,"subtask, ranking 2nd out of 8 teams on F1 score. For toponym disambiguation and end-to-end resolution, we officially ranked 2nd and 3rd, respectively. 1 • we show that training separate models for table and non-table portions of the paper is better than simply training one model over the full text; Introduction Toponym resolution (TR) refers to the task of automatically assigning geographic references to place names in text, which has applications in question answering and information retrieval tasks (Leidner, 2008; Daoud and Huang, 2013; Vasardani et al., 2013), user geolocation prediction (Roller et al., 2012; Han et al., 2014; Rahimi et al., 2015), and historical research (Grover et al., 2010). This paper describes our system entry to the Toponym resolution in scientific paper task of SemEval 2019 (Weissenbacher et al., 2019). The task consists of three subtasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution. For the toponym detection task, we extract tables from the full text and train separate BiLSTMATTN models for each. For tables, the model captures the horizontal row-wise structure of the table. For non-table content, the model can capture syntactic and semanti"
S19-2231,S19-2155,0,0.114313,"Missing"
U03-1007,P03-1059,1,0.551708,"a lossy mapping. Our aim in this is to determine the most effective fast-track solution when faced with a novel task in a given language for which high-quality annotated data exists in a closely-related language. We illustrate this issue by way of a type-level noun countability classification task in Dutch for which we have moderate amounts of high-quality annotated data in English and large amounts of medium-quality annotated data in Dutch (see §2.4). For English, previous research has shown that corpus evidence can be applied successfully to classify unannotated noun types for countability (Baldwin and Bond, 2003a; Baldwin and Bond, 2003b). We extend this research to Dutch and address the question of which of high-quality out-of-language English data and lower-quality in-language Dutch data produces the best Dutch countability classification results, realising that the feature mapping from English-to-Dutch in the first case will necessarily be lossy. We treat noun countability as a lexical property that determines determiner co-occurrence, the ability to pluralise, and enumeration effects. Each Dutch noun type is classified as being countable and/or uncountable, noting that different senses/usages of"
U03-1007,W03-1010,1,0.546376,"a lossy mapping. Our aim in this is to determine the most effective fast-track solution when faced with a novel task in a given language for which high-quality annotated data exists in a closely-related language. We illustrate this issue by way of a type-level noun countability classification task in Dutch for which we have moderate amounts of high-quality annotated data in English and large amounts of medium-quality annotated data in Dutch (see §2.4). For English, previous research has shown that corpus evidence can be applied successfully to classify unannotated noun types for countability (Baldwin and Bond, 2003a; Baldwin and Bond, 2003b). We extend this research to Dutch and address the question of which of high-quality out-of-language English data and lower-quality in-language Dutch data produces the best Dutch countability classification results, realising that the feature mapping from English-to-Dutch in the first case will necessarily be lossy. We treat noun countability as a lexical property that determines determiner co-occurrence, the ability to pluralise, and enumeration effects. Each Dutch noun type is classified as being countable and/or uncountable, noting that different senses/usages of"
U03-1007,P01-1005,0,0.0218518,"is of word-to-word or featureto-feature mappings between English and Dutch performs at least as well as in-language classification based on gold-standard Dutch countability data. 1 Introduction The performance of supervised learning methods is conditioned on the quality of annotation and also volume of training data (Hastie et al., 2001). This effect is felt particularly keenly in tasks of high feature dimensionality or low feature–class correlation. In many cases, high-quality data is not available in large quantities, but a large volume of lower-quality data can be accessed (Mitchell, 1999; Banko and Brill, 2001). Alternatively, high-quality data may exist for some parallel task which can be adapted to the task at hand through a lossy feature mapping. This strategy has been adopted successfully in NLP applications such as part-of-speech tagging involving languages with a relative paucity of language resources or annotated data (Yarowsky et al., 2001; Cucerzan and Yarowsky, 2002). This paper takes a supervised learning task and contrasts the use of a restricted volume of inlanguage training data with the use of a larger volume of out-of-language training data adapted to the task through a lossy mapping"
U03-1007,briscoe-carroll-2002-robust,0,0.0128856,"e a variety of pre-processors to map the raw data onto the types of constructions targeted in the feature clusters, namely a POS tagger and a full-text chunker for both English and Dutch, and additionally a dependency parser for English. For Dutch, POS tags, lemmata and chunk data were extracted from automatically generated, fully parsed Alpino output (Bouma et al., 2000). For English, we used a custom-built fnTBL-based tagger (Ngai and Florian, 2001) with the Penn tagset, morph (Minnen et al., 2001) as our lemmatiser, an fnTBL-based chunker which runs over the output of the tagger, and RASP (Briscoe and Carroll, 2002) as the dependency parser. These data sets are then used independently to test the efficacy of the different systems at capturing features used in the classification process, or in tandem to consolidate the strengths of the individual methods and reduce system-specific idiosyncrasies in the feature values. When combining Dutch and English in classification, we invariably combine like systems (e.g. Dutch POS data with English POS data). The English data was extracted from the written component of the British National Corpus (90m words: Burnard (2000)), and the Dutch data from the newspaper comp"
U03-1007,W02-2006,0,0.0151959,"ularly keenly in tasks of high feature dimensionality or low feature–class correlation. In many cases, high-quality data is not available in large quantities, but a large volume of lower-quality data can be accessed (Mitchell, 1999; Banko and Brill, 2001). Alternatively, high-quality data may exist for some parallel task which can be adapted to the task at hand through a lossy feature mapping. This strategy has been adopted successfully in NLP applications such as part-of-speech tagging involving languages with a relative paucity of language resources or annotated data (Yarowsky et al., 2001; Cucerzan and Yarowsky, 2002). This paper takes a supervised learning task and contrasts the use of a restricted volume of inlanguage training data with the use of a larger volume of out-of-language training data adapted to the task through a lossy mapping. Our aim in this is to determine the most effective fast-track solution when faced with a novel task in a given language for which high-quality annotated data exists in a closely-related language. We illustrate this issue by way of a type-level noun countability classification task in Dutch for which we have moderate amounts of high-quality annotated data in English and"
U03-1007,J01-2002,0,0.0730074,"Missing"
U03-1007,W02-0811,0,0.0248377,"(3) ENBIN (featDET ) is based on only aligned determiner features, plus the aligned cluster totals; (4) ENBIN (featPREP ) is based on only aligned preposition features, plus the aligned cluster totals; and (5) ENBIN (featPRON ) is based on only aligned pronoun features, plus the aligned cluster totals.8 3.3 System combination System combination takes the outputs of heterogeneous classifiers and makes a consolidated classification based upon them. It has been shown to be effective in tasks ranging from word sense disambiguation to tagging in consolidating the performance of component systems (Klein et al., 2002; van Halteren et al., 2001). In our case, we first take the outputs of all unsupervised (i.e. evidence-based) and crosslingual classifiers—a total of 12 classifiers—for each countability class (ENBIN (combined)). We test the effects of system classification by way of 10-fold cross-validation over the 196 annotated Dutch nouns. This provides an estimate of the classification performance we could expect over unannotated Dutch noun data using the 196 annotated nouns as our sole source of annotated Dutch data. We also test combining the outputs of the 12 unsupervised and crosslingual classifiers"
U03-1007,N01-1006,0,0.0136434,"in the two languages express the same concept (a quantity of something) and bring about the same restrictions with respect to countability. 2.3 Feature extraction We use a variety of pre-processors to map the raw data onto the types of constructions targeted in the feature clusters, namely a POS tagger and a full-text chunker for both English and Dutch, and additionally a dependency parser for English. For Dutch, POS tags, lemmata and chunk data were extracted from automatically generated, fully parsed Alpino output (Bouma et al., 2000). For English, we used a custom-built fnTBL-based tagger (Ngai and Florian, 2001) with the Penn tagset, morph (Minnen et al., 2001) as our lemmatiser, an fnTBL-based chunker which runs over the output of the tagger, and RASP (Briscoe and Carroll, 2002) as the dependency parser. These data sets are then used independently to test the efficacy of the different systems at capturing features used in the classification process, or in tandem to consolidate the strengths of the individual methods and reduce system-specific idiosyncrasies in the feature values. When combining Dutch and English in classification, we invariably combine like systems (e.g. Dutch POS data with English"
U03-1007,H01-1035,0,0.0536249,"Missing"
U03-1007,S01-1021,0,\N,Missing
U05-1008,P98-1029,0,0.0457294,"Missing"
U05-1008,J95-4004,0,0.0956766,"me diverse algorithms which have been applied to POS tagging; Section 3 gives some motivation for attempting increases in accuracy; Section 4 describes details of the tagset used in the Penn Treebank; Section 5 outlines our method; in Section 6 we show results for various strategies and in Section 7 we discuss further work. 2 Tagging Algorithms A large number of algorithms have been applied to POS tagging; a brief treatment of those which are relevant follows. 2.1 Transformation-Based POS tagging The transformation-based learning (TBL) paradigm as applied to POS tagging was first described in Brill (1995); like all of the taggers described here it is a corpus-based method. In the learning phase, a TBL tagger assigns each word the most-likely unigram tag from the training data, and generates a large set of possible transformational rules which Proceedings of the Australasian Language Technology Workshop 2005, pages 40–48, Sydney, Australia, December 2005. 40 map the unigram tagger assignments onto the gold-standard assignments, conditioned on contextual word and tag features. It iteratively selects from these the rule which minimizes the number of errors, and applies that rule to modify the ass"
U05-1008,E03-1009,0,0.0250956,"Results Baseline and Benchmark The benchmark results from running each of the publicly available taggers with the default or recommended parameter settings are shown in Table 1, with results over specific POSs in Table 2. For a point of comparison, we also applied a suite of naively conceived modifications to illustrate the effects of data sparseness. The idea is borrowed from POS induction, which involves determining word clusters (i.e. POSs) from unannotated data. The task here is similar except that we are looking for patterns of regularity within a particular POS, so the baseline used in Clark (2003) may be informative. To subdivide a part of speech into n subclasses, we assign each of the (n − 1) most frequently seen word tokens from the class into (n − 1) separate new classes and the remainder to a final subclass. In Table 3, we present the results for n = 2, 3, 4 over closed-class POSs of reasonable size, after training and tagging with fnTBL using the same broad indicators shown in Table 1. The best-performing modification from this selection, (i.e. for subdividing PRP, with n = 1) was additionally tested using SVMTool and the Stanford MaxEnt Tagger; these results are shown in Table 1"
U05-1008,gimenez-marquez-2004-svmtool,0,0.0416402,"Missing"
U05-1008,P03-1054,0,0.0508352,", are examples of tagsets which are highly tractable in computational terms, but of very little use linguistically, which perhaps serves to indicate that these requirements sometimes conflict. However, the aim here is to test whether there is always an inverse relationship between the two. A tagset which encodes more subtle distinctions is almost inevitably more useful in linguistic terms unless the additional distinctions are entirely random; here we will test whether the accuracy can be increased by certain carefully selected tagset subdivisions motivated by linguistic intuition. Indeed, in Klein and Manning (2003) it was demonstrated that a finer-grained set of category labels can markedly improve performance in the related application of parsing, by providing more contextual information upon which to base decisions in cases of ambiguity. This, along with the demonstration by Toutanova and Manning (2000) that there is potential to improve POS tagging performance by adding linguistically motivated features to the tagger suggests that it may be possible to apply an analogous version of Klein and Manning’s method to POS tagging. If we alter the tagset to encode more subtle distinctions within the word cla"
U05-1008,J93-2004,0,0.0254085,"erbs which is made in most other tagsets derived from the Brown tagset (Francis and Kuˇcera, 1979; Garside et al., 1987; Garside et al., 1997). Additionally the presence of syntactic informa1 For comparison with the Penn Treebank, where the ’s suffix is split from the host noun, this figure excludes 12 possesive variants of other tags such as NN$ 42 tion means that the traditional distinction between prepositions and subordinating conjunctions can be removed as it can be recovered from the phrasal category of the sibling (SBAR for a subordinating conjunction and NP for a preposition). However Marcus et al. (1993, p315) stress that all of this information is available to users of the corpus via additional sources: ...the lexical and syntactic recoverability inherent in the POS-tagged version of the Penn Treebank corpus allows end users to employ a much richer tagset than the small one described ... if the need arises. What is interesting here is that the tagset was not designed to differentiate all possible distributional differences when other information is available, but in examples of POS tagging in the literature, the tagset is invariably used in unaltered form despite the tagger having no explic"
U05-1008,N01-1006,0,0.0346779,"Missing"
U05-1008,W96-0213,0,0.25979,"Missing"
U05-1008,W00-1308,0,0.481228,"Missing"
U05-1008,C98-1029,0,\N,Missing
U05-1022,briscoe-carroll-2002-robust,0,0.148951,"Missing"
U05-1022,J96-2004,0,0.0144404,"Missing"
U05-1022,N03-1013,0,0.0729976,"002), who manually extracted and annotated a sample of 796 binary compound nominalisations out of about 170,000 candidates automatically identified in the BNC. In the original Lapata data, the underlying verb form of the head noun was uniquely identified using a combination of Celex (Burnage, 1990) and Nomlex (Macleod et al., 1998) data, sometimes resulting in sub-optimal results (e.g. the base verb of question is given as quest). In order to ameliorate such quirks in morphological analysis and expand the coverage of our method, we mined Celex and Nomlex, and also the word clusters in CatVar (Habash and Dorr, 2003) for morphologically-related noun– verb pairs. This culminated in a total of about 14,000 deverbal nouns, many of which are listed with multiple base verb forms (e.g. divination is listed as all of divine, divinise and divinize). To validate the data for consistency, we removed those nominalisations which were not associated with a context sentence in the data set, those which did not occur in the same chunk in that sentence, according to the chunker above, those for which we did not find a verbal form (e.g. decision-maker ), and those consisting of one or more proper nouns. We were left with"
U05-1022,J02-3004,0,0.635399,"em, but have no way of checking for direct derivational correspondence between a given noun and verb. As a result, the output of the filter tends to have excellent recall, but diminished precision. We combat this effect by additionally checking for the plausibility of a subject or object interpretation against corpus data and thresholding over the probability for each interpretation type. We evaluated the detection method by attempting to re-extract the gold standard twoway classification data from the BNC, and over our annotated data set for the three-way classification. 4.2 Paraphrase Tests Lapata (2002) and Grover et al. (2005) provide the usual statistical interpretation of a compound nominalisation: that of the most attested relation in corpus frequencies for the verb-argument pair. Other paraphrases are also used, however: as Lauer (1995) notes, the system by Leonard (1984) has paraphrasing as a goal, whereby mountain vista is interpreted via paraphrasing as vista of a mountain or mountains. Lauer himself also attempts to paraphrase compounds based on corpus statistics. We notice that the other direction is also productive: instances of vista of mountains occur in the corpus, and they sup"
U05-1022,W04-2609,0,0.0314621,"Missing"
U05-1022,N01-1006,0,0.104946,"Missing"
U05-1022,W01-0511,0,0.234107,"Missing"
U05-1022,C94-2125,0,0.679006,"Missing"
U05-1022,2003.mtsummit-systems.9,0,\N,Missing
U06-1011,P03-1059,1,0.778864,"gories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply it to a lexical type system (e.g. Carroll and Fang (2004) for verb subcategorisation frames), restricting possible target types according to evidence, and unifying to a consolidated entry (e.g. Fouvry (2003) for precision grammar lexical types), or applying the lexical category of similar in"
U06-1011,W05-1008,1,0.930574,"LPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property (e.g. noun countability, or subcategorisation properties); and generalised DLA, in which lexemes are classified according to the full range of lexical properties captured in a given DLR (e.g. the full range of lexical relations in a lexical ontology, or the system of lexical types in an HPSG). As we attest in Section 2, most work in deep lexical acquisition has focussed on the English language. This can be explained in part by the ready availability of tar"
U06-1011,W06-1620,1,0.808124,"es defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply i"
U06-1011,W02-1503,0,0.0351909,"osyntax, in order to create language resources in a language-independent manner. 1 Introduction As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main ca"
U06-1011,W04-3205,0,0.02577,". These results describe further scope in analysing other properties in languages displaying a more challenging morphosyntax, in order to create language resources in a language-independent manner. 1 Introduction As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic"
U06-1011,W02-2203,0,0.0183067,"m of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular propert"
U06-1011,copestake-flickinger-2000-open,0,0.0742019,"8 2 Background 2.1 Deep Lexical Acquisition As mentioned above, DLA traditionally takes two forms: targeted toward a specific lexical property, or generalised to map a term to an amalgam of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies var"
U06-1011,N03-1006,0,0.666612,"dictions of language users when confronted with unseen words.1 When contextual or lexicographic information is available for a language, this is usually a reliable method for the prediction of gender. Consequently, automatic prediction of gender in languages which have inflectional morphology is usually seen as the domain of the POS tagger (such as Haji˘c and Hladk´a (1998)), or morphological analyser (e.g. GERTWOL (Haapalainen and Majorin, 1995) for German and FLEMM (Namer, 2000) for French). One work in automatic gender prediction that is similar to this one is the bootstrapping approach of Cucerzan and Yarowsky (2003). Starting with a seed set of nouns whose gender is presumably language–invariant, they mine contextual features to hypothesise the gender of novel instances. They then extract simple morphological features of their larger predicted set, and use these to predict the gender of all nouns in their corpora. The major differences between this work and our own are in the approach Cucerzan and Yarowsky use, and the classes that they can handle. First, their semi-automatic approach relies on 1 See Tucker et al. (1977), among others, for detailed studies of L1 and L2 gender acquisition. 69 a bilingual"
U06-1011,P98-1080,0,0.0966055,"Missing"
U06-1011,hockenmaier-steedman-2002-acquiring,0,0.0133019,"otation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property (e.g. noun countability, or subcategorisation"
U06-1011,J05-1004,0,0.00912527,"on As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property ("
U06-1011,P05-1016,0,0.0130401,"of which we made use, and Section 4 details the feature set. Finally, we evaluate our method in Section 5, and supply a discussion and brief conclusion in Sections 6 and 7. 68 2 Background 2.1 Deep Lexical Acquisition As mentioned above, DLA traditionally takes two forms: targeted toward a specific lexical property, or generalised to map a term to an amalgam of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schul"
U06-1011,W04-2104,0,0.0144916,"hology or syntax. This corpus is heavily domain–specific, and the lack of annotation provides particular problems, which we explain below. Note that we make no use of the English component of BAF in this paper. 3.2 Inflectional Lexicons Whereas our German corpus has gold-standard judgements of gender for each token, the French corpus has no such information. Consequently, we use a semi-automatic method to match genders to nouns. Using the Lefff syntactic lexi2 3 http://www.ims.uni-stuttgart.de/projekte/TIGER http://rali.iro.umontreal.ca/Ressources/BAF con4 (Sagot et al., 2006) and Morphalou5 (Romary et al., 2004), a lexicon of inflected forms, we automatically annotate tokens for which the sources predict an unambiguous gender, and handannotate ambiguous tokens using contextual information. These ambiguous tokens are generally animate nouns like coll`egue, which are masculine or feminine according to their referent, or polysemous nouns like aide, whose gender depends on the applicable sense. 3.3 POS Taggers Again, TIGER comes annotated with handcorrected part-of-speech tags, while the BAF does not. For consistency, we tag both corpora with TreeTagger6 (Schmid, 1994), a decision tree– based probabilist"
U06-1011,sagot-etal-2006-lefff,0,0.056038,"Missing"
U06-1011,A92-1011,0,0.271756,"g, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply it to a lexical type system (e.g. Carroll and Fang (2004) for verb subcategorisation frames), restricting possible target types according to evidence, and unifying to a consolidated entry (e.g. Fouvry (2003) for precision grammar lexical types), or applying the lexical category of similar instances, based on some notion of similarity (e.g. Baldwin (2005), also for lexical types). It is this last approach that we use in this work. Implicit in all of these methods is a notion of the secondary language r"
U06-1011,P06-1040,0,0.0218069,"Missing"
U06-1011,J06-2001,0,\N,Missing
U06-1011,C98-1077,0,\N,Missing
U06-1020,P03-1059,1,0.79877,"250 movie scripts, but due to limited resources, only 3 scripts were sense tagged, and only 8 high frequency and highly polysemous verbs were chosen for this study. 144 is deemed necessary. Due to the high numbers of individual binary features and feature types, it would be impractical to generate all possible combinations of the individual features or feature types. Therefore, we propose two automatic feature selection algorithms here: the individual feature ranking algorithm and feature type coverage algorithm. 5.1 Individual Feature Ranking Algorithm This algorithm is based on the work of Baldwin and Bond (2003) and works by first calculating the information gain (IG), gain ratio (GR) and Chisquare statistics (Chi) for each binary feature as 3 separate scores. Then, each score is separately used to rank the features in a way such that the greater the score, the higher the rank. Features which have the same value for a particular score are given the same rank. Once individual ranks have been determined for each feature, the ranks themselves are summed up and used as a new score which is then used to re-rank all the features one last time. This final ranking will be used to perform the feature selectio"
U06-1020,J96-1002,0,0.00581016,"ning of the machine learner’s parameters. Since feature extraction is explained in detail in Section 3, we will only disscuss the other two components here. Within our framework, feature selection is performed only on the training set. We first use the feature selection algorithms described in Section 5 to generate different feature sets, which are used to generate separate datasets. We then perform cross validation (CV) on each dataset, and the feature set with the best performance is chosen as the final feature set. The machine learning algorithm used in our study is Maxium Entropy (MaxEnt: Berger et al. (1996)1 ). MaxEnt classifiers work by modelling the probability distribution of labels with respect to disambiguation features, the distribution of which is commonly smoothed based on a Gaussian prior. Different values for the Gaussian prior often lead to significant differences in the classification of new data, motivating us to include the tuning of the Gaussian prior in VSD framework.2 The tuning of the Gaussian prior is performed in conjunction with the feature selection. The CV for each feature set is performed multiple times, each time with a different parameterisation of the Gaussian prior. T"
U06-1020,A00-2018,0,0.0577272,"set and parameterisation of the Gaussian prior for the given dataset. 3 Features Types Since selectional preference based WSD features and general WSD features are not mutually exclusive of each other, and it would be less convincing to evaluate the impact of selectional preference based features without a baseline derived from general WSD features, we decided to include a number of general WSD features in our experiments. The sources of these features include: Part-of-Speech tags extracted using a tagger described in Gimnez and Mrquez (2004); parse trees extracted using the Charniak Parser (Charniak, 2000); chunking information extracted using a statistical chunker trained on the Brown Corpus and the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993); and named entities extracted using the system described in Cohn et al. (2005). 3.1 General WSD Features There are 3 broad types of general WSD features: n-gram based features of surrounding words and WordNet noun synsets, parse-tree-based syntactic features, and non-parse-tree based syntactic features. 3.1.1 N -gram based features The following n-gram based features have been experimented with: Bag of Words Lemmatized ope"
U06-1020,P05-1002,0,0.288149,"e impact of selectional preference based features without a baseline derived from general WSD features, we decided to include a number of general WSD features in our experiments. The sources of these features include: Part-of-Speech tags extracted using a tagger described in Gimnez and Mrquez (2004); parse trees extracted using the Charniak Parser (Charniak, 2000); chunking information extracted using a statistical chunker trained on the Brown Corpus and the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993); and named entities extracted using the system described in Cohn et al. (2005). 3.1 General WSD Features There are 3 broad types of general WSD features: n-gram based features of surrounding words and WordNet noun synsets, parse-tree-based syntactic features, and non-parse-tree based syntactic features. 3.1.1 N -gram based features The following n-gram based features have been experimented with: Bag of Words Lemmatized open class words in the entire sentence of the target verb. Words that occur multiple times are only counted once. Bag of Synsets The WordNet (Fellbaum, 1998) synsets for all the open class words in the entire 2 1 We used Zhang Le’s implementation which i"
U06-1020,gimenez-marquez-2004-svmtool,0,0.019097,"herefore, the final classifier incorporates the best combination of feature set and parameterisation of the Gaussian prior for the given dataset. 3 Features Types Since selectional preference based WSD features and general WSD features are not mutually exclusive of each other, and it would be less convincing to evaluate the impact of selectional preference based features without a baseline derived from general WSD features, we decided to include a number of general WSD features in our experiments. The sources of these features include: Part-of-Speech tags extracted using a tagger described in Gimnez and Mrquez (2004); parse trees extracted using the Charniak Parser (Charniak, 2000); chunking information extracted using a statistical chunker trained on the Brown Corpus and the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993); and named entities extracted using the system described in Cohn et al. (2005). 3.1 General WSD Features There are 3 broad types of general WSD features: n-gram based features of surrounding words and WordNet noun synsets, parse-tree-based syntactic features, and non-parse-tree based syntactic features. 3.1.1 N -gram based features The following n-gram based"
U06-1020,W04-2416,0,0.0193775,"ks. If a chunk is a PP, then the head preposition will also be part of the feature; if a chunk is an NP and the head word is a question word (who, what, when, how, where or why), the head word itself will be part of the feature, but if the head word is not a question word, its POS tag will be part of the feature; if a chunk is a VP, then the POS tag of the head verb will be part of the 143 feature. Using the above kick example, this feature will be: (NP, PRP) (PP, out) (NP, NN) (PP, through) (NP, which) (NP, PRP) (VP, VBD). 3.2 Selectional Preference Based Features We used the ASSERT5 system (Hacioglu et al., 2004) to extract the semantic roles from the target sentences. The following selectional preference based features have been experimented with: WordNet synsets of the head nouns of the SRs For each semantic role, the WordNet synsets of its head noun, paired with the corresponding semantic role. Semantic role’s relative positions These features are designed to capture the syntactic patterns of the target verb and its semantic roles. The relative position is set up such that the first semantic role to the left of the target verb will be given the position of −1, and the first one to the right will be"
U06-1020,P03-1007,0,0.0244782,"(1999)) surrounding and including the target verb within a window of 5 words, paired with their relative positions. Bag of Chunk Types The chunk types (e.g. NP, VP) surrounding and including the target verb within a window of 5 words. Bag of Named Entities The named entities in the entire sentence of the target verb. 3.1.2 Parse tree based features Right POS-tags The POS tag of each word to the right of the target verb within a predefined window is paired with its relative position. The parse tree based syntactic features are inspired by research on verb subcategorization acquisition such as Korhonen and Preiss (2003), and are intended to capture the differences in syntactic patterns of the different senses of the same verb. Given the position of the target verb v in the parse tree, the basic form of the corresponding parse tree feature is just the list of nodes of v’s siblings in the tree. Figure 1 shows the parse tree for a sentence containing the ambiguous verb call. Given the position of the target verb called in the parse tree, the basic form of the features that can be created will be (N P, P P ). However, there are 3 additional types of variations that can be applied to the basic features. The first"
U06-1020,J93-2004,0,0.0254118,"mutually exclusive of each other, and it would be less convincing to evaluate the impact of selectional preference based features without a baseline derived from general WSD features, we decided to include a number of general WSD features in our experiments. The sources of these features include: Part-of-Speech tags extracted using a tagger described in Gimnez and Mrquez (2004); parse trees extracted using the Charniak Parser (Charniak, 2000); chunking information extracted using a statistical chunker trained on the Brown Corpus and the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993); and named entities extracted using the system described in Cohn et al. (2005). 3.1 General WSD Features There are 3 broad types of general WSD features: n-gram based features of surrounding words and WordNet noun synsets, parse-tree-based syntactic features, and non-parse-tree based syntactic features. 3.1.1 N -gram based features The following n-gram based features have been experimented with: Bag of Words Lemmatized open class words in the entire sentence of the target verb. Words that occur multiple times are only counted once. Bag of Synsets The WordNet (Fellbaum, 1998) synsets for all t"
U06-1020,J03-4004,0,0.0216098,"stands for “to collide with”, with the only difference between the two instances of hit being their manner modifiers. Intuitively, the inclusion of verb adjuncts can enrich the semantic roles (SRs) and provide additional disambiguation information for verbs. Therefore, in the rest of this paper, the concept of “semantic role” will include both the arguments and adjuncts of verbs. All the selectional preference based WSD systems to date have only used the subject and direct object of verbs as semantic roles, extracting the necessary argument structure via hand-crafted heuristics (Resnik, 1997; McCarthy and Carroll, 2003, inter alia). As a result, it is difficult to extend the selectional preferences to anything else. However, with recent progress in Semantic Role Labelling (SRL) technology, it is now possible to obtain additional semantic roles such as the indirect object of ditransitive verbs and the locational, temporal and manner adjuncts. Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 139–148. 139 Given the lack of research in VSD using multisemantic-role based selectional preferences, the main contribution of the work presented in this paper is to show that it is pos"
U06-1020,W97-0209,0,0.045009,"ith a car, it stands for “to collide with”, with the only difference between the two instances of hit being their manner modifiers. Intuitively, the inclusion of verb adjuncts can enrich the semantic roles (SRs) and provide additional disambiguation information for verbs. Therefore, in the rest of this paper, the concept of “semantic role” will include both the arguments and adjuncts of verbs. All the selectional preference based WSD systems to date have only used the subject and direct object of verbs as semantic roles, extracting the necessary argument structure via hand-crafted heuristics (Resnik, 1997; McCarthy and Carroll, 2003, inter alia). As a result, it is difficult to extend the selectional preferences to anything else. However, with recent progress in Semantic Role Labelling (SRL) technology, it is now possible to obtain additional semantic roles such as the indirect object of ditransitive verbs and the locational, temporal and manner adjuncts. Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 139–148. 139 Given the lack of research in VSD using multisemantic-role based selectional preferences, the main contribution of the work presented in this pap"
U06-1020,E99-1023,0,0.0173981,"OS) features are the union of the Left-Words (POS) features and the Right-Words (POS) features. However, this redundancy of features makes it convenient to investigate the disambiguation effectiveness of the word collocations before and after the target verb, as well as the syntactic pattern before and after the target verb. Furthermore, we have also experimented with different window sizes for the Surrounding-Words (POS), Left-Words (POS) and Right-Words (POS) to determine the most appropriate window size.4 Bag of Chunk Tags The chunk tags (in Inside-Outside-Beginning (IOB) format: Tjong Kim Sang and Veenstra (1999)) surrounding and including the target verb within a window of 5 words, paired with their relative positions. Bag of Chunk Types The chunk types (e.g. NP, VP) surrounding and including the target verb within a window of 5 words. Bag of Named Entities The named entities in the entire sentence of the target verb. 3.1.2 Parse tree based features Right POS-tags The POS tag of each word to the right of the target verb within a predefined window is paired with its relative position. The parse tree based syntactic features are inspired by research on verb subcategorization acquisition such as Korhone"
U07-1009,P07-1072,0,0.0126833,"o enhance NC interpretation; the noun components that comprise the NCs are disambiguated using these WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual informat"
U07-1009,S07-1003,0,0.0305768,"Missing"
U07-1009,P06-2064,1,0.702941,"WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to"
U07-1009,J02-3004,0,0.154449,"elated. As English noun phrases are rightheaded, the head noun occurs after all modifying 49 Research on NCs can be categorised into four main groups: deﬁning SRs, disambiguating the syntax of NCs, disambiguating the semantics of NCs, and interpreting NCs via SRs. Each task is detailed in Section 2.1. Interpreting NCs has received much attention of late, and the problem has been addressed in areas of machine translation (MT), information extraction (IE), and applications such as questionanswering (QA). NCs pose a considerable challenge to computational linguistics due to the following issues (Lapata, 2002): (1) NCs are extremely productive; (2) the semantic relationship between the head noun and its modiﬁer(s) is implicit; and (3) the interpretation of an NC can vary due to contextual and pragmatic factors. Due to these challenges, current NC interpretation methods are too error-prone to be employed directly in NLP applications without human intervention or preprocessing. In this paper, we investigate the task of NC interpretation based on sense collocation. It has been shown that NCs with semantically similar components share the same SR (Kim and Baldwin, 2007); this is encapsulated by the phr"
U07-1009,W04-2609,0,0.0660329,"elb.edu.au Abstract nouns. For example, brick house is interpreted as a house that is modiﬁed by the word brick, which exhibits a P RODUCT-P RODUCER relationship between the two nouns in the compound. In contrast, the modiﬁer and head in house brick exhibits a PARTW HOLE relationship, which is interpreted as a brick from a house, rather than the former interpretation of a house made of bricks. The set of SRs that we are concerned with in this paper is deﬁned in Section 5.1. This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by Moldovan et al. (2004). Our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity of the classiﬁers by expanding the range of sense collocations via different semantic relations. Our method combines hypernyms, hyponyms and sister words of the component nouns, based on W ORD N ET. The data used in our experiments was taken from the nominal pair interpretation task of S EM E VAL -2007 (4th International Workshop on Semantic Evaluation 2007). In our evaluation, we test 7-way and 2-way class data, and show that the inclusion of hypernyms improves the perfo"
U07-1009,W05-0603,0,0.074031,"Missing"
U07-1009,W01-0511,0,0.0450922,"Missing"
U07-1009,W07-1108,0,0.0635081,"Missing"
U07-1009,C94-2125,0,0.0419458,"Cs are disambiguated using these WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from"
U07-1009,I05-1082,1,\N,Missing
U07-1009,P98-1015,0,\N,Missing
U07-1009,C98-1015,0,\N,Missing
U07-1018,2005.mtsummit-papers.11,0,0.00552814,"to look up in the dictionary in the ﬁrst place. Third, the user may be unable to lemmatise the word to determine the form in which it is listed in a dictionary. There are several alternatives to help decipher webpages in unfamiliar languages. The ﬁrst one is to use an online machine translation system such as Altavista’s Babel Fish1 or Google Translate.2 1 2 While web-based machine translation services occasionally produce good translations for linguisticallysimilar languages such as English and French, they do not perform very well in translating languages which are removed from one another (Koehn, 2005). The second alternative is a pop-up glossing application. The application takes raw text or a URL, parses the words, and returns the pop-up translation of each word as the mouse hovers over it. Some example pop-up glossing applications for Japanese source text and English glosses are Rikai3 and POPjisyo.4 With the aid of these pop-up translations, the manual effort of segmenting words (if necessary) and looking up each can be avoided. This application is also useful as an educational aid for learners of that language. The drawback with these applications is they display all possible translati"
U07-1018,C02-1162,0,0.0254017,"and Japanese, based on Chinese hanzi. There have been numerous attempts to manually 126 develop multilingual resources that include crosslingual sense alignments (Vossen, 1998; Stamou et al., 2002), and the import of cross-lingual semantic alignment has been ably demonstrated by the high impact of these resources. Due to the high overhead in manually constructing such resources, there have been various attempts at automatic crosslingual sense alignment. The methods are predominantly corpus-driven, based either on cross-lingual distributional similarity in a comparable corpus (e.g. Ngai et al. (2002)) or word alignment over a parallel corpus (e.g. Gliozzo et al. (2005)). There is a lesser amount of research on crosslingually aligning ontologies without using largescale corpus data, which we discuss in greater detail as it is more closely related to that proposed in this research. Asanoma (2001) aligned the Japanese Goi-Taikei ontology with WordNet by ﬁrst translating a signiﬁcant subset of the WordNet synonym sets (synsets) into Japanese, automatically matching these based on (monolingual Japanese) lexical overlap, and “ﬁlling in the gaps” for the remaining classes based on their hierarch"
U07-1018,W06-0608,0,0.0551737,"Missing"
U07-1018,I08-2108,1,\N,Missing
U08-1011,M95-1012,0,0.0149328,"Missing"
U08-1011,A00-1011,0,0.034833,"top word list of van Rijsbergen (1979). Model Precision Recall F-Score Baseline STEP EVITA 0.800 0.820 0.740 0.550 0.706 0.873 0.652 0.759 0.801 Table 1: Performance of comparable event identiﬁcation systems and our baseline system, as evaluated over the TimeBank data Recall = # words correctly classiﬁed as events # words that are events F-Score = 2 × Precision × Recall Precision + Recall (2) (3) Our baseline takes the form of a unigram tagger, where each word is labeled with its most frequent label. Table 1 shows the performance of the baseline system, in addition to the the comparable STEP (Aone and Ramos-Santacruz, 2000) and EVITA (Sauir et al., 2005) results — as described in Section 2.2 — both of which use the TimeBank data in the development and testing of their systems. 4 Results 4.1 Sentence Level Classiﬁcation of Events In this section we ﬁrst present the results of our sentence-level event identiﬁcation system, where each sentence is classiﬁed as containing one or • POS more event references, or alternatively containing Parts of speech (POSs) are assigned to each no event references. Three learners — C4.5 deciword using the Stanford maxent POS tagger, sion tree, SVM and naive Bayes — were run over and"
U08-1011,W06-1618,0,0.141204,"Missing"
U08-1011,W04-3205,0,0.0244773,"e 2), increasing the number of training instances has the potential to improve the performance of the models which include context. The two-word POS context model misclassiﬁes a different subset of words to the threeword POS context model. Therefore, combining different models may increase the performance of the overall system. The majority of errors came from previously unseen words in the training data. Future work will thus focus on how the system can better handle unseen words. 5 Future Work We intend to look at the effect verb types obtained from WordNet (Fellbaum, 1998) or V ERB O CEAN (Chklovski and Pantel, 2004) have on word-level event identiﬁcation, to assist in better handling of unseen words. We will also look at how the system performance scales over a larger data set. 6 Conclusion We presented a set of experiments involving realworld event reference identiﬁcation at the word level in newspaper and newswire documents. We addressed the issue of effective text representation for classiﬁcation of events using support vector machines. Our ﬁnal system combined two word POS prior context, with positional information and stop word removal. It achieved an F-score of 0.764, signiﬁcantly exceeding that of"
U08-1011,C96-1079,0,0.0260794,"sentation to use in a purely statistical approach to word-level event identiﬁcation in newswire text. In the remainder of this paper, we review the notion of “event” in the literature, and present an overview of the different approaches to event identiﬁcation. We then describe our experimental methodology, including the TimeBank corpus. Finally we describe a comparative evaluation. 2 Related Work 2.1 Event Deﬁnition The deﬁnition of an event varies in granularity depending on the desired application of event extraction. The Scenario Template task of the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996) relied on speciﬁed domaindependent templates which deﬁne events for the purpose of information extraction. The events that are extracted depend on the templates and documents supplied. MUC consists of domainspeciﬁc scenario (topic) based templates that systems are required to ﬁll. An example domain is ﬁnancial news in the Wall Street Journal, e.g. with the scenario of change in the corporate executive management personnel of an organization (Grishman and Sundheim, 1996). In the context of Topic Detection and Tracking (TDT) (Fiscus et al., 1998), an event corresponds to an instance of a topic"
U08-1011,W03-0418,0,0.0631513,"Missing"
U08-1011,H05-1088,0,0.260464,"el Precision Recall F-Score Baseline STEP EVITA 0.800 0.820 0.740 0.550 0.706 0.873 0.652 0.759 0.801 Table 1: Performance of comparable event identiﬁcation systems and our baseline system, as evaluated over the TimeBank data Recall = # words correctly classiﬁed as events # words that are events F-Score = 2 × Precision × Recall Precision + Recall (2) (3) Our baseline takes the form of a unigram tagger, where each word is labeled with its most frequent label. Table 1 shows the performance of the baseline system, in addition to the the comparable STEP (Aone and Ramos-Santacruz, 2000) and EVITA (Sauir et al., 2005) results — as described in Section 2.2 — both of which use the TimeBank data in the development and testing of their systems. 4 Results 4.1 Sentence Level Classiﬁcation of Events In this section we ﬁrst present the results of our sentence-level event identiﬁcation system, where each sentence is classiﬁed as containing one or • POS more event references, or alternatively containing Parts of speech (POSs) are assigned to each no event references. Three learners — C4.5 deciword using the Stanford maxent POS tagger, sion tree, SVM and naive Bayes — were run over and optionally used as an alternati"
U08-1011,N03-1033,0,0.00827237,"er event or time to which it is temporally related. There are seven event class types (each of which is illustrated with a set of examples): • I-State: believe, intend, want • Aspectual: begin, ﬁnish, stop, continue • Perception: see, hear, watch, feel We currently disregard the event classes and use a binary classiﬁcation, identifying each word as an event or non-event. 3.2 System Description We preprocessed the TimeBank data to remove extraneous XML tags, decompose it into sentences and words, and discard punctuation. Finally we assign each word with a POS tag using the Stanford POS tagger (Toutanova et al., 2003). In our experiments, we use three classiﬁcation algorithms: decision trees, naive Bayes and support vector machines. The chosen training and evaluation platform was WEKA (Witten and Frank, 2005). Stratiﬁed 10-fold cross-validation was used throughout, where the labeled text fragments were randomly allocated to different combinations of training and testing splits. We report on the average performance over the 10 runs. Preliminary experiments over the three classiﬁcation algorithms indicated that SVMs were more effective in the classiﬁcation of event references in text. BSVM (Hsu and Lin, 2002"
U08-1011,P05-3021,0,0.0607338,"Missing"
U08-1011,doddington-etal-2004-automatic,0,\N,Missing
U08-1011,U06-1005,0,\N,Missing
U08-1015,baldwin-awab-2006-open,1,0.417785,"ground 2.1 NLP for Malay As the basis of our evaluation, we use the KAMI Malay–English translation dictionary (Quah et al., 2001). Although primarily a translation dictionary, with translations of Malay headwords into both English and Chinese, KAMI contains syntactic information and semantics in terms of a large ontology. Of the total of around 90K lexical entries in KAMI, we make use of around 19K nominal lexical entries which are annotated for headword, lemma and POS tag when speciﬁed, and count classiﬁer. As a corpus of Malay text, we use the 1.2M-token web document collection described in Baldwin and Awab (2006).5 The corpus has been sentence- and word-tokenised, and additionally lemmatised based on a hand-crafted set of inﬂectional and derivational morphology rules for Malay, developed using KAMI and an electronic version of a standard Malay dictionary (Taharin, 1996). While little NLP has been performed for Malay, Indonesian has seen some interesting work in the past few years. Adriani et al. (2007) examined stemming Indonesian, somewhat overlapping with Baldwin and Awab above, evaluating on the information retrieval testbed from Asian et al. (2004). Recently, a probabilistic parser of Indonesian h"
U08-1015,P03-1059,1,0.780835,"se noun from an ontology to predict its count classiﬁer, and Paik and Bond (2001) extend this strategy to include both Japanese and Korean. Finally, Sornlertlamvanich et al. (1994) propose their own typology for classiﬁers within Thai, and an automatic method to predict these using corpus evidence. As for comparable structures in English, Flickinger and Bond (2003) analyse measure nouns within a Head-Driven Phrase Structure Grammar framework. Countability, which is a salient feature of nouns in English, undergoes many of the same structural syntax and semantic preferences as count classiﬁers. Baldwin and Bond (2003) motivate and examine a variety of surface cues in English that can be used to predict typewise countability of a given noun. Taking a mapped cross-lingual ontology, van der Beek and Baldwin (2004) use the relatedness of Dutch and English to cross-lingually predict countability, and observe comparable performance to monolingual prediction. 3 Methodology 3.1 Data Set First, we constructed a data set based on the resources available. As we chose to approach the task in a supervised learning framework, we required a set of labelled exemplars in order to train and test our classiﬁers. To obtain th"
U08-1015,Y07-1001,1,0.277707,"Missing"
U08-1015,C00-1014,0,0.49936,"the theory and difﬁculties associated with automatically developing a broad taxonomy of count classiﬁers suitable for three languages: Chinese, Japanese, and Thai. They ﬁnd that relatively high agreement between Chinese and Japanese, but Thai remains resistant to a universal hierarchy of count classes. Otherwise, work on count classiﬁers has mostly focussed on a single language at a time, primarily Japanese. Bond and Paik (1997) consider the lexical implications of the typology of kind and shape classiﬁers, and propose a hierarchy for Japanese classiﬁers that they extend to Korean classiﬁers. Bond and Paik (2000) examine using the semantic class of a Japanese noun from an ontology to predict its count classiﬁer, and Paik and Bond (2001) extend this strategy to include both Japanese and Korean. Finally, Sornlertlamvanich et al. (1994) propose their own typology for classiﬁers within Thai, and an automatic method to predict these using corpus evidence. As for comparable structures in English, Flickinger and Bond (2003) analyse measure nouns within a Head-Driven Phrase Structure Grammar framework. Countability, which is a salient feature of nouns in English, undergoes many of the same structural syntax a"
U08-1015,C94-1002,0,0.0389127,"Missing"
U08-1015,I08-1052,0,0.570704,"s. Adriani et al. (2007) examined stemming Indonesian, somewhat overlapping with Baldwin and Awab above, evaluating on the information retrieval testbed from Asian et al. (2004). Recently, a probabilistic parser of Indonesian has been developed, as discussed in Gusmita and Manu5 An alternative corpus of Malay would have been the Dewan Bahasa & Pustaka Corpus, with about 114M word tokens. As it is not readily accessible, however, we were unable to use it in this research. rung (2008), and used for information extraction and question answering (Larasati and Manurung, 2007). 2.2 Count Classiﬁers Shirai et al. (2008) discuss the theory and difﬁculties associated with automatically developing a broad taxonomy of count classiﬁers suitable for three languages: Chinese, Japanese, and Thai. They ﬁnd that relatively high agreement between Chinese and Japanese, but Thai remains resistant to a universal hierarchy of count classes. Otherwise, work on count classiﬁers has mostly focussed on a single language at a time, primarily Japanese. Bond and Paik (1997) consider the lexical implications of the typology of kind and shape classiﬁers, and propose a hierarchy for Japanese classiﬁers that they extend to Korean cla"
U08-1015,C94-1091,0,0.772447,"inese and Japanese, but Thai remains resistant to a universal hierarchy of count classes. Otherwise, work on count classiﬁers has mostly focussed on a single language at a time, primarily Japanese. Bond and Paik (1997) consider the lexical implications of the typology of kind and shape classiﬁers, and propose a hierarchy for Japanese classiﬁers that they extend to Korean classiﬁers. Bond and Paik (2000) examine using the semantic class of a Japanese noun from an ontology to predict its count classiﬁer, and Paik and Bond (2001) extend this strategy to include both Japanese and Korean. Finally, Sornlertlamvanich et al. (1994) propose their own typology for classiﬁers within Thai, and an automatic method to predict these using corpus evidence. As for comparable structures in English, Flickinger and Bond (2003) analyse measure nouns within a Head-Driven Phrase Structure Grammar framework. Countability, which is a salient feature of nouns in English, undergoes many of the same structural syntax and semantic preferences as count classiﬁers. Baldwin and Bond (2003) motivate and examine a variety of surface cues in English that can be used to predict typewise countability of a given noun. Taking a mapped cross-lingual o"
U09-1006,W04-2209,0,\N,Missing
U09-1006,U08-1008,0,\N,Missing
U09-1007,U08-1018,1,0.896901,"a breakdown of the components that make up the surface word, and (c) the surface word (in italics). Note that the first-line representation for (2) and (3) is identical, but the surface words differ on the basis of the order in which the reduplication and meN affixation are applied. Note also that, as is apparent in the gloss, (3) involves a different process to the other two examples, and yet all three are dealt with using the same reduplication strategy in our implementation. We return to discuss these and other issues in Section 3. The morphological analyser is based on the system built by Pisceldo et al. (2008), whose implementation of reduplication follows closely that suggested for Malay by Beesley and Karttunen (2003). However, (3) is not dealt with by Beesley and Karttunen (2003), and the solution of Pisceldo et al. (2008) requires an overlay of corrections to account for the distinct argument structure of (3). This paper outlines a method for reorganising the morphological analyser to account for these facts in a manner which is more elegant and faithful to the data. The marking on the verb indicates the semantic role of the subject, in square braces [ ] the agent in (4), and the theme and pati"
U09-1013,drouin-2004-detection,0,0.0319108,"the Reuters document collection using term frequency and inverse document frequency. 1 Introduction Automatic domain-specific term extraction is a categorization/classification task where terms are categorized into a set of predefined domains. It has been employed in tasks such as keyphrase extraction (Frank et al., 1999; Witten et al., 1999), word sense disambiguation (Magnini et al., 2002), and query expansion and cross-lingual text categorization (Rigutini et al., 2005). Even though the approach shows promise, relatively little research has been carried out to study its effects in detail (Drouin, 2004; Milne et al., 2006; Rigutini et al., 2006; Kida et al., 2007; Park et al., 2008). Most of the research to date on domainspecific term extraction has employed supervised machine learning, within the fields of term categorization and text mining. However, to date, the only research to approach the task in an unsupervised manner is that of Park et al. (2008). Unsupervised methods have the obvious advantage that they circumvent the need for laborious manual classification of training instances, and are thus readily applicable to arbitrary sets of domains, tasks and languages. In this paper, we p"
U09-1013,P06-1068,0,0.0754223,"Missing"
U09-1013,S01-1027,0,0.0880807,"Missing"
U10-1003,N10-1027,1,0.476818,"Missing"
U10-1003,W09-0307,0,0.153077,"ed in. This dataset formed the basis of the 2010 Australasian Language Technology Workshop shared task. Multilingual language identification is relevant in a number of contexts. “Word spotting” of foreign words in multilingual documents has been shown to improve parsing performance (Alex et al., 2007), and multilingual language identification is a first step in this direction. It can also be used as part of a linguistic corpus creation pipeline for low-density languages, e.g. to determine the language used in interlinear glossed text (IGT) embedded in language documentation (Xia et al., 2009; Xia and Lewis, 2009). The ideal vehicle for multilingual language identification research would be a dataset genuinely representative of the true multilingualism of resources such as the web. Creating such a resource, however, would require: (a) a multilingual crawl without language bias; and (b) a largescale document collection with gold-standard annotations over the full range of languages extant on the web, including sub-document extents for the individual languages contained in a document. While we would ultimately like to generate such a dataset for general usage, in this paper we describe a more modest effo"
U10-1003,E09-1099,0,0.0615363,"document is authored in. This dataset formed the basis of the 2010 Australasian Language Technology Workshop shared task. Multilingual language identification is relevant in a number of contexts. “Word spotting” of foreign words in multilingual documents has been shown to improve parsing performance (Alex et al., 2007), and multilingual language identification is a first step in this direction. It can also be used as part of a linguistic corpus creation pipeline for low-density languages, e.g. to determine the language used in interlinear glossed text (IGT) embedded in language documentation (Xia et al., 2009; Xia and Lewis, 2009). The ideal vehicle for multilingual language identification research would be a dataset genuinely representative of the true multilingualism of resources such as the web. Creating such a resource, however, would require: (a) a multilingual crawl without language bias; and (b) a largescale document collection with gold-standard annotations over the full range of languages extant on the web, including sub-document extents for the individual languages contained in a document. While we would ultimately like to generate such a dataset for general usage, in this paper we descr"
U10-1003,D07-1016,0,0.137743,"arch to move towards a more realistic task setting where a document can be authored in one or more languages (Hughes et al., 2006). This paper describes such a dataset, based around the task of multilingual language identification, where the task is to determine which one or two languages a given document is authored in. This dataset formed the basis of the 2010 Australasian Language Technology Workshop shared task. Multilingual language identification is relevant in a number of contexts. “Word spotting” of foreign words in multilingual documents has been shown to improve parsing performance (Alex et al., 2007), and multilingual language identification is a first step in this direction. It can also be used as part of a linguistic corpus creation pipeline for low-density languages, e.g. to determine the language used in interlinear glossed text (IGT) embedded in language documentation (Xia et al., 2009; Xia and Lewis, 2009). The ideal vehicle for multilingual language identification research would be a dataset genuinely representative of the true multilingualism of resources such as the web. Creating such a resource, however, would require: (a) a multilingual crawl without language bias; and (b) a la"
U10-1006,W10-0508,1,0.600811,"line users to share information on the Internet. Users post their questions or problems onto online forums and get possible solutions from other users. Through this simple mechanism, great volumes of data with customised answers to highly specialised domain-specific questions are created on a daily basis. However, it is not an easy job to extract the information latent in the threads. The aim of our research is to help users to more easily access existing information in forums which relate to their questions, by text mining troubleshooting-oriented, computer-related technical user forum data (Baldwin et al., 2010). An example thread from a real-world forum is shown in Figure 1, which is made up of 4 posts with 3 distinct participants. Our proposed strategy is to model the “content structure” of forum threads by analysing requests for information and provision of solutions in the thread data. We devise an ontology of problem sources and solution types with which to analyse individual threads, paving the way for users to spell out the general nature of their support need in their queries. The main contributions of this paper are: (1) designing a modular thread-level class set; (2) constructing and publis"
U10-1006,P05-1037,0,0.0204605,"evel class set; (2) constructing and publishing an annotated dataset; and (3) performing preliminary threadlevel experiments over the dataset. 2 Related Work There is very little work that is specifically targeted at the thread-level analysis of web user forum data. The most closely-related work is that performed by Baldwin et al. (2007), and our thread class set was created based on this original work. Another research line that relates to the thread classification is discussion summarisation. For example, technical online IRC (Internet Relay Chat) discussions are summarised and segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawle"
U10-1006,N04-4027,0,0.02075,"ions are summarised and segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawled threads were then preprocessed. Only the title and sub-forum information of each thread, and the body, title, and author information of each post were preserved. Finally, we randomly selected 500 threads from 4 sub-forums of the CNET forums: Operating Systems, Software, Hardware, and Web Development. Two annotators performed a pilot annotation using a seed set of 150 threads and a dedicated web annotation tool. The κ value for the pilot annotation (indicating the relative agreement between the two annotators) was 0.43. The annotators sat down together to"
U10-1006,C04-1079,0,0.0195437,"nd segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawled threads were then preprocessed. Only the title and sub-forum information of each thread, and the body, title, and author information of each post were preserved. Finally, we randomly selected 500 threads from 4 sub-forums of the CNET forums: Operating Systems, Software, Hardware, and Web Development. Two annotators performed a pilot annotation using a seed set of 150 threads and a dedicated web annotation tool. The κ value for the pilot annotation (indicating the relative agreement between the two annotators) was 0.43. The annotators sat down together to go over every thread wh"
U10-1006,C00-2137,0,\N,Missing
U10-1009,W10-2923,1,0.908684,"ation. However, even in IR it has long been known that much more information than simple term occurrence is available. In the modern era of web search, for example, extensive use is made of link structure (Brin and Page, 1998), anchor text, document zones, and a plethora of other document (and query, click stream and user) features (Manning et al., 2008). The natural question to ask at this point is, What additional structure can we extract from web forum data? Previous work has been done in extracting useful information from various dimensions of web forums, such as the post-level structure (Kim et al., 2010). One dimension that has received relatively little attention is how we can use information about the identity of the participants to extract useful information from a web forum. In this work we will examine how we can utilize such user-level structure to improve performance over a user classification task. We have used the term threaded discourse to describe online data that represents a record of messages exchanged between a group of participants. In this work, we examine data from LinuxQuestions, a popular Internet forum for Linux-related troubleshooting. Aside from a limited set of feature"
U10-1009,W06-1639,0,0.0315724,"incorporating network-derived features. In a similar vein, Carvalho et al. (2007) used a combination of textual features (in the form of “email speech acts”) and network-based features to learn which users were team leaders. They found that the network-based features enhanced classification accuracy. Sentiment analysis (Pang and Lee, 2008) relates to this work as one of our user characteristics (P OSITIVITY) is an expression of user sentiment. However, sentiment analysis has tended to focus on individual documents, and rarely takes into account the author. An exception to this is the work of Thomas et al. (2006), who attempted to predict 50 4 Dataset which way each speaker in a U.S. Congressional debate on a proposed bill voted, on the basis of both what was said and the indication of agreement between speakers. Their task is related to ours in that it involves a user-level classification, but it focused on extracting information identifying where the speakers agree and disagree. Expert finding is the task of ranking experts relative to each of a series of queries, and has been part of the TREC Enterprise Track (Craswell et al., 2005; Soboroff et al., 2006; Balog et al., 2006; Fang and Zhai, 2007). T"
U10-1009,P07-2032,0,0.284648,"Missing"
U10-1009,W10-0508,1,0.818108,"n utilize such user-level structure to improve performance over a user classification task. We have used the term threaded discourse to describe online data that represents a record of messages exchanged between a group of participants. In this work, we examine data from LinuxQuestions, a popular Internet forum for Linux-related troubleshooting. Aside from a limited set of features specific to the Linux-related troubleshooting domain, however, our techniques are domaininspecific and expected to generalize to any data that can be interpreted as a threaded discourse. This work is part of ILIAD (Baldwin et al., 2010), an ongoing effort to improve information access in linux forums. Our contribution to the project is techniques to identify characteristics of forum users, building on earlier work in the space (Lui, 2009). The problem that we face here is twofold: Firstly, there is no established ontology for Marco Lui and Timothy Baldwin. 2010. Classifying User Forum Participants: Separating the Gurus from the Hacks, and Other Tales of the Internet. In Proceedings of Australasian Language Technology Association Workshop, pages 49−57 This is conceptually very similar to our task, and we build on this feature"
U10-1009,H93-1012,0,0.111918,"Missing"
U11-1011,W10-2923,1,0.729062,"Missing"
U11-1011,P93-1041,0,0.194791,"tional model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th Edition (1977). Because of the lack of a machine-readable copy of the thesaurus at the time, the lexical chains were built by hand. Research in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), text summarisation (Barzilay and Elhadad, 1997), topic detection and tracking (Stokes and Carthy, 2001), information retrieval (Stairmand, 1997), malapropism detection (Hirst and St-Onge, 1998), and question answering (Moldovan and Novischi, 2002). Many types of lexical chaining algorithms rely on examining lexicographical relationships (i.e. semantic measures) between words using domainindependent thesauri such as the Longmans Dictionary of Contemporay English (Kozima, 1993), Roget’s Thesaurus (Jarmasz and Szpakowic"
U11-1011,P98-2127,0,0.0550504,"., 2011b; Wang et al., 2011a; Aumayr et al., 2011), with CRF models frequently being reported to deliver superior performance. While there is research that attempts to conduct cross-forum classification (Wang et al., 2011a) — where classifiers are trained over linking labels from one forum and tested over threads from other forums — the results have not been promising. This research explores unsupervised methods for thread 77 cept or category) inventory from the Macquarie Thesaurus (Bernard, 1986) to build a word-category cooccurrence matrix (WCCM), based on the British National Corpus (BNC). Lin (1998a)’s measure of distributional similarity based on point-wise mutual information (PMI) is then used to measure the association between words. This research will explore two thesaurus-based lexical chaining algorithms, as well as a novel lexical chaining approach which relies solely on statistical word associations. linking structure recovery, by exploiting lexical cohesion between posts via lexical chaining. The first computational model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th E"
U11-1011,W97-0703,0,0.69674,"erent solution again to the original question (link = 4). Lexical chaining is a technique for identifying lists of related words (lexical chains) within a given discourse. The extracted lexical chains represent the discourse’s lexical cohesion, or “cohesion indicated by relations between words in the two units, such as use of an identical word, a synonym, or a hypernym” (Jurafsky and Martin, 2008, pp. 685). Lexical chaining has been investigated in many research tasks such as text segmentation (Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), and text summarisation (Barzilay and Elhadad, 1997). The lexical chaining algorithms used usually rely on domain-independent thesauri such as Roget’s Thesaurus, the Macquarie Thesaurus (Bernard, 1986) and WordNet (Fellbaum, 1998), with some algorithms also utilising statistical associations between words (Stokes et al., 2004; Marathe and Hirst, 2010). 2 Related Work The linking structure of web user forum threads can be used in tasks such as IR (Xi et al., 2004; Seo et al., 2009; Elsas and Carbonell, 2009) and threading visualisation. However, many user forums don’t support the user input of linking information. Automatically recovering the li"
U11-1011,W06-1605,0,0.0603238,"pon, and often only apply to nouns. Some lexical chaining algorithms also make use of statistical associations (i.e. distributional measures) between words which can be automatically generated from domain-specific corpora. For example, Stokes et al. (2004)’s lexical chainer extracts significant noun bigrams based on the G2 statistic (Pedersen, 1996), and uses these statistical word associations to find related words in the preceding context, building on the work of Hirst and StOnge (1998). Marathe and Hirst (2010) use distributional measures of conceptual distance, based on the methodology of Mohammad and Hirst (2006) to compute the relation between two words. This framework uses a very coarse-grained sense (con3 Lexical Chaining Algorithms Three lexical chaining algorithms are experimented with in this research, as detailed in the following sections. 3.1 ChainerRoget ChainerRoget is a Roget’s Thesaurus based lexical chaining algorithm (Jarmasz and Szpakowicz, 2003) based on an off-the-shelf package, namely the Electronic Lexical Knowledge Base (ELKB) (Jarmasz and Szpakowicz, 2001). The underlying methodology of ChainerRoget is shown in Algorithm 1. Methods used to calculate the chain strength/weight are p"
U11-1011,C02-1167,0,0.0291959,"h in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown, 2003), text summarisation (Barzilay and Elhadad, 1997), topic detection and tracking (Stokes and Carthy, 2001), information retrieval (Stairmand, 1997), malapropism detection (Hirst and St-Onge, 1998), and question answering (Moldovan and Novischi, 2002). Many types of lexical chaining algorithms rely on examining lexicographical relationships (i.e. semantic measures) between words using domainindependent thesauri such as the Longmans Dictionary of Contemporay English (Kozima, 1993), Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003), Macquarie Thesaurus (Marathe and Hirst, 2010) or WordNet (Barzilay and Elhadad, 1997; Hirst and StOnge, 1998; Moldovan and Novischi, 2002; Galley and McKeown, 2003). These lexical chaining algorithms are limited by the linguistic resources they depend upon, and often only apply to nouns. Some lexical chaining algo"
U11-1011,J91-1002,0,0.785889,"1986) to build a word-category cooccurrence matrix (WCCM), based on the British National Corpus (BNC). Lin (1998a)’s measure of distributional similarity based on point-wise mutual information (PMI) is then used to measure the association between words. This research will explore two thesaurus-based lexical chaining algorithms, as well as a novel lexical chaining approach which relies solely on statistical word associations. linking structure recovery, by exploiting lexical cohesion between posts via lexical chaining. The first computational model for lexical chain extraction was proposed by Morris and Hirst (1991), based on the use of the hierarchical structure of Roget’s International Thesaurus, 4th Edition (1977). Because of the lack of a machine-readable copy of the thesaurus at the time, the lexical chains were built by hand. Research in lexical chaining has then been investigated by researchers from different research fields such as information retrieval, and natural language processing. It has been demonstrated that the textual knowledge provided by lexical chains can benefit many tasks, including text segmentation (Kozima, 1993; Stokes et al., 2004), word sense disambiguation (Galley and McKeown"
U11-1011,J98-1004,0,0.109223,"Missing"
U11-1011,D11-1002,1,0.817214,"heer scale and diversity of the data. Previous research shows that the thread linking structure can be used to improve information retrieval (IR) in forums, at both the post level (Xi et al., 2004; Seo et al., 2009) and thread level (Seo et al., 2009; Elsas and Carbonell, 2009). These interpost links also have the potential to enhance threading visualisation, thereby improving information access over complex threads. Unfortunately, linking information is not supported in many forums. While researchers have started to investigate the task of thread linking structure recovery (Kim et al., 2010; Wang et al., 2011b), most research efforts focus on supervised methods. To illustrate the task of thread linking recovery, we use an example thread, made up of 5 posts from 4 distinct participants, from the CNET forum dataset of Kim et al. (2010), as shown in Figure 1. The linking structure of the thread is modelled as a rooted directed acyclic graph (DAG). In this example, UserA initiates the thread with a question in the first post, by asking how to create an interactive input box on a webpage. This post is linked to a virtual root with link label 0. In response, UserB and UserC proLi Wang, Diana Mccarthy an"
U11-1011,widdows-ferraro-2008-semantic,0,0.0125278,"ple.virginia.edu/˜ma5ke/ classes/files/cs65lexicalChain.pdf 78 Algorithm 1 ChainerRoget select a set of candidate nouns for each candidate noun do build all the possible chains, where each pair of nouns in each chain are either the same word or included in the same Head of Roget’s Thesaurus, and select the strongest chain for each candidate noun. end for merge two chains if they contain at least one noun in common derived from words that co-occur with w. A dimensionality reduction technique is often used to reduce the dimension of the vector. We build the WORDSPACE model with SemanticVectors (Widdows and Ferraro, 2008), which is based on Random Projection dimensionality reduction (Bingham and Mannila, 2001). The underlying methodology of ChainerSV is shown in Algorithm 2. This algorithm requires a method to calculate the similarity between two tokens (i.e. words): simtt (x, y), which is done by computing the cosine similarity of the two tokens’ semantic vectors. The similarity between a token ti and a lexical chain cj is then calculated by: the discourse one by one. Each node in the graph represents a noun instance with all its senses, and each weighted edge represents the semantic relation between two sens"
U11-1011,C00-2137,0,0.0531232,"Missing"
U11-1011,J00-4006,0,\N,Missing
U11-1011,C98-2122,0,\N,Missing
U12-1006,E09-1005,0,0.029899,"a multinomial distribution over words, and LDA places a Dirichlet prior on word distributions in topics. Although exact inference of LDA parameters is intractable, the model has gained prominence due to the availability of computationally efﬁcient approximations, the most popular being based on Gibbs sampling (Grifﬁths and Steyvers, 2004). For brevity, we do not give a detailed description of the LDA model. 2.3 Related Work Stevenson (2011) experimented with the use of LDA topic modelling in word sense disambiguation, where he used topic models to provide context for a graph-based WSD system (Agirre and Soroa, 2009), replacing a local context derived from adjacent words. This approach is of limited relevance to our work, as the graph-based approach considered state-of-the-art in unsupervised WSD (De Cao et al., 2010) maps senses to individual nodes in a graph. This presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme ("
U12-1006,E09-1013,0,0.0261533,"Related Work Stevenson (2011) experimented with the use of LDA topic modelling in word sense disambiguation, where he used topic models to provide context for a graph-based WSD system (Agirre and Soroa, 2009), replacing a local context derived from adjacent words. This approach is of limited relevance to our work, as the graph-based approach considered state-of-the-art in unsupervised WSD (De Cao et al., 2010) maps senses to individual nodes in a graph. This presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributio"
U12-1006,W10-2304,0,0.047835,"Missing"
U12-1006,D10-1113,0,0.0466316,"mbines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributional semantics of words in context (Erk and Pad´o, 2008). Dinu and Lapata (2010) propose a probabilistic framework for representing word meaning and measuring similarity of words in context. One of the parametrizations of their framework uses LDA to automatically induce latent senses, which is conceptually very similar to our approach. One key difference is that Dinu and Lapata focus on inferring the similarity in use of different words given their context, whereas in this work we focus on estimating the similarity of use of a single word in a number of different contexts. 3 Methodology Our basic framework is to produce a vector representation for each item in a L EX S UB"
U12-1006,D08-1094,0,0.170801,"Missing"
U12-1006,P09-1002,1,0.850568,"ord sense induction (WSI), but differs in that Usim does not pre-suppose a predeﬁned sense inventory. It also captures the fact that word senses may not always be distinct, and that the applicability of word senses is not necessarily mutually exclusive. In Usim, we consider pairs of sentences at a time, and quantify the similarity of the sense of the target word being used in each sentence. An example of a sentence pair (SPAIR) using similar but not identical senses of the word dry is given in Figure 1. Usim is a relatively new NLP task, partly due to the lack of resources for its evaluation. Erk et al. (2009) recently produced a corpus of sentence Figure 1: Example of an SPAIR judged by annotators to use similar but not identical senses of the word dry. pairs annotated for usage similarity judgments, allowing Usim to be formulated as a distinct task from the related tasks of word sense disambiguation and word sense induction. In this work, we propose a method to estimate word usage similarity in an entirely unsupervised fashion through the use of a topic model. We make use of the well-known Latent Dirichlet Allocation (LDA) model (Blei et al., 2003) to model the distribution of topics in a sentenc"
U12-1006,J06-4002,0,0.0318208,"ixture is too dry, add some water ; if it is too soft, add some ﬂour. ���� ���� ρ Figure 5: Sentences for dry(a) with a strong component of Topic 0 given the 8-topic model illustrated in ﬁgure 3 ���� ���� tor averages and the automated word usage similarity computation was a statistically signiﬁcant 0.202. A detailed breakdown of the best overall result is given in Table 1. Alongside this breakdown, we also provide: (1) the average inter-annotator agreement (IAA); and (2) the Spearman’s ρ for the optimal number of topics for the given lemma. The IAA is computed using leave-one-out resampling (Lapata, 2006), and is a detailed breakdown of the result reported by Erk et al. (2009). In brief, the IAA reported is the mean Spearman’s ρ between the ratings given by each annotator and the average rating given by all other annotators. We also present the Spearman’s ρ for the best number of topics in order to illustrate the impact of the number of topics parameter for the model of the background collection. We ﬁnd that for some lemmas, a lower topic count is optimal, whereas for other lemmas, a higher topic count is preferred. In aggregate terms, we found that verbs, adverbs and nouns performed better wi"
U12-1006,E12-1060,1,0.841355,"presupposes the existence of a ﬁxed sense inventory, and thus does 34 not lend itself to determining unsupervised word usage similarity. Brody and Lapata (2009) proposed an LDA topic modelling approach to WSI which combines feature sets such as unigram tokens and dependency relations, using a layered feature representation. Yao and Van Durme (2011) extended this work in applying a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)) to the WSI task, whereby the topic model dynamically determines how many topics to model the data with, rather than relying on a preset topic number. Recently, Lau et al. (2012) further extended this work and applied it to the task of novel sense detection. More broadly, this work is related to the study of distributional semantics of words in context (Erk and Pad´o, 2008). Dinu and Lapata (2010) propose a probabilistic framework for representing word meaning and measuring similarity of words in context. One of the parametrizations of their framework uses LDA to automatically induce latent senses, which is conceptually very similar to our approach. One key difference is that Dinu and Lapata focus on inferring the similarity in use of different words given their conte"
U12-1006,S07-1009,1,0.888227,"Missing"
U12-1006,W11-1102,0,0.0498483,"Missing"
U12-1009,P01-1005,0,0.0314841,"Missing"
U12-1009,C04-1086,0,0.197049,"apanese loanword MWEs and construct likely English translations, with the ultimate aim of being part of a toolkit to aid the lexicographer. The system builds on the availability of large collections of translated loanwords and a large English n-gram corpus, and in testing is performing with high levels of precision and recall. 2 Prior Work There has not been a large amount of work published on the automatic and semiautomatic extraction and translation of Japanese loanwords. Much that has been reported has been in areas such as backtransliteration (Matsuo et al., 1996; Knight and Graehl, 1998; Bilac and Tanaka, 2004), or on extraction from parallel bilingual corpora (Brill et al., 2001). More recently work has been carried out exploring combinations of dictionaries and corpora (Nakazawa et al., 2005), although this lead does not seem to have been followed further. Both Bilac and Tanaka (2004) and Nakazawa et al. (2005) address the issue of segmentation of MWEs. This is discussed in 3.1 below. 1 In addition to katakana, loanwords use the ー (chōoN) character for indicating lengthened vowels, and on rare occasions the ヽ and ヾ syllable repetition characters. 3 Role and Nature of Katakana Words in Japanese As"
U12-1009,W04-2209,0,0.0863188,"Missing"
U12-1009,W04-3230,0,0.141664,"aka (2004) report a precision and recall of approximately 0.65 on the segmentation, with a tendency to undersegment being the main problem. Nakazawa et al. (2005) report a similar tendency with the JUMAN morphological analyzer (Kurohashi and Nagao, 1998). The problem was most likely due to the relatively poor representation of loanwords in the morpheme lexicons used by these systems. For example the IPADIC lexicon (Asahara and Matsumoto, 2003) used at that time only had about 20,000 words in katakana, and many of those were proper nouns. In this study, we use the MeCab morphological analyzer (Kudo et al., 2004) with the recently-developed UniDic lexicon (Den et al., 2007), as discussed below. As they were largely dealing with nonlexicalized words, Bilac and Tanaka (2004) used a dynamic programming model trained on a relatively small (13,000) list of katakana words, and reported a high precision in their segmentation. Nakazawa et al. (2005) used a larger lexicon in combination with the JUMAN analyzer and reported a similar high precision. 3.2 and Satō, 2003) indicates a similar proportion. (In both dictionaries loanwords from languages other than English are marked with their source language.) This r"
U12-1009,N04-1016,0,0.019131,"Missing"
U12-1009,I05-1060,0,0.292118,"ollections of translated loanwords and a large English n-gram corpus, and in testing is performing with high levels of precision and recall. 2 Prior Work There has not been a large amount of work published on the automatic and semiautomatic extraction and translation of Japanese loanwords. Much that has been reported has been in areas such as backtransliteration (Matsuo et al., 1996; Knight and Graehl, 1998; Bilac and Tanaka, 2004), or on extraction from parallel bilingual corpora (Brill et al., 2001). More recently work has been carried out exploring combinations of dictionaries and corpora (Nakazawa et al., 2005), although this lead does not seem to have been followed further. Both Bilac and Tanaka (2004) and Nakazawa et al. (2005) address the issue of segmentation of MWEs. This is discussed in 3.1 below. 1 In addition to katakana, loanwords use the ー (chōoN) character for indicating lengthened vowels, and on rare occasions the ヽ and ヾ syllable repetition characters. 3 Role and Nature of Katakana Words in Japanese As mentioned above, loan words in Japanese are currently written in the katakana script. This is an orthographical convention that has been applied relatively strictly since the late 1940s,"
U12-1009,W09-2900,0,\N,Missing
U12-1009,J98-4003,0,\N,Missing
U12-1010,2010.amta-papers.20,0,0.785958,"endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of items that together can be used to indirectly measure it. In psychology, the term construct validity refers to the degree to which inferen"
U12-1010,W06-3114,0,0.156869,"rence set of translation pairs (Ikehara et al., 1994). While this methodology is capable of capturing longitudinal progress for a given MT system, it is prohibitively expensive and doesn’t scale well to multisystem comparison. The annual workshop for statistical machine translation (WMT) has, over recent years, been the main forum for collection of human assessment of translation quality, despite this not being the main focus of the workshop (which is to provide a regular cross-system comparison over standardized datasets for a variety of language pairs by means of a shared translation task) (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012). Figure 2 shows the approaches used for human judgments of translation quality at the annual workshops. To summarize, across the ﬁeld of machine translation human judges have been asked to assess translation quality in a variety of ways: adequacy being a two-item assessment); • Using different labels (for example, asking which translation is better or asking which is more adequate); • Ordinal level scales (ranking a numbe"
U12-1010,P02-1040,0,0.0942272,"ations. To alleviate these concerns, direct human judgments of translation quality are also collected when possible. During the evaluation of MT shared tasks, for example, human judgments of MT outputs have been used to determine the ranking of participating systems. The same human judgments can also be used in the evaluation of automatic measures, by comparing the degree to which automatic scores (or ranks) of translations correlate with them. This aspect of MT measurement is discussed shortly. One well-known example of an automatic metric is the BLEU (bilingual evaluation understudy) score (Papineni et al., 2002). Computation of a BLEU score for a system, based on a set of candidate translations it has generated, requires only that sets of corresponding reference translations be made available, one per candidate. The ease – and repeatability – of such testing has meant that BLEU is popular as a translation effectiveness measure. But that popularity does not bestow any particular superiority, and, BLEU suffers from drawbacks (Callison-Burch et al., 2006). (As an aside, we note that in all such repeatable scoring arrangements, every subsequent experiment must be designed so that there is clear separatio"
U12-1010,2003.mtsummit-papers.44,0,0.0172878,"scratch (White et al., 1994). Adequacy is the degree to which the information in the source language string is preserved in the translation,1 while ﬂuency is the determination of whether the translation is a well-formed utterance in the target language and ﬂuent in context. Subsequently, many of the large corporate machine translation systems used regression testing to establish whether changes or new modules had a positive impact on machine translation quality. Annotators were asked to select which of two randomly-ordered translations (one from each system) they preferred (Bond et al., 1995; Schwartz et al., 2003), and this was often performed over a reference set of translation pairs (Ikehara et al., 1994). While this methodology is capable of capturing longitudinal progress for a given MT system, it is prohibitively expensive and doesn’t scale well to multisystem comparison. The annual workshop for statistical machine translation (WMT) has, over recent years, been the main forum for collection of human assessment of translation quality, despite this not being the main focus of the workshop (which is to provide a regular cross-system comparison over standardized datasets for a variety of language pair"
U12-1010,1994.amta-1.25,0,0.679144,"Missing"
U12-1010,W11-2101,0,0.0624707,"Missing"
U12-1010,E06-1032,0,0.0414097,"them. This aspect of MT measurement is discussed shortly. One well-known example of an automatic metric is the BLEU (bilingual evaluation understudy) score (Papineni et al., 2002). Computation of a BLEU score for a system, based on a set of candidate translations it has generated, requires only that sets of corresponding reference translations be made available, one per candidate. The ease – and repeatability – of such testing has meant that BLEU is popular as a translation effectiveness measure. But that popularity does not bestow any particular superiority, and, BLEU suffers from drawbacks (Callison-Burch et al., 2006). (As an aside, we note that in all such repeatable scoring arrangements, every subsequent experiment must be designed so that there is clear separation between training and test data, to avoid any risk of hill-climbing and hence over-ﬁtting.) Automatic Measurement of MT The automatic evaluation of MT system output has long been an objective of MT research, with several of the recommendations of the early ALPAC Report (ALPAC, 1966), for example, relating to evaluation: 1. Practical methods for evaluation of translations; . . . 3. Evaluation of quality and cost of various sources of translation"
U12-1010,W07-0718,0,0.47099,"a more robust approach is to ﬁnd ways of increasing the reliability of the human judgments we use as the yard-stick for automatic metrics by endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of"
U12-1010,W08-0309,0,0.291115,"ﬁnd ways of increasing the reliability of the human judgments we use as the yard-stick for automatic metrics by endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of items that together can be u"
U12-1010,W12-3102,0,\N,Missing
U12-1010,W09-0401,0,\N,Missing
U12-1010,W10-1703,0,\N,Missing
U12-1016,P05-1045,0,0.113548,"Missing"
U12-1016,C00-2137,0,0.0415274,"tures boosts accuracy. Ultimately, the best-performing classiﬁer utilised the top result from both DBpedia and GeoNames, using the bag-of-toponyms and topresult frequency features, achieving an accuracy of 0.892, well above the accuracy of both the majority class baseline at 0.415 and the simple bagof-words classiﬁer at 0.729, and only slightly below the human-based upper bound of 0.969. The difference between this best-performing SVM classifer and the majority vote classiﬁer of the same toponym resolution approach was found to be statistically signiﬁcant (p = .001) using randomization tests (Yeh, 2000). 5 Conclusion and Future Work We have demonstrated that NLP approaches paired with toponym resolution are highly successful at identifying the study region from the abstracts of publications within the environmental science domain, with our best classiﬁer achieving an accuracy of 0.892, compared to a human-based upper bound of 0.969. Possible future work could include weighting of different toponym granularities, exploiting geo-spatial relationships between identiﬁed toponyms, and domain-adapting a NER for the environmental sciences. Acknowledgments NICTA is funded by the Australian Governmen"
U13-1004,2001.mtsummit-papers.68,0,0.0462489,"between systems, and to be an accurate proxy for the properties of the systems being studied. For machine translation (MT), measurement has been a combination of human judgments and automated measurements. With the aim of removing system biases and creating robust comparisons, there has been extensive use of workshops and shared tasks such as the ongoing Workshops on Statistical Machine Translation (WMT) and the NIST Open Machine Translation (OpenMT) evaluations. The basis of system evaluation is generally human judgments, which have also been used to evaluate automatic metrics such as BLEU (Papineni et al., 2001), under the assumption that a metric that correlates strongly with human judgments is more valid than a metric with weak correlation. Human evaluation of MT thus forms the foundation of evaluation in empirical MT, regardless of whether a particular evaluation makes use of human judges or automatic metrics. The current methodology used for the task of human evaluation in MT is problematic, however, as assessments carried out by expert judges are highly inconsistent. Even when a single expert judge is asked to assess the same pair of translations in two separate sittings, the second judgment is"
U13-1004,P11-2040,0,0.0303281,"Figure 1: Screen shot for base configuration for fluency assessments, including 100 point visual analog scale (VAS), marked but not labeled at 25-50-75. ments for Spanish and French. The reverse occurs for translation into German, however, where less than one third of completed Human Intelligence Tasks (HITs) were carried out by workers that reached the quality control threshold. clude seven language pairs across all participating systems from WMT 2012 (Callison-Burch et al., 2012). Previous work has shown the advantages of collecting judgments on a continuous rating scale for NLP evaluation (Belz and Kow, 2011) in general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency"
U13-1004,D08-1027,0,0.0785006,"Missing"
U13-1004,W10-0701,0,0.0139675,"ference of means test undertaken, and the resulting p-value then used as a reliability estimate. A threshold (for example, p < 0.05) can then be applied to select the reliable workers. Careless judges have a high p-value, while judges who are both skilled and conscientious have a low p-value. This relationship can be validated by direct inspection of the judgments performed. 4 (1) 5 AMT Lessons Amazon’s Mechanical Turk and other crowdsourcing services are widely used in NLP to collect data (Snow et al., 2008), with guides available that provide advice on how best to make use of such services (Callison-Burch and Dredze, 2010). Whether engaging with crowd-sourcing services such as AMT as a requester or worker, however, there is some degree of risk, primarily because of the anonymity that is assured by the services. The requester, in providing payment for potentially large volumes of work, is vulnerable to substandard or even robotically completed HITs. In this regard there is a clear sense of “buyer beware” that is part and parcel of using crowd-sourcing services. The worker, on the other hand, earns a relatively low hourly rate, and faces an ongoing risk of having completed HITs declined and of not being reimburse"
U13-1004,W07-0718,0,0.0382128,"nt” with a phrase more commonly used to refer to language, that is, whether the text is clearly written, denoted in Table 1 as the written approach. The third dimension is whether or not to include a reference translation (the “south” variant in Figure 2). An assessment of fluency independent of adequacy and without a reference translation provides at least one part of an overall evaluation that will not be biased in favor of systems that happen to produce reference-like translations. However, in the past, fluency judgments have generally been carried out with a reference translation present (Callison-Burch et al., 2007). In this part of the evaluation the instructions described the task as assessing automatic translations as opposed to a simple rating of the fluency of the text, since without this context it would be difficult to explain what a reference in fact was. With each translation that was presented a note was displayed on screen to the users as follows: An equivalent piece of fluent text is provided in gray for your reference. The final dimension explored (the “north” variant in Figure 2) is the effect of the presence of source language words in translations. Many of 18 Configuration Anchor labels Q"
U13-1004,J11-2010,0,0.0141478,"part and parcel of using crowd-sourcing services. The worker, on the other hand, earns a relatively low hourly rate, and faces an ongoing risk of having completed HITs declined and of not being reimbursed for diligently completed work. Recently developed online tools provide slightly more power to workers, by enabling requester reviewing and hence allowing workers to identify requesters who too readily reject completed HITs (Irani and Silberman, 2013). And even when workers are paid, rather than volunteers, payment rates are well below the minimum wages that apply in most developed countries (Fort et al., 2011). Judge Consistency Table 2 shows consistency of human judges for judgments of translations repeated by the same and distinct judges. Mean scores for same judge ask again repeat items show no significant difference. At the same time, mean scores for degraded 20 Human Ethics Posting HITs on a service such as AMT amounts to research involving humans, and human ethics potentially becomes a concern (Gilles et al., 2011; Fort et al., 2011). Research institutes tend to evolve their own specific human ethics policies for crowd-sourcing tasks. In our particular institution, a two-stage procedure for h"
U13-1004,U12-1010,1,0.848717,"n general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency. Graham et al. (2012) suggest translation quality should be measured as a hypothetical construct, where measurements that employ more items (dimensions of measurement) as opposed to fewer are considered more valid. Under this criterion, a two-item (fluency and adequacy) scale is more valid than a single-item translation quality measure, further motivating the inclusion of fluency as an assessment item for measurement of translation quality. Overall, just under half of the Turkers carried out the human evaluation to a standard that met our quality control threshold. In addition, proportions of good quality workers"
U13-1004,W13-2305,1,0.798038,"ding 100 point visual analog scale (VAS), marked but not labeled at 25-50-75. ments for Spanish and French. The reverse occurs for translation into German, however, where less than one third of completed Human Intelligence Tasks (HITs) were carried out by workers that reached the quality control threshold. clude seven language pairs across all participating systems from WMT 2012 (Callison-Burch et al., 2012). Previous work has shown the advantages of collecting judgments on a continuous rating scale for NLP evaluation (Belz and Kow, 2011) in general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency. Graham et al. (2012) suggest translation quality should be measured as a h"
U13-1004,W12-3102,0,\N,Missing
U13-1004,P02-1040,0,\N,Missing
U13-1004,W13-2201,0,\N,Missing
U13-1018,N10-1124,0,0.0238308,"or moving towards an evidence-based model of environmental management have obvious parallels to the motivation for the practice of EBM. Although the structure of evidence will differ between the domains, many of the techniques applied in research for EBM are likely to have application for our current task. Successful applications of NLP to EBM include sentence categorization for information on randomized controlled trials (Chung, 2009; Kim et al., 2011), the labelling of sentences with “PICO” (Patient/Problem, Intervention, Comparison and Outcome) labels to aid clinical information retrieval (Boudin et al., 2010), and the automatic assignment of Medical Subject Headings (MeSH) terms to PubMed abstracts (Gaudinat and Boyer, 2002). 3 Resources In this section, we provide details of key resources used in this paper, namely: • Eco Evidence, a manually-curated database of metadata for environmental science literature, which provides the basis of the data used in our experiments • DBpedia and GeoNames, as resources for toponym resolution • the K¨oppen-Geiger Climate Map of Peel et al. (2007) 124 3.1 Eco Evidence Eco Evidence (Webb et al., 2011) is a tool for literature review and evidence synthesis, consist"
U13-1018,P05-1045,0,0.00890167,"5 21 0 39 13 0 236 NA 1055 51 162 5 278 102 1 1654 SA 9 98 7 1 3 1 0 119 OC 98 10 0 1 0 1 0 110 MU 13 7 1 2 1 113 0 137 OT 1 1 0 0 2 1 0 5 T OTAL 2478 380 356 12 463 286 2 3977 Table 1: Distribution for the gold standard climate classifications across the gold standard study region classifications (EU = Europe, AU = Australia, AF = Africa, AN = Antarctica, AS = Asia, NA = North America, SA = South America, OC = Oceania [other than Australia], MU = Multiple, OT = Other; the boldfaced number indicates the majority-class for a given continent) (2012). First, the Stanford Named Entity Recogniser (Finkel et al., 2005) is used to identify location-type NEs in each abstract. Each NE is then mapped to a set of toponyms, based on DBpedia or GeoNames, and the counts of toponyms are aggregated into bag-of-toponyms (BoT) features. Finally, a linear-kernel support vector machine (SVM) is used to train a supervised classifier. We experiment with both: (1) study region classification (at the continent level), and a majorityclass classification for that continent; and (2) replacement of continent-level classes from the original paper with climate-based classes. In the latter case, the toponyms are resolved to climate"
U13-1018,U12-1016,1,0.893088,"eek to automate the climate annotation process with natural language processing (NLP) techniques. The task of climate type classification is complex as although the label set is relatively small, the geographic granularity is fine and toponym ambiguity becomes a significant problem — toponyms commonly mentioned in the environmental sciences (e.g. Murray River) are often large and cover multiple climates, which presents difficulties for a point-based representation of toponyms. Initially, experiments are run to examine the effectiveness of the direct application of the classifiers developed by Willett et al. (2012) for study region classification. We then investigate methods for adapting these techniques to the climate task through the modification of the toponym resolution component of our classifiers. These approaches include utilizing a K¨oppen-Geiger climate classification world map to resolve toponyms to climate instead of region, in addition to experiments with targeting types of toponyms reliable for identifying climate. 2 Related Work The methodology used to extract and disambiguate toponyms is based on a standard approach to geographic information retrieval, which was presented, e.g., by Stokes"
U15-1010,W06-1615,0,0.150222,"r annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 words) were randomly selected for manual annotation, based on the four NE types provided in the CoNLL-2003 dataset: LOCATION ( LOC ), ORGANISATION (ORG), PERSON (PER), and MISCELLANEOUS (MISC). The annotation was c"
U15-1010,D14-1096,1,0.827424,"of ORG, though in the gold standard they don’t belong to any entity type. This error was reduced drastically through the addition of the in-domain financial data in training, improving the overall performance of the model. Ultimately, the purely in-domain training stratagem in Experiment4 outperforms the mixed data setup (Experiment3), indicating that domain context is critical for the task. Having said that, the results of our study inform the broader question of out-of-domain applicability of NER models. Furthermore, they point to the value of even a small amount of in-domain training data (Duong et al., 2014). 6 Conclusions Risk assessment is a crucial task for financial institutions such as banks because it helps to estimate the amount of capital they should hold to promote their stability and protect their clients. Manual extraction of relevant information from text88 LOC MISC Actual ORG PER NA NE Precision LOC 20 0 0 0 12 0.625 Predicted MISC ORG PER 0 3 2 7 0 0 0 16 0 0 0 202 2 24 8 0.778 0.372 0.953 O 14 0 40 14 – Recall 0.513 1.000 0.286 0.935 Table 3: Confusion matrix for the predictions over F IN 3 using the model from Experiment3, including the precision and recall for each class (“NA NE”"
U15-1010,demiros-etal-2000-named,0,0.0734988,"d, but the number of words in the corpus is 30,000 words for training and 140,000 for testing. The approach involved the creation of rules by hand; this is a time-consuming task, and the overall recall is low compared to other extraction methods. Related Work Another rule-based approach was proposed by Sheikh and Conlon (2012) for extracting information from financial data (combined quarterly reports from companies and financial news) with the aim of assisting in investment decision-making. Most prior approaches to information extraction in the financial domain make use of rule-based methods. Farmakiotou et al. (2000) extract entities from financial news using grammar rules 85 ing over unlabelled target domain data. Qu et al. (2015) showed that a graph transformer NER model trained over word embeddings is more robust cross-domain than a model based on simple lexical features. Our approach is based on large amounts of labelled data from a source domain and small amounts of labelled data from the target domain (i.e. financial agreements), drawing inspiration from previous research that has shown that using a modest amount of labelled in-domain data to perform transfer learning can substantially improve class"
U15-1010,N06-1010,0,0.0316701,"of the text and linguistic forms, and then creates text grammars. Finally, the approach uses a parser to process the document content. Although the authors do not present results, they argue that when applied to a test set of 1,000 criminal cases, they were able to identify the required information. P P p(s|o) = In order to reduce the need for annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset o"
U15-1010,P05-3015,0,0.0421595,"t. Although the authors do not present results, they argue that when applied to a test set of 1,000 criminal cases, they were able to identify the required information. P P p(s|o) = In order to reduce the need for annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 w"
U15-1010,K15-1009,1,0.84904,"Missing"
U15-1010,E12-2021,0,0.0977471,"Missing"
U15-1015,P12-1042,0,0.0514246,"Missing"
U15-1015,C12-1003,0,0.0398439,"Missing"
U15-1015,N15-1171,0,0.0305336,"Missing"
U15-1015,P15-2072,0,0.0220386,"Missing"
U15-1015,N09-1057,0,0.0602606,"Missing"
U15-1015,W06-2915,0,0.0869091,"Missing"
U15-1015,P09-1026,0,0.0798604,"Missing"
U15-1015,P15-1157,0,0.0605363,"Missing"
U15-1015,W15-4614,0,0.0609236,"Missing"
U17-1002,W14-4337,0,0.0495726,"Missing"
U17-1002,D14-1181,0,0.00276342,"ch performs consistently well over all tasks. We propose a unified model generalising weight tying and in doing so, make the model more expressive. The proposed model achieves uniformly high performance, improving on the best results for memory network-based models on the bAbI dataset, and competitive results on Dialog bAbI. 1 Introduction Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016), speech recognition (Graves et al., 2013), and various natural language processing tasks (Bahdanau et al., 2014; Kim, 2014; Xiong et al., 2016). Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014, 2016; Rae et al., 2016). Of particular interest to this work is the work by Sukhbaatar et al. (2015), on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning (Weston et al., 2016) and goal-oriented dialogue tasks (Bordes and Weston, 2016). Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question,"
U17-1002,E17-1001,1,0.652188,"ayer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in Sukhbaatar et al. (2015), the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed. In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task. This is realised through the use of a gating vector, inspired by Liu and Perez (2017). Our method achieves the best performance for a memory network-based model on the bAbI dataset, superior to both adjacent and layer-wise weight tying, and competitive results on Dialog bAbI. The paper is organised as follows: after we review N2N and related reasoning models in Section 2, we describe our motivation and detail the elements of our proposed model in Section 3. Section 4 and 5 present the experimental results on the bAbI and Dialog bAbI datasets with analyses in Section 6. Lastly, Section 7 concludes the paper. 2 Related Work End-to-End Memory Networks: Building on top of memory n"
U17-1002,W14-4012,0,0.0143448,"Missing"
U17-1003,W13-1106,0,0.028906,"Missing"
U17-1003,I11-1019,0,0.0318106,"dings averaged with TF-IDF scores as weights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different manifestos across countries over time can provide deeper comparative insights into"
U17-1003,W16-5612,0,0.0450127,"eights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different manifestos across countries over time can provide deeper comparative insights into political change. Existing classi"
U17-1003,W17-2906,0,0.447667,"Missing"
U17-1003,E17-2109,0,0.109668,"nguages. In this work, we focus on cross-lingual fine-grained thematic classification (57 categories in total), where we have labeled data for all the languages. For the document-level quantification task, much work has used label count aggregation of manually-annotated sentences as features (Lowe et al., 2011; Benoit and D¨aubler, 2014), while other work has used dictionary- based supervised methods, or unsupervised factor analysis based techniques (Hjorth et al., 2015; Bruinsma and Gemenis, 2017). The latter method uses discrete word representations and deals with mono-lingual text only. In Glavas et al. (2017), the authors lever3 Manifesto Text Analysis In the CMP, trained annotators manually label manifesto sentences according to the 57 finegrained political categories (shown in Table 5), which are grouped into seven policy areas: External Relations, Freedom and Democracy, Political System, Economy, Welfare and Quality of Life, Fabric of Society, and Social Groups. Political parties either write their promises as a bulleted list of individual sentences, or structured as paragraphs (an example is given in Figure 4), providing more information on topic coherence. Also the length of documents, measur"
U17-1003,W14-2715,0,0.178933,"or extreme left and right positions. They represent the documents using word embeddings averaged with TF-IDF scores as weights. All these approaches model the sentence and documentlevel tasks separately. Related Work The recent adoption of NLP methods has led to significant advances in the field of Computational Social Science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013). Some popular tasks addressed with political text include: party position analysis (Biessmann, 2016); political leaning categorization (Akoglu, 2014; Zhou et al., 2011); stance classification (Sridhar et al., 2014); identifying keywords, themes & topics (Karan et al., 2016; Ding et al., 2011); emotion analysis (Rheault, 2016); and sentiment analysis (Bakliwal et al., 2013). The source data includes manifestos, political speeches, news articles, floor debates and social media posts. With the increasing availability of large-scale datasets and computational resources, large-scale comparative political text analysis has gained the attention of political scientists (Lucas et al., 2015). For example, rather than analyzing the political manifestos of a particular party during an election, mining different man"
U17-1003,N16-1149,1,0.88579,"Missing"
U17-1003,W16-2102,0,0.177345,"Missing"
U17-1005,P06-4018,0,0.0172496,"Missing"
U17-1005,N04-1024,0,0.102107,"ntent, they require a large training dataset. Due to the limited availability of labeled Wikipedia articles, we supplement the document embedding with hand-engineered features, which can lead to better quality prediction even with a relatively small volume of training data. Cheng et al. (2016) adopt a similar idea in an app recommendation scenario. and complexity. The effects of grammatical and mechanics errors on the quality of an essay are measured via word and POS n-gram features and “mechanics” features (e.g., spelling, capitalization, and punctuation), respectively (Persing and Ng, 2013; Higgins et al., 2004). To measure content quality in cQA, researchers exploit various features from different sources, such as the content itself, the user’s profile, asking and answering interaction among users, and usage of the content. The most common feature used is the content length (Jeon et al., 2006; Suryanto et al., 2009). Agichtein et al. (2008) explore syntactic and semantic complexity features, such as the entropy of word lengths and various readability scores. Le et al. (2016) exploit user’s characteristic features (e.g., the grade level or the rank of the user in cQA) and the user’s historical featur"
U17-1005,P82-1020,0,0.831616,"Missing"
U17-1005,D14-1162,0,0.0846814,"Missing"
U17-1005,P13-1026,0,0.0181532,"es from the article content, they require a large training dataset. Due to the limited availability of labeled Wikipedia articles, we supplement the document embedding with hand-engineered features, which can lead to better quality prediction even with a relatively small volume of training data. Cheng et al. (2016) adopt a similar idea in an app recommendation scenario. and complexity. The effects of grammatical and mechanics errors on the quality of an essay are measured via word and POS n-gram features and “mechanics” features (e.g., spelling, capitalization, and punctuation), respectively (Persing and Ng, 2013; Higgins et al., 2004). To measure content quality in cQA, researchers exploit various features from different sources, such as the content itself, the user’s profile, asking and answering interaction among users, and usage of the content. The most common feature used is the content length (Jeon et al., 2006; Suryanto et al., 2009). Agichtein et al. (2008) explore syntactic and semantic complexity features, such as the entropy of word lengths and various readability scores. Le et al. (2016) exploit user’s characteristic features (e.g., the grade level or the rank of the user in cQA) and the u"
U17-1005,P14-1144,0,0.0161101,"d for quality class prediction. Dalip et al. (2009) find that textual features extracted from articles are the best indicators to distinguish articles of different quality classes. For the related task of automatic essay scoring, the following dimensions are often captured: topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mechanics. To measure whether an essay is relevant to its “prompt” (i.e., the description of the essay topic), lexical overlap and semantic overlap between an essay and its corresponding prompt can be used (Phandi et al., 2015; Persing and Ng, 2014). Lexical overlap and semantic similarity features are exploited to measure coherence between different discourse elements, sentences, and paragraphs (Higgins et al., 2004; McNamara et al., 2015). Attali and Burstein (2004) explore word features, such as the number of verb formation errors, average word frequency, and average word length, to measure word usage and lexical complexity. Intelli-Metric (Rudner et al., 2006) uses sentence structure features, such as syntactic variety and readability, to measure sentence variety 3 Problem Definition We formulate quality assessment of Wikipedia artic"
U17-1005,D15-1049,0,0.0236607,"sion model is proposed for quality class prediction. Dalip et al. (2009) find that textual features extracted from articles are the best indicators to distinguish articles of different quality classes. For the related task of automatic essay scoring, the following dimensions are often captured: topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mechanics. To measure whether an essay is relevant to its “prompt” (i.e., the description of the essay topic), lexical overlap and semantic overlap between an essay and its corresponding prompt can be used (Phandi et al., 2015; Persing and Ng, 2014). Lexical overlap and semantic similarity features are exploited to measure coherence between different discourse elements, sentences, and paragraphs (Higgins et al., 2004; McNamara et al., 2015). Attali and Burstein (2004) explore word features, such as the number of verb formation errors, average word frequency, and average word length, to measure word usage and lexical complexity. Intelli-Metric (Rudner et al., 2006) uses sentence structure features, such as syntactic variety and readability, to measure sentence variety 3 Problem Definition We formulate quality assess"
U17-1008,W17-2320,0,0.0276588,"and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detection methods over different domains, and found that performance often suffers without in-domain training data. Miller et al. (2017) also investigated the use of different unsupervised domain adaptation algorithms for negation detection in the clinical domain and found that such algorithms only achieved marginal increase in performance compared to systems that use in-domain training data. ]], adv [[SPEC Such differences in usage between veterinary clinicians and other medical professionals such as radiologists are a major focus of this work, in adapting the annotation framework from BioScope to this new domain. This paper attempts to address the following research questions: (1) Can the task of negation/speculation detecti"
U17-1008,W09-1304,0,0.0263696,"al infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detectio"
U17-1008,W09-1105,0,0.0264146,"al infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detectio"
U17-1008,D08-1075,0,0.0312125,"ncratic in nature. Second, while radiology clinical notes are often professionally transcribed from an oral account by the clinician, in the veterinary general practice context, notes are authored directly by the clinician as text. Inevitably, this is done under time pressure, meaning that the text is often ungrammatical and lacks punctuation. Examples (3) and (4) exemplify negation and speculation in VetCompass: (3) Mm - moist [[NEG no skin tent NEG ]] (4) Adv [[SPEC poss bacterial infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante a"
U17-1008,C12-2096,0,0.0600722,"Missing"
U17-1008,N15-1168,0,0.0224595,"otes? This paper describes the process of annotating negation and speculation in veterinary clinical records. We then demonstrate that the task of negation and speculation detection can be successfully applied to veterinary clinical notes using a simple conditional random field (CRF) model. We additionally show that models trained on a related out-of-domain corpus such as the BioScope have utility over veterinary clinical records, in particular for negation detection. 2 2.2 Veterinary NLP We are only aware of a few papers that have applied natural language processing in the veterinary domain. Ding and Riloff (2015) conducted work on detecting mentions of medication usage in a discussion forum for veterinarians, and categorizing the usage of the medication. A classifier determines whether each word is part of a medication mention using features such as the POS tags and neighbouring words The output of the medication mention detector is used by another classifier to determine its usage category such as whether the clinician prescribed the medication or changed it. Text classification is a task that had been previously applied to veterinary clinical records. Anholt et al. (2014) performed classification of"
U17-1008,E12-2021,0,0.115484,"Missing"
U17-1008,W08-0606,0,0.567458,"data, in the veterinary clinical note domain, and describes a series of experiments whereby we port a CRF-based method across from the BioScope corpus to this novel domain. 1 Introduction Negation and speculation are common in clinical texts, yet pose a challenge for natural language processing of these texts. Negation indicates the absence or opposite of something, and is defined within the previously released BioScope corpus (a collection of biomedical and clinical documents annotated for the task of negation/speculation detection) to be the “implication of the non-existence of something” (Szarvas et al., 2008). For example, the statement no abnormalities were found in the patient indicates the absence of abnormalities in the patient. Speculation is used to indicate uncertainty or the possibility of something, and is defined within BioScope to be statements of “the possible existence of something”. For example, there is possible bacterial infection indicates that an infection might be present, without any certainty that it is. Both are commonly used in clinical texts as a means of ruling out diagnostic possibilities and hypothesising. This paper will discuss a method for detecting negation and specu"
U18-1009,W11-0815,0,0.0318377,"., 2016; Cordeiro et al., to appear). In the case of couch potato “an idler who spends much time on a couch (usually watching television)”, e.g., on a scale of [0, 1] the overall compositionality may be judged to be 0.3, and the compositionality of couch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in p"
U18-1009,P14-1023,0,0.132531,"s to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in performing a comparative study of the performance of a range of off-the-shelf representation learning methods over the task of MWE compositionality prediction. Our contributions are as follows: (1) we show that, despite their effectiveness over a range of other tasks, recent off-the-shelf character- and document-level embedding learning methods are i"
U18-1009,Q17-1010,0,0.279544,"compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in performing a comparative study of the performance of a range of off-the-shelf representation learning methods over the task of MWE compositionality prediction. Our contributions are as follows: (1) we show that, despite their effectiveness over a range of other tasks, recent off-the-shelf character- and document-level embedding learning methods are inferior to simple word2vec at modelling MWE com"
U18-1009,D17-1070,0,0.486399,"relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in performing a comparative study of the performance of a range of off-the-shelf representation learning methods over the task of MWE compositionality prediction. Our contributions are as follows: (1) we show that, despite their effectiveness over a range of other tasks, recent off-the-shelf character- and document-level embedding learning methods are inferior to simple word2vec at modelling MWE compositionality; and (2)"
U18-1009,D14-1162,0,0.102007,"ch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in performing a comparative study of the performance of a range of off-the-shelf representation learning methods over the task of MWE compositionality prediction. Our contributions are as follows: (1) we show that, despite their effectiveness over a range of ot"
U18-1009,N18-1202,0,0.0423555,"and Cook, 2018). There has also been work in the extension of word embeddings to document embeddings that map entire sentences or documents to vectors (Le and Mikolov, 2014; Conneau et al., 2017). 3 In a character embedding model, the vector for a word is constructed from the character n-grams that compose it. Since character n-grams are shared across words, assuming a closed-world alphabet,2 these models can generate embeddings for OOV words, as well as words that occur infrequently. The two character-level embedding models we experiment with are fastText (Bojanowski et al., 2017) and ELMo (Peters et al., 2018), as detailed below. Embedding Methods We use two character-level embedding models (fastText and ELMo) and two document-level models (doc2vec and infersent) to compare with word-level word2vec, as used in the state-of-theart method of Salehi et al. (2015a). In each case, we use canonical pre-trained models, with the exception of word2vec, which must be trained over data with appropriate tokenisation to be able to generate MWE embeddings, as it treats words atomically and cannot generate OOV words. fastText We used the 300-dimensional model pre-trained on Common Crawl and Wikipedia using CBOW."
U18-1009,P16-2026,0,0.45014,"g varying levels of compositionality, but ultimately are not as effective as a simple word2vec baseline. However they have the advantage over word-level models that they do not require token-level identification of MWEs in the training corpus. 1 Introduction In recent years, the study of the semantic idiomaticity of multiword expressions (“MWEs”: Baldwin and Kim (2010)) has focused on compositionality prediction, a regression task involving the mapping of an MWE onto a continuous scale, representing its compositionality either as a whole or for each of its component words (Reddy et al., 2011; Ramisch et al., 2016; Cordeiro et al., to appear). In the case of couch potato “an idler who spends much time on a couch (usually watching television)”, e.g., on a scale of [0, 1] the overall compositionality may be judged to be 0.3, and the compositionality of couch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta"
U18-1009,I11-1024,0,0.126187,"ffective at modelling varying levels of compositionality, but ultimately are not as effective as a simple word2vec baseline. However they have the advantage over word-level models that they do not require token-level identification of MWEs in the training corpus. 1 Introduction In recent years, the study of the semantic idiomaticity of multiword expressions (“MWEs”: Baldwin and Kim (2010)) has focused on compositionality prediction, a regression task involving the mapping of an MWE onto a continuous scale, representing its compositionality either as a whole or for each of its component words (Reddy et al., 2011; Ramisch et al., 2016; Cordeiro et al., to appear). In the case of couch potato “an idler who spends much time on a couch (usually watching television)”, e.g., on a scale of [0, 1] the overall compositionality may be judged to be 0.3, and the compositionality of couch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy an"
U18-1009,N15-1099,1,0.89387,"Missing"
U18-1009,W15-0909,1,0.941522,"al., to appear). In the case of couch potato “an idler who spends much time on a couch (usually watching television)”, e.g., on a scale of [0, 1] the overall compositionality may be judged to be 0.3, and the compositionality of couch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper builds on directly, in performing a comparati"
U18-1009,W06-1204,0,0.0255126,"dy et al., 2011; Ramisch et al., 2016; Cordeiro et al., to appear). In the case of couch potato “an idler who spends much time on a couch (usually watching television)”, e.g., on a scale of [0, 1] the overall compositionality may be judged to be 0.3, and the compositionality of couch and potato as 0.8 and 0.1, respectively. The main motivation for the study of compositionality is to better understand the semantic of the compound and the semantic relationships between the component words of the MWEs, which has applications in various information retrieval and natural language processing tasks (Venkatapathy and Joshi, 2006; Acosta et al., 2011; Salehi et al., 2015b). Separately, there has been burgeoning interest {salehi.b,tbaldwin}@unimelb.edu.au in learning distributed representations of words and their meanings, starting out with word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and now also involving the study of character- and document-level models (Baroni et al., 2014; Le and Mikolov, 2014; Bojanowski et al., 2017; Conneau et al., 2017). This work has been applied in part to predicting the compositionality of MWEs (Salehi et al., 2015a; Hakimi Parizi and Cook, 2018), work that this paper bui"
U18-1010,W16-2302,0,0.0389002,"Missing"
U18-1010,N15-1124,1,0.828874,"is the mean standardised score of its translations after discarding scores that do not meet quality control criteria. The noise in worker scores is cancelled out when a large number of translations are averaged. To obtain accurate scores of individual translations, multiple judgments are collected and averaged. As we increase the number of annotators per translation, there is greater consistency and reliability in the mean score. This was empirically tested by showing that there is high correlation between the mean of two independent sets of judgments, when the sample size is greater than 15 (Graham et al., 2015). However, both these tests are based on a sample-size of 10 items, and, as such, the first test has low power; we show that it filters out a large proportion of the total workers. One solution would be to increase the sample size of the degraded-reference-pairs, but this would be at the expense of the number of useful worker annotations. It is better to come up with a model that would use the scores of all workers, and is more robust to low quality scores. Automatic metrics such as BLEU (Papineni et al., 2002) are generally evaluated using the Pear2016a), and has replaced RR since 2017 (Bojar"
U18-1010,P13-1139,0,0.0302029,"ilter out unreliable workers. Background The Conference on Machine Translation (WMT) annually collects human judgements to evaluate the MT systems and metrics submitted to the shared tasks. The evaluation methodology has evolved over the years, from 5 point adequacy and fluency rating, to relative rankings (“RR”), to DA. With RR, annotators are asked to rank translations of 5 different MT systems. In earlier years, the final score of a system was the expected number of times its translations score better than translations by other systems (expected wins). Bayesian models like Hopkins and May (Hopkins and May, 2013) and Trueskill (Sakaguchi et al., 2014) were then proposed to learn the relative ability of the MT systems. Trueskill was adopted by WMT in 2015 as it is more stable and efficient than the expected wins heuristic. DA was trialled at WMT 2016 (Bojar et al., Nitika Mathur, Timothy Baldwin and Trevor Cohn. 2018. Towards Efficient Machine Translation Evaluation by Modelling Annotators. In Proceedings of Australasian Language Technology Association Workshop, pages 77−82. The paired Wilcoxon rank-sum test is used to test whether the worker scored degraded translations worse than the corresponding sy"
U18-1010,P02-1040,0,0.117992,"e mean of two independent sets of judgments, when the sample size is greater than 15 (Graham et al., 2015). However, both these tests are based on a sample-size of 10 items, and, as such, the first test has low power; we show that it filters out a large proportion of the total workers. One solution would be to increase the sample size of the degraded-reference-pairs, but this would be at the expense of the number of useful worker annotations. It is better to come up with a model that would use the scores of all workers, and is more robust to low quality scores. Automatic metrics such as BLEU (Papineni et al., 2002) are generally evaluated using the Pear2016a), and has replaced RR since 2017 (Bojar et al., 2017a). It is more scalable than RR as the number of systems increases (we need to obtain one annotation per system, instead of one annotation per system pair). Each translation is rated independently, minimising the risk of being influenced by the relative quality of other translations. Ideally, it is possible that evaluations can be compared across multiple datasets. For example, we can track the progress of MT systems for a given language pair over the years. Another probabilistic model, EASL (Sakag"
U18-1010,Q14-1025,0,0.0196834,"with Trueskill. Annotators score translations from 5 systems at the same time on a sliding scale, allowing users to explicitly specify the magnitude of difference between system translations. Active learning to select the systems in each comparison to increase efficiency. But it does not model worker reliability, and is, very likely, not compatible with longitudinal evaluation, as the systems are effectively scored relative to each other. In NLP, most other research on learning annotator bias and reliability has been on categorical data (Snow et al., 2008; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014). 3 Direct Assessment To measure adequacy, in DA, annotators are asked to rate how adequately an MT output expresses the meaning of a reference translation using a continuous slider, which maps to an underlying scale of 0–100. These annotations are crowdsourced using Amazon Mechanical Turk, where “workers” complete “Human Intelligence Tasks” (HITs) in the form of one or more micro-tasks. Each HIT consists of 70 MT system translations, along with an additional 30 control items: 1. degraded versions of 10 of these translations; 2. 10 reference translations by a human expert, corresponding to 10"
U18-1010,W14-3301,0,0.0382128,"Missing"
U18-1010,P18-1020,0,0.0518819,"Missing"
U18-1010,D08-1027,0,0.246619,"Missing"
U19-1007,N19-1423,0,0.0216904,"detailed below. The reasons we chose these particular tasks are as follows. First, extensive domain-specific feature engineering had taken place in each case, that we could use as the basis of our feature indicators. Second, strong neural benchmarks have been established, based on extensive experimentation with both neural and non-neural models. Our experiments in this paper are based on the state-of-the-art. We aim to explore the relative gains of our proposed method relative to the current state-of-theart for the task, which in both cases is not based on contextualised embeddings. For BERT (Devlin et al., 2019) or other contextualised encoders, the same word in different contexts will end with different embeddings, leading to localized representations of feature indicators. As such, the proposed method is not directly applicable to models such as BERT, and novel research would be required to adapt the method to such models. 3.1 Wikipedia Document Quality Assessment Dataset The Wikipedia dataset (Shen et al., 2019) consists of 29,794 English Wikipedia articles and their corresponding quality labels: Featured Article, Good Article, B-class Article, Cclass Article, Start Article, and Stub Article, in d"
U19-1007,P82-1020,0,0.802269,"Missing"
U19-1007,P14-1062,0,0.0153581,"onsistently outperform those that simply use hand-crafted features as side information. 1 Introduction and Background Text classification/regression is a fundamental problem in natural language processing. Traditional methods make use of hand-crafted features, such as the length of a document, to represent a document. A classifier/regressor is built on top of such features to learn a model (Wang and Manning, 2012; Warncke-Wang et al., 2013, 2015; Dang and Ignat, 2016). Recently, neural models such as LSTMs (Hochreiter and Schmidhuber, 1997) and convolutional neural networks (CNNs: Kim (2014); Kalchbrenner et al. (2014)) have become the de facto for text classification/regression tasks, with one oft-cited advantage being that they are able to learn implicit features as part of the representation learning. Studies employing neural models either eschew hand-crafted features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LSTM (“bi-LSTM”) to classify Wikipedia articles by their quality classes, and Shen et al. (2017) concatenate structural features (e.g., article length) and readability scores with bi-LSTMlearned document represen"
U19-1007,D14-1181,0,0.010889,"odels that consistently outperform those that simply use hand-crafted features as side information. 1 Introduction and Background Text classification/regression is a fundamental problem in natural language processing. Traditional methods make use of hand-crafted features, such as the length of a document, to represent a document. A classifier/regressor is built on top of such features to learn a model (Wang and Manning, 2012; Warncke-Wang et al., 2013, 2015; Dang and Ignat, 2016). Recently, neural models such as LSTMs (Hochreiter and Schmidhuber, 1997) and convolutional neural networks (CNNs: Kim (2014); Kalchbrenner et al. (2014)) have become the de facto for text classification/regression tasks, with one oft-cited advantage being that they are able to learn implicit features as part of the representation learning. Studies employing neural models either eschew hand-crafted features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LSTM (“bi-LSTM”) to classify Wikipedia articles by their quality classes, and Shen et al. (2017) concatenate structural features (e.g., article length) and readability scores with bi-L"
U19-1007,D14-1162,0,0.0866984,"Missing"
U19-1007,U17-1005,1,0.824366,"odels such as LSTMs (Hochreiter and Schmidhuber, 1997) and convolutional neural networks (CNNs: Kim (2014); Kalchbrenner et al. (2014)) have become the de facto for text classification/regression tasks, with one oft-cited advantage being that they are able to learn implicit features as part of the representation learning. Studies employing neural models either eschew hand-crafted features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LSTM (“bi-LSTM”) to classify Wikipedia articles by their quality classes, and Shen et al. (2017) concatenate structural features (e.g., article length) and readability scores with bi-LSTMlearned document representations for the same task. Subramanian et al. (2018) hand-engineer a set of features (e.g., the ratio of indefinite and definite articles), and concatenate them with CNNlearned document representations to predict the popularity of online petitions. Wu et al. (2018) explore the utility of hand-crafted features in NER by concatenating these features with character representations learned via an CNN and word embeddings. These representations are then fed into a biLSTM to identify na"
U19-1007,P18-2030,1,0.9109,"for text classification/regression tasks, with one oft-cited advantage being that they are able to learn implicit features as part of the representation learning. Studies employing neural models either eschew hand-crafted features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LSTM (“bi-LSTM”) to classify Wikipedia articles by their quality classes, and Shen et al. (2017) concatenate structural features (e.g., article length) and readability scores with bi-LSTMlearned document representations for the same task. Subramanian et al. (2018) hand-engineer a set of features (e.g., the ratio of indefinite and definite articles), and concatenate them with CNNlearned document representations to predict the popularity of online petitions. Wu et al. (2018) explore the utility of hand-crafted features in NER by concatenating these features with character representations learned via an CNN and word embeddings. These representations are then fed into a biLSTM to identify named entities (with the help of a CRF) and re-construct the hand-crafted features in the output simultaneously, which is achieved by combining an auto-encoder loss with"
U19-1007,P18-1216,0,0.0115575,"h value represents. To make better use of hand-crafted features, we propose a feature-guided neural training method that guides the network to map feature indicators onto (explicit or implicit) features in the document. We evaluate the effectiveness of the proposed method over two datasets for two different tasks: (1) quality assessment of Wikipedia articles, and (2) popularity prediction of online petitions. Taking stateof-the-art approaches for the respective tasks, we achieve consistent improvements when using our model. The closest work to our approach is the labelguided model training of Wang et al. (2018). They embed words and labels in the same embedding space, and compute a label-based attention score between a word and all possible labels, which is used to weight word embeddings in obtaining document representations. Our work differs in two aspects: (1) Wang et al. (2018) capture direct associations between labels and words, while we use the proxy of (potentially much higher-level) handcrafted features to guide network learning; and (2) our method does not rely on the target variable being closely related to the semantics of a document, leading to better generalisability. 2 word embedding f"
U19-1007,P12-2018,0,0.0361215,"riables. In experiments over two different tasks — quality assessment of Wikipedia articles and popularity prediction of online petitions— we demonstrate that the proposed method yields neural models that consistently outperform those that simply use hand-crafted features as side information. 1 Introduction and Background Text classification/regression is a fundamental problem in natural language processing. Traditional methods make use of hand-crafted features, such as the length of a document, to represent a document. A classifier/regressor is built on top of such features to learn a model (Wang and Manning, 2012; Warncke-Wang et al., 2013, 2015; Dang and Ignat, 2016). Recently, neural models such as LSTMs (Hochreiter and Schmidhuber, 1997) and convolutional neural networks (CNNs: Kim (2014); Kalchbrenner et al. (2014)) have become the de facto for text classification/regression tasks, with one oft-cited advantage being that they are able to learn implicit features as part of the representation learning. Studies employing neural models either eschew hand-crafted features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LS"
U19-1007,D18-1310,0,0.0220244,"features or simplistically use handcrafted features as side information. For example, Dang and Ignat (2017) propose to use a bidirectional LSTM (“bi-LSTM”) to classify Wikipedia articles by their quality classes, and Shen et al. (2017) concatenate structural features (e.g., article length) and readability scores with bi-LSTMlearned document representations for the same task. Subramanian et al. (2018) hand-engineer a set of features (e.g., the ratio of indefinite and definite articles), and concatenate them with CNNlearned document representations to predict the popularity of online petitions. Wu et al. (2018) explore the utility of hand-crafted features in NER by concatenating these features with character representations learned via an CNN and word embeddings. These representations are then fed into a biLSTM to identify named entities (with the help of a CRF) and re-construct the hand-crafted features in the output simultaneously, which is achieved by combining an auto-encoder loss with the NER loss. The motivation underlying this work is that when hand-crafted features are represented by numerical vectors and concatenated with neural network representations, there is no information on what kind"
U19-1007,N16-1174,0,0.108106,"r. 1. CNN BI LSTM: apply convolution kernels with width 2, 3, and 4 (32 for each width size) to word embeddings within a sentence, and a tanh activation function to each; pass the output of the filters through a bi-LSTM. 2. AVERAGE BI LSTM (Shen et al., 2017): average word embeddings to get the sentence representation, and run a bi-LSTM over the sequence of sentence representations. 3. S TACKED BI LSTM: feed the word embeddings in a sentence through a bi-LSTM, and the output through a max-pooling layer; finally, apply another bi-LSTM over the sentence representations. 4. S TACKED BI LSTM ATT (Yang et al., 2016): use a hierarchical S TACKED BI LSTM, except that an attention mechanism with a context size of 100 is applied to the output of each bi-LSTM to weight words/sentences based on their importance in the sentence/document. A max-pooling layer is applied to the output of the bi-LSTM at the sentence level for all models except S TACKED BI LSTM ATT to get the document level representation, which is followed by two dense layers, one with a ReLU activation and one without any activation function. For all models, dropout layers are applied at both the sentence and document levels with a rate of 0.5 dur"
U19-1010,D15-1263,0,0.0737828,"arser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Classification Bhatia et al. (2015) show that discourse analyses produced by an RST parser can improve document-level sentiment analysis. Based on DPLP (Discourse Parsing from Linear Projection) — an RST parser by Ji and Eisenstein (2014) — they recursively propagate sentiment scores up to the root via a neural network. A similar idea was proposed by Lee et al. (2018), where a recursive neural network is used to learn a discourse-aware representation. Here, DPLP is utilized to obtain discourse structures, and a recursive neural network is applied to the doc2vec (Le and Mikolov, 2014) representations for each EDU. The proposed a"
U19-1010,N18-2097,0,0.020144,"nslation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connecting words. Current approaches (Nallapati et al., 2016; See et al., 2017) capture topic sentence identification by leveraging the pointer network to do content selection, but the model is left to largely figure out the rest by providing it wi"
U19-1010,P81-1022,0,0.618278,"Missing"
U19-1010,P12-1007,0,0.161621,"course parser in sequence training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form of RST parsing, has been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates imp"
U19-1010,D18-1443,0,0.0190638,"in the original document; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connect"
U19-1010,D14-1168,0,0.0247245,"n, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Clas"
U19-1010,P18-1013,0,0.0162431,"t; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observe four core activities involved in creating a summary: (1) topic sentence identification; (2) deletion of unnecessary details; (3) paragraph collapsing; and (4) paraphrasing and insertion of connecting words. Current"
U19-1010,P14-1002,0,0.565893,"nce training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form of RST parsing, has been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obta"
U19-1010,P13-1048,0,0.0734756,"and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and included in the summary based on a template-based generation framework. Although the experiment shows that the RST can be beneficial for content selection, the proposed feature is rule-based and highly tailored to review documents. Instead, in this work, we extract a latent representation of the discourse directly from the Yu et al. (2018) parser, and incorporate this into the abstractive summarizer. 2.2 Discourse Analysis for Document Classification Bhatia et al. (2015) show t"
U19-1010,D14-1220,0,0.0969918,"eural RST parser. It is most closely related to the work of Bhatia et al. (2015) and Lee et al. (2018), but intuitively, our discourse representations contain richer information, and we evaluate over more tasks such as popularity prediction of online petitions. 3 Discourse Feature Extraction To incorporate discourse information into our models (for summarization or document regression), we use the RST parser developed by Yu et al. (2018) to extract shallow and latent discourse features. The parser is competitive with other traditional parsers that use heuristic features (Feng and Hirst, 2012; Li et al., 2014; Ji and Eisenstein, 2014) and other neural network-based parsers (Li et al., 2016b). 3.1 Shallow Discourse Features Given a discourse tree produced by the RST parser (Yu et al., 2018), we compute several shallow features for an EDU: (1) the nuclearity score; (2) the relation score for each relation; and (3) the node type and that of its sibling. Intuitively, the nuclearity score measures how informative an EDU is, by calculating the (relative) number of ancestor nodes that are nuclei:2 P x∈ancestor(e) 1nucleus (x) h(root) where e is an EDU; h(x) gives the height from node x;3 and 1nucleus (x)"
U19-1010,D16-1035,0,0.527009,"been the target of research over a long period of time, including pre-neural feature engi1 The details of each relation can be found on the RST website http://www.sfu.ca/rst/index.html neering approaches (Hernault et al., 2010; Feng and Hirst, 2012; Ji and Eisenstein, 2014). Two approaches have been proposed to construct discourse parses: (1) bottom-up construction, where EDU merge operations are applied to single units; and (2) transition parser approaches, where the discourse tree is constructed as a sequence of parser actions. Neural sequence models have also been proposed. In early work, Li et al. (2016a) applied attention in an encoder–decoder framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearit"
U19-1010,W04-1013,0,0.0188753,"–100 words for beam search. Our experiment has two pointer–generator network baselines: (1) one without the coverage mechanism (“PG”); and (2) one with the coverage mechanism (“PG+Cov”; Section 4.1). For each baseline, we incorporate the latent and shallow discourse features separately in 3 ways (Section 4.2), giving us 6 additional results. We train the models for approximately 240,000270,000 iterations (13 epochs). When we include the coverage mechanism (second baseline), we train for an additional 3,000–3,500 iterations using the coverage penalty, following See et al. (2017). We use ROUGE (Lin, 2004) as our evaluation metric, which is a standard measure based on overlapping n-grams between the generated summary and the reference summary. We assess unigram (R-1), bigram (R-2), and longest-commonsubsequence (R-L) overlap, and present F1, recall and precision scores in Table 1. For the first baseline (PG), we see that incorporating discourse features consistently improves recall and F1. This observation is consistent irrespective of how (e.g. M1 or M2) and what (e.g. shallow or latent features) we add. These improvements do come at the expense of precision, with the exception of M2-latent (w"
U19-1010,W97-0713,0,0.637797,"ssical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (201"
U19-1010,C94-1056,0,0.468496,"framework and slightly improved on a classical featureengineering approach. The current state of the art is a neural transition-based discourse parser (Yu et al., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014)"
U19-1010,D14-1162,0,0.0895044,"f examples and yˆi (yi ) the predicted (true) value. Note that in both cases, lower numbers are better. Similar to the abstractive summarization task, we experiment with incorporating the discourse features of the petition text to the petition regression model, under the hypothesis that discourse structure should benefit the model. 5.1 Deep Regression Model As before, our model is based on the model of Subramanian et al. (2018). The input is a concatenation of the petition’s title and content words, and the output is the log number of signatures. The input sequence is mapped to GloVe vectors (Pennington et al., 2014) and processed by several convolution filters with max-pooling to create a fixed-width hidden representation, which is then fed to fully connected layers and ultimately activated by an exponential linear unit to predict the output. The model is optimized with mean squared error (MSE). In addition to the MSE loss, the authors include an auxiliary ordinal regression objective that predicts the scale of signatures (e.g. {10, 100, 1000, 10000, 100000}), and found that it improves performance. Our model is based on the best model that utilizes both the MSE and ordinal regression loss. 5.2 Incorpora"
U19-1010,D15-1044,0,0.0387593,"sk of creating a concise version of a document that encapsulates its core content. Unlike extractive summarization, abstractive summarization has the ability to create new sentences that are not in the original document; it is closer to how humans summarize, in that it generates paraphrases and blends multiple sentences in a coherent manner. Current sequence-to-sequence models for abstractive summarization work like neural machine translation models, in largely eschewing symbolic 5 https://github.com/XuezheMax/ NeuroNLP2 analysis and learning purely from training data. Pioneering work such as Rush et al. (2015), for instance, assumes the neural architecture is able to learn main sentence identification, discourse structure analysis, and paraphrasing all in one model. Studies such as Gehrmann et al. (2018); Hsu et al. (2018) attempt to incorporate additional supervision (e.g. content selection) to improve summarization. Although there are proposals that extend sequence-to-sequence models based on discourse structure — e.g. Cohan et al. (2018) include an additional attention layer for document sections — direct incorporation of discourse information is rarely explored. Hare and Borchardt (1984) observ"
U19-1010,W18-0313,0,0.0150495,"l., 2018) which incorporates implicit syntax features obtained from a bi-affine dependency parser (Dozat and Manning, 2017). In this work, we employ this discourse parser to generate discourse representations. 2.1 Discourse and Summarization Research has shown that discourse parsing is valuable for summarization. Via the RST tree, the salience of a given text can be determined from the nuclearity structure. In extractive summarization, Ono et al. (1994), O’Donnell (1997), and Marcu (1997) suggest introducing penalty scores for each EDU based on the nucleus–satellite structure. In recent work, Schrimpf (2018) utilizes the topic relation to divide documents into sentences with similar topics. Every chunk of sentences is then summarized in extractive fashion, resulting in a concise summary that covers all of the topics discussed in the passage. Although the idea of using discourse information in summarization is not new, most work to date has focused on extractive summarization, where our focus is abstractive summarization. Gerani et al. (2014) used the parser of Joty et al. (2013) to RST-parse product reviews. By extracting graph-based features, important aspects are identified in the review and in"
U19-1010,P17-1099,0,0.457801,"example of a discourse tree, from (Yu et al., 2018); elab = elaboration; attr = attribute. work, we use our reimplementation of the state of the art neural RST parser of Yu et al. (2018), which is based on eighteen relations: purp, cont, attr, evid, comp, list, back, same, topic, mann, summ, cond, temp, eval, text, cause, prob, elab.1 This research investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whether summarization benefits from discourse parsing; and (2) how a pointer–generator network guides the summarization model when discourse information is provided. For document classification, we investigate the content-based popularity prediction of online petitions with a deep regression model (Subramanian et al., 2018). We argue that document structure is a key predictor of the societal influence (as measured by signatures to the petition) of a document such as a petition. Our primary contributions are as follows: (1) we are the first to incorporate a neural"
U19-1010,P18-2030,1,0.940627,"ch investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whether summarization benefits from discourse parsing; and (2) how a pointer–generator network guides the summarization model when discourse information is provided. For document classification, we investigate the content-based popularity prediction of online petitions with a deep regression model (Subramanian et al., 2018). We argue that document structure is a key predictor of the societal influence (as measured by signatures to the petition) of a document such as a petition. Our primary contributions are as follows: (1) we are the first to incorporate a neural discourse parser in sequence training; (2) we empirically demonstrate that a latent representation of discourse structure enhances the summaries generated by an abstractive summarizer; and (3) we show that discourse structure is an essential factor in modelling the popularity of online petitions. 2 Related Work Discourse parsing, especially in the form"
U19-1010,C18-1047,0,0.335395,"The Nucleus unit is considered more prominent than the Satellite, indicating that the Satellite is a supporting sentence for the Nucleus. Nuclearity relationships between two EDUs can take the following three forms: Nucleus–Satellite, Satellite–Nucleus, and Nucleus–Nucleus. In this elab attr EDU-1 EDU-2 elab EDU-3 EDU-4 EDU-1: American Telephone & Telegraph Co. said it EDU-2: will lay off 75 to 85 technicians here, effective Nov. 1 EDU-3: The workers install, maintain and repair its branch, EDU-4: which are large intracompany telephone networks Figure 1: An example of a discourse tree, from (Yu et al., 2018); elab = elaboration; attr = attribute. work, we use our reimplementation of the state of the art neural RST parser of Yu et al. (2018), which is based on eighteen relations: purp, cont, attr, evid, comp, list, back, same, topic, mann, summ, cond, temp, eval, text, cause, prob, elab.1 This research investigates the impact of discourse representations obtained from an RST parser on natural language generation and document classification. We primarily experiment with an abstractive summarization model in the form of a pointer–generator network (See et al., 2017), focusing on two factors: (1) whe"
U19-1011,D18-2029,0,0.023827,"Missing"
U19-1011,N19-1423,0,0.186292,"ial heterogeneity. They found that error rates do not correlate with the sequential heterogeneity of tasks. Rebuffi et al. (2017) and Li and Hoiem (2018) found that training on a subsequent task doesn’t update the classification layer of the previous task, and only updates the encoder layer, increasing catastrophic forgetting. Knowledge distillation loss (Hinton et al., 2015) is a commonly used technique to avoid dramatic changes in the encoder layer, while adapting the classification layer for the new task. Yogatama et al. (2019) found catastrophic forgetting while fine-tuning ELMo and BERT (Devlin et al., 2019) embeddings, similar to our findings. They also found that sampling examples from a different task (with uniform probability) enables a network to learn all tasks reasonably well; this requires access to all task simultaneously, which is different from our setup. We consider whether to fine-tune embeddings or not, which is similar to the question posed by Peters et al. (2019), who focused on various types of task. In contrast, we address the same question in a continual learning setup. 2.2 Transfer Learning ULMFit (Howard and Ruder, 2018) was an effort to enable transfer learning in a pre-trai"
U19-1011,W18-2501,0,0.0502212,"Missing"
U19-1011,P84-1044,0,0.0605453,"Missing"
U19-1011,P18-1031,0,0.265356,"Transfer learning — transferring knowledge from a source task to a target task — has become an essential technique in both computer vision and NLP. Earlier attempts at transfer learning were limited in their applicability (Mou et al., 2016), as the transfer only worked for very similar tasks: if the source and target tasks were not very similar, training on the target task resulted in catastrophic forgetting (Ratcliff, 1990; McCloskey and Cohen, 1989), whereby the neural network abruptly forgets previously-acquired knowledge during training on a new task, limiting inductive transfer. ULMFit (Howard and Ruder, 2018) developed specialised techniques to reduce forgetting during the fine-tuning process, resulting in successful 1 All code associated with this paper is available at https://github.com/gauravaror/catastrophic forgetting transfer learning. A general finding of this work was that uncovering underlying causes of catastrophic forgetting can result in improved architectures for transfer learning. Previous studies found that using dropout (Goodfellow et al., 2014) and sharp activation functions (French, 1991) help reduce catastrophic forgetting. Sharp activation functions effectively distribute each"
U19-1011,D16-1046,0,0.0323241,"ration which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning (Bengio et al., 2009), placing a hard task towards the end of task sequence, reduces forgetting. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting, and found that using fixed word embeddings is preferable to fine-tuning.1 1 Introduction Transfer learning — transferring knowledge from a source task to a target task — has become an essential technique in both computer vision and NLP. Earlier attempts at transfer learning were limited in their applicability (Mou et al., 2016), as the transfer only worked for very similar tasks: if the source and target tasks were not very similar, training on the target task resulted in catastrophic forgetting (Ratcliff, 1990; McCloskey and Cohen, 1989), whereby the neural network abruptly forgets previously-acquired knowledge during training on a new task, limiting inductive transfer. ULMFit (Howard and Ruder, 2018) developed specialised techniques to reduce forgetting during the fine-tuning process, resulting in successful 1 All code associated with this paper is available at https://github.com/gauravaror/catastrophic forgetting"
U19-1011,P04-1035,0,0.0132463,"ctures (Section 7), ways to use embeddings (Section 8), network configurations (Section 9), and task sequences (Section 10). We compare the amount of forgetting of various architectural design choices using the evaluation metric proposed in Section 5. 4 Tasks We selected four text classification tasks of different nature, each targeting different language learning tasks for English. • Stanford Sentiment Treebank (“SST”): fine-grained sentiment classification over five classes (Socher et al., 2013). • Subjectivity (“SUBJ”): binary classification of Subjectivity vs. Objectivity in IMDB reviews (Pang and Lee, 2004). • TREC Question classification (“TREC”): coarse-grained classification of questions, based on 6 classes (Voorhees and Tice, 1999). • Corpus of Linguistic Acceptability (“CoLA”): prediction of whether a sentence is grammatical or not (Warstadt et al., 2018). Table 1 contains state-of-the-art (SOTA), majority class voting, and single-task performance using a CNN for all four tasks. We consider a task difficult for our setup if we cannot attain performance close to SOTA with a simple architecture like an LSTM or CNN. SST is the most challenging task in our setup: achieving SOTA performance requ"
U19-1011,N18-1202,0,0.384715,"for various neural architectures and hyperparameter configurations in a continual learning setup. We train the network without access to data from the previous tasks, and measure how much of the knowledge learned in previous tasks is forgotten. After performing initial experiments, we conduct further experiments to understand the underlying reason for the differences in forgetting. We found that CNNs forget less than LSTMs, because of max pooling. Max-pooling decreases forgetting as the gradient doesn’t update all the shared parameters. Further, adding contextual word embeddings such as ELMo (Peters et al., 2018a) with either an LSTM or a CNN as the top layer, reduces the forgetting for both architectures. Surprisingly, the LSTM forgets less when the ELMo embeddings are frozen, and fine-tuning performs worse than randomly initialised embeddings in a continual learning setup. We also found that, contrary to common wisdom, more network capacity doesn’t always result in less forgetting. For CNNs, sequence forgetting increases as we increase the number of layers, whereas for the dimensionality of hidden layers, the degree of forgetting depends on the task sequence: the choice of which task to train first"
U19-1011,W19-4302,0,0.0220852,"Missing"
U19-1011,D13-1170,0,0.00656607,"r-parameters, as described in Section 6. We performed experiments to find how forgetting changes for different architectures (Section 7), ways to use embeddings (Section 8), network configurations (Section 9), and task sequences (Section 10). We compare the amount of forgetting of various architectural design choices using the evaluation metric proposed in Section 5. 4 Tasks We selected four text classification tasks of different nature, each targeting different language learning tasks for English. • Stanford Sentiment Treebank (“SST”): fine-grained sentiment classification over five classes (Socher et al., 2013). • Subjectivity (“SUBJ”): binary classification of Subjectivity vs. Objectivity in IMDB reviews (Pang and Lee, 2004). • TREC Question classification (“TREC”): coarse-grained classification of questions, based on 6 classes (Voorhees and Tice, 1999). • Corpus of Linguistic Acceptability (“CoLA”): prediction of whether a sentence is grammatical or not (Warstadt et al., 2018). Table 1 contains state-of-the-art (SOTA), majority class voting, and single-task performance using a CNN for all four tasks. We consider a task difficult for our setup if we cannot attain performance close to SOTA with a si"
U19-1014,N18-2075,0,0.0279898,"zed as part of CLEF-IP 2012 (Piroi et al., 2012). In the sub-task titled “Passage Retrieval Starting From Claims”, participants were required to extract passages from chemical patents that are relevant to a given claim. The difference between our task and theirs is that the output of our task is all chemical reactions mentioned in a given patent, independent of any claim. In addition, the CLEF-IP task does not require the identification of reaction spans. That is, they deal with each passage independently, ignoring ordering. The proposed task can also be viewed as a text segmentation problem. Koshorek et al. (2018) formulated the text segmentation task on general domain corpora such as Wikipedia as a supervised learning problem, and proposed a twolevel bidirectional LSTM model to learn to detect text spans. In particular, they used a softmax layer on top of a standard BiLTSM architecture for segmentation prediction. We experiment with a BiLTSM-CRF architecture as a document-level training method as described in Section 4, i.e. we use a CRF layer to obtain the document-level label sequence, instead of applying a softmax classifier on top of the BiLSTM. 3 3.1 Task and Dataset Task Formulation A patent doc"
U19-1014,P09-1113,0,0.0277244,"kground knowledge. Indeed, the BiLSTM-CRF model trained at the documentlevel performed much better than the paragraphlevel classification methods. The performance of the baseline methods presented in this paper is still not satisfactory considering the complex downstream tasks such as event extraction. We believe that both the models and the corpus have potential to be improved. As future work, we plan to explore more efficient document-level training methods, and, in particular, methods that work well on noisy training sets. For instance, techniques successfully used for distant supervision (Mintz et al., 2009) may be effective. Furthermore, although we used only textual information, patent documents contain substantial visual information (e.g., images of compounds, or tables) that may be helpful to properly understand a reaction description. Longer term, we will also tackle finer-grained information extraction for chemical reactions utilizing the output of this task. This step involves extracting the details of the detected reactions, that is, inferring the underlying structure of the reactions themselves. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. T"
U19-1014,N18-1202,0,0.0449915,"Missing"
U19-1014,E99-1023,0,0.134349,"copper(I)bromide dimethyl sulfide (2.17 g, 10.56 mmol) was dissolved ... I Figure 2: Illustration of our reaction span detection task. would not be able to detect reactions as a whole or capture reaction substructure. Figure 2 shows an example of an input and goldstandard output of the reaction span detection task. A patent document is given as a sequence of paragraphs. The task is to detect a span of contiguous paragraphs that describe a single chemical reaction. In our corpus, we provide paragraph-level label sequences over paragraphs in patent documents, following the IOB2 tagging scheme (Tjong et al., 1999). The definition of “reaction spans” in our dataset follows the extraction rules of the original database. In principle, a reaction is extracted from a patent if the requisite information about the reaction (e.g., starting materials, reaction conditions and target compounds) is provided within the patent document and there is no obvious error or inconsistency in the description. Typically a reaction constitutes an example section or a subsection beginning with a title paragraph such as Example 1, Step 1 and Preparation of [product name], as shown in Figure 1. However, it is also commonly the c"
U19-1014,W19-5035,1,0.833679,"xt of each paragraph as input, with a maximum length of 128 tokens.5 For tokenization, we used the OSCAR4 tokenizer (Jessop et al., 2011), as it is customized to chemical text mining. Equation (1) formulates the input token-level representation for the BiLSTM paragraph encoder in the form of (context-insensitive) word embeddings, contextualized word embeddings, and feature embeddings. For the word embeddings eWE wi and contextualized embeddings eCW wi |p , we employ Word2Vec (Mikolov et al., 2013) and ELMo (Peters et al., 2018), respectively, both pre-trained on chemical patent documents from Zhai et al. (2019). These embeddings are fixed during training. We denote our encoder employing only the pre-trained word and contextualized embeddings CW (i.e. ei = eWE wi ⊕ ewi |p ) as W 2 V +ELM O . We also explore additional learnable feature embeddings eFT fi (in Equation 1) based on the output of a chemical named entity recognizer (Zhai et al., 2019). This named entity recognizer was trained on a patent corpus named Reaxys® Gold data (Akhondi et al., 2019). For self-containment purpose we show the entity label set of Reaxys® Gold data in Table 4 in the Appendix. As the label set has two levels of granular"
villavicencio-etal-2004-multilingual,copestake-etal-2002-multiword,1,\N,Missing
villavicencio-etal-2004-multilingual,W03-1803,1,\N,Missing
villavicencio-etal-2004-multilingual,copestake-etal-2004-lexicon,1,\N,Missing
W02-2001,J95-4004,0,0.103412,"ise extraposed) for the head verb of the VPC. Here and for the subsequent methods, we assume that the maximum word length for NP complements in the split configuration for transitive VPCs is 5,2 i.e. that an NP “heavier” than this would occur more naturally in the joined configuration. We thus discount all particles which are more than 5 words from their governing verb. Additionally, we extracted a set of 73 canonical particles from the LinGO-ERG, and used this to filter out extraneous particles in the POS data. In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank. We further lemmatise the data using morph (Minnen et al., 2001) and extract VPCs based on the Brill tags. This produces a total of 135 VPCs, which we evaluate according to the standard metrics of precision (Prec), recall (Rec) and F-score (Fβ=1 ). Note that here and for the remainder of this paper, precision is calculated according to the manual annotation for the combined total of 4,173 VPC candidate types extracted by the various methods described in this paper, whereas recall is"
W02-2001,calzolari-etal-2002-towards,0,0.0380294,"ut, chunker output and a chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb–preposition pairs in ambiguous constructs. We then combine the three methods together into a single classifier, and add in a number of extra lexical and frequentistic features, producing a final F-score of 0.865 over the WSJ. 1 Introduction There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need for a robust, structured handling thereof (Sag et al., 2002; Calzolari et al., 2002; Copestake et al., 2002). Examples of MWEs are lexically fixed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement). MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts. Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendoff, 1997)), productivity and domain-specificity. Ideally, therefore, we would like to have some means o"
W02-2001,copestake-flickinger-2000-open,0,0.00846794,"city. Ideally, therefore, we would like to have some means of automatically extracting MWEs from a given domain or corpus, allowing us to pre-tune our grammar prior to deployment. It is this task of extraction that we target in this paper. This research represents a component of the LinGO multiword expression project,1 which is targeted at extracting, adequately handling and representing MWEs of all types. As a research testbed and target resource to expand/domain-tune, we use the LinGO English Resource Grammar (LinGOERG), a linguistically-precise HPSG-based grammar under development at CSLI (Copestake and Flickinger, 2000; Flickinger, 2000). The particular MWE type we target for extraction is the English verb-particle construction. Verb-particle constructions (“VPCs”) consist of a 1 http://lingo.stanford.edu/mwe head verb and one or more obligatory particles, in the form of intransitive prepositions (e.g. hand in), adjectives (e.g. cut short) or verbs (e.g. let go) (Villavicencio and Copestake, 2002a; Villavicencio and Copestake, 2002b; Huddleston and Pullum, 2002); for the purposes of this paper, we will focus exclusively on prepositional particles—by far the most common and productive of the three types— and"
W02-2001,copestake-etal-2002-multiword,1,0.771,"chunk grammar, respectively, with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verb–preposition pairs in ambiguous constructs. We then combine the three methods together into a single classifier, and add in a number of extra lexical and frequentistic features, producing a final F-score of 0.865 over the WSJ. 1 Introduction There is growing awareness of the pervasiveness and idiosyncrasy of multiword expressions (MWEs), and the need for a robust, structured handling thereof (Sag et al., 2002; Calzolari et al., 2002; Copestake et al., 2002). Examples of MWEs are lexically fixed expressions (e.g. ad hoc), idioms (e.g. see double), light verb constructions (e.g. make a mistake) and institutionalised phrases (e.g. kindle excitement). MWEs pose a challenge to NLP due to their syntactic and semantic idiosyncrasies, which are often unpredictable from their component parts. Largescale manual annotation of MWEs is infeasible due to their sheer volume (at least equivalent to the number of simplex words (Jackendoff, 1997)), productivity and domain-specificity. Ideally, therefore, we would like to have some means of automatically extractin"
W02-2001,J93-1003,0,0.0296048,"itive prepositional verb hand in or simple transitive verb with PP adjunct), and (3) hand [the paper in here] (simple transitive verb). In such cases, we can choose to either (a) avoid committing ourselves to any one analysis, and ignore all such ambiguous cases, or (b) use some means to resolve the attachment ambiguity (i.e. whether the NP is governed by the verb, resulting in a VPC, or the preposition, resulting in a prepositional verb or free verb–preposition combination). In the latter case, we use an unsupervised attachment disambiguation method, based on the log-likelihood ratio (“LLR”, Dunning (1993)). That is, we use the chunker output to enumerate all the verb–preposition, preposition– noun and verb–noun bigrams in the WSJ data, based on chunk heads rather than strict word bigrams. We then use frequency data to pre-calculate the LLR for each such type. In the case that the verb and “particle” are joined (i.e. no NP occurs between them), we simply compare the LLR of the verb–noun and particle–noun pairs, and assume a VPC analysis in the case that the former is strictly larger than the latter. In the case that the verb and “particle” are split (i.e. we have the chunk sequence VC NC1 PC NC"
W02-2001,kaalep-muischnek-2002-using,0,0.029314,"with our research impossible. The work of Blaheta and Johnson (2001) is closer in its objectives to our research, in that it takes a parsed corpus and extracts out multiword verbs (i.e. VPCs and prepositional verbs) through the use of log-linear models. Once again, direct comparison with our results is difficult, as Blaheta and Johnson output a ranked list of all verb–preposition pairs, and subjectively evaluate the quality of different sections of the list. Additionally, they make no attempt to distinguish VPCs from prepositional verbs. The method which is perhaps closest to ours is that of Kaalep and Muischnek (2002) in extracting Estonian multiword verbs (which are similar to English VPCs in that the components of the multiword verb can be separated by other words). Kaalep and Muischnek apply the “mutual expectation” test over a range of “positioned bigrams”, similar to those used by Smadja. They test their method over three different corpora, with results ranging from a precision of 0.21 and recall of 0.86 (F-score=0.34) for the smallest corpus, to a precision of 0.03 and recall of 0.85 (F-score=0.06) for the largest corpus. That is, high levels of noise are evident in the system output, and the F-score"
W02-2001,J93-2004,0,0.0296154,"n and Radev (2000)) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle. We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains. Any linguistic annotation required during the extraction process, therefore, is produced through automatic means, and it is only for reasons of accessibility and comparability with other research that we choose to work over the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). That is, other than in establishing upper bounds on the performance of the different extraction methods, we use only the raw text component of the treebank. In this paper, we first outline distinguishing features of VPCs relevant to the extraction process (§ 2). We then present and evaluate a number of simple methods for extracting VPCs based on, respectively, POS tagging (§ 3), the output of a full text chunk parser (§ 4), and a chunk grammar (§ 5). Finally, we detail enhancements to the basic methods (§ 6) and give a brief description of related research (§ 7) before concluding the paper ("
W02-2001,J93-1007,0,0.847998,"rbs (e.g. let go) (Villavicencio and Copestake, 2002a; Villavicencio and Copestake, 2002b; Huddleston and Pullum, 2002); for the purposes of this paper, we will focus exclusively on prepositional particles—by far the most common and productive of the three types— and further restrict our attention to single-particle VPCs (i.e. we ignore VPCs such as get along together ). We define VPCs to optionally select for an NP complement, i.e. to occur both transitively (e.g. hand in the paper ) and intransitively (e.g. battle on). One aspect of VPCs that makes them difficult to extract (cited in, e.g., Smadja (1993)) is that the verb and particle can be non-contiguous, e.g. hand the paper in and battle right on. This sets them apart from conventional collocations and terminology (see, e.g., Manning and Sch¨ utze (1999) and McKeown and Radev (2000)) in that they cannot be captured effectively using N-grams, due to the variability in the number and type of words potentially interceding between the verb and particle. We are aiming for an extraction technique which is applicable to any raw text corpus, allowing us to tune grammars to novel domains. Any linguistic annotation required during the extraction pro"
W02-2001,W00-0726,0,0.0208529,"rill tagger in identifying particles, we next look to full chunk 2 Note, this is the same as the maximum span length of 5 used by Smadja (1993), and above the maximum attested NP length of 3 from our corpus study (see Section 2.2). Prec 0.889 WSJ Rec 0.911 Fβ=1 0.900 Prec 0.912 CoNLL Rec Fβ=1 0.925 0.919 Table 2: Chunking performance parsing. Full chunk parsing involves partitioning up a text into syntactically-cohesive, head-final segments (“chunks”), without attempting to resolve inter-chunk dependencies. In the chunk inventory devised for the CoNLL-2000 test chunking shared task (Tjong Kim Sang and Buchholz, 2000), a dedicated particle chunk type once again exists. It is therefore possible to adopt an analogous approach to that from Method-1, in identifying particle chunks then working back to locate the verb each particle chunk is associated with. 4.1 Chunk parsing method In order to chunk parse the WSJ, we first tagged the full WSJ and Brown corpora using the Brill tagger, and then converted them into chunks based on the original Penn Treebank parse trees, with the aid of the conversion script used in preparing the CoNLL-2000 shared task data.3 We next lemmatised the data using morph (Minnen et al.,"
W02-2001,W00-1308,0,0.0146913,"ools data as described above. As indicated in the first line of Table 1 (“Brill”), the simple POS-based method results in a precision of 1.000, recall of 0.177 and F-score of 0.301. In order to determine the upper bound on performance for this method, we ran the extraction method over the original tagging from the Penn Treebank. This resulted in an F-score of 0.774 (“Penn” in Table 1). The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). Over the WSJ, the Brill tagger achieves a modest tag recall of 0.103 for particles, and tag precision of 0.838. That is, it is highly conservative in allocating particle tags, to the extent that it recognises only two particle types for the whole of the WSJ: out and down. 4 Method-2: Simple Chunk-based Extraction To overcome the shortcomings of the Brill tagger in identifying particles, we next look to full chunk 2 Note, this is the same as the maximum span length of 5 used by Smadja (1993), and above the maximum attested NP length of 3 from our corpus study (see Section 2.2). Prec 0.889 WSJ"
W02-2001,W00-1427,0,\N,Missing
W02-2001,W00-0735,0,\N,Missing
W03-1010,P03-1059,1,0.881675,"which simply looks at the distribution of features in the corpus data, and agreement-based representation which analyses the level of tokenwise agreement between multiple preprocessor systems. We additionally compare a single multiclass classifier architecture with a suite of binary classifiers, and combine analyses from multiple preprocessors. Finally, we present and evaluate a feature selection method. with differences in meaning: I submitted two papers “documents” (countable) vs. Please use white paper “substance to be written on” (uncountable). This research complements that described in Baldwin and Bond (2003), where we present the linguistic foundations and features drawn upon in the countability classification task, and motivate the claim that countability preferences can be learned from corpus evidence. In this paper, we focus on the methods used to tackle the task of countability classification based on this fixed feature set. The remainder of this paper is structured as follows. Section 2 outlines the countability classes, resources and pre-processors. Section 3 presents two methods of representing the feature space. Section 4 details the different classifier designs and the dataset, which are"
W03-1010,C02-1052,1,0.32975,"Missing"
W03-1010,C02-1013,0,0.019961,"ates to extract out the features from the processed data. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks. For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision. We read dependency tuples directly off the output of RASP (Briscoe and Carroll, 2002b) in grammatical relation mode.1 RASP has the advantage that recall is high, although precision is potentially lower 1 We used the first parse in the experiments reported here. An alternative method would be to use weighted dependency tuples, as described in Briscoe and Carroll (2002a). than chunking or tagging as the parser is forced into resolving phrase attachment ambiguities and committing to a single phrase structure analysis. After generating the different feature vectors for each noun based on the above configurations, we filtered out all nouns which did not occur at least 10 times in"
W03-1010,briscoe-carroll-2002-robust,0,0.0389787,"ates to extract out the features from the processed data. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks. For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision. We read dependency tuples directly off the output of RASP (Briscoe and Carroll, 2002b) in grammatical relation mode.1 RASP has the advantage that recall is high, although precision is potentially lower 1 We used the first parse in the experiments reported here. An alternative method would be to use weighted dependency tuples, as described in Briscoe and Carroll (2002a). than chunking or tagging as the parser is forced into resolving phrase attachment ambiguities and committing to a single phrase structure analysis. After generating the different feature vectors for each noun based on the above configurations, we filtered out all nouns which did not occur at least 10 times in"
W03-1010,J96-2004,0,0.0162335,"te 3n independent feature values. In the case of a two-dimensional feature matrix (e.g. subject-position noun number vs. verb number agreement), each unit feature f s,t for target noun w is translated into corpfreq(f s,t , w ), wordfreq(f s,t , w ) and featfreq(f s,t , w ) as above, and 2 additional feature values: featdimfreq  (f s,t , w ) featdimfreq  (f s,t , w ) agr (f s ,w ) (sys  , sys  ) = |tok (f s ,w ) (sys  ) ∩ tok (f s ,w ) (sys  )| |tok (f s ,w ) (sys  ) ∪ tok (f s ,w ) (sys  )| where tok (f s ,w ) (sys i ) returns the set of token instances of (f s , w ). The κ statistic (Carletta, 1996) is recast as: P κ(f s ,w ) (sys  , sys  ) = agr (f s ,w ) (sys  , sys  ) − P − agr (f ,∗) (sys  ,sys  ) s N agr (f ,∗) (sys  ,sys  ) s N In this modified form, κ(f s ,w ) represents the divergence in relative agreement wrt f s for target noun w , relative to the mean relative agreement wrt f s over all words. Correlated frequency is defined to be: cfreq (f s ,w ) (sys  , sys  ) = |tok (f s ,w ) (sys  ) ∩ tok (f s ,w ) (sys  )| freq(w ) It describes the occurrence of tokens in agreement for (f s , w ) relative to the total occurrence of the target word. The metrics are used to der"
W03-1010,1991.mtsummit-papers.16,0,0.0583768,"y have a plural form, such as goods, and cannot be either denumerated or modified by much; many plural only nouns, such as clothes, use the plural form even as modifiers: a clothes horse. Bipartite nouns are plural when they head a noun phrase (trousers), but generally singular when used as a modifier (trouser leg); they can be denumerated with the classifier pair: a pair of scissors. 2.2 Gold standard data Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998) and the common noun part of ALTJ/E’s Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991). Of the approximately 22,000 noun entries in COMLEX, 13,622 are marked as countable, 710 as uncountable and the remainder are unmarked for countability. ALT-J/E has 56,245 English noun types with distinct countability. 2.3 Feature space Features used in this research are divided up into feature clusters, each of which is conditioned on the occurrence of a target noun in a given construction. Feature clusters are either one-dimensional (describing a single multivariate feature) or twodimensional (describing the interaction between two multivariate features), with each dimension describing a le"
W03-1010,N01-1006,0,0.134442,"is the number of the target noun for each 2.4 Feature extraction The values for the features described above were extracted from the written component of the British National Corpus (BNC, Burnard (2000)) using three different pre-processors: (a) a POS tagger, (b) a fulltext chunker and (c) a dependency parser. These are used independently to test the efficacy of the different systems at capturing features used in the classification process, and in tandem to consolidate the strengths of the individual methods. With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tagset. We then lemmatised this data using a Penn tagset-customised version of morph (Minnen et al., 2001). Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or appl"
W03-1010,W00-0726,0,0.0450574,"process, and in tandem to consolidate the strengths of the individual methods. With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tagset. We then lemmatised this data using a Penn tagset-customised version of morph (Minnen et al., 2001). Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000style (Tjong Kim Sang and Buchholz, 2000) chunkconverted versions of the full Brown and WSJ corpora. For the NP-internal features (e.g. determiners, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks. For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision. We read dependency tuples directly off the output of RASP (Briscoe and Carroll, 2002b) in grammatical relation mode.1 RASP has the advantage that recall is high, although precision is potentially lower 1 We used the first parse in the experiment"
W03-1803,P98-1015,0,0.0214104,"only in that the source NN compounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP ) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP , DMTINTERP suffers from undergeneration. With DMTINTERP , context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantic"
W03-1803,C02-1011,0,0.041227,"entence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002):  1. extract NN compounds from the source language corpus  by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”)  2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N  N  ]J are [N  N  ]E and [N of N  ]E , where the numeric subscripts indicate word coindexation between Japanese and English (resulting in, e.g., machine translation and translation of machine) 3. use empirical evidence from the"
W03-1803,P97-1018,0,0.0282932,"DMTCOMP ) differs from MBMTCOMP only in that the source NN compounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP ) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP , DMTINTERP suffers from undergeneration. With DMTINTERP , context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sh"
W03-1803,W97-0119,0,0.0857546,"rs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002):  1. extract NN compounds from the source language corpus  by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”)  2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N  N  ]J are [N  N  ]E and [N of N  ]E , where the numer"
W03-1803,P95-1032,0,0.0494064,"late only those source language strings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT ). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo,"
W03-1803,1991.mtsummit-papers.16,0,0.0694742,"Missing"
W03-1803,N01-1006,0,0.0252117,"Missing"
W03-1803,1999.tmi-1.9,0,0.0305109,"trings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT ). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract tr"
W03-1803,P99-1067,0,0.0681017,"a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002):  1. extract NN compounds from the source language corpus  by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”)  2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N  N  ]J are [N  N  ]E and [N of N  ]E , where the numeric subscript"
W03-1803,W01-0511,0,0.0390835,"mpounds are fed directly into the system rather than extracted out of a source language corpus. That is, it applies Steps 2 and 3 of the method for MBMTCOMP to an arbitrary source language string. Interpretation-driven DMT (or DMTINTERP ) offers the means to deal with NN compounds where strict word-to-word alignment does not hold. It generally does this in two stages: 1. use semantics and/or pragmatics to carry out deep analysis of the source NN compound, and map it into some intermediate (i.e. interlingual) semantic representation (Copestake and Lascarides, 1997; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001) 2. generate the translation directly from the semantic representation DMTINTERP removes any direct source/target language interdependence, and hence solves the problem of overgeneration due to crosslingual bias. At the same time, it is forced into tackling idiomaticity headon, by way of interpreting each individual NN compound. As for DMTCOMP , DMTINTERP suffers from undergeneration. With DMTINTERP , context must often be called upon in interpreting NN compounds (e.g. apple juice seat (Levi, 1978; Bauer, 1979)), and minimal pairs with sharply-differentiated semantics such as colour/group phot"
W03-1803,J96-1001,0,0.0239236,"ose source language strings contained in the translation database. There are a number of ways to populate the translation database used in MBMT, the easiest of which is to take translation pairs directly from a bilingual dictionary (dictionary-driven MBMT or MBMTDICT ). MBMTDICT offers an extremist solution to the idiomaticity problem, in treating all NN compounds as being fully lexicalised. Overgeneration is not an issue, as all translations are manually determined. As an alternative to a precompiled bilingual dictionary, translation pairs can be extracted from a parallel corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002)."
W03-1803,1999.tmi-1.11,1,0.869329,"corpus (Fung, 1995; Smadja et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002):  1. extract NN compounds from the source language corpus  by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”)  2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N  N  ]J are [N  N  ]E and [N of N  ]E , where the numeric subscripts indicate word coindexat"
W03-1803,C02-1065,1,0.858237,"a et al., 1996; Ohmori and Higashida, 1999), that is a bilingual document set that is translation-equivalent at the sentence or paragraph level; we term this MT configuration alignment-driven MBMT (or MBMTALIGN ). While this method alleviates the problem of limited scalability, it relies on the existence of a parallel corpus in the desired domain, which is often an unreasonable requirement. Whereas a parallel corpus assumes translation equivalence, a comparable corpus is simply a crosslingual pairing of corpora from the same domain (Fung and McKeown, 1997; Rapp, 1999; Tanaka and Matsuo, 1999; Tanaka, 2002). It is possible to extract translation pairs from a comparable corpus by way of the following process (Cao and Li, 2002):  1. extract NN compounds from the source language corpus  by searching for NN bigrams (e.g. kikai hoNyaku “machine translation”)  2. compositionally generate translation candidates for each NN compound by accessing translations for each component word and slotting these into translation templates; example JE translation templates for source Japanese string [N  N  ]J are [N  N  ]E and [N of N  ]E , where the numeric subscripts indicate word coindexation between Jap"
W03-1809,W02-2001,1,0.657125,"(2000), and applied to VPCs by Lohse et al. (in preparation). 3.1 Experimental Materials In an attempt to normalise the annotators’ entailment judgements, we decided upon an experimental setup where the subject is, for each VPC type, presented with a fixed selection of sentential contexts for that VPC. So as to avoid introducing any bias into the experiment through artificially-generated sentences, we chose to extract the sentences from naturallyoccurring text, namely the written component of the British National Corpus (BNC, Burnard (2000)). Extraction of the VPCs was based on the method of Baldwin and Villavicencio (2002). First, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to (re)tag the BNC. This allowed us to extract VPC tokens through use of: (a) the particle POS in the POS tagged output, for each instance of which we simply then look for the rightmost verb within a fixed window to the left of the particle, and (b) the particle chunk tag in the chunker output, where we similarly locate the rightmost verb associated with each particle chunk occurrence. Finally, we ran a stochastic chunk-based grammar over the chunker output to extend extraction coverage to include m"
W03-1809,W03-1812,1,0.748125,"Missing"
W03-1809,P98-1015,0,0.00691467,"on, notably in the area of hand-written grammar development (Sag et al., 2002; Villavicencio and Copestake, 2002). Such items cause considerable problems for any semantically-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts. However, corpus-based or empirical NLP has shown limited interest in the problem. While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g. Lauer (1995), Barker and Szpakowicz (1998), Rosario and Hearst (2001)), the more idiosyncratic items have been largely ignored beyond attempts at identification (Melamed, 1997; Lin, 1999; Schone and Jurafsky, 2001). And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter. The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes. Part of the reason for the lack of interest by computational linguists in the"
W03-1809,J96-2004,0,0.00583313,".677 .376 .575 .393 .032 Verbs only .703 .372 .655 .319 .026 Particles only .650 .352 .495 .467 .038 Table 2: Summary of judgements for all VPCs The experiment was conducted remotely over the Web, using the experimental software package WebExp (Corley et al., 2000). Experimental sessions lasted approximately 20 minutes and were self-paced. The order in which the forty sets of sentences were presented was randomised by the software. 3.4 Annotator agreement We performed a pairwise analysis of the agreement between our 28 participants. The overall mean agreement was .655, with a kappa (κ) score (Carletta, 1996) of .329. An initial analysis showed that two participants strongly disagreed with the other, achieving a mean pairwise κ score of less than .1. We decided therefore to remove these from the set before proceeding. The overall results for the remaining 26 participants can be seen in Table 2. The κ score over these 26 participants (.376) is classed as fair (0.2– 0.4) and approaching moderate (0.4–0.6) according to Altman (1991). As mentioned above, a major problem with lexical semantic studies is that items tend to occur with more than one meaning. In order to test the effects of polysemy in the"
W03-1809,P98-2127,0,0.0286959,"taking VPC tokens automatically extracted from the BNC and using an automatically acquired thesaurus to classify their relative compositionality. One significant divergence from our research is that they consider compositionality to be an indivisible property of the overall VPC, and not the individual parts. Gold-standard data was generated by asking human annotators to describe the compositionality of a given VPC according to a 11point scale, based upon which the VPCs were ranked in order of compositionality. Similarly to this research, McCarthy et al. in part used the similarity measure of Lin (1998a) to model compositionality, e.g., in taking the top N similar words to each VPC and looking at overlap with the top N similar words to the head verb. They also examine the use of statistical tests such as mutual information in modelling compositionality, and find the similarity-based methods to correlate more highly with the human judgements. Baldwin et al. (2003) use LSA as a technique for analysing the compositionality (or decomposability) of a given MWE. LSA is suggested to be a construction-inspecific test for compositionality, which is illustrated by testing its effectivity over both En"
W03-1809,P99-1041,0,0.618055,"emantically-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts. However, corpus-based or empirical NLP has shown limited interest in the problem. While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g. Lauer (1995), Barker and Szpakowicz (1998), Rosario and Hearst (2001)), the more idiosyncratic items have been largely ignored beyond attempts at identification (Melamed, 1997; Lin, 1999; Schone and Jurafsky, 2001). And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter. The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes. Part of the reason for the lack of interest by computational linguists in the semantics of MWEs is that there is no established gold standard data from which to construct or evaluate models. Evaluation to date has tended"
W03-1809,J93-2004,0,0.0285524,"Missing"
W03-1809,W03-1810,0,0.692404,"Missing"
W03-1809,W97-0311,0,0.140911,"blems for any semantically-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts. However, corpus-based or empirical NLP has shown limited interest in the problem. While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g. Lauer (1995), Barker and Szpakowicz (1998), Rosario and Hearst (2001)), the more idiosyncratic items have been largely ignored beyond attempts at identification (Melamed, 1997; Lin, 1999; Schone and Jurafsky, 2001). And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter. The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes. Part of the reason for the lack of interest by computational linguists in the semantics of MWEs is that there is no established gold standard data from which to construct or evaluate models. Evaluation to date"
W03-1809,N01-1006,0,0.0166794,"to normalise the annotators’ entailment judgements, we decided upon an experimental setup where the subject is, for each VPC type, presented with a fixed selection of sentential contexts for that VPC. So as to avoid introducing any bias into the experiment through artificially-generated sentences, we chose to extract the sentences from naturallyoccurring text, namely the written component of the British National Corpus (BNC, Burnard (2000)). Extraction of the VPCs was based on the method of Baldwin and Villavicencio (2002). First, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to (re)tag the BNC. This allowed us to extract VPC tokens through use of: (a) the particle POS in the POS tagged output, for each instance of which we simply then look for the rightmost verb within a fixed window to the left of the particle, and (b) the particle chunk tag in the chunker output, where we similarly locate the rightmost verb associated with each particle chunk occurrence. Finally, we ran a stochastic chunk-based grammar over the chunker output to extend extraction coverage to include mistagged particles and also more reliably determine the valence of the VPC. The token output o"
W03-1809,pearce-2002-comparative,0,0.0580333,"Missing"
W03-1809,W01-0511,0,0.00640498,"d-written grammar development (Sag et al., 2002; Villavicencio and Copestake, 2002). Such items cause considerable problems for any semantically-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts. However, corpus-based or empirical NLP has shown limited interest in the problem. While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g. Lauer (1995), Barker and Szpakowicz (1998), Rosario and Hearst (2001)), the more idiosyncratic items have been largely ignored beyond attempts at identification (Melamed, 1997; Lin, 1999; Schone and Jurafsky, 2001). And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter. The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes. Part of the reason for the lack of interest by computational linguists in the semantics of MWEs is that"
W03-1809,W01-0513,0,0.277511,"-grounded NLP application (including applications where semantic information is implicit, such as information retrieval) because their meaning is often not simply a function of the meaning of the constituent parts. However, corpus-based or empirical NLP has shown limited interest in the problem. While there has been some work on statistical approaches to the semantics of compositional compound nominals (e.g. Lauer (1995), Barker and Szpakowicz (1998), Rosario and Hearst (2001)), the more idiosyncratic items have been largely ignored beyond attempts at identification (Melamed, 1997; Lin, 1999; Schone and Jurafsky, 2001). And yet the identification of noncompositional phrases, while valuable in itself, would by no means be the end of the matter. The unique challenge posed by MWEs for empirical NLP is precisely that they do not fall cleanly into the binary classes of compositional and non-compositional expressions, but populate a continuum between the two extremes. Part of the reason for the lack of interest by computational linguists in the semantics of MWEs is that there is no established gold standard data from which to construct or evaluate models. Evaluation to date has tended to be fairly ad hoc. Another"
W03-1809,J98-1004,0,0.0319759,"Missing"
W03-1809,C98-2122,0,\N,Missing
W03-1809,C98-1015,0,\N,Missing
W03-1812,W02-2001,1,0.543878,"ite from gerund forms) led to data sparseness, and instead we considered “verb” as a single part-of-speech type. 3.3 MWE extraction methods NN compounds were extracted from the WSJ by first tagging the data with fnTBL 1.0 (Ngai and Florian, 2001) and then simply taking noun bigrams (adjoined on both sides by non-nouns to assure the bigram is not part of a larger compound nominal). Out of these, we selected those compounds that are listed in WordNet, resulting in 5,405 NN compound types (208,000 tokens). Extraction of the verb-particles was considerably more involved, and drew on the method of Baldwin and Villavicencio (2002). Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag the BNC. This allowed us to extract verb-particle tokens through use of the particle POS and chunk tags returned by the two systems. This produces highprecision, but relatively low-recall results, so we performed the additional step of running a chunkbased grammar over the chunker output to detect candidate mistagged particles. In the case that a noun phrase followed the particle candidate, we performed attachment disambiguation to determine the transitivity of the particle ca"
W03-1812,W03-1809,1,0.733408,"Missing"
W03-1812,calzolari-etal-2002-towards,0,0.0270477,"greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. 1 Introduction This paper is concerned with an empirical model of multiword expression decomposability. Multiword expressions (MWEs) are defined to be cohesive lexemes that cross word boundaries (Sag et al., 2002; Copestake et al., 2002; Calzolari et al., 2002). They occur in a wide variety of syntactic configurations in different languages (e.g. in the case of English, compound nouns: post office, verbal idioms: pull strings, verb-particle constructions: push on, etc.). Decomposability is a description of the degree to which the semantics of an MWE can be ascribed to those of its parts (Riehemann, 2001; Sag et al., 2002). Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (Nunberg et al., 1994; Lin, 1999). Our claim here is that the semantics"
W03-1812,copestake-etal-2002-multiword,1,0.666565,"r similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. 1 Introduction This paper is concerned with an empirical model of multiword expression decomposability. Multiword expressions (MWEs) are defined to be cohesive lexemes that cross word boundaries (Sag et al., 2002; Copestake et al., 2002; Calzolari et al., 2002). They occur in a wide variety of syntactic configurations in different languages (e.g. in the case of English, compound nouns: post office, verbal idioms: pull strings, verb-particle constructions: push on, etc.). Decomposability is a description of the degree to which the semantics of an MWE can be ascribed to those of its parts (Riehemann, 2001; Sag et al., 2002). Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (Nunberg et al., 1994; Lin, 1999). Our claim h"
W03-1812,W97-0311,0,0.0474037,"Missing"
W03-1812,N01-1006,0,0.012557,"onfuses this distinction — the neighbours of fire treated just as a string include words related to both the meaning of fire as a noun (more frequent in the BNC) and as a verb. The appropriate granularity of syntactic classifications is an open question for this kind of research: treating all the possible verbs categories as different (e.g. distinguishing infinitive from finite from gerund forms) led to data sparseness, and instead we considered “verb” as a single part-of-speech type. 3.3 MWE extraction methods NN compounds were extracted from the WSJ by first tagging the data with fnTBL 1.0 (Ngai and Florian, 2001) and then simply taking noun bigrams (adjoined on both sides by non-nouns to assure the bigram is not part of a larger compound nominal). Out of these, we selected those compounds that are listed in WordNet, resulting in 5,405 NN compound types (208,000 tokens). Extraction of the verb-particles was considerably more involved, and drew on the method of Baldwin and Villavicencio (2002). Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag the BNC. This allowed us to extract verb-particle tokens through use of the particle POS and ch"
W03-1812,W01-0513,0,0.281547,"its head verb. Bannard et al. (2003) extended this research in looking explicitly at the task of classifying verb-particles as being compositional or not. They successfully combined statistical and distributional techniques (including LSA) with a substitution test in analysing compositionality. McCarthy et al. (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verbparticle and head verb, e.g., to determine compositionality. We are not the first to consider applying LSA to MWEs. Schone and Jurafsky (2001) applied LSA to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from a corpus. The major point of divergence from this research is that Schone and Jurafsky focused specifically on MWE extraction, whereas we are interested in the downstream task of semantically classifying attested MWEs. 3 Resources and Techniques In this section, we outline the resources used in evaluation, give an informal introduction to the LSA model, sketch how we extracted the MWEs from corpus data, and describe a number of methods for modelling decomposability within a hierarchical l"
W03-1812,P98-2127,0,0.0210829,"and made available for general use as the Perl package distance-0.11. 2 We focused in particular on the following three measures, the first two of which are based on information theoretic principles, and the third on sense topology: • Resnik (1995) combined WordNet with corpus statistics. He defines the similarity between two words as the information content of the lowest superordinate in the hierarchy, defining the information content of a concept c (where a concept is the WordNet class containing the word) to be the negative of its log likelihood. This is calculated over a corpus of text. • Lin (1998c) also employs the idea of corpusderived information content, and defines the similarity between two concepts in the following way: sim(C1 , C2 ) = 2 log P (C0 ) log P (C1 ) + log P (C2 ) (1) where C0 is the lowest class in the hierarchy that subsumes both classes. 2 http://www.d.umn.edu/˜tpederse/ distance.html • Hirst and St-Onge (1998) use a system of “relations” of different strength to determine the similarity of word senses, conditioned on the type, direction and relative distance of edges separating them. The Patwardhan et al. (2003) implementation that we used calculates the informati"
W03-1812,P99-1041,0,0.571186,"Copestake et al., 2002; Calzolari et al., 2002). They occur in a wide variety of syntactic configurations in different languages (e.g. in the case of English, compound nouns: post office, verbal idioms: pull strings, verb-particle constructions: push on, etc.). Decomposability is a description of the degree to which the semantics of an MWE can be ascribed to those of its parts (Riehemann, 2001; Sag et al., 2002). Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (Nunberg et al., 1994; Lin, 1999). Our claim here is that the semantics of the MWE are deconstructed and the parts coerced into often idiosyncratic interpretations to attain semantic alignment, rather than the other way around. One idiom which illustrates this process is spill the beans, takaaki@cslab.kecl.ntt.co.jp where the semantics of reveal0 (secret0 ) are decomposed such that spill is coerced into the idiosyncratic interpretation of reveal0 and beans into the idiosyncratic interpretation of secret 0 . Given that these senses for spill and beans are not readily available at the simplex level other than in the context of"
W03-1812,W03-1810,0,0.701013,"Missing"
W03-1812,J98-1004,0,0.0149535,"Missing"
W03-1812,widdows-etal-2002-using,1,0.65846,"es could be measured. LSA is a method for representing words as points in a vector space, whereby words which are related in meaning should be represented by points which are near to one another, first developed as a method for improving the vector model for information retrieval (Deerwester et al., 1990). As a technique for measuring similarity between words, LSA has been shown to capture semantic properties, and has been used successfully for recognising synonymy (Landauer and Dumais, 1997), word-sense disambiguation (Sch¨utze, 1998) and for finding correct translations of individual terms (Widdows et al., 2002). The LSA model we built is similar to that described in (Sch¨utze, 1998). First, 1000 frequent content words (i.e. not on the stoplist) 1 were chosen as “content-bearing words”. Using these contentbearing words as column labels, the 50,000 most frequent terms in the corpus were assigned row vectors by counting the number of times they oc1 A “stoplist” is a list of frequent words which have little independent semantic content, such as prepositions and determiners (Baeza-Yates and Ribiero-Neto, 1999, p167). curred within the same sentence as a content-bearing word. Singular-value decomposition"
W03-1812,N03-1036,1,0.557356,"between them, in the same way as the similarity between a query and a document is often measured in information retrieval (Baeza-Yates and Ribiero-Neto, 1999, p28). Effectively, we could use LSA to measure the extent to which two words or MWEs x and y usually occur in similar contexts. Since the corpora had been tagged with parts-ofspeech, we could build syntactic distinctions into the LSA models — instead of just giving a vector for the string test we were able to build separate vectors for the nouns, verbs and adjectives test. This combination of technologies was also used to good effect by Widdows (2003): an example of the contribution of part-of-speech information to extracting semantic neighbours of the word fire is shown in Table 1. As can be seen, the noun fire (as in the substance/element) and the verb fire (mainly used to mean firing some sort of weapon) are related to quite different areas of meaning. Building a single vector for the string fire confuses this distinction — the neighbours of fire treated just as a string include words related to both the meaning of fire as a noun (more frequent in the BNC) and as a verb. The appropriate granularity of syntactic classifications is an ope"
W03-1812,C98-2122,0,\N,Missing
W04-0404,briscoe-carroll-2002-robust,0,0.0195661,". 3.3 Corpus data The corpus frequencies were extracted from the same three corpora as were described in 1: the BNC and Reuters Corpus for English, and Mainichi Shimbun Corpus for Japanese. We chose to use the BNC and Reuters Corpus because of their complementary nature: the BNC is a balanced corpus and hence has a rounded coverage of NN compounds (see Table 1), whereas the Reuters Corpus contains newswire data which aligns relatively well in content with the newspaper articles in the Mainichi Shimbun Corpus. We calculated the corpus frequencies based on the tag and dependency output of RASP (Briscoe and Carroll, 2002) for English, and CaboCha (Kudo and Matsumoto, 2002) for Japanese. RASP is a tag sequence grammar-based stochastic parser which attempts to exhaustively resolve inter-word dependencies in the input. CaboCha, on the other hand, chunks the input into head-annotated “bunsetsu” or base phrases, and resolves only inter-phrase dependencies. We thus independently determined the intra-phrasal structure from the CaboCha output based on POS-conditioned templates. - 4 Evaluation We evaluate the method over both JE and EJ translation selection, using the two sets of 750 NN compounds described in 2.2. In e"
W04-0404,C02-1011,0,0.0214601,"or or MT system to select between based on the full translation context. In the remainder of the paper, we describe the translation procedure and resources used in this research ( 2), and outline the translation candidate selection method, a benchmark selection method and pre-processors our method relies on ( 3). We then evaluate the method using a variety of data sources ( 4), and finally compare our method to related research ( 5). - - 2 - - Preliminaries 2.1 Translation procedure We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)). Generation consists of looking up word-level translations for each word in the NN compound to be translated, and running them through a set of constructional translation templates to generate translation candidates. In order to translate kaNkei kaizeN “improvement in relations”, for example, possible word-level translations for are relation, connection and relationship, and translations for are improvement and betterment. Constructional templates are of the form [N in N ] (where N indicates that the word is a noun (N) in English ( ) and corresponds to the th-"
W04-0404,1999.tc-1.8,0,0.409038,"e to those used in this research, and has the potential to further improve the accuracy of our method. Nagata et al. (2001) use “partially bilingual” web pages, that is web pages which are predominantly Japanese, say, but interspersed with English words, to extract translation pairs. They do this by accessing web pages containing a given Japanese expression, and looking for the English expression which occurs most reliably in its immediate vicinity. The method achieves an impressive gold-standard accuracy of 0.62, at a recall of 0.68, over a combination of simplex nouns and compound nominals. Grefenstette (1999) uses web data to select English translations for compositional German and Spanish noun compounds, and achieves an impressive accuracy of 0.86–0.87. The translation task Grefenstette targets is intrinsically simpler than that described in this paper, however, in that he considers only those compounds which translate into NN compounds in English. It is also possible that the historical relatedness of languages has an effect on the difficulty of the translation task, although further research would be required to confirm this prediction. Having said this, the successful use of web data by a vari"
W04-0404,1991.mtsummit-papers.16,0,0.02313,"Missing"
W04-0404,P03-1040,0,0.0484347,"Missing"
W04-0404,W02-2016,0,0.0128086,"ed from the same three corpora as were described in 1: the BNC and Reuters Corpus for English, and Mainichi Shimbun Corpus for Japanese. We chose to use the BNC and Reuters Corpus because of their complementary nature: the BNC is a balanced corpus and hence has a rounded coverage of NN compounds (see Table 1), whereas the Reuters Corpus contains newswire data which aligns relatively well in content with the newspaper articles in the Mainichi Shimbun Corpus. We calculated the corpus frequencies based on the tag and dependency output of RASP (Briscoe and Carroll, 2002) for English, and CaboCha (Kudo and Matsumoto, 2002) for Japanese. RASP is a tag sequence grammar-based stochastic parser which attempts to exhaustively resolve inter-word dependencies in the input. CaboCha, on the other hand, chunks the input into head-annotated “bunsetsu” or base phrases, and resolves only inter-phrase dependencies. We thus independently determined the intra-phrasal structure from the CaboCha output based on POS-conditioned templates. - 4 Evaluation We evaluate the method over both JE and EJ translation selection, using the two sets of 750 NN compounds described in 2.2. In each case, we first evaluate system performance accor"
W04-0404,P98-1116,0,0.0145103,"ect between based on the full translation context. In the remainder of the paper, we describe the translation procedure and resources used in this research ( 2), and outline the translation candidate selection method, a benchmark selection method and pre-processors our method relies on ( 3). We then evaluate the method using a variety of data sources ( 4), and finally compare our method to related research ( 5). - - 2 - - Preliminaries 2.1 Translation procedure We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)). Generation consists of looking up word-level translations for each word in the NN compound to be translated, and running them through a set of constructional translation templates to generate translation candidates. In order to translate kaNkei kaizeN “improvement in relations”, for example, possible word-level translations for are relation, connection and relationship, and translations for are improvement and betterment. Constructional templates are of the form [N in N ] (where N indicates that the word is a noun (N) in English ( ) and corresponds to the th-occurring noun in the original J"
W04-0404,W01-1413,0,0.0282693,"their method is based on contextual similarity in the two languages, without assuming parallelism or comparability in the corpus data. They report an impressive F-score of 0.73 over a dataset of 1000 instances, although they also cite a prior-based Fscore (equivalent to our Baseline) of 0.70 for the task, such that the particular data set they are dealing with would appear to be less complex than that which we have targeted. Having said this, contextual similarity is an orthogonal data source to those used in this research, and has the potential to further improve the accuracy of our method. Nagata et al. (2001) use “partially bilingual” web pages, that is web pages which are predominantly Japanese, say, but interspersed with English words, to extract translation pairs. They do this by accessing web pages containing a given Japanese expression, and looking for the English expression which occurs most reliably in its immediate vicinity. The method achieves an impressive gold-standard accuracy of 0.62, at a recall of 0.68, over a combination of simplex nouns and compound nominals. Grefenstette (1999) uses web data to select English translations for compositional German and Spanish noun compounds, and a"
W04-0404,N01-1006,0,0.018268,"Missing"
W04-0404,rose-etal-2002-reuters,0,0.0225353,"Missing"
W04-0404,W03-1803,1,0.66047,"thin the and context of the translation template (8 "" # + ) is to capture the subcategorisation prop8 "" # + erties of "" # and "" # relative to . For example, if ""$# and ""$# were Bandersnatch and relation, 29( for all , we respectively, and 8 ""# +-""# + would hope to score relation to (the) Bandersnatch as being more likely than relation on (the) Bandersnatch. We could hope to achieve this by virtue of the fact that relation occurs in the form relation to ... much more frequently than relation on ..., making the value of 8 "" # + greater for the template [N to N ] than [N on N ]. In evaluation, Tanaka and Baldwin (2003b) found the principal failing of this method to be its treatment of all translations contained in the transfer dictionary as being equally likely, where in fact        3 In the         original formulation, the product was included as a third term, but Tanaka and Baldwin (2003b) found it to have negligible impact on translation accuracy, so we omit it here. 4 = and = are assumed to be POS-compatible with A . ? :<;>= ? :<;A ? :<;>= @    Template (JE) [N N ]J [N [N N ]J [N [N N ]J [N Template (EJ) [N N ]E [N [N N ]E [N [N N ]E [N"
W04-0404,2003.mtsummit-papers.50,1,0.767849,"thin the and context of the translation template (8 "" # + ) is to capture the subcategorisation prop8 "" # + erties of "" # and "" # relative to . For example, if ""$# and ""$# were Bandersnatch and relation, 29( for all , we respectively, and 8 ""# +-""# + would hope to score relation to (the) Bandersnatch as being more likely than relation on (the) Bandersnatch. We could hope to achieve this by virtue of the fact that relation occurs in the form relation to ... much more frequently than relation on ..., making the value of 8 "" # + greater for the template [N to N ] than [N on N ]. In evaluation, Tanaka and Baldwin (2003b) found the principal failing of this method to be its treatment of all translations contained in the transfer dictionary as being equally likely, where in fact        3 In the         original formulation, the product was included as a third term, but Tanaka and Baldwin (2003b) found it to have negligible impact on translation accuracy, so we omit it here. 4 = and = are assumed to be POS-compatible with A . ? :<;>= ? :<;A ? :<;>= @    Template (JE) [N N ]J [N [N N ]J [N [N N ]J [N Template (EJ) [N N ]E [N [N N ]E [N [N N ]E [N"
W04-0404,W97-0119,0,\N,Missing
W04-0404,C98-1112,0,\N,Missing
W04-0907,P92-1028,0,0.0489175,"gate the success of various parameter configurations in interpreting RCCs. One feature of the proposed method is that it is based on shallow analysis, centring principally around a basic case frame and verb class description. That is, we attempt to make maximum use of surface information in performing a deep semantic task, in the same vein, e.g., as Joanis and Stevenson (2003) for English verb classification and Lapata (2002) in disambiguating nominalisations. Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series (Cardie, 1992; Hobbs et al., 1997). It also has immediate applications in, e.g., Japanese–English machine translation: for case-slot gapping RCCs such as (1), we extrapose the head NP from the appropriate argument position in the English relative clause (producing, e.g., “the hat [ bought yesterday]”), and for attributive RCCs such as (2), we generate the English relative clause without extraposition and select the relative pronoun according to the head NP (producing, e.g., “the reason that the hat was bought”). RCC interpretation is dogged by analytical ambiguity, in particular for phrase boundary, phrase"
W04-0907,E03-1040,0,0.0238802,"1997), although we ignore its effects, and indeed the impact of context, in this research. Our objective in this paper is, given a taxonomy of Japanese RCC semantic types (Baldwin, 1998) and a gold-standard set of Japanese RCC instances, to investigate the success of various parameter configurations in interpreting RCCs. One feature of the proposed method is that it is based on shallow analysis, centring principally around a basic case frame and verb class description. That is, we attempt to make maximum use of surface information in performing a deep semantic task, in the same vein, e.g., as Joanis and Stevenson (2003) for English verb classification and Lapata (2002) in disambiguating nominalisations. Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series (Cardie, 1992; Hobbs et al., 1997). It also has immediate applications in, e.g., Japanese–English machine translation: for case-slot gapping RCCs such as (1), we extrapose the head NP from the appropriate argument position in the English relative clause (producing, e.g., “the hat [ bought yesterday]”), and for attributive RCCs such as (2), we generate the English relative claus"
W04-0907,W02-2016,0,0.0300486,"e the head NP from the appropriate argument position in the English relative clause (producing, e.g., “the hat [ bought yesterday]”), and for attributive RCCs such as (2), we generate the English relative clause without extraposition and select the relative pronoun according to the head NP (producing, e.g., “the reason that the hat was bought”). RCC interpretation is dogged by analytical ambiguity, in particular for phrase boundary, phrase head/attachment and word sense ambiguity. The first two of these concerns can be dealt with by a parser such as KNP (Kurohashi and Nagao, 1998) or CaboCha (Kudo and Matsumoto, 2002), or alternatively a tag sequence-based technique such as that proposed by Siddharthan (2002) for English. Word sense ambiguity is an issue if we wish to determine the valence of the verb and make use of selectional restrictions. We sidestep full-on verb sense disambiguation by associating a unique case frame with each verb stem type and encoding common alternations in the verb class. Even here, however, we must have some means of dealing with verb homonymy and integrating analyses for cosubordinated relative clauses. We investigate various techniques to resolve such ambiguity and combine the"
W04-0907,J02-3004,0,0.0337892,"context, in this research. Our objective in this paper is, given a taxonomy of Japanese RCC semantic types (Baldwin, 1998) and a gold-standard set of Japanese RCC instances, to investigate the success of various parameter configurations in interpreting RCCs. One feature of the proposed method is that it is based on shallow analysis, centring principally around a basic case frame and verb class description. That is, we attempt to make maximum use of surface information in performing a deep semantic task, in the same vein, e.g., as Joanis and Stevenson (2003) for English verb classification and Lapata (2002) in disambiguating nominalisations. Relative clause interpretation is a core component of text understanding, as demonstrated in the context of the MUC conference series (Cardie, 1992; Hobbs et al., 1997). It also has immediate applications in, e.g., Japanese–English machine translation: for case-slot gapping RCCs such as (1), we extrapose the head NP from the appropriate argument position in the English relative clause (producing, e.g., “the hat [ bought yesterday]”), and for attributive RCCs such as (2), we generate the English relative clause without extraposition and select the relative pr"
W04-0907,C98-2120,0,0.0764987,"Missing"
W04-0907,P98-2125,0,\N,Missing
W05-1008,2003.mtsummit-systems.9,0,0.0154053,"we have a word list (ideally lemmatised). 3.2 Derviational morphology The second morphology-based DLA method makes use of derivational morphology and analysis of the process of word formation. As an example of how derivational information could assist DLA, knowing that the noun achievement is deverbal and incorporates the -ment suffix is a strong predictor of it being optionally uncountable and optionally selecting for a PP argument (i.e. being of lexical type n mass count ppof le). We generate derivational morphological features for a given lexeme by determining its word cluster in CATVAR7 (Habash and Dorr, 2003) and then for each sister lexeme (i.e. lexeme occurring in the 5 Although this would inevitably lose lexical generalisations among the different word forms of a given lemma. 6 We also experimented with syllabification, but found the character n-grams to produce superior results. 7 In the case that the a given lemma is not in CATVAR, we attempt to dehyphenate and then deprefix the word to find a match, failing which we look for the lexeme of smallest edit distance. same cluster as the original lexeme with the same word stem), determine if there is a series of edit operations over suffixes and p"
W05-1008,E03-1040,0,0.0396193,"eneral methods for DLA, aligned more closely with this research. Fouvry (2003) proposed a method of token-based DLA for unification-based precision grammars, whereby partially-specified lexical features generated via the constraints of syntacticallyinteracting words in a given sentence context, are combined to form a consolidated lexical entry for that word. That is, rather than relying on indirect feature signatures to perform lexical acquisition, the DLR itself drives the incremental learning process. Also somewhat related to this research is the general-purpose verb feature set proposed by Joanis and Stevenson (2003), which is shown to be applicable in a range of DLA tasks relating to English verbs. 2.1 English Resource Grammar All experiments in this paper are targeted at the English Resource Grammar (ERG; Flickinger (2002), Copestake and Flickinger (2000)). The ERG is an implemented open-source broad-coverage precision Head-driven Phrase Structure Grammar Secondary LR type Description Preprocessor(s) Word list∗∗∗ Morphological lexicon∗ Compiled corpus∗∗∗ List of words with basic POS Derivational and inflectional word relations Unannotated text corpus WordNet-style ontology∗ Lexical semantic word linkage"
W05-1008,J93-2004,0,0.0510449,"14 Table 2: Feature types used in syntax-based DLA for the different preprocessors and chunker, and also dependency-derived features, namely the modifier of all dependency tuples the target word occurs as head of, and conversely, the head of all dependency tuples the target word occurs as modifier in, along with the dependency relation in each case. See Table 2 for full details. 4.4 Corpora We ran the three syntactic preprocessors over a total of three corpora, of varying size: the Brown corpus (∼460K tokens) and Wall Street Journal corpus (∼1.2M tokens), both derived from the Penn Treebank (Marcus et al., 1993), and the written component of the British National Corpus (∼98M tokens: Burnard (2000)). This selection is intended to model the effects of variation in corpus size, to investigate how well we could expect syntax-based DLA methods to perform over both smaller and larger corpora. Note that the only corpus annotation we make use of is sentence tokenisation, and that all preprocessors are run automatically over the raw corpus data. This is in an attempt to make the methods maximally applicable to lower-density languages where annotated corpora tend not to exist but there is at least the possibil"
W05-1008,P03-1059,1,0.823715,"vailable for languages of varying density, and aim to minimise assumptions as to the pre-existence of particular preprocessing tools. The basic types of resources and tools we experiment with in this paper are detailed in Table 1. Past research on DLA falls into two basic categories: expert system-style DLA customised to learning particular linguistic properties, and DLA via resource translation. In the first instance, a specialised methodology is proposed to (automatically) learn a particular linguistic property such as verb subcategorisation (e.g. Korhonen (2002)) or noun countability (e.g. Baldwin and Bond (2003a)), and little consideration is given to the applicability of that method to more general linguistic properties. In the second instance, we take one DLR and map it onto another to arrive at the lexical information in the desired format. This can take the form of a onestep process, in mining lexical items directly from a DLR (e.g. a machine-readable dictionary (Sanfilippo and Pozna´nski, 1992)), or two-step process in reusing an existing system to learn lexical properties in one format and then mapping this onto the DLR of choice (e.g. Carroll and Fang (2004) for verb subcategorisation learnin"
W05-1008,W03-1010,1,0.861369,"vailable for languages of varying density, and aim to minimise assumptions as to the pre-existence of particular preprocessing tools. The basic types of resources and tools we experiment with in this paper are detailed in Table 1. Past research on DLA falls into two basic categories: expert system-style DLA customised to learning particular linguistic properties, and DLA via resource translation. In the first instance, a specialised methodology is proposed to (automatically) learn a particular linguistic property such as verb subcategorisation (e.g. Korhonen (2002)) or noun countability (e.g. Baldwin and Bond (2003a)), and little consideration is given to the applicability of that method to more general linguistic properties. In the second instance, we take one DLR and map it onto another to arrive at the lexical information in the desired format. This can take the form of a onestep process, in mining lexical items directly from a DLR (e.g. a machine-readable dictionary (Sanfilippo and Pozna´nski, 1992)), or two-step process in reusing an existing system to learn lexical properties in one format and then mapping this onto the DLR of choice (e.g. Carroll and Fang (2004) for verb subcategorisation learnin"
W05-1008,briscoe-carroll-2002-robust,0,0.00764438,"s (such as surrounding chunk types when the target word is chunk head). See Table 2 for full details. Note that while chunk parsers are theoretically easier to develop than full phrase-structure or treebank parsers, only high-density languages such as English and Japanese have publicly available chunk parsers. 4.3 Dependency parsing The third and final form of syntactic preprocessing is dependency parsing, which represents the pinnacle of both robust syntactic sophistication and inaccessibility for any other than the highest-density languages. The particular dependency parser we use is RASP9 (Briscoe and Carroll, 2002), which outputs head–modifier dependency tuples and further classifies each tuple according to a total of 14 relations; RASP also outputs the POS tag of each word token. As our features, we use both local word and POS features, for comparability with the POS tagger 9 RASP is, strictly speaking, a full syntactic parser, but we use it in dependency parser mode Feature type TAGGER POS tag Word POS bi-tag Positions/description Total (−4, −3, −2, −1, 0, 1, 2, 3, 4) (−4, −3, −2, −1, 1, 2, 3, 4) ( (−4, −1), (−4, 0), (−3, −2), (−3, −1), (−3, 0), (−2, −1), (−2, 0), (−1, 0), (0, 1), (0, 2), (0, 3), (0,"
W05-1008,copestake-flickinger-2000-open,0,0.285703,"cticallyinteracting words in a given sentence context, are combined to form a consolidated lexical entry for that word. That is, rather than relying on indirect feature signatures to perform lexical acquisition, the DLR itself drives the incremental learning process. Also somewhat related to this research is the general-purpose verb feature set proposed by Joanis and Stevenson (2003), which is shown to be applicable in a range of DLA tasks relating to English verbs. 2.1 English Resource Grammar All experiments in this paper are targeted at the English Resource Grammar (ERG; Flickinger (2002), Copestake and Flickinger (2000)). The ERG is an implemented open-source broad-coverage precision Head-driven Phrase Structure Grammar Secondary LR type Description Preprocessor(s) Word list∗∗∗ Morphological lexicon∗ Compiled corpus∗∗∗ List of words with basic POS Derivational and inflectional word relations Unannotated text corpus WordNet-style ontology∗ Lexical semantic word linkages — — POS tagger∗∗ Chunk parser∗ Dependency parser∗ — Table 1: Secondary LR and tool types targeted in this research (∗∗∗ = high expectation of availability for a given language; ∗∗ = medium expectation of availability; ∗ = low expectation of av"
W05-1008,N01-1006,0,0.0127973,"f a given lexeme, and filters over each feature type to produce a maximum of 50 feature instances of highest saturation (e.g. if the feature type is the word immediately proceeding the target word, the feature instances are the 50 words which proceed the most words in our lexicon). The feature signature associated with a word for a given preprocessor type will thus have a maximum of 3,900 items (39 × 50 × 2).8 4.1 Tagging The first and most basic form of syntactic preprocessing is part-of-speech (POS) tagging. For our purposes, we use a Penn treebank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001), and further lemmatise the output of the tagger using morph (Minnen et al., 2000). 8 Note that we will have less than 50 feature instances for some feature types, e.g. the POS tag of the target word, given that the combined size of the Penn POS tagset is 36 elements (not including punctuation). 71 The second form of syntactic preprocessing, which builds directly on the output of the POS tagger, is CoNLL 2000-style full text chunking (Tjong Kim Sang and Buchholz, 2000). The particular chunker we use was custom-built using fnTBL 1.0 once again, and operates over the lemmatised output of the POS"
W05-1008,A92-1011,0,0.253314,"Missing"
W05-1008,W00-0726,0,0.00501176,"processing is part-of-speech (POS) tagging. For our purposes, we use a Penn treebank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001), and further lemmatise the output of the tagger using morph (Minnen et al., 2000). 8 Note that we will have less than 50 feature instances for some feature types, e.g. the POS tag of the target word, given that the combined size of the Penn POS tagset is 36 elements (not including punctuation). 71 The second form of syntactic preprocessing, which builds directly on the output of the POS tagger, is CoNLL 2000-style full text chunking (Tjong Kim Sang and Buchholz, 2000). The particular chunker we use was custom-built using fnTBL 1.0 once again, and operates over the lemmatised output of the POS tagger. The feature set for the chunker output includes a subset of the POS tagger features, but also makes use of the local syntactic structure in the chunker input in incorporating both intra-chunk features (such as modifiers of the target word if it is the head of a chunk, or the head if it is a modifier) and inter-chunk features (such as surrounding chunk types when the target word is chunk head). See Table 2 for full details. Note that while chunk parsers are the"
W05-1008,W00-1427,0,\N,Missing
W06-1201,W01-0513,0,\N,Missing
W06-1201,W03-1810,0,\N,Missing
W06-1201,P01-1025,0,\N,Missing
W06-1201,W03-1812,1,\N,Missing
W06-1201,W03-1809,1,\N,Missing
W06-1201,P99-1041,0,\N,Missing
W06-1208,briscoe-carroll-2002-robust,0,0.0351492,"e translations in a machine translation task through Web statistics, and avoids some data sparseness within that context. Zhu and Rosenfeld (2001) train a language model from a large corpus, and use the Web to estimate low–density trigram frequencies. Keller and Lapata (2003) show that Web counts can obviate data sparseness for syntactic predicate– 3 Resources We make use of a number of lexical resources in our implementation and evaluation. For corpus statistics, we use the written component of the BNC, a balanced 90M token corpus. To find verb–argument frequencies, we parse this using RASP (Briscoe and Carroll, 2002), a tag sequence grammar–based statistical parser. We contrast the corpus statistics with ones collected from the 55 4 Proposed Method Web, using an implementation of a freely available Google “scraper” from CPAN.1 For a given compound nominalisation, we wish to determine all possible verbal forms of the head. We do so using the combination of the morphological component of C ELEX (Burnage, 1990), a lexical database, N OMLEX (Macleod et al., 1998), a nominalisation database, and C AT VAR (Habash and Dorr, 2003), an automatically–constructed database of clusters of inflected words based on the"
W06-1208,J02-3004,0,0.280991,"rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object, or a number of prepositional objects in the latter. 2.2 2.3 Confidence Intervals Maximum likelihood statistics are not robust when many sparse vectors are under consideration, i.e. naively “choosing the largest number” may not be accurate in contexts when the relative value across samplings may be relevant, for example, in machine learning. As such, we apply a statistical test wit"
W06-1208,W05-0603,0,0.075792,"Missing"
W06-1208,U05-1022,1,0.662277,"“the investor hesitates”), compared to the past participle for the DOB relation (the spoken language ≡ “[SO] speaks the language”). The corresponding prepositional object construction is unusual in English, but still possible: compare ?the operatedon leg and the lived-in village. 1. items that did not occur in the same chunk, according to a chunker based on fnTBL 1.0 (Ngai and Florian, 2001), 2. items whose head did not have a verbal form according to our lexical resources, and 3. items which consisted in part of proper nouns, 4.2 to end up with 695 consistent compounds. We used the method of Nicholson and Baldwin (2005) to derive a small data set of 129 compound nominalisations, also from the BNC, which we instructed three unskilled annotators to identify each as one of subjective (SUB), direct object (DOB), or prepositional object (POB, e.g. side show “[SO] show [ST] on the side”). The annotators identified nine prepositional relations: {about,against,for,from,in,into,on,to,with}. The Algorithm Given a compound nominalisation, we perform a number of steps to arrive at an interpretation. First, we derive a set of verbal forms for the head from the combination of C ELEX , N OMLEX, and C ATVAR. We find the par"
W06-1208,W01-0511,0,0.0121952,"he compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepositions. Lapata (2002) and Grover et al. (2005) again use a corpus statistical paraphrase–based approach, but with verb– argument relations for compound nominalisations — attempting to define the relation as one of subject, direct object,"
W06-1208,W03-1803,1,0.931416,"Missing"
W06-1208,C94-2125,0,0.052221,"mpound Noun Interpretation Compound nouns were seminally and thoroughly analysed by Levi (1978), who hand–constructs a nine–way set of semantic relations that she identifies as broadly defining the observed relationships between the compound head and modifier. Warren (1978) also inspects the syntax of compound nouns, to create a somewhat different set of twelve conceptual categories. Early attempts to automatically classify compound nouns have taken a semantic approach: Finin (1980) and Isabelle (1984) use “role nominals” derived from the head of the compound to fill a slot with the modifier. Vanderwende (1994) uses a rule–based technique that scores a compound on possible semantic interpretations, while Jones (1995) implements a graph–based unification procedure over semantic feature structures for the head. Finally, Rosario and Hearst (2001) make use of a domain–specific lexical resource to classify according to neural networks and decision trees. Syntactic classification, using paraphrasing, was first used by Leonard (1984), who uses a prioritised rule–based approach across a number of possible readings. Lauer (1995) employs a corpus statistical model over a similar paraphrase set based on prepos"
W06-1208,1999.tc-1.8,0,\N,Missing
W06-1208,E03-1073,0,\N,Missing
W06-1208,N01-1006,0,\N,Missing
W06-1208,P84-1109,0,\N,Missing
W06-1208,J03-3005,0,\N,Missing
W06-1208,N03-1013,0,\N,Missing
W06-1208,2003.mtsummit-systems.9,0,\N,Missing
W06-1620,W05-1008,1,0.835659,"to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. We propose a conditional random fieldbased method for supertaggi"
W06-1620,J99-2004,0,0.47271,"re that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Metho"
W06-1620,W02-1502,0,0.0269699,"tems for unknown words, by generating underspecified lexical items for all unknown words and parsing with them. Syntactico-semantic interaction between unknown words and pre-existing lexical items during parsing provides insight into the nature of each unknown word. By combining such fragments of information, it is possible to incrementally arrive at a consolidated lexical entry for that word. That is, the precision grammar itself drives the incremental learning process within a parsing context. In this research, we focus particularly on the Grammar Matrix-based DELPH - IN family of grammars (Bender et al., 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean. The Grammar Matrix is a framework for streamlining and standardising HPSG-based multilingual grammar development. One property of Grammar Matrixbased grammars is that they are strongly lexicalist and adhere to a highly constrained lexicongrammar interface via a unique (terminal) lexical type for each lexical item. As such, lexical item creation in any of the Grammar Matrix-based grammars, irrespective of language, consists predominantly of predicting the appropriate lexical type for each lexical ite"
W06-1620,N01-1006,0,0.0469605,"ing, such as Tagalog, we may want to include -gram infixes in addition to -gram prefixes and suffixes. Here again, however, the decision about what range of affixes is appropriate for a given language requires only superficial knowledge of its morphology. 5 is predicted by:  ?BA""C DF ?    In the instance that  was not observed in the training data, we back off to the majority lexical type in the training data. 5.2 Benchmark: fnTBL In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FN TBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. FN TBL is a transformation-based learner that is distributed with pre-optimised POS tagging modules for English and other European languages that can be redeployed over the task of supertagging. Following Baldwin (2005b), the only modifications we make to the default English POS tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to n - c le and n - pn le, respectively; and (2) reduce the threshold score for lexical and context transformation rules to 1. It is important to realise that"
W06-1620,C04-1180,0,0.0274301,"natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing c"
W06-1620,P02-1035,0,0.0264849,"win Computer Science and Software Engineering University of Melbourne, Victoria 3010 Australia {pcbl,tim}@csse.unimelb.edu.au Abstract Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on s"
W06-1620,N03-1028,0,0.0502315,"Missing"
W06-1620,C04-1041,0,0.289443,"lude DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171, c Sydney, July 2006. 2006 Association for Computational Linguistics eithe"
W06-1620,W02-1210,0,0.0715803,"th the left-adjacent word to form a single lexeme, and shares the same lexical type. This tagging convention is based on that used, e.g., in the CLAWS7 part-of-speech tagset. Optionally discontinuous lexical items are less of a concern, as selection of each of the discontinuous “components” is done via lexical types. E.g. in the case of pick up, the lexical entry looks as follows: Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For example, in the English grammar, the lexical item for the noun"
W06-1620,copestake-flickinger-2000-open,0,0.137222,"indicates that the current word combines (possibly recursively) with the left-adjacent word to form a single lexeme, and shares the same lexical type. This tagging convention is based on that used, e.g., in the CLAWS7 part-of-speech tagset. Optionally discontinuous lexical items are less of a concern, as selection of each of the discontinuous “components” is done via lexical types. E.g. in the case of pick up, the lexical entry looks as follows: Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For ex"
W06-1620,P04-1057,0,0.0922746,"Missing"
W06-1620,E03-1040,0,0.031734,"r corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 countable, ""dog"" specifies the lexical stem, and "" dog n 1 rel"" introduces an ad hoc predicate name for the lexical item to use in constructing a semantic representation. In the context of the ERG and JACY"
W06-1620,U05-1006,0,0.0611088,"ion for Computational Linguistics either: (a) off-line for the purposes of rounding out the coverage of a static lexicon, in which case we are generally interested in globally maximising precision over a given corpus and hence predicting the single most plausible lexical type for each word token (off-line DLA: Baldwin (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. supertagger-based DLA methods. The remainder of this paper is structured as follows. Section 2 outlines past work relative to this research, and Section 3 reviews the resources used in our supertagging experiments. Section 4 outlines the proposed supertagger model and reviews previous research on supertaggerbased DLA. Section 5 then outlines the set-up and results of our evaluation. 2 Past Research Ac"
W06-1620,zhang-kordoni-2006-automated,0,0.0794524,"Missing"
W06-1620,N04-1016,0,0.0313107,"n that they do not rely on preprocessing of any form), and check for corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 countable, ""dog"" specifies the lexical stem, and "" dog n 1 rel"" introduces an ad hoc predicate name for the lexical item to use in cons"
W06-1620,W02-2018,0,0.034304,"le, oqpsrqt “mouldy” would be flagged as containing katakana character(s), kanji character(s) and hiragana character(s) only. Note that the only language-dependent component of J  4.2 b c6de.fhgji C   &lt;   2  g""i   C  R  (4) In order to train the model, we maximize (4). While the log-pseudo-likelihood cannot be maximised for the parameters, * , in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and 168 Baseline FN TBL  CRF !  CRF ACC 0.802 0.915 0.911 0.917 ACC 0.053 0.236 0.427 0.489 ERG P REC 0.184 0.370 0.339 0.509 R EC 0.019 0.038 0.053 0.059 F- SCORE 0.034 0.068 0.092 0.105 ACC 0.866 — 0.920 0.932 ACC 0.592 — 0.816 0.827 JACY P REC 0.680 — 0.548 0.696 R EC 0.323 — 0.414 0.424 F- SCORE 0.438 — 0.471 0.527 Table 3. Results of supertagging for the ERG and JACY (best result in each column in bold) the lexical features is the character sets, which requires little or no specialist knowledge of the language. Note also that for languages with infixing, such as Tagalog,"
W06-2110,W02-2001,1,0.869882,"vant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1 The learning of lexical items in a form that can be fed directly into a de"
W06-2110,W03-1812,1,0.860051,"to be used with objects with the semantics of object and prepositional phrases containing NPs with the semantics of place. Also, as observed above, the valence of a VPC can differ from that of the head verb. (3) and (4) illustrate two different senses of take off with intransitive and transitive syntax, respectively. Note that take cannot occur as a simplex intransitive verb. wearing (3) take off = lift off The airplane takes off. ARGS : When verbs co-occur with particles to form VPCs, their meaning can be significantly different from the semantics of the head verb in isolation. According to Baldwin et al. (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. In one example cited in the paper, the cosine similarity between cut and cut out, based on word co-occurrence vectors, was found to be greater than that between cut and cut off, mirroring the intuitive compositionality of these VPCs. (1) and (2) illustrate the difference in the selectional preferences of the verb put in isolation as compared with the VPC put on.3 book OBJ = book, publication, object ANALYSIS : EX : 2 Put the book on the table."
W06-2110,W03-1809,1,0.923199,"lay lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) o"
W06-2110,briscoe-carroll-2002-robust,0,0.0226196,"off co-occurs with a subject of the class airplane, aeroplane. In (4), on the other hand, take off = remove and the corresponding object noun is of class garment or clothing. From the above, we can see that head nouns in the subject and object argument positions can be used to distinguish VPCs from simplex verbs with prepositional phrases (i.e. verb-PPs). 3 Approach Our goal is to distinguish VPCs from verb-PPs in corpus data, i.e. to take individual inputs such as Kim handed the paper in today and tag each as either a VPC or a verb-PP. Our basic approach is to parse each sentence with RASP (Briscoe and Carroll, 2002) to obtain a first-gloss estimate of the VPC and verb-PP token instances, and also identify the head nouns of the arguments of each VPC and simplex verb. For the head noun of each subject and object, as identified by RASP, we use WordNet 2.1 (Fellbaum, 1998) to obtain the word sense. Finally we build a supervised classifier using TiMBL 5.1 (Daelemans et al., 2004). raw text corpus RASP parser Preprocessing Verbs Particles v+p with Semantics e.g. take_off := [.. put_on := [.. look_after := [.. Subjects Objects WordNet Word Senses TiMBL Classifier 3.1 Method Compared to the method proposed by Ba"
W06-2110,calzolari-etal-2002-towards,0,0.0448145,"Missing"
W06-2110,P97-1018,0,0.0360332,"multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He"
W06-2110,P03-1065,0,0.05032,"that of the head verb (e.g. walk off ) or alternatively diverge (e.g. lift off ). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs"
W06-2110,J93-2004,0,0.0273478,"Missing"
W06-2110,W03-1810,0,0.357159,"and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put th"
W06-2110,P04-1036,0,0.0526714,"Missing"
W06-2110,W03-0411,0,0.241644,"Missing"
W06-2110,W03-1808,0,0.329858,"the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. In the case that the object is pronominal, however, the VPC must occur in split word order (c.f. *He handed in it) (Huddleston and Pullum, 2002; Villavicencio, 2003). The semantics of the VPC can either derive transparently from the semantics of the head verb and particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of VPCs can mirror those of their head verbs or alternatively diverge markedly. The syntax of the VPC can also coincide with that of the head verb (e.g. walk off ) or alternatively diverge (e.g. lift off ). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of"
W06-2110,W05-1006,0,0.0129273,"ncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order."
W07-0907,J98-1006,0,0.0220878,"exhibits in the path. For each word in the keyword set of each exhibit, the WordNet (Fellbaum, 1998) similarity is calculated against each word in another exhibit. The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits (K1 , K2 ), normalised over the length of both keyword sets: P k1 ∈K1 P W N sim(k1 , k2 ) |K1 ||K2 | k2 ∈K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits. The Lin (Lin, 1998) and Leacock-Chodorow (Leacock et al., 1998) similarity measures and the BanerjeePedersen (Patwardhan and Pedersen, 2003) relatedness measures were used. The similarities were normalised and transformed into probability P matrices such that j PW N sim (e|cj ) = 1 for each next exhibit ci . The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways. The history of the visitor at any given time is essential in keeping the visitor’s conceptual model of the exhibit space current. The recency of a given exhibi"
W07-0907,P98-2127,0,0.00507288,"based on the meanings of previous exhibits in the path. For each word in the keyword set of each exhibit, the WordNet (Fellbaum, 1998) similarity is calculated against each word in another exhibit. The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits (K1 , K2 ), normalised over the length of both keyword sets: P k1 ∈K1 P W N sim(k1 , k2 ) |K1 ||K2 | k2 ∈K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits. The Lin (Lin, 1998) and Leacock-Chodorow (Leacock et al., 1998) similarity measures and the BanerjeePedersen (Patwardhan and Pedersen, 2003) relatedness measures were used. The similarities were normalised and transformed into probability P matrices such that j PW N sim (e|cj ) = 1 for each next exhibit ci . The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways. The history of the visitor at any given time is essential in keeping the visitor’s conceptual model of the exhibit"
W07-0907,C98-2122,0,\N,Missing
W07-1220,baldwin-etal-2004-road,1,0.94279,"s within NLP tasks, to arrive at a detailed (=deep) syntactic and semantic analysis of the data. It is conventionally driven by deep grammars, which encode linguistically-motivated predictions of language behaviour, are usually capable of both parsing and generation, and generate a highlevel semantic abstraction of the input data. While enjoying a resurgence of interest due to advances in parsing algorithms and stochastic parse pruning/ranking, deep grammars remain an underutilised resource predominantly because of their lack of coverage/robustness in parsing tasks. As noted in previous work (Baldwin et al., 2004), a significant cause It is often the case that the different measures lead to significantly different assessments of the quality of DLA, even for a given DLA approach. Additionally, it is far from clear how the numbers generated by these evaluation metrics correlate with actual parsing performance when the output of a given DLA method is used. This makes standardised comparison among the various different approaches to DLA very difficult, if not impossible. It is far from clear which evaluation metrics are more indicative of the true “goodness” of the lexicon. The aim of this research, theref"
W07-1220,W05-1008,1,0.791921,"y may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. The whole procedure gets even more complicated when two unknown words occur next to each other, potentially allowing almost any constituent to be constructed. The evaluation of these proposals has tended to be small-scale and somewhat brittle. No concrete results have been presented relating to the improvement in grammar performance, either for parsing or for generation. Baldwin (2005) took a statistical approach to automated lexical acquisition for deep grammars. Focused on generalising the method of deriving DLA models on various secondary language resources, Baldwin used a large set of binary classifiers to predict whether a given unknown word is of a particular lexical type. This data-driven approach is grammar independent and can be scaled up for large grammars. Evaluation was via type precision, type recall, type F-measure and token accuracy, resulting in different interpretations of the data depending on the evaluation metric used. Zhang and Kordoni (2006) tackled th"
W07-1220,P98-1014,0,0.0260799,"over a carefully designed test suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computational"
W07-1220,E03-1041,0,0.0135372,"est suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. Th"
W07-1220,P06-1064,0,0.0613453,"Missing"
W07-1220,C02-2025,0,0.0585456,"Missing"
W07-1220,W02-1210,0,0.77711,"disation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance. Specifically, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. Our results show convincingly that traditional Fscore-based evaluation of lexical acquisition does not correlate with actual parsing performance. What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms. • Type precision: the proportion of correctly hypothesised lexical entries • Type recall: the proportion of gold-standard lexical entries that are correctly hypothesised • Type F-measure: the harmonic mean of the type precision and type recall • Toke"
W07-1220,P04-1057,0,0.265235,"Missing"
W07-1220,2006.jeptalnrecital-invite.2,0,0.0410557,"Missing"
W07-1220,zhang-kordoni-2006-automated,1,0.930026,"and the Lexicon: Standardising Deep Lexical Acquisition Evaluation Yi Zhang† and Timothy Baldwin‡ and Valia Kordoni† † Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany ‡ Dept of Computer Science and Software Engineering, University of Melbourne, Australia {yzhang,kordoni}@coli.uni-sb.de tim@csse.unimelb.edu.au Abstract of diminished coverage is the lack of lexical coverage. Various attempts have been made to ameliorate the deficiencies of hand-crafted lexicons. More recently, there has been an explosion of interest in deep lexical acquisition (DLA; (Baldwin, 2005; Zhang and Kordoni, 2006; van de Cruys, 2006)) for broad-coverage deep grammars, either by exploiting the linguistic information encoded in the grammar itself (in vivo), or by using secondary language resources (in vitro). Such approaches provide (semi-)automatic ways of extending the lexicon with minimal (or no) human interference. One stumbling block in DLA research has been the lack of standardisation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual pars"
W07-1220,A00-2018,0,\N,Missing
W07-1220,C98-1014,0,\N,Missing
W07-1602,J96-2004,0,0.111485,"Missing"
W07-1602,J03-3001,0,0.0372605,"and it is more natural-sounding to receive route instruc10 The only research we are aware of which has addressed this same topic of landmark interpretation is that of Tezuka and Tanaka (2005). In an investigation of the spatial use of landmarks in sentences, Tezuka and Tanaka (2005) modiﬁed existing web mining methods to include spatial context in order to obtain landmark information. It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). However, the vast amount of data available can nevertheless give better results than more theoretically motivated techniques (Lapata and Keller, 2004). And importantly, the data that can be gleaned from the web does not mirror the view of a single person or a select group, but of the entire global community (or at least the best available representation of it). 3 Methodology The prepositions and verbs which accompany a landmark in spatial sentences capture that landmark’s implicit conceptualisation. We use this implicit conceptualisation, as represented on the web, to develop two automated c"
W07-1602,N04-1016,0,0.0167913,"t of Tezuka and Tanaka (2005). In an investigation of the spatial use of landmarks in sentences, Tezuka and Tanaka (2005) modiﬁed existing web mining methods to include spatial context in order to obtain landmark information. It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). However, the vast amount of data available can nevertheless give better results than more theoretically motivated techniques (Lapata and Keller, 2004). And importantly, the data that can be gleaned from the web does not mirror the view of a single person or a select group, but of the entire global community (or at least the best available representation of it). 3 Methodology The prepositions and verbs which accompany a landmark in spatial sentences capture that landmark’s implicit conceptualisation. We use this implicit conceptualisation, as represented on the web, to develop two automated classiﬁcation schemes: a simple voting classiﬁer and a neural network classiﬁer. We compile a set of gold standard classiﬁcations in order to evaluate th"
W07-1602,J93-2004,0,0.0320235,"were then summed over the conceptual categories in which each preposition and verb appeared. The result of this was a probabilistic categorisation of each landmark as point, line or area, according to its usage in spatial sentences on the web. It is difﬁcult to determine the context of sentences using a search engine. It is uncertain whether the documents found by Google use the searched-for linguistic chunks in a spatial context or in some other context. For this reason, each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the Penn Treebank (Marcus et al., 1993) which are labelled with a spatial meaning. This weighting should give an approximation to the proportion of spatial usages of that word on the web. Automated Classiﬁcation proceed, which specify a motion but require a preposition for clariﬁcation. This second type of verb is of no interest to the study as they tell us nothing about the conceptualisation of landmarks. As with the prepositions, the verbs were grouped into the conceptual classes of point, line and area according to the requirements they place on reference objects, including enter for an area-like object, follow for a line-like o"
W07-2205,W02-1503,1,0.818483,"Baldwin University of Melbourne tim@csse.unimelb.edu.au Julia Hockenmaier University of Pennsylvania juliahr@cis.upenn.edu Mark Dras Macquarie University madras@ics.mq.edu.au Tracy Holloway King PARC thking@parc.com Abstract Gertjan van Noord University of Groningen vannoord@let.rug.nl dependencies or the underlying predicate-argument structure directly. Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al., 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al., 2005), ParGram (Butt et al., 2002) and DELPH-IN (Oepen et al., 2002) projects and meetings. However, the fundamental approaches to building a linguistically-founded system and many of the techniques used to engineer efficient systems are common across these projects and independent of the specific grammar formalism chosen. As such, we felt the need for a common meeting in which experiences could be shared among a wider community, similar to the role played by recent meetings on grammar engineering (Wintner, 2006; Bender and King, 2007). As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007"
W09-1410,P06-4020,0,0.0414435,"Missing"
W09-1410,copestake-flickinger-2000-open,0,0.00911965,"he ERG It seemed likely that syntactico-semantic analysis would be useful for task 3. To identify negation or speculation with relatively high precision, it is probable that knowledge of the relationships of possibly distant elements (such as the negation particle not) to a particular target word would provide valuable information for classification. Further to this, it was our intention to evaluate the utility of deep parsing in such an approach, 78 rather than a shallower annotation such as the output of a dependency parser. With this in mind, we selected the English Resource Grammar1 (ERG: Copestake and Flickinger (2000)), an open-source, broad-coverage high-precision grammar of English in the HPSG framework. While the ERG is relatively robust across different domains, it is a general-purpose resource, and there are some aspects of the language used in the biomedical abstracts that cause difficulties; unknown word handling is especially important given the nature of terms in the domain. Fortunately we can make some optimisations to mitigate this. The GENIA tagger mentioned in Section 2.1.1 provides both POS and named entity annotations, which we used to constrain the input to the ERG in two ways: • Biological"
W09-1410,C04-1185,0,0.0412547,"Missing"
W10-0508,P95-1005,0,0.516276,"Missing"
W10-0508,J05-2005,0,0.0605956,"ce IR effectiveness. 3 Conclusions This paper provides an outline of the ILIAD project, focusing on the tasks of crawling, thread-level analysis, post-level analysis, user-level analysis and IR reranking. We have designed a series of class sets for the component tasks, and carried out experimentation over a range of data sources, achieving encouraging results. 2.3 Post-level analysis Acknowledgements We automatically analyse the post-to-post discourse structure of each thread, in terms of which (preceding) post(s) each post relates to, and how, building off the work of Ros´e et al. (1995) and Wolf and Gibson (2005). For example, a given post may refute the solution proposed in an earlier post, and also propose a novel solution in response to the initiating post. Separately, we are developing techniques for identifying whether a new post to a given forum is sufficiently similar to other (ideally resolved) threads that the author should be prompted to first check the existing threads for redundancy before a new thread is initiated. Our experiments on post-level analysis are, once again, based on data from LinuxQuestions and CNET. NICTA is funded by the Australian Government as represented by the Departmen"
W10-2923,W10-0508,1,0.824462,"Missing"
W10-2923,P08-1095,0,0.032955,"s for post dependency linking and labelling, which achieve strong results for the respective tasks. In this work, we draw on existing work (esp. Xi et al. (2004)) in proposing a novel DA tag set customised to the analysis of troubleshootingoriented web user forums (Section 3), and compare a range of text classification and structured classification methods for post-level DA classification. Discourse disentanglement is the process of automatically identifying coherent sub-discourses in a single thread (in the context of user forums/mailing lists), chat session (in the context of IRC chat data: Elsner and Charniak (2008)), system interaction (in the context of HCI: Lemon et al. (2002)) or document (Wolf and Gibson, 2005). The exact definition of what constitutes a subdiscourse varies across domains, but for our purposes, entails an attempt to resolve the informa2 Related Work Related work exists in the broad fields of dialogue processing, discourse analysis and information retrieval, and can be broken down into the following tasks: (1) dialogue act tagging; (2) discourse “disentanglement”; (3) community question answering; and (4) newsgroup/user forum search. 193 ority of this method over a model which ignore"
W10-2923,N09-1035,0,0.0122661,"indicator of which post a given post responds (links) to, and can aid in DA tagging. We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is c"
W10-2923,J86-3001,0,0.188004,"structure was superior to that using a monolithic document representation. The observations and results of Xi et al. (2004) and Seo et al. (2009) that threading information (or in our case “disentangled” DAG structure) enhances IR effectiveness is a core motivator for this research. tion need of the initiator by a particular approach; if there are competing approaches proposed in a single thread, multiple sub-discourses will necessarily arise. The data structure used to represent the disentangled discourse varies from a simple connected sub-graph (Elsner and Charniak, 2008), to a stack/tree (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), to a full directed acyclic graph (DAG: Ros´e et al. (1995), Wolf and Gibson (2005), Schuth et al. (2007)). Disentanglement has been carried out via analysis of direct citation/user name references (Schuth et al., 2007; Seo et al., 2009), topic modelling (Lin et al., 2009), and clustering over content-based features for pairs of posts, optionally incorporating various constraints on post recency (Elsner and Charniak, 2008; Wang et al., 2008; Seo et al., 2009). 3 Post Label Set Our post label set contains 12 categories, intended to capture the typical int"
W10-2923,W01-1605,0,0.0604213,"is task of enhanced support sharing, in the form of text mining over troubleshootingoriented web user forum data (Baldwin et al., to appear). One facet of our proposed strategy for enhancing information access to troubleshooting-oriented web user forum data is to preprocess threads to uncover the “content structure” of the thread, in the form of its post-to-post discourse structure. Specifically, we identify which earlier post(s) a given post responds to (linking) and in what manner (tagging), in an amalgam of dialogue act tagging (Stolcke et al., 2000) and coherence-based discourse analysis (Carlson et al., 2001; Wolf and Gibson, 2005). The reason we do this is gauge the relative role/import of individual posts, to index and weight component terms accordingly, ultimately in an attempt to enhance information access. Evidence to suggest that this structure can enhance information retrieval effectiveness comes from Xi et al. (2004) and Seo et al. (2009) (see Section 2). To illustrate the task, consider the thread from the CNET forum shown in Figure 1, made up of 5 posts (Post 1, ..., Post 5) with 4 distinct participants (A, B, C, D). In the first post, A initiates the thread by requesting assistance in"
W10-2923,W04-3240,0,0.540286,"Missing"
W10-2923,W02-0216,0,0.171406,"s for the respective tasks. In this work, we draw on existing work (esp. Xi et al. (2004)) in proposing a novel DA tag set customised to the analysis of troubleshootingoriented web user forums (Section 3), and compare a range of text classification and structured classification methods for post-level DA classification. Discourse disentanglement is the process of automatically identifying coherent sub-discourses in a single thread (in the context of user forums/mailing lists), chat session (in the context of IRC chat data: Elsner and Charniak (2008)), system interaction (in the context of HCI: Lemon et al. (2002)) or document (Wolf and Gibson, 2005). The exact definition of what constitutes a subdiscourse varies across domains, but for our purposes, entails an attempt to resolve the informa2 Related Work Related work exists in the broad fields of dialogue processing, discourse analysis and information retrieval, and can be broken down into the following tasks: (1) dialogue act tagging; (2) discourse “disentanglement”; (3) community question answering; and (4) newsgroup/user forum search. 193 ority of this method over a model which ignores thread structure. Finally, Seo et al. (2009) automatically deri"
W10-2923,P08-1081,0,0.0291932,"05) in adopting a DAG representation of discourse structure, and draw on the wide set of features used in discourse entanglement to model coherence. Community question answering (cQA) is the task of identifying question–answer pairs in a given thread, e.g. for the purposes of thread summarisation (Shrestha and McKeown, 2004) or automated compilation of resources akin to Yahoo! Answers. cQA has been applied to both mailing list and user forum threads, conventionally based on question classification, followed by ranking of candidate answers relative to each question (Shrestha and McKeown, 2004; Ding et al., 2008; Cong et al., 2008; Cao et al., 2009). The task is somewhat peripheral to our work, but relevant in that it involves the implicit tagging of certain posts as containing questions/answers, as well as linking the posts together. Once again, we draw on the features used in cQA in this research. There has been a spike of recent interest in newsgroup/user forum search. Xi et al. (2004) proposed a structured information retrieval (IR) model for newsgroup search, based on author features, thread structure (based on the tree defined by the reply-to structure), thread “topology” features and content-b"
W10-2923,N06-1047,0,0.0138883,"ser A Thank You! Post 4 Thanks a lot for that . . . I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site?. . . User D A little more help Post 5 . . . You would simply do it this way: . . . You could also just . . . An example of this is:. . . Figure 1: Snippeted posts in a CNET thread 53EBF2753EBF2 1234567123456 123457 123456 Dialogue act (DA) tagging is a means of capturing the function of a given utterance relative to an encompassing discourse, and has been proposed variously as a means of enhancing dialogue summarisation (Murray et al., 2006), and tracking commitments and promises in email (Cohen et al., 2004; Lampert et al., 2008), as well as being shown to improve speech recognition accuracy (Stolcke et al., 2000). A wide range of DA tag sets have been proposed, usually customised to a particular medium such as speech dialogue (Stolcke et al., 2000; Shriberg et al., 2004), taskfocused email (Cohen et al., 2004; Wang et al., 2007; Lampert et al., 2008) or instant messaging (Ivanovic, 2008). The most immediately relevant DA-based work we are aware of is that of Xi et al. (2004), who proposed a 5-way classification for newsgroup da"
W10-2923,J00-3003,0,0.486125,"Missing"
W10-2923,P07-2019,0,0.094754,"information (or in our case “disentangled” DAG structure) enhances IR effectiveness is a core motivator for this research. tion need of the initiator by a particular approach; if there are competing approaches proposed in a single thread, multiple sub-discourses will necessarily arise. The data structure used to represent the disentangled discourse varies from a simple connected sub-graph (Elsner and Charniak, 2008), to a stack/tree (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), to a full directed acyclic graph (DAG: Ros´e et al. (1995), Wolf and Gibson (2005), Schuth et al. (2007)). Disentanglement has been carried out via analysis of direct citation/user name references (Schuth et al., 2007; Seo et al., 2009), topic modelling (Lin et al., 2009), and clustering over content-based features for pairs of posts, optionally incorporating various constraints on post recency (Elsner and Charniak, 2008; Wang et al., 2008; Seo et al., 2009). 3 Post Label Set Our post label set contains 12 categories, intended to capture the typical interactions that take place in troubleshooting-oriented threads on technical forums. There are 2 super-categories (Q UESTION, A NSWER) and 3 single"
W10-2923,P95-1005,0,0.167737,"Missing"
W10-2923,P98-2188,0,0.0229291,"Missing"
W10-2923,P07-2032,0,0.0226497,"Missing"
W10-2923,J05-2005,0,0.34868,"pport sharing, in the form of text mining over troubleshootingoriented web user forum data (Baldwin et al., to appear). One facet of our proposed strategy for enhancing information access to troubleshooting-oriented web user forum data is to preprocess threads to uncover the “content structure” of the thread, in the form of its post-to-post discourse structure. Specifically, we identify which earlier post(s) a given post responds to (linking) and in what manner (tagging), in an amalgam of dialogue act tagging (Stolcke et al., 2000) and coherence-based discourse analysis (Carlson et al., 2001; Wolf and Gibson, 2005). The reason we do this is gauge the relative role/import of individual posts, to index and weight component terms accordingly, ultimately in an attempt to enhance information access. Evidence to suggest that this structure can enhance information retrieval effectiveness comes from Xi et al. (2004) and Seo et al. (2009) (see Section 2). To illustrate the task, consider the thread from the CNET forum shown in Figure 1, made up of 5 posts (Post 1, ..., Post 5) with 4 distinct participants (A, B, C, D). In the first post, A initiates the thread by requesting assistance in creating a web form. In"
W10-2923,P04-1088,0,0.0273801,"Missing"
W10-2923,C00-2137,0,0.0155274,"arge numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given iteration, all other posts in that same thread are also in the test data (or more pertinently, not in the training data). We evaluate using micro-averaged precision, recall and Fscore (β = 1). We test the statistical significance of all above-baseline results using randomised estimation (p &lt; 0.05; Yeh (2000)), and present all such results in bold in our results tables. In our experiments, we first look at the post classification task in isolation (i.e. we predict which labels to associate with each post, underspecifying which posts those labels relate to). We then move on to look at the link classification task, again in isolation (i.e. we predict which previous posts each post links to, underspecifying the nature of the link). Finally, we perform preliminary investigation of the joint task of DA and link classification, by incorporating DA class features into the link classification task. Post S"
W10-2923,N03-1028,0,0.0134673,"sponds (links) to, and can aid in DA tagging. We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given i"
W10-2923,W06-0127,0,0.0154486,"We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given iteration, all other posts in that same thr"
W10-2923,C04-1128,0,0.042657,"post C should exist only in the case that the link between them is not inferrable transitively. Detailed definitions of each post tag are given below. Note that initiator refers to the user who started the thread with the first post. In this work, we follow Ros´e et al. (1995) and Wolf and Gibson (2005) in adopting a DAG representation of discourse structure, and draw on the wide set of features used in discourse entanglement to model coherence. Community question answering (cQA) is the task of identifying question–answer pairs in a given thread, e.g. for the purposes of thread summarisation (Shrestha and McKeown, 2004) or automated compilation of resources akin to Yahoo! Answers. cQA has been applied to both mailing list and user forum threads, conventionally based on question classification, followed by ranking of candidate answers relative to each question (Shrestha and McKeown, 2004; Ding et al., 2008; Cong et al., 2008; Cao et al., 2009). The task is somewhat peripheral to our work, but relevant in that it involves the implicit tagging of certain posts as containing questions/answers, as well as linking the posts together. Once again, we draw on the features used in cQA in this research. There has been"
W10-2923,W04-2319,0,\N,Missing
W10-2923,C98-2183,0,\N,Missing
W13-2305,U12-1010,1,0.77545,"gree to which that output was better than the other. In addition, direct estimation of quality within the context of machine translation extends the usefulness of the annotated data to other tasks such as quality-estimation (CallisonBurch et al., 2012). 33 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33–41, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics reference translation is needed, and the bias is removed. In earlier work, we consider the possibility that translation quality is a hypothetical construct (Graham et al., 2012), and suggest applying methods of validating measurement of psychological constructs to the validation of measurements of translation quality. In psychology, a scale that employs more items as opposed to fewer is considered more valid. Under this criteria, a two-item (fluency and adequacy) scale is more valid than a single-item translation quality measure. the conventional 5-point interval-level scale and a continuous visual analog scale (VAS) are used for human evaluation. We collected data via Amazon’s Mechanical Turk, where the quality of annotations is known to vary considerably (CallisonB"
W13-2305,W07-0718,0,0.214127,"llison-Burch et al., 2012). Inconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for human assessors. In MT evaluation, five and seven-point interval-level scales are common (Callison-Burch et al., 2007; Denkowski and Lavie, 2010). However, the interval-level scale commonly used for direct estimation of translation quality (and other NLP annotation tasks) forces human judges to discretize their assessments into a fixed number of categories, and this process could be a cause of inconsistency in human judgments. In particular, an assessor may be repeatedly forced to choose between two categories, neither of which really fits their judgment. The continuous nature of translation quality assessment, as well as the fact that many statistical methods exist that can be applied to continuous data but"
W13-2305,W08-0309,0,0.0214353,"fat Justin Zobel Department of Computing and Information Systems, The University of Melbourne {ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au Abstract For an evaluation to be credible, the annotations must be credible. The simplest way of establishing this is to have the same data point annotated by multiple annotators, and measure the agreement between them. There has been a worrying trend in recent MT shared tasks – whether the evaluation was structured as ranking translations from best-to-worst, or by direct estimation of fluency and adequacy – of agreement between annotators decreasing (Callison-Burch et al., 2008; CallisonBurch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012). Inconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for"
W13-2305,D08-1027,0,0.0288369,"Missing"
W13-2305,D09-1030,0,0.0229152,"T output, and Figure 2 shows an equivalent VAS using the two most extreme anchor labels, strongly disagree and strongly agree. 4 Crowd-sourcing Judgments The volume of judgments required for evaluation of NLP tasks can be large, and employing experts to undertake those judgments may not always be feasible. Crowd-sourcing services via the Web offer an attractive alternative, and have been used in conjunction with a range of NLP evaluation and annotation tasks. Several guides exist for instructing researchers from various backgrounds on using Amazon’s Mechanical Turk (AMT) (Gibson et al., 2011; Callison-Burch, 2009), and allowance for the use of AMT is increasingly being made in research grant applications, as a cost-effective way of gathering data. Issues remain in connection with low payment levels (Fort et al., 2011); nevertheless, Ethics Approval Boards are typically disinterested in projects that make use of AMT, regarding AMT as being a purchased service rather than a part of the experimentation that may affect human subjects. The use of crowd-sourced judgments does, however, introduce the possibility of increased inconsistency, with service requesters typically havFor example, if the task at hand"
W13-2305,2010.amta-papers.20,0,0.207951,"nconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for human assessors. In MT evaluation, five and seven-point interval-level scales are common (Callison-Burch et al., 2007; Denkowski and Lavie, 2010). However, the interval-level scale commonly used for direct estimation of translation quality (and other NLP annotation tasks) forces human judges to discretize their assessments into a fixed number of categories, and this process could be a cause of inconsistency in human judgments. In particular, an assessor may be repeatedly forced to choose between two categories, neither of which really fits their judgment. The continuous nature of translation quality assessment, as well as the fact that many statistical methods exist that can be applied to continuous data but not interval-level data, mo"
W13-2305,W12-3102,0,\N,Missing
W13-2305,W09-0401,0,\N,Missing
W13-2305,J11-2010,0,\N,Missing
W13-2305,W10-1703,0,\N,Missing
W14-1303,N10-1027,1,0.353658,"to work with monolingual subsets of historical data, we investigate the most practical means of carrying out LangID of Twitter messages, balancing accuracy with ease of implementation. In this work, we present an evaluation of “off-the-shelf” language identifiers, combined with techniques that have been proposed for boosting accuracy on Twitter messages. A major challenge that we have had to overcome is the lack of annotated data for evaluation. Bergsma et al. (2012) point out that in LangID research on microblog messages to date, only a small number of European languages has been considered. Baldwin and Lui (2010) showed that, when considering full documents, good performance on just European languages does not necessarily imply equally good performance when a larger set of languages is considered. This does not detract from work to date on European languages (Tromp and Pechenizkiy, 2011; Carter et al., 2013), but rather highlights the need for further research in LangID for microblog messages. 2 Background LangID is the problem of mapping a document onto the language(s) it is written in. The bestknown technique classifies documents according to rank order statistics over character n-gram sequences bet"
W14-1303,I13-1041,1,0.0952229,"news event detection (Petrovi´c et al., 2010), and prediction of sporting match outcomes (Sinha et al., 2013). Text analysis of social media has quickly become one of the “frontier” areas of Natural Language Processing (NLP), with major conferences opening entire tracks for it in recent years. The challenges in NLP for social media are many, stemming primarily from the “noisy” nature of the content. Research indicates that English Twitter in particular is more dissimilar to the kinds of reference corpora used in NLP to date, compared to other forms of social media such as blogs and comments (Baldwin et al., 2013). This has led to the development of techniques to “normalize” Twitter messages (Han et al., 2013), as well as Twitter-specific approaches to conventional NLP tasks such as part-of-speech tagging (Gimpel et al., 2011) and information extraction (Bontcheva et al., 2013). Even so, a precondition of NLP techniques is that the language of the input data is known, and this has led to interest in “language identification” (LangID) of Twitter messages. Research has shown that “off-the-shelf” LangID systems appear to perform fairly well on Twitter (Lui and Baldwin, 2012), but Twitter-specific systems"
W14-1303,W12-2108,0,0.540826,"sages (Han et al., 2013), as well as Twitter-specific approaches to conventional NLP tasks such as part-of-speech tagging (Gimpel et al., 2011) and information extraction (Bontcheva et al., 2013). Even so, a precondition of NLP techniques is that the language of the input data is known, and this has led to interest in “language identification” (LangID) of Twitter messages. Research has shown that “off-the-shelf” LangID systems appear to perform fairly well on Twitter (Lui and Baldwin, 2012), but Twitter-specific systems seem to perform better (Carter et al., 2013; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Goldszmidt et al., 2013). Twitter recognizes the utility of language metadata in enabling new applications, and as of March 2013 includes language predictions with results from its API (Roomann-Kurrik, 2013). These predictions are not perfect (see Section 3.2), and at time of writing do not cover some languages (e.g. Romanian). Furthermore, some research groups We present an evaluation of “off-theshelf” language identification systems as applied to microblog messages from Twitter. A key challenge is the lack of an adequate corpus of messages annotated for language that reflects the linguisti"
W14-1303,R13-1011,0,0.130802,"racted recent interest from the research community. Hammarstrom (2007) describes a method that augments a dictionary with an affix table, and tests it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compare a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different LangID approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. Carter et al. (2013) focus specifically on LangID in Twitter messages by augmentManual annotation of Twitter messages is a challenging and laborious process. Furthermore, Twitter is highly multilingual, making it very difficult to obtain annotators for all of the languages represented. Previous work has attempted to crowdsource part of this process (Bergsma et al., 2012), but such an approach requires substantial monetary investment, as well as care in ensuring the quality of the final annotations. In this paper, we propose an alternative, “mostly-automated” approach to gathering language-labeled Twitter messages"
W14-1303,P09-1120,0,0.0250296,"roposed, such as the use of stop word lists (Johnson, 1993), where a document is classified according to its degree of overlap with lists for different languages. Other approaches include word and part-of-speech (POS) correlation (Grefenstette, 1995), cross-language tokenization (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID of short strings has attracted recent interest from the research community. Hammarstrom (2007) describes a method that augments a dictionary with an affix table, and tests it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compare a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different LangID approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. Carter et al. (2013) focus specifically on LangID in Twitter messages by augmentManual annotation of Twitter messages is a challenging and laborious process. Furthermore, Twitter is highly multilingual, making it very difficult to obtain annota"
W14-1303,P11-2008,0,0.0146937,"Missing"
W14-1303,N13-1131,0,0.0302651,"about the relative order of tokens. Bergsma et al. (2012) examine LangID for creating languagespecific twitter collections, finding that a compressive method trained over out-of-domain data from Wikipedia and standard text corpora performed better than the off-the-shelf language identifiers they tested. Goldszmidt et al. (2013) propose a method based on rank-order statistics, using a bootstrapping process to acquire in-domain training data from unlabeled Twitter messages. Recent work has also put some emphasis on word-level rather than document-level LangID (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), including research on identifying the language of each word in multilingual online communications (Nguyen and Dogruoz, 2013; Ling et al., 2013). In this paper, we focus on monolingual messages, as despite being simpler, LangID of monolingual Twitter messages is far from solved. Initial Post-review English 0.906 0.930 Chinese 0.773 0.916 Japanese 0.989 0.998 Table 1: Inter-annotator agreement measured using Fleiss’ kappa (Fleiss, 1971) over annotations for T WITTER. 2.1 Manual annotation of Z H E N JA A manual approach to constructing a LangID dataset from Twitter data is difficult due to the"
W14-1303,P13-1018,0,0.00617743,"ive method trained over out-of-domain data from Wikipedia and standard text corpora performed better than the off-the-shelf language identifiers they tested. Goldszmidt et al. (2013) propose a method based on rank-order statistics, using a bootstrapping process to acquire in-domain training data from unlabeled Twitter messages. Recent work has also put some emphasis on word-level rather than document-level LangID (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), including research on identifying the language of each word in multilingual online communications (Nguyen and Dogruoz, 2013; Ling et al., 2013). In this paper, we focus on monolingual messages, as despite being simpler, LangID of monolingual Twitter messages is far from solved. Initial Post-review English 0.906 0.930 Chinese 0.773 0.916 Japanese 0.989 0.998 Table 1: Inter-annotator agreement measured using Fleiss’ kappa (Fleiss, 1971) over annotations for T WITTER. 2.1 Manual annotation of Z H E N JA A manual approach to constructing a LangID dataset from Twitter data is difficult due to the wide variety of languages present on Twitter — Bergsma et al. (2012) report observing 65 languages in a 10M message sample, and Baldwin et al. ("
W14-1303,I11-1062,1,0.625836,"Missing"
W14-1303,P12-3005,1,0.615942,"dia such as blogs and comments (Baldwin et al., 2013). This has led to the development of techniques to “normalize” Twitter messages (Han et al., 2013), as well as Twitter-specific approaches to conventional NLP tasks such as part-of-speech tagging (Gimpel et al., 2011) and information extraction (Bontcheva et al., 2013). Even so, a precondition of NLP techniques is that the language of the input data is known, and this has led to interest in “language identification” (LangID) of Twitter messages. Research has shown that “off-the-shelf” LangID systems appear to perform fairly well on Twitter (Lui and Baldwin, 2012), but Twitter-specific systems seem to perform better (Carter et al., 2013; Tromp and Pechenizkiy, 2011; Bergsma et al., 2012; Goldszmidt et al., 2013). Twitter recognizes the utility of language metadata in enabling new applications, and as of March 2013 includes language predictions with results from its API (Roomann-Kurrik, 2013). These predictions are not perfect (see Section 3.2), and at time of writing do not cover some languages (e.g. Romanian). Furthermore, some research groups We present an evaluation of “off-theshelf” language identification systems as applied to microblog messages f"
W14-1303,E12-3006,0,0.0424607,"Missing"
W14-1303,D13-1084,0,0.198715,"s, finding that a compressive method trained over out-of-domain data from Wikipedia and standard text corpora performed better than the off-the-shelf language identifiers they tested. Goldszmidt et al. (2013) propose a method based on rank-order statistics, using a bootstrapping process to acquire in-domain training data from unlabeled Twitter messages. Recent work has also put some emphasis on word-level rather than document-level LangID (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), including research on identifying the language of each word in multilingual online communications (Nguyen and Dogruoz, 2013; Ling et al., 2013). In this paper, we focus on monolingual messages, as despite being simpler, LangID of monolingual Twitter messages is far from solved. Initial Post-review English 0.906 0.930 Chinese 0.773 0.916 Japanese 0.989 0.998 Table 1: Inter-annotator agreement measured using Fleiss’ kappa (Fleiss, 1971) over annotations for T WITTER. 2.1 Manual annotation of Z H E N JA A manual approach to constructing a LangID dataset from Twitter data is difficult due to the wide variety of languages present on Twitter — Bergsma et al. (2012) report observing 65 languages in a 10M message sample,"
W14-1303,N10-1021,0,0.0284605,"Missing"
W14-1303,vatanen-etal-2010-language,0,0.0295433,"efenstette, 1995), cross-language tokenization (Giguet, 1995) and grammatical-class models (Dueire Lins and Gonc¸alves, 2004). LangID of short strings has attracted recent interest from the research community. Hammarstrom (2007) describes a method that augments a dictionary with an affix table, and tests it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compare a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different LangID approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. Carter et al. (2013) focus specifically on LangID in Twitter messages by augmentManual annotation of Twitter messages is a challenging and laborious process. Furthermore, Twitter is highly multilingual, making it very difficult to obtain annotators for all of the languages represented. Previous work has attempted to crowdsource part of this process (Bergsma et al., 2012), but such an approach requires substantial monetary investment, as well as care in ensuring the quality"
W14-1303,P12-1102,0,0.0459517,"del of text to include information about the relative order of tokens. Bergsma et al. (2012) examine LangID for creating languagespecific twitter collections, finding that a compressive method trained over out-of-domain data from Wikipedia and standard text corpora performed better than the off-the-shelf language identifiers they tested. Goldszmidt et al. (2013) propose a method based on rank-order statistics, using a bootstrapping process to acquire in-domain training data from unlabeled Twitter messages. Recent work has also put some emphasis on word-level rather than document-level LangID (Yamaguchi and Tanaka-Ishii, 2012; King and Abney, 2013), including research on identifying the language of each word in multilingual online communications (Nguyen and Dogruoz, 2013; Ling et al., 2013). In this paper, we focus on monolingual messages, as despite being simpler, LangID of monolingual Twitter messages is far from solved. Initial Post-review English 0.906 0.930 Chinese 0.773 0.916 Japanese 0.989 0.998 Table 1: Inter-annotator agreement measured using Fleiss’ kappa (Fleiss, 1971) over annotations for T WITTER. 2.1 Manual annotation of Z H E N JA A manual approach to constructing a LangID dataset from Twitter data"
W14-3333,1993.eamt-1.1,0,0.361232,"would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is. The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the ex3 3.1 Randomized Significance Tests Bootstrap Resampling Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993). The test statistic is taken as the difference in scores of the two systems, SX − SY , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (Xb , Yb ) of a bootstrapped test set (Koehn, 2004), constructed by sampling with replacement from the original test set translations. The bootstrap distribution Sboot of the test statistic is estimated by calculating the value of the pseudo-statistic SXb − SYb for each pseudo-sample. 267 Set c = 0 Set c = 0 Compute actual statistic o"
W14-3333,D08-1089,0,0.029392,"ct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shar"
W14-3333,N03-1010,0,0.0799423,"rison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with"
W14-3333,W13-2305,1,0.847589,"l., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency assessments were collected. After the removal of quality control items and filtering of judgments from low-quality workers, this resulted in an average of 1,280 adequacy and 1,013 fluency assessments per system for Spanishto-English (12 systems), and 1,483 adequacy and 1,534 fluency assessments per system for Englishto-Spanish (11 systems). To remove bias with respect to individual human judge preference scoring severity/leniency, scores provided by each human assessor were standardized according to the mean and standard d"
W14-3333,N10-1129,0,0.0132479,"esampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for"
W14-3333,W06-3114,0,0.0385282,"rap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scal"
W14-3333,W04-3250,0,0.911376,"ference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational g"
W14-3333,N04-1022,0,0.0532519,"th one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply pr"
W14-3333,W05-0909,0,0.0290276,"han those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: B LEU (Papineni et al., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessment"
W14-3333,D11-1080,0,0.0138168,"lude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language"
W14-3333,P03-1021,0,0.0852537,"put text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the origin"
W14-3333,P11-2031,0,0.0238496,"nce approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language pairs to provide a g"
W14-3333,W05-0908,0,0.476407,"lation Yvette Graham Nitika Mathur Timothy Baldwin Department of Computing and Information Systems The University of Melbourne ygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.net Abstract for a pair of systems how likely a difference in B LEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, B LEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice. Within MT, approximate randomization for the purpose of statistical testing is also less common. Riezler and Maxwell (2005) provide a comp"
W14-3333,N06-1032,0,0.0153741,"e randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We"
W14-3333,2006.amta-papers.25,0,0.034494,". The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: B LEU (Papineni et al., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency asse"
W14-3333,W12-3102,0,\N,Missing
W14-3333,P02-1040,0,\N,Missing
W14-5315,N10-1027,1,0.833342,"gual POS tagging as well as fine-grained tags extracted from a deep grammar of English, and discuss additional data we collected for the open submissions, utilizing custombuilt web corpora based on top-level domains as well as existing corpora. 1 Introduction Language identification (LangID) is the problem of determining what natural language a document is written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994; Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accuracy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further work is accurate discrimination between closely-related languages (Ljubeˇsi´c et al., 2007; Tiedemann and Ljubeˇsi´c, 2012). The problem has been explored for specific groups of confusable languages, such as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and Ljubeˇsi´c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre, 2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task (Zampieri et al., 2014) was hosted at the VarDial workshop at CO"
W14-5315,C10-3010,1,0.824871,"ciated with those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this approach is that text found in the top-level domains (TLDs) of those countries will primarily be of the sub-language dominant in that country. For instance, we assume that Portuguese text found when crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will be primarily Brazilian Portuguese. The process of creating a corpus for each sub-language involved translating a sample of 200 of the original ukWaC queries into each language using Panlex (Baldwin et al., 2010).5 These queries were then submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a Heritrix 3.1.16 instance with default settings other than constraining the crawled content to the relevant TLD. Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach, only documents with a MIME type of HTML and size between 5k and 200"
W14-5315,baroni-bernardini-2004-bootcat,0,0.0821185,"this approach is that text found in the top-level domains (TLDs) of those countries will primarily be of the sub-language dominant in that country. For instance, we assume that Portuguese text found when crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will be primarily Brazilian Portuguese. The process of creating a corpus for each sub-language involved translating a sample of 200 of the original ukWaC queries into each language using Panlex (Baldwin et al., 2010).5 These queries were then submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a Heritrix 3.1.16 instance with default settings other than constraining the crawled content to the relevant TLD. Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach, only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Justext (Pomik´alek, 2011) was used to extract text from the selected documents. langid.py (Lu"
W14-5315,D13-1120,0,0.0477973,"Missing"
W14-5315,P13-2112,1,0.893753,"Missing"
W14-5315,2005.mtsummit-papers.11,0,0.0102014,"Missing"
W14-5315,I11-1062,1,0.875837,"proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 129 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129–138, Dublin, Ireland, August 23 2014. 2 Overview Our main focus was to explore novel methods and sources of training data for discriminating similar languages. In this section, we describe techniques and text representations that we tested, as well as the external data sources that we used to build language identifiers for this task. 2.1 Language-Indicative Byte Sequences Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is robust to variation in languages across different sources of text. The LD feature set can be thought of as language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin (2012) present langid.py,1 an off-the-shelf LangID system that utilizes the LD feature set. In this work, we re-train langid.py using the training data provided by the shared task organizers, and use thi"
W14-5315,P12-3005,1,0.938117,"e describe techniques and text representations that we tested, as well as the external data sources that we used to build language identifiers for this task. 2.1 Language-Indicative Byte Sequences Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is robust to variation in languages across different sources of text. The LD feature set can be thought of as language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin (2012) present langid.py,1 an off-the-shelf LangID system that utilizes the LD feature set. In this work, we re-train langid.py using the training data provided by the shared task organizers, and use this as a baseline result representative of the state-of-the-art in LangID. 2.2 Hierarchical LangID In LangID research to date, systems generally do not take into account any form of structure in the class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use of this structure by introducing a two-level LangID model. The first level implements a single grouplev"
W14-5315,U13-1003,1,0.927895,"h accuracy (Cavnar and Trenkle, 1994; Dunning, 1994; Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accuracy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further work is accurate discrimination between closely-related languages (Ljubeˇsi´c et al., 2007; Tiedemann and Ljubeˇsi´c, 2012). The problem has been explored for specific groups of confusable languages, such as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and Ljubeˇsi´c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre, 2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task (Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the work on these various language groups by proposing a task on a single dataset containing text from 13 languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014). In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took part in both the closed and the open categories, submitting to the main component (Gr"
W14-5315,J03-1002,0,0.00581025,"Missing"
W14-5315,petrov-etal-2012-universal,0,0.0912549,"Missing"
W14-5315,skadins-etal-2014-billions,0,0.0262828,"Missing"
W14-5315,C12-1160,0,0.370094,"Missing"
W14-5315,tiedemann-2012-parallel,0,0.0311357,"of participation: (1) Closed, using only training data provided by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To participate in the latter category, we sourced additional training data through: (1) collection of data relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The information about the additional training data is shown in Table 2. 2.4.1 Existing Corpora We collected training data from a number of existing corpora, as shown in Table 3. Many of the corpora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text corpora commonly used for research in machine translation. The exceptions are: (1) debian, which was constructed using translations of message strings from the Debian operating system,3 ; (2) BNC — the British National Corpus (Burnard, 2000); (3) OANC — the open component of the Second Release of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);4 a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences from each of the BNC and OANC, which we used as training"
W14-5315,N03-1033,0,0.120728,"Missing"
W14-5315,U09-1008,0,0.0384822,"Missing"
W14-5315,W14-5307,0,0.184466,"Missing"
W15-0909,W11-0815,0,0.0498963,"egrating compositionality scores for English noun compounds into the T ESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is"
W15-0909,C10-3010,1,0.827638,"dataset, and the subset of the dataset that contains noun compounds SS+DS: the arithmetic mean of DS and string similarity (“SS”), based on the findings of Salehi et al. (2014). SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013), under the hypothesis that compositional MWEs are more likely to be word-forword translations in a given language than noncompositional MWEs. Following Salehi and Cook (2013), the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014). In Salehi and Cook (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgem"
W15-0909,W05-0909,0,0.680956,"ative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality o"
W15-0909,N10-1029,0,0.0448741,"machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatche"
W15-0909,W08-0332,0,0.083093,"Missing"
W15-0909,D14-1020,1,0.828248,"oerover, the N→P sentences for DS are a subset of the N→P sentences for 0/1; the same is true for the P→N sentences. 6 Table 3: The number of judgements that were ranked correctly by T ESLA originally, but incorrectly with the incorporation of compositionality scores (“P→N”) and vice versa (“N→P”), and the absolute improvement with compositionality scores (“∆”) judgements, as shown in Table 2, again over the full dataset and also the subset of data containing compound nouns. The improvement here is slightly greater than for our first experiment, but not at a level of statistical significance (Graham and Baldwin, 2014). Perhaps surprisingly, the exact compositionality predictions produce a higher correlation than the continuous-valued compositionality predictions, but again, even with the inclusion of the compositionality features, T ESLA is outperformed by M ETEOR. The correlation over the subset of the data containing compound nouns is markedly higher than that over the full dataset, but the r values with the inclusion of compositionality values are actually all slightly below those for the basic T ESLA. As a final analysis, we examine the relative impact on T ESLA of the three compositionality methods, i"
W15-0909,E14-1047,1,0.848875,"ok (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output, as collected by Graham et al. (2014). We used the Stanford CoreNLP parser (Klein and Manning, 2003) to identify English noun compounds in the translations. Among the 3000 sentences, 579 sentences contain at least one noun compound. Dataset We evaluate our method over the data from WMT 2013, which is made up of a total of 3000 transla56 Results We performed two evaluations, based on the two sets of judgements (pairwise preference or continuousvalued judgement for each MT output). In each case, we use three baselines (each applied at the segment level, meaning that individual sentences get a score): (1) M ETEOR (Banerjee and Lavie"
W15-0909,I11-1024,0,0.255284,"for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores o"
W15-0909,kamholz-etal-2014-panlex,0,0.0128974,"et of the dataset that contains noun compounds SS+DS: the arithmetic mean of DS and string similarity (“SS”), based on the findings of Salehi et al. (2014). SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013), under the hypothesis that compositional MWEs are more likely to be word-forword translations in a given language than noncompositional MWEs. Following Salehi and Cook (2013), the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014). In Salehi and Cook (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output"
W15-0909,P03-1040,0,0.0329895,"dgements that were ranked correctly originally, but incorrectly when the compositionality score was incorporated (“P→N”); and also the number of pairwise judgements that were ranked incorrectly originally, and corrected with the incorpo57 Discussion As shown in the previous section, the incorporation of compositionality scores can improve the quality of MT evaluation based on T ESLA. However, the improvements are very small and not statistically significant. Part of the reason is that we focus exclusively on noun compounds, which are contiguous and relatively easy to translate for MT systems (Koehn and Knight, 2003). Having said that, preliminary error analysis would suggest that most MT systems have difficulty translating non-compositional noun compounds, although then again, most noun compounds in the WMT 2013 shared task are highly compositional, limiting the impact of compositionality scores. We speculate that, for the method to have greater impact, we would need to target a larger set of MWEs, including non-contiguous MWEs such as split verb particle constructions (Kim and Baldwin, 2010). Further error analysis suggests that incorrect identification of noun compounds in a reference sentence can have"
W15-0909,P07-2045,0,0.00407743,"s NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations, based on compositionality. For example, an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the re"
W15-0909,W10-1754,0,0.0176187,"for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the reference translation. On the other hand, a partial mismatch between traffic signal and traffic light should be rewarded, as the usage of traffic is highly compositional in both cases. That is, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M"
W15-0909,P09-1034,0,0.0605029,"Missing"
W15-0909,P02-1040,0,0.0974854,"s, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching"
W15-0909,W12-3311,0,0.0149819,"Abstract In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the T ESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could im"
W15-0909,S13-1039,1,0.903598,"nguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (Sm"
W15-0909,E14-1050,1,0.897614,"(Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (Sms , Slem and Spos ) fo"
W15-0909,2006.amta-papers.25,0,0.0599367,"can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different"
W15-0909,W06-1204,0,0.120698,"ion metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the refer"
W15-0909,W14-5709,0,0.151776,"Missing"
W15-0909,W13-2201,0,\N,Missing
W15-4319,W15-4308,0,0.196632,"et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supra"
W15-4319,P06-2005,0,0.224961,"Missing"
W15-4319,W15-4312,0,0.040755,"d using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the basis of n-grams tatistics. BEKLI (Beckley, 2015) A substitution dictionary is constructed in which keys are non-standard words and values are lists of potential normalizations. Frequent morphology errors are captured by hand-crafted rules. Finally, the Viterbi algorithm is applied to bigram sequences to decode the normalized sentence with maximum probability. LYSGROUP (Mosquera et al., 2015) A system originally developed for Spanish text normalization was adapted to English text normalization. The method consists of a cascaded pipeline of several data adaptors and processors, such as a Twitter POS tagger and a spell checker. 3 Named Entity"
W15-4319,W15-4318,0,0.105136,"Missing"
W15-4319,N15-1075,0,0.0319326,"Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Twitter NER (Ritter et al., 2011), which disti"
W15-4319,W15-4307,0,0.270391,"tain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-competition analysis of the effect of training on development sets is presented in the NRC system description paper (Cherry et al., 2015). 131 Figure 2: Annotation interface. POS Orthographic Gazetteers Brown clustering – X X X – – – X X X – X X – X – X X X – X – – X X X X – X – X – X X – X BASELINE Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Word embedding ML – correlation analysis – – word2vec word2vec & GloVe word2vec X – CRFsuite CRFsuite CRF++ CRF wapiti FFNN CRF++ semi-Markov MIRA entity linking CRF L-BFGS Table 7: Features and machine learning approach taken by each team. Precision Recall F ousia NLANGP nrc multimedialab USFD iitp Hallym lattice 57.66 63.62 53.24 49.52 45.72 60.68 39.59 55.17 55.22 43.12 38.5"
W15-4319,P14-2111,0,0.0445576,"r data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods,"
W15-4319,W09-2010,0,0.0437694,"guistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-"
W15-4319,W15-4306,0,0.105059,"hallenge is concept drift (Dredze et al., 2010; Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Tw"
W15-4319,W15-4322,0,0.0251441,"Missing"
W15-4319,P11-1038,1,0.912118,"Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can b"
W15-4319,D12-1039,1,0.757294,"zation examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words tha"
W15-4319,P13-1155,0,0.0178022,"rds can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task"
W15-4319,W15-4313,0,0.0798376,"shown in Tables 3 and 4. Overall, common approaches were lexicon-based methods, CRFs, and neural network-based approaches. Among the constrained systems, neural networks achieved strong results, even without off-the-shelf tools. In contrast, CRF- and lexicon-based approaches were shown to be effective in the unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et"
W15-4319,D14-1108,0,0.0271234,"new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited t"
W15-4319,W15-4323,0,0.0427639,"ING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015)"
W15-4319,D10-1057,0,0.157913,"Missing"
W15-4319,D13-1008,0,0.00982812,"level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on Engli"
W15-4319,P11-2013,0,0.247801,"dard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined paralle"
W15-4319,W15-4321,0,0.0536682,"s first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as activation function in combination with dropout to prevent overfitting. A context window of 5 words was used As input (2 words left and right). The output is a single tag of the middle word. Afterwards, a rule-based post-processing step was executed to ensure every I-tag has a B-tag in front of it and that all tags within a single span are of the same type. Train and dev were used as training data and used dev 2015 as validation set. NLANGP (Toh et al., 2015) The NLANGP team modeled the problem as a sequential labeling task and used Conditional Random Fields. Several post-processing steps (e.g. rulebased matching) were applied to refine the system output. Besides Brown clusters, Kmeans clusters were also used; the K-means clusters were generated based on word embeddings. nrc (Cherry et al., 2015) NRC applied a MIRAtrained semi-Markov tagger with Gazetteer, Brown cluster and Word Embedding features. The Word Embeddings were built over phrases using Word2Vec’s phrase finder tool, and were modified using an auto-encoder to be predictive of Gazetteer"
W15-4319,P12-1109,0,0.0482015,"es. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical no"
W15-4319,P10-1040,0,0.00514115,"d resources. There were 6 official submissions in the constrained category, and 5 official submissions in the unconstrained category. Overall, deep learning methods and methods based on lexicon-augmented conditional random fields (CRFs) achieved the best results. The winning team achieved a precision of 0.9061 precision, recall of 0.7865, and F1 of 0.8421. The named entity recognition task attracted 8 participants. The majority of teams built their systems using linear-chain conditional random fields (Lafferty et al., 2001), and many teams also used brown clusters and word embedding features (Turian et al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al."
W15-4319,P12-3005,1,0.690086,"e their system categories: • Constrained: participants could not use any data other than the provided training data to perform the text normalization task. They were allowed to use pre-trained tools (e.g., Twitter POS taggers), but no normalization lexicons or extra tweet data. • Unconstrained: participants could use any publicly accessible data or tools to perform the text normalization task. Evaluation was based on token-level precision, recall and F-score. 2.2.1 Preprocessing We first collected tweets using the Twitter Streaming API over the period 23–29 May, 2014, and then used langid.py (Lui and Baldwin, 2012)1 to remove all non-English tweets. Tokenization was performed with CMU-ARK tokeniser.2 To ensure that tweets had a high likelihood of requiring lexical normalization, we filtered out tweets with less than 2 non-standard words (i.e. words not occurring in our dictionary — see Section 2.2.3). While this biases the sample of tweets, the decision was made at a pragmatic level to ensure a reasonable level of lexical normalization and “annotation density”. This was based on a pilot study over a random sample of English tweets, in which we found that many non-standard words were actually unknown nam"
W15-4319,W15-4314,0,0.0181512,"Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction mo"
W15-4319,W15-4317,0,0.0288445,"unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al."
W15-4319,W15-4315,0,0.0571066,"Missing"
W15-4319,C14-1168,0,0.0359301,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,E14-1078,0,0.0294125,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,D11-1141,1,0.828298,"Missing"
W15-4319,N13-1050,0,0.00844789,"(e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One ma"
W15-4319,W15-4320,0,0.0994733,"all 56.64 57.52 57.07 =1 Table 5: Precision and recall comparing one annotator against the other. Cohen’s kappa between the annotators was 0.607. Disagreements between the annotators resolved by a 3rd adjudicator for the final datasets. Team ID Affiliation Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Hallym University Indian Institute of Technology Patna University Paris 3 UGent - iMinds Institute for Infocomm Research National Research Council Canada Studio Ousia University of Sheffield Table 6: Team ID and affiliation of the named entity recognition shared task participants. sia (Yamada et al., 2015). All the other teams used CRFs. On top of a CRF, the iitp team used a differential evolution based technique to obtain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-co"
W15-4319,D13-1007,0,0.160415,"anslation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words that happen to coincide in spel"
W15-4319,W15-4311,0,0.0298261,"2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the ba"
W15-4319,W15-4309,0,0.0208179,".14 54.31 56.28 55.22 54.61 51.44 48.5 25.72 70.63 60.29 59.81 58.82 58.13 56.81 53.01 35.71 BASELINE 53.86 46.44 49.88 =1 Table 8: Results segmenting and categorizing entities into 10 types. Hallym (Yang and Kim, 2015) The Hallym team used an approach based on CRFs using both Brown clusters and word embeddings trained using Canonical Correlation Analysis as features. iitp (Akhtar et al., 2015a) The iitp team pro=1 Table 9: Results on segmentation only (no types). posed a multi-objective differential evolution based technique for feature selection in twitter named entity recognition. lattice (Tian, 2015) Lattice employed a CRF model using Wapiti. The feature templates consisted of standard features used in stateof-the-art. They trained first a model with 132 dev 2015 and evaluated this model on train and dev. multimedialab (Godin et al., 2015) The goal of the multimedia lab system was to only use neural networks and word embeddings to show the power of automatic feature learning and semi-supervised methods. A FeedForward Neural Network was first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as act"
W15-4319,W03-0419,0,0.581711,"Missing"
W15-4319,fromreide-etal-2014-crowdsourcing,0,\N,Missing
W15-4319,W15-4316,0,\N,Missing
W16-1609,S14-2010,0,0.012764,"est score in each row. Domain DLS headlines ans-forums ans-students belief images .83 .74 .77 .74 .86 doc2vec dbow dmpv .77 .78 .66 .65 .65 .60 .76 .75 .78 .75 word2vec sg cbow .74 .69 .62 .52 .69 .64 .72 .59 .73 .69 Description Dimension of word vectors Left/right context window size Minimum frequency threshold for word types Threshold to downsample high frequency words No. of negative word samples Number of training epochs Semantic Textual Similarity The Semantic Textual Similarity (STS) task is a shared task held as part of *SEM and SemEval over a number of iterations (Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). In STS, the goal is to automatically predict the similarity of a pair of sentences in the range [0, 5], where 0 indicates no similarity whatsoever and 5 indicates semantic equivalence. The top systems utilise word alignment, and further optimise their scores using supervised learning (Agirre et al., 2015). Word embeddings are employed, although sentence embeddings are often taken as the average of word embeddings (e.g. Sultan et al. (2015)). We evaluate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5"
W16-1609,N13-1092,0,0.0151087,"Missing"
W16-1609,S15-2027,0,0.0310091,"tic Textual Similarity (STS) task is a shared task held as part of *SEM and SemEval over a number of iterations (Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). In STS, the goal is to automatically predict the similarity of a pair of sentences in the range [0, 5], where 0 indicates no similarity whatsoever and 5 indicates semantic equivalence. The top systems utilise word alignment, and further optimise their scores using supervised learning (Agirre et al., 2015). Word embeddings are employed, although sentence embeddings are often taken as the average of word embeddings (e.g. Sultan et al. (2015)). We evaluate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5 domains, and each domain has 375–750 annotated pairs. Sentences are much shorter than our previous task, at an average of only 13 words in each test sentence. As the dataset is also much smaller, we combine sentences from all 5 domains and also sentences from previous years (2012–2014) to form the training data. We use the headlines domain from 2014 as development, and test on all 2015 domains. For pre-processing, we tokenise and lowercase the words using Stanfor"
W16-1609,P16-1158,1,0.826708,"Missing"
W16-1609,C14-1179,0,0.00949912,"dings were first proposed by Bengio et al. (2003), in the form of a feed-forward neural network language model. Modern methods use a simpler and more efficient neural architecture to learn word vectors (word2vec: Mikolov et al. (2013b); GloVe: Pennington et al. (2014)), based on objective functions that are designed specifically to produce high-quality vectors. Neural embeddings learnt by these methods have been applied in a myriad of NLP applications, including initialising neural network models for objective visual recognition (Frome et al., 2013) or machine translation (Zhang et al., 2014; Li et al., 2014), as well as directly modelling word-to-word relationships (Mikolov et al., 1 The term doc2vec was popularised by Gensim ˇ uˇrek and Sojka, 2010), a widely-used implementation of (Reh˚ paragraph vectors: https://radimrehurek.com/gensim/ 2 The authors of Gensim found dbow outperforms dmpv: https://github.com/piskvorky/gensim/blob/ develop/docs/notebooks/doc2vec-IMDB.ipynb 3 https://groups.google.com/forum/#!topic/ gensim/bEskaT45fXQ 4 For a detailed discussion on replicating the results of Le and Mikolov (2014), see: https://groups.google.com/ forum/#!topic/word2vec-toolkit/Q49FIrNOQRo 78 Proce"
W16-1609,P14-1011,0,0.0121569,"duction Neural embeddings were first proposed by Bengio et al. (2003), in the form of a feed-forward neural network language model. Modern methods use a simpler and more efficient neural architecture to learn word vectors (word2vec: Mikolov et al. (2013b); GloVe: Pennington et al. (2014)), based on objective functions that are designed specifically to produce high-quality vectors. Neural embeddings learnt by these methods have been applied in a myriad of NLP applications, including initialising neural network models for objective visual recognition (Frome et al., 2013) or machine translation (Zhang et al., 2014; Li et al., 2014), as well as directly modelling word-to-word relationships (Mikolov et al., 1 The term doc2vec was popularised by Gensim ˇ uˇrek and Sojka, 2010), a widely-used implementation of (Reh˚ paragraph vectors: https://radimrehurek.com/gensim/ 2 The authors of Gensim found dbow outperforms dmpv: https://github.com/piskvorky/gensim/blob/ develop/docs/notebooks/doc2vec-IMDB.ipynb 3 https://groups.google.com/forum/#!topic/ gensim/bEskaT45fXQ 4 For a detailed discussion on replicating the results of Le and Mikolov (2014), see: https://groups.google.com/ forum/#!topic/word2vec-toolkit/Q4"
W16-1609,P14-5010,0,0.00835232,"ate doc2vec and word2vec embeddings over the English STS sub-task of SemEval2015 (Agirre et al., 2015). The dataset has 5 domains, and each domain has 375–750 annotated pairs. Sentences are much shorter than our previous task, at an average of only 13 words in each test sentence. As the dataset is also much smaller, we combine sentences from all 5 domains and also sentences from previous years (2012–2014) to form the training data. We use the headlines domain from 2014 as development, and test on all 2015 domains. For pre-processing, we tokenise and lowercase the words using Stanford CoreNLP (Manning et al., 2014). As a benchmark, we include results from the overall top-performing system in the competition, referred to as “DLS” (Sultan et al., 2015). Note, however, that this system is supervised and highly customised to the task, whereas our methods are completely unsupervised. Results are presented in Table 2. Unsurprisingly, we do not exceed the overall performance of the supervised benchmark system DLS, although doc2vec outperforms DLS over ngram .61 .50 .65 .67 .62 Table 2: Pearson’s r of the STS task across 5 domains. DLS is the overall best system in the competition. Boldface indicates the best r"
W16-1609,D14-1162,0,0.121305,"Missing"
W16-1609,N15-1099,1,0.679102,"Missing"
W16-3928,W16-3930,0,0.0746246,"96.3 2121.6 849.8 2075.7 3413.3 2232.8 3333.9 5503.0 5586.8 0.534 0.524 0.640 0.638 0.502 0.631 0.417 0.254 0.106 0.088 0.090 0.0 0.0 12.9 13.0 0.0 13.7 85.7 689.2 2289.3 6229.9 6514.4 1325.6 1108.9 1861.2 2021.3 1318.8 2409.0 2835.6 3487.6 4670.9 6819.8 6519.8 Table 4: User-level results comparison for pre 2016 (left) and 2016 (right) test sets method in the validation stage, and if no results are found, backing off to the next classifier. As highly accurate methods rely on location-based services or reliable social network information, this ensemble approach is primarily precision-oriented. Chi et al. (2016) (“IBM”) applied multinomial naive Bayes methods over different sets of features. Unlike other teams, they only used tweet text data in their system. The system used an existing set of location-indicative words, gazetted location names from GeoNames, hashtags, users mentions, and a combination of all of these features. In addition, a frequency-based method was used to filter the combined feature set. Experimental results show feature selection over the methods combined achieved the best results across all metrics. Tweet-level and user-level results are shown in Table 1 and Table 2, respectivel"
W16-3928,N16-1122,0,0.0251185,"Missing"
W16-3928,C12-1064,1,0.679011,"Missing"
W16-3928,W16-3929,0,0.0594753,"among which 3 teams submitted system description papers. Miura et al. (2016) (“F UJI X EROX”) applied vectorised inputs in linear models for both tweet-level and user-level geotagging tasks. They used tweet text, user self-declared locations, timezone values, and user self-descriptions as input sources. Each source of input is transformed from a one-hot bag-ofword representation into a vector. Vectors from the same source are averaged, and vectors from different sources then are concatenated and used as the input to linear models. A softmax function is used to select the most probable class. Jayasinghe et al. (2016) (“CSIRO”) adopted ensemble learning methods with carefully extracted features from various available sources in both text and metadata. Namely, the authors implemented label propagation methods among tweet posts, location name mappings, text retrieval methods that assume similar text comes from the same region, and language-based classifiers. Having obtained outputs from each of the individual methods, the authors combined the resultant features in different ways. The best accuracy is achieved by using accuracy-ordered predictions, i.e. taking the prediction from the best 215 SUBMISSION ACCUR"
W16-3928,W16-3931,0,0.191324,"30.6 262.7 630.2 1711.1 4000.2 5714.9 1122.3 963.8 1928.8 2071.5 1084.3 2242.4 3124.4 2860.2 4002.4 6161.4 6053.3 Table 2: User-level results ranked by median error distance wildly-wrong predictions. Specifically, as our distance-based metrics, we measure (in kilometres): (a) the median error distance; and (b) the mean error distance. Although distance-based metrics are more intuitive, class-based predictions such as city labels are often easier to use in downstream applications. 4 Systems and Results In total, 5 teams uploaded 21 runs, among which 3 teams submitted system description papers. Miura et al. (2016) (“F UJI X EROX”) applied vectorised inputs in linear models for both tweet-level and user-level geotagging tasks. They used tweet text, user self-declared locations, timezone values, and user self-descriptions as input sources. Each source of input is transformed from a one-hot bag-ofword representation into a vector. Vectors from the same source are averaged, and vectors from different sources then are concatenated and used as the input to linear models. A softmax function is used to select the most probable class. Jayasinghe et al. (2016) (“CSIRO”) adopted ensemble learning methods with car"
W17-1002,W04-1013,0,0.0146504,"coder, so ot = Wo xt + Uo ht−1 + bo jt = Wj xt + Uj ht−1 + bj (1) ct = ct−1 ∗ σ(ft ) + tanh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time"
W17-1002,D15-1044,0,0.571169,"the document vector to a recurrent neural network decoder. With this decoupled architecture, we decrease the number of parameters in the decoder substantially, and shorten its training time. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents. 1 Introduction Abstractive document summarization is a challenging natural language understanding task. Abstractive methods first encode the original document into a high-level representation, and then decode it into the target summary. Rush et al. (2015) proposed the task of headline generation as the first step towards abstractive summarization. Instead of using the full document, the authors experimented with using the first sentence as input, with the aim of generating a coherent headline given the sentence. 2 Attentive Recurrent Neural Network: A Joint Encoder–decoder Architecture The attentive recurrent neural network is composed of an attentive encoder and a recurrent decoder (Chopra et al., 2016), where the encoder is 1 The training time is decreased from 4 days (with full G I GAWORD ) for the coupled model (Rush et al., 2015) to 2 day"
W17-1002,N16-1012,0,0.13036,"nh(jt ) ∗ σ(it ) ht = tanh(ct ) ∗ σ(ot ) 2 The unnormalised attention weights are computed by combining ht−1 with the convolutional embedding of each source word via dot product. where it , ft and ot are input, forget and output gates, respectively; jt , ct and ht represent the new 8 Combination add-input add-hidden stack-input stack-hidden mlp-input mlp-hidden Equation i0t = it + d h0t = ht + d i0t = [it ; d] h0t = [ht ; d] i0t = tanh(Wi it + Wd d + b) h0t = tanh(Wi ht + Wd d + b) using the ROUGE metric (Lin, 2004), following the same evaluation style of benchmark systems (Rush et al., 2015; Chopra et al., 2016). For outof-domain experiments, we use the same models trained from G IGAWORD, but tune using DUC 03 and test on DUC 04; DUC 03 and DUC 04 each have 500 examples. For the doc2vec encoder, we train using G IGA WORD and infer document vectors for validation and test examples using the trained model. Valid and test examples are excluded from the doc2vec training data. Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respecti"
W17-1002,N16-1149,1,0.834556,"Table 1: Incorporation of doc2vec signal in the decoder. d denotes the doc2vec vector; it (ht ) is the input (hidden) vector at time t; and “[·; ·]” denotes vector concatenation. 4.2 input, new context and new hidden state, respectively; ∗ is the elementwise vector product; and σ is the sigmoid activation function. Given an input word and previous hidden state, the decoder predicts the next word and generates the summary one word at a time. To generate summaries that are related to the document, we incorporate the doc2vec input document signal to the decoder using several methods proposed by Hoang et al. (2016). There are two layers where we can incorporate doc2vec: in the input layer (input), or hidden layer (hidden). There are three methods of incorporation: addition (add), stacking (stack), or via a multilayer perceptron (mlp). Table 1 illustrates the 6 possible approaches to incorporation. Note that add requires doc2vec to have the same vector dimensionality as the layer it is combined with, and stack-hidden doubles the hidden size (assuming they have the same dimensions), resulting in a large output projection matrix and longer training time. 4 4.1 Hyper-parameter tuning For the encoder, we exp"
W17-1002,W16-1609,1,\N,Missing
W17-1726,W14-0807,0,0.0705624,"Missing"
W17-1726,W06-2920,0,0.129258,"Missing"
W17-1726,J13-1009,0,0.0459691,"Missing"
W17-1726,D07-1116,0,0.0302854,"Missing"
W17-1726,P14-1070,0,0.11271,"idered to be a weak expression as it is largely compositional — one can highly recommend a product — as indicated by the presence of alternatives such as greatly recommend which are also acceptable though less idiomatic. A total of 3,626 MWE instances were identified in STREUSLE, across 2,334 MWE types. Related Work Our long-term goal is in building reliable resources for joint MWE/syntactic parsing. Explicit modelling of MWEs has been shown to improve parser accuracy (Nivre and Nilsson, 2004; Finkel and Manning, 2009; Korkontzelos and Manandhar, 2010; Green et al., 2013; Vincze et al., 2013; Candito and Constant, 2014; Constant and Nivre, 2016). Treatment of MWEs has typically involved parsing MWEs as single lexical units (Nivre and Nilsson, 2004; Eryi˘git et al., 2011; Aggeliki Fotopoulou, 2014), however this flattened, “words with spaces” (Sag et al., 2002) approach is inflexible in its coverage of MWEs where components have some level of flexibility. The English Web Treebank (Bies et al., 2012) represents a gold-standard annotation effort over informal web text. The original syntactic constituency annotation of the corpus was based on hand-correcting the output of the Stanford Parser (Manning et al., 20"
W17-1726,L16-1629,0,0.020962,"syntactic annotations. We make available a tool that applies these fixes in the process of joining the two annotations into a single harmonized, corrected annotation, and release the harmonized annotations in the form of HAMSTER (the HArmonized Multiword and Syntactic TreE Resource): https: //github.com/eltimster/HAMSTER. 2 two annotators, which resulted in an average interannotator F1 agreement of 0.7. The idiosyncratic nature of MWEs lends itself to challenges associated with their interpretation, and this was readily acknowledged by those involved in the development of the STREUSLE corpus (Hollenstein et al., 2016). Two important aspects of the MWE annotation are that it includes both contiguous and non-contiguous MWEs (e.g. check ⇤ out), and that it supports both weak and strong annotation; both of these are considered in scope for our inconsistency analysis. A variety of cues are employed to determine this associative strength. The primary factor relates to the degree in which the expression is semantically opaque and/or morphosyntactically idiosyncratic. An example of a strong MWE would be top notch, as used in the sentence: We stayed at a top notch hotel. The semantics of this expression are not imm"
W17-1726,P16-1016,0,0.556212,"Melbourne chanking@gmail.com, julian.brooke@unimelb.edu.au, tb@ldwin.net Abstract small number of errors in a gold-standard syntactic annotation can, for example, result in significant changes in downstream applications (Habash et al., 2007). This paper presents the results of a harmonization effort for the overlapping STREUSLE annotation (Schneider et al., 2014) of multiword expressions (“MWEs”: Baldwin and Kim (2010)) and dependency parse structure in the English Web Treebank (“EWT”: Bies et al. (2012)), with the long-term goal of building reliable resources for joint MWE/syntactic parsing (Constant and Nivre, 2016). As part of merging these two sets of annotations, we use analysis of cross-annotation and type-level consistency to identify instances of potential annotation inconsistency, with an eye to improving the quality of the component and combined annotations. It is important to point out that our approach to identifying and handling inconsistencies does not involve re-annotating the corpus; instead we act as arbitrators, resolving inconsistency in only those cases where human intervention is necessary. Our three methods for identifying potentially problematic annotations are: • a cross-annotation"
W17-1726,N06-2015,0,0.0583086,"Missing"
W17-1726,de-marneffe-etal-2006-generating,0,0.185456,"Missing"
W17-1726,W07-1501,0,0.0494997,"tion of dependency parsers (Buchholz and Marsi, 2006). In recent years, there has been a focus on multiannotation of a single corpus, such as joint syntactic, semantic role, named entity, coreference and word sense annotation in Ontonotes (Hovy et al., 2006) or constituency, semantic role, discourse, opinion, temporal, event and coreference (among others) annotation of the Manually Annotated Sub-Corpus of the ANC (Ide et al., 2010). As part of this, there has been an increased focus on harmonizing and merging existing annotated data sets as a means of extending the scope of reference corpora (Ide and Suderman, 2007; Declerck, 2008; Simi et al., 2015). This effort sometimes presents an opportunity to fix conflicting annotations, a worthwhile endeavour since even a 187 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 187–193, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics about a hundred errors in the original syntactic annotations. We make available a tool that applies these fixes in the process of joining the two annotations into a single harmonized, corrected annotation, and release the harmonized annotations in the form of HAMSTER (the HArmonized"
W17-1726,P10-2013,0,0.0296141,"es in the joint corpus. 1 Introduction The availability of gold-standard annotations is important for the training and evaluation of a wide variety of NLP tasks, including the evaluation of dependency parsers (Buchholz and Marsi, 2006). In recent years, there has been a focus on multiannotation of a single corpus, such as joint syntactic, semantic role, named entity, coreference and word sense annotation in Ontonotes (Hovy et al., 2006) or constituency, semantic role, discourse, opinion, temporal, event and coreference (among others) annotation of the Manually Annotated Sub-Corpus of the ANC (Ide et al., 2010). As part of this, there has been an increased focus on harmonizing and merging existing annotated data sets as a means of extending the scope of reference corpora (Ide and Suderman, 2007; Declerck, 2008; Simi et al., 2015). This effort sometimes presents an opportunity to fix conflicting annotations, a worthwhile endeavour since even a 187 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 187–193, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics about a hundred errors in the original syntactic annotations. We make available a tool that appli"
W17-1726,declerck-2008-framework,0,0.0323769,"rs (Buchholz and Marsi, 2006). In recent years, there has been a focus on multiannotation of a single corpus, such as joint syntactic, semantic role, named entity, coreference and word sense annotation in Ontonotes (Hovy et al., 2006) or constituency, semantic role, discourse, opinion, temporal, event and coreference (among others) annotation of the Manually Annotated Sub-Corpus of the ANC (Ide et al., 2010). As part of this, there has been an increased focus on harmonizing and merging existing annotated data sets as a means of extending the scope of reference corpora (Ide and Suderman, 2007; Declerck, 2008; Simi et al., 2015). This effort sometimes presents an opportunity to fix conflicting annotations, a worthwhile endeavour since even a 187 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 187–193, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics about a hundred errors in the original syntactic annotations. We make available a tool that applies these fixes in the process of joining the two annotations into a single harmonized, corrected annotation, and release the harmonized annotations in the form of HAMSTER (the HArmonized Multiword and Sy"
W17-1726,P10-2014,0,0.0579268,"Missing"
W17-1726,W11-3806,0,0.0440104,"Missing"
W17-1726,N10-1089,0,0.0446954,"for a Harmonized Multiword Expression and Dependency Parse Annotation King Chan, Julian Brooke, and Timothy Baldwin Department of Computing and Information Systems, The University of Melbourne chanking@gmail.com, julian.brooke@unimelb.edu.au, tb@ldwin.net Abstract small number of errors in a gold-standard syntactic annotation can, for example, result in significant changes in downstream applications (Habash et al., 2007). This paper presents the results of a harmonization effort for the overlapping STREUSLE annotation (Schneider et al., 2014) of multiword expressions (“MWEs”: Baldwin and Kim (2010)) and dependency parse structure in the English Web Treebank (“EWT”: Bies et al. (2012)), with the long-term goal of building reliable resources for joint MWE/syntactic parsing (Constant and Nivre, 2016). As part of merging these two sets of annotations, we use analysis of cross-annotation and type-level consistency to identify instances of potential annotation inconsistency, with an eye to improving the quality of the component and combined annotations. It is important to point out that our approach to identifying and handling inconsistencies does not involve re-annotating the corpus; instead"
W17-1726,A00-2020,0,0.113419,"sson, 2004), or in the case of Candito and Constant (2014), allow for distinctions in the granularity of syntactic representation for regular vs. irregular MWE types. The identification of inconsistencies in annotation requires comparisons to be made between similar instances that are labeled differently. Boyd et al. (2007) employed an alignment-based approach to assess differences in the annotation of n-gram word sequences in order to establish the likelihood of error occurrence. Other work in the syntactic inconsistency detection domain includes those related to POS tagging (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parse structure (Ule and Simov, 2004; Kato and Mat188 amod conj root nn nsubj prep pobj prep pobj cc Deep tissue massage helps with pain in neck and shoulders JJ NN NN VBZ IN NN IN NN CC NNS Figure 1: An example where the arc count heuristic is breached. Deep tissue has been labeled in the sentence here as an MWE in STREUSLE. Deep and tissue act as modifiers to massage, a term that has not been included as part of the MWE. subara, 2010). Dickinson and Meurers (2003) outline various approaches for detecting inconsistencies in parse structure within treebanks. In general,"
W17-1726,E09-1060,0,0.0355818,"d (Nivre and Nilsson, 2004), or in the case of Candito and Constant (2014), allow for distinctions in the granularity of syntactic representation for regular vs. irregular MWE types. The identification of inconsistencies in annotation requires comparisons to be made between similar instances that are labeled differently. Boyd et al. (2007) employed an alignment-based approach to assess differences in the annotation of n-gram word sequences in order to establish the likelihood of error occurrence. Other work in the syntactic inconsistency detection domain includes those related to POS tagging (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parse structure (Ule and Simov, 2004; Kato and Mat188 amod conj root nn nsubj prep pobj prep pobj cc Deep tissue massage helps with pain in neck and shoulders JJ NN NN VBZ IN NN IN NN CC NNS Figure 1: An example where the arc count heuristic is breached. Deep tissue has been labeled in the sentence here as an MWE in STREUSLE. Deep and tissue act as modifiers to massage, a term that has not been included as part of the MWE. subara, 2010). Dickinson and Meurers (2003) outline various approaches for detecting inconsistencies in parse structure within treebanks."
W17-1726,P14-5010,0,0.00360132,"and Constant, 2014; Constant and Nivre, 2016). Treatment of MWEs has typically involved parsing MWEs as single lexical units (Nivre and Nilsson, 2004; Eryi˘git et al., 2011; Aggeliki Fotopoulou, 2014), however this flattened, “words with spaces” (Sag et al., 2002) approach is inflexible in its coverage of MWEs where components have some level of flexibility. The English Web Treebank (Bies et al., 2012) represents a gold-standard annotation effort over informal web text. The original syntactic constituency annotation of the corpus was based on hand-correcting the output of the Stanford Parser (Manning et al., 2014); for our purposes we have converted this into a dependency parse using the Stanford Typed Dependency converter (de Marneffe et al., 2006). We considered the use of the Universal Dependencies representation (Nivre et al., 2016), however we noted that several aspects of that annotation (in particular the treatment of all prepositions as case markers dependent on their noun) make it inappropriate for joint MWE/syntactic parsing since it results in large numbers of MWEs that are non-contiguous in their syntactic structure (despite being contiguous at the token-level). As such, the Stanford Typed"
W17-1726,L16-1262,0,0.0479772,"Missing"
W17-1726,schneider-etal-2014-comprehensive,0,0.0712667,"the use of the Universal Dependencies representation (Nivre et al., 2016), however we noted that several aspects of that annotation (in particular the treatment of all prepositions as case markers dependent on their noun) make it inappropriate for joint MWE/syntactic parsing since it results in large numbers of MWEs that are non-contiguous in their syntactic structure (despite being contiguous at the token-level). As such, the Stanford Typed Dependencies are the representation which has the greatest currency for joint MWE/syntactic parsing work (Constant and Nivre, 2016). The STREUSLE corpus (Schneider et al., 2014) is based entirely on the Reviews subset of the EWT, and comprises of 3,812 sentences representing 55,579 tokens. The annotation was completed by six linguists who were native English speakers. Every sentence was assessed by at least Other MWE-aware dependency treebanks include the various UD treebanks (Nivre et al., 2016), the Prague Dependency Treebank (Bejˇcek et al., 2013), and others (Nivre and Nilsson, 2004; Eryi˘git et al., 2011; Candito and Constant, 2014). The representation of MWEs, and the scope of types covered by these treebanks, can vary significantly. For example, the internal s"
W17-1726,ule-simov-2004-unexpected,0,0.0580952,"(2014), allow for distinctions in the granularity of syntactic representation for regular vs. irregular MWE types. The identification of inconsistencies in annotation requires comparisons to be made between similar instances that are labeled differently. Boyd et al. (2007) employed an alignment-based approach to assess differences in the annotation of n-gram word sequences in order to establish the likelihood of error occurrence. Other work in the syntactic inconsistency detection domain includes those related to POS tagging (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parse structure (Ule and Simov, 2004; Kato and Mat188 amod conj root nn nsubj prep pobj prep pobj cc Deep tissue massage helps with pain in neck and shoulders JJ NN NN VBZ IN NN IN NN CC NNS Figure 1: An example where the arc count heuristic is breached. Deep tissue has been labeled in the sentence here as an MWE in STREUSLE. Deep and tissue act as modifiers to massage, a term that has not been included as part of the MWE. subara, 2010). Dickinson and Meurers (2003) outline various approaches for detecting inconsistencies in parse structure within treebanks. In general, inconsistencies associated with MWE annotation fall under t"
W17-1726,I13-1024,0,0.0166045,"hly recommend is considered to be a weak expression as it is largely compositional — one can highly recommend a product — as indicated by the presence of alternatives such as greatly recommend which are also acceptable though less idiomatic. A total of 3,626 MWE instances were identified in STREUSLE, across 2,334 MWE types. Related Work Our long-term goal is in building reliable resources for joint MWE/syntactic parsing. Explicit modelling of MWEs has been shown to improve parser accuracy (Nivre and Nilsson, 2004; Finkel and Manning, 2009; Korkontzelos and Manandhar, 2010; Green et al., 2013; Vincze et al., 2013; Candito and Constant, 2014; Constant and Nivre, 2016). Treatment of MWEs has typically involved parsing MWEs as single lexical units (Nivre and Nilsson, 2004; Eryi˘git et al., 2011; Aggeliki Fotopoulou, 2014), however this flattened, “words with spaces” (Sag et al., 2002) approach is inflexible in its coverage of MWEs where components have some level of flexibility. The English Web Treebank (Bies et al., 2012) represents a gold-standard annotation effort over informal web text. The original syntactic constituency annotation of the corpus was based on hand-correcting the output of the Stanfor"
W17-1726,N09-1037,0,\N,Missing
W17-4122,W04-2106,0,0.0156755,"is the use of a relatively limited set of bushu. Additionally, unlike the other datasets, KRADFILE does not list bushu in an order consistent with their appearance in the kanji. Furthermore, KRADFILE provides a single exhaustive decomposition for all kanji characters and their bushu. Because of this, we consider KRADFILE to have only a single, indivisible layer of decomposition. KanjiVG KanjiVG3 is a collection of images that provides information about kanji and their decompositions. Decomposition information for KanjiVG is derived from the analysis of strokes used to write kanji characters (Apel and Quint, 2004). Although KanjiVG does allow for the decomposition of characters down 2 3 2.5 Dataset Comparison Table 1 shows descriptive statistics for the four datasets and their decompositions. https://github.com/cjkvi/cjkvi-ids http://kanjivg.tagaini.net/ 4 149 http://users.monash.edu/~jwb/kradinf.html Characters Unique bushu Average bushu per kanji Average branching factor Average depth GlyphWiki IDS KanjiVG KRADFILE 18761 2834 1.9 1.5 2.7 20970 3104 2.1 2.1 3.1 6744 1327 2.2 1.9 2.9 12156 254 4.5 4.5 1.0 Table 1: Statistics for the four kanji datasets The Characters row of the table describes the tota"
W17-4122,W07-1522,0,0.0289022,"Missing"
W17-4122,P09-2022,0,0.0328244,"Missing"
W17-4122,D16-1100,0,0.25421,"Missing"
W17-4122,D15-1098,0,0.102272,"Missing"
W17-4122,D15-1176,0,0.032776,"tagging. Given the correspondence between kanji and Chinese characters, a comparison of the two languages with regards to the usefulness of decomposition would be worth exploring. We are also interested in performing an intrinsic evaluation of the character- and bushu-level embeddings learned through language modelling, e.g. relative to character-level similarity datasets such as that of Yencken and Baldwin (2006). Related Work Working at the character level has proven useful in language modelling in English, as well as related applications such as building word representations (Graves, 2013; Ling et al., 2015). With regards to ideographic languages, there is work in information retrieval that has considered the appropriate representation for indexing; the focus has typically been word versus character (Kwok, 1997; Baldwin, 2009), but Fujii and Croft (1993) considered (though ultimately rejected) subcharacter based indexing. In terms of investigations of the usefulness of sub-character representations for neural network models in ideographic languages, relevant work includes recent papers that use sub-character information to assist in the training of character embeddings for Chinese (Sun et al., 20"
W17-4122,I11-1085,0,0.0292116,"Missing"
W17-4122,P15-2098,0,0.168463,"Missing"
W17-5404,P06-4018,0,0.00742436,"utputs to fix minor grammar errors, such as adding or removing the determiner a or the. 2.2.2 3 Token Substitution Method Once the algorithm has decided which tokens should be changed, the next move is to find appropriate substitutions. As described above, most systems are based on n-grams, making them very sensitive to unknown tokens. Therefore, we came up with some heuristics. The first approach draws on our earlier work on learning robust text representations (Li et al., 2017), and is based on synonyms of the given token, based on Princeton WordNet (Miller et al., 1990) using the NLTK API (Bird, 2006). Here, we test possible synonyms, considering their part-ofspeech tag, asking the system s whether the loss is reduced after substitution. We also tried to find antonyms that cannot be recognized by the system, causing the predicted sentiment label to not flip. Finally, we add a small amount of human superviResults and Analysis In this section, we detail the results of our methods, and perform error analysis. 3.1 Builder The results for the builder systems over the test set are shown in Table 1. To evaluate the robustness of the builder systems, there are two evaluation criteria: average F-sc"
W17-5404,P15-2030,1,0.839782,"minimal pair, with positive (+1) sentiment: This paper describes our submission to the sentiment analysis sub-task of “Build It, Break It: The Language Edition (BIBI)”, on both the builder and breaker sides. As a builder, we use convolutional neural nets, trained on both phrase and sentence data. As a breaker, we use Q-learning to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the robustness of NLP systems. 1 Introduction Recently, deep learning models have made impressive gains over a range of NLP tasks (Bahdanau et al., 2015; Bitvai and Cohn, 2015). However, recent studies have exposed brittleness in the models, e.g. through adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015). In these papers, researchers construct cognitively implausible perturbations of raw image inputs to fool state-of-the-art deep learning models. These perturbations are cheap and easy to generate using a “fast-gradient” method, based on analysis of the derivative of the loss with respect to the input. One issue with the generation of adversarial examples for NLP has been the fact that language data is discrete, and hence difficult to map the contin"
W17-5404,P11-1038,1,0.70899,"ples, making our approach limited in application. Therefore, we can’t really conclude that our automatic approach is a success, and we should explore more flexible approaches in the future. However, the approach itself still achieves a break rate higher than the error rate on the origiIt is almost the same situation when the systems encounter out-of-vocabulary words (OOV). Although OOVs are a significant challenge, we believe they can be overcome by training better sentiment-sensitized word embeddings (Mrkˇsi´c et al., 2016), or combining the system with character-level normalization methods (Han and Baldwin, 2011). However, CNNs are not good at dealing with complex grammatical structures or long-distance dependencies. For instance, changing a comparative from more than to less than flips the sentiment and is something that humans are sensitized to, but CNNs tend not to capture this difference. Also, CNNs are not sensitive to tense, such as changing the present tense is to the past tense was to capture pragmatic/connotative effects. For these kinds of examples, we expected to see higher performance among models which better capture syntactic structure, such as recursive neural nets (“RNNs”: Socher et al"
W17-5404,P14-1062,0,0.226354,"Missing"
W17-5404,E17-2004,1,0.797792,"etable as ironic — which remains a challenging problem for us to totally eliminate during generation. Last, we slightly modify the outputs to fix minor grammar errors, such as adding or removing the determiner a or the. 2.2.2 3 Token Substitution Method Once the algorithm has decided which tokens should be changed, the next move is to find appropriate substitutions. As described above, most systems are based on n-grams, making them very sensitive to unknown tokens. Therefore, we came up with some heuristics. The first approach draws on our earlier work on learning robust text representations (Li et al., 2017), and is based on synonyms of the given token, based on Princeton WordNet (Miller et al., 1990) using the NLTK API (Bird, 2006). Here, we test possible synonyms, considering their part-ofspeech tag, asking the system s whether the loss is reduced after substitution. We also tried to find antonyms that cannot be recognized by the system, causing the predicted sentiment label to not flip. Finally, we add a small amount of human superviResults and Analysis In this section, we detail the results of our methods, and perform error analysis. 3.1 Builder The results for the builder systems over the te"
W17-5404,N16-1018,0,0.0232632,"Missing"
W17-5404,P05-1015,0,0.312829,"Timothy Baldwin School of Computing and Information Systems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.au Abstract different inputs, and breakers try to construct instances which will cause the builders’ systems to make incorrect predictions. In this paper, we describe our builder and breaker submissions to the sentiment analysis subtask, which is a sentence-level binary classification task, to predict whether a given review sentence is positive or negative with respect to a given movie. The data set is derived from movie review data (Pang and Lee, 2005) and the Stanford Sentiment Treebank (Socher et al., 2013). We participated both as a builder and breaker because we are interested in testing the robustness of state-of-the-art neural models, such as convolutional neural networks (“CNNs”: Kim (2014)). Also, we were interested in the breaker task as an avenue for exploring how well we can automatically construct adversarial test instances. In the sentiment sub-task, the main job of breakers is to construct minimally-changed pairs that are able to fool the builders’ sentiment analysers. For example, the following sentences can be considered to"
W17-5404,D13-1170,0,0.112127,"ems The University of Melbourne, Australia yitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.au Abstract different inputs, and breakers try to construct instances which will cause the builders’ systems to make incorrect predictions. In this paper, we describe our builder and breaker submissions to the sentiment analysis subtask, which is a sentence-level binary classification task, to predict whether a given review sentence is positive or negative with respect to a given movie. The data set is derived from movie review data (Pang and Lee, 2005) and the Stanford Sentiment Treebank (Socher et al., 2013). We participated both as a builder and breaker because we are interested in testing the robustness of state-of-the-art neural models, such as convolutional neural networks (“CNNs”: Kim (2014)). Also, we were interested in the breaker task as an avenue for exploring how well we can automatically construct adversarial test instances. In the sentiment sub-task, the main job of breakers is to construct minimally-changed pairs that are able to fool the builders’ sentiment analysers. For example, the following sentences can be considered to be a minimal pair, with positive (+1) sentiment: This pape"
W18-6102,I13-1041,1,0.733732,"y speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization for proper names, and with non-standard syntax (Baldwin et al., 2013, 2015). 2 2.1 Network-based methods Related Work Text-based methods Text-based geolocation methods use text features to estimate geolocation. Unsupervised topic modeling approaches (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013) are one successful approach in text-based geolocation estimation, although they tend not to scale to larger data sets. It is also possible to use semi-supervised learning over gazetteers (Lieberman et al., 2010; Quercini et al., 2010), whereby gazetted terms are identified and used to construct a distribution over possible locations, and clustering or"
W18-6102,W15-4319,1,0.868415,"Missing"
W18-6102,C12-1064,1,0.851836,"l networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weight NN weight e Input layer e1, e2 Input layer Weighted sum Channel 2: Relation R0 (out) …"
W18-6102,P17-1116,0,0.709437,"label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization"
W18-6102,W16-3928,1,0.822564,"dges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if t"
W18-6102,C16-2055,0,0.0607572,"Missing"
W18-6102,D17-1016,1,0.757551,"opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE"
W18-6102,P15-2104,1,0.958467,"entity (“NE”) tokens, which would only be applicable to NEs attested in the training data. Twitter, as a social media platform, supports a number of different modalities for interacting with other users, such as mentioning another user in the body of a tweet, retweeting the message of another user, or following another user. If we consider the users of the platform as nodes in a graph, these define edges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaki"
W18-6102,P17-2033,1,0.730706,"opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE"
W18-6102,P18-1187,1,0.837067,"pton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaking, network-based methods are empirically superior to text-based methods over the same data set, but don’t scale as well to larger data sets (Rahimi et al., 2015a). Our contributions are as follows: (1) we propose a joint knowledge-based neural network model for Twitter user geolocation, that outperforms conventional text-based user geolocation; and (2) we show that our method works well even if the accuracy of the NE recognition is low — a common situation with Twitter, because many posts are written colloquially, without capitalization for proper names, and with non-standar"
W18-6102,W15-1527,0,0.0300504,"onal graph convolutional networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weight NN weight e Input layer e1, e2 Input layer Weighted sum Channel 2:"
W18-6102,N15-1153,1,0.95748,"entity (“NE”) tokens, which would only be applicable to NEs attested in the training data. Twitter, as a social media platform, supports a number of different modalities for interacting with other users, such as mentioning another user in the body of a tweet, retweeting the message of another user, or following another user. If we consider the users of the platform as nodes in a graph, these define edges in the graph, opening the way for network-based methods to estimate geolocation. The simplest and most common network-based approach is label propagation (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015b), or related methods such as modified adsorption (Talukdar and Crammer, 2009; Rahimi et al., 2015a). Network-based methods are often combined with text-based methods, with the simplest methods being independently trained and combined through methods such as classifier combination, or the integration of text-based predictions into the network to act as priors on individual nodes (Han et al., 2016; Rahimi et al., 2017a). More recent work has proposed methods for jointly training combined text- and network-based models (Miura et al., 2017; Do et al., 2017; Rahimi et al., 2018). Generally speaki"
W18-6102,W17-4415,0,0.0270451,"Missing"
W18-6102,P13-2005,0,0.0648627,"Missing"
W18-6102,D14-1039,0,0.0886635,"et al., 2015) and quantum structure learning (Gilmer et al., 2017). Relational graph convolutional networks (“R-GCNs”: Schlichtkrull et al. (2017)) are a simple implementation of a graph convolutional network, where a weight matrix is constructed for each channel, and combined via a normalised sum to generate an embedding. Kipf and Welling (2016) adapted graph convolutional networks for text based on a layer-wise propagation rule. Supervised approaches tend to be based on bagof-words modelling of the text, in combination with a machine learning method such as hierarchical logistic regression (Wing and Baldridge, 2014) or a neural network with denoising autoencoder (Liu and Inkpen, 2015). Han et al. (2012) focused on explicitly identifying “location indicative words” using multinomial naive Bayes and logistic regression classifiers combined with feature selection methods, while Rahimi et al. (2015b) extended this work using multi-level regularisation and a multi-layer perceptron architecture (Rahimi et al., 2017b). 3 Methods In this paper, we use the following notation to describe the methods: U is the set of users in the 8 Input entities Input words Semantic relation database e2 e1 R0 R0 e R0 e3 Rk e4 Weig"
W18-6119,D15-1075,0,0.0188086,"xt Only We experiment with two methods for constructing our text embeddings: an attentional approach, and a benchmark approach using a simple paragraph vector representation. 3 139 https://www.tensorflow.org/ Let G be a feed-forward network with relu activations. We define the representation for each word as follows: 4.1.1 Decompositional Attentional Model Parikh et al. (2016) proposed a decompositional attentional model for identifying near-duplicate questions. It is based on a bag-of-words model, and has been shown to perform well over the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015; Tomar et al., 2017). We adapt their architecture for our task, running it on question/answer pairs instead of entailment pairs. Note that, in our case, the best answer is in no way expected to be a near-duplicate of the question, and rather, the attention mechanism over word embeddings is used to bridge the “lexical gap” between questions and answers (Shtok et al., 2012), as well as to automatically determine the sorts of answer words that are likely to align with particular question words. Henceforth we refer to our adapted model as “decatt”. The model works as follows: first it attends wor"
W18-6119,P14-1092,0,0.0608892,"Missing"
W18-6119,W16-1616,0,0.0184598,"P Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 137–147 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics compared using cosine similarity. The resulting score was used as a feature in a learn-to-rank setup, together with a set of hand-crafted features including metadata, which did not have a positive effect on the results. In another approach, Bao and Wu (2016) mapped questions and answers to multiple lower dimensional layers of variable size. They then used a 3-way tensor transformation to combine the layers and produce one output layer. Nassif et al. (2016) used stacked bidirectional LSTMs with a multilayer perceptron on top, with the addition of a number of extra features including a small number of metadata features, to classify and re-rank answers. Although the model performed well, it was no better than a far simpler classification model using only features based on text (Belinkov et al., 2015). Compared to these past deep learning approaches for answer retrieval, our work differs in that we include metadata features directly within our deep learning model. We include a large number of such features and show, contrary to the results of previ"
W18-6119,N16-1084,0,0.0610742,"Missing"
W18-6119,D16-1244,0,0.052512,"Missing"
W18-6119,D14-1162,0,0.0829498,"5 Experiments To train our models, we used the Adam Optimiser (Kingma and Ba, 2014). For decatt, we used dropout over F, G, H after every dense layer. For the doc2vec MLP, we included batch normalisation before, and dropout after, each dense layer. For testing, we picked the best model according to the validation results after the end of each epoch. The parameters for decatt were initialised with a Gaussian distribution of mean 0 and variance 0.01, and for the doc2vec MLP we used Glorot normal initialization. For Stack Overflow, the parameters for Word embeddings were pretrained using GloVe (Pennington et al., 2014) with the F ULL data (by combining all questions and answers in the sequence they appeared) for 50 epochs. Word embeddings were set to 150 dimensions. The co-occurrence weighting function’s maximum value xmax was kept at the default of 10. For S EM E VAL, we used pretrained Common Crawl cased embeddings with 840G tokens and 300 dimensions (Pennington et al., 2014). To train the decatt model for Stack Overflow we split the data into 3 partitions based on the size of the question/answer text, with separate partitions where the total length of the question/answer text was: (1) ≤ 500 words; (2) >"
W18-6119,S17-2058,0,0.015578,"e develop two novel benchmark datasets for cQA answer ranking/selection; • we adapt a deep learning method proposed for near-duplicate/paraphrase detection, and achieve state-of-the-art results for text-based answer selection; and • we demonstrate that metadata is critical in identifying preferred answers, but at the same time text-based representations complement metadata to achieve the best overall results for the task. The data and code used in this research will be made available on acceptance. 2 Related work The work that is most closely related to ours is Bogdanova and Foster (2016) and Koreeda et al. (2017). In this first case, Le and Mikolov’s paragraph2vec was used to convert question–answer pairs into fixed-size vectors in a word-embedding vector space, which were then fed into a simple feed-forward neural network. In the second case, a decompositional attentional model is applied to the SemEval question–comment re-ranking task, and achieved respectable results for text alone. We improve on the standalone results for these two methods through better training and hyperparameter optimisation. We additionally extend both methods by incorporating metadata features in the training of the neural mo"
W18-6119,W16-1609,1,0.923095,"on and answer by summing them: v1 = Plb k=1 exp(ei,k ) la X v1,i ; i=1 v2 = lb X v2,i j=1 Finally, we concatenate both vectors, vtext = [v1 ; v2 ]. This text vector is used as the input in the classification network H. 4.1.2 Paragraph Vectors Our second approach uses the method of Bogdanova and Foster (2016), who achieved state-ofthe-art performance on the Yahoo! Answers corpus of Jansen et al. (2014). The method, which we will refer to as “doc2vec”, works by independently constructing vector representations of both the question and answer texts, using paragraph vectors (Le and Mikolov, 2014; Lau and Baldwin, 2016) in the same vector space. The training is unsupervised, only requiring an unlabelled pretraining corpus to learn the vectors. The doc2vec method is an extension of word2vec (Mikolov et al., 2013) for learning document embeddings. The document embeddings are generated along with word embeddings in the same vector space. word2vec learns word embeddings that can separate the words appearing in contexts of the target word from randomly sampled words, while doc2vec learns document embeddings that can separate the words appearing in the document from randomly sampled words. Given the doc2vec questi"
W18-6119,W17-4121,0,0.0313843,"Missing"
W18-6119,P14-5010,0,0.0103402,"and Foster (2016). The two evaluation datasets, which we denote as “S MALL” and “L ARGE”, contain 10K and 70K questions, respectively, each with a predefined 50/25/25 split between train, val, and test questions. On average, there are approximately six answers per question. In addition to the sampled sub-sets, we also used the full Stack Overflow dump (containing a full month of questions and answers) for pretraining; we will refer to this dataset as “F ULL”. This full dataset consists of approximately 300K questions and 1M answers. In all cases, we tokenised the text using Stanford CoreNLP (Manning et al., 2014). Stack Overflow contains rich metadata, including user-level information and question- and answer-specific data. We leverage this metadata in our model, as detailed in Section 4.2. Summary statistics of S MALL, L ARGE and F ULL are presented in Table 1. In addition to the Stack Overflow dataset, we also experiment with an additional complementary dataset: the S EM E VAL 2017 Task 3A QuestionComment reranking dataset (Nakov et al., 2017). 4 Methodology We treat the answer ranking problem as a classification problem, where given a question/answer pair, the model tries to predict how likely the"
W18-6119,P10-1125,0,0.0365423,"SemEval question–comment re-ranking task, and achieved respectable results for text alone. We improve on the standalone results for these two methods through better training and hyperparameter optimisation. We additionally extend both methods by incorporating metadata features in the training of the neural model, instead of extracting neural features for use in a non-deep learning model, as is commonly done in re-ranking tasks (Koreeda et al., 2017). In addition to this, there is a variety of other recent work on deep learning methods for answer ranking or best answer selection. For instance, Wang et al. (2010) used a network based on restricted Bolzmann machines (Hinton, 2002), using binary vectors of the most frequent words in the training data as input. This model was trained by trying to reconstruct question vectors from answer vectors, then at test time question vectors were compared against answer vectors to determine their relevance. Elsewhere, Zhou et al. (2016) used Denoising Auto-Encoders (Vincent et al., 2008) to learn how to map both questions and answers to lowdimensional representations, which were then 138 (2009), Wang et al. (2009) and Joty et al. (2016). In this kind of approach, qu"
W19-2004,C18-1139,0,0.0261271,"ses for each NC, presented in decreasing order of popularity among the annotators. 29 4.2.1 Word-level A word embedding captures the context of a word in a document (in relation to other words) in the form of a vector representation. It tokenises text at the word level. dings capture the semantics of a word or phrase in a manner which is sensitised to the context of usage. We used the pretrained implementations of ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) found in the Flair framework.5 The framework also has a contextualised string embedding model of its own, also named Flair (Akbik et al., 2018). We supplied sentences extracted from the Brown corpus where available in order to derive a contextualised interpretation. We extracted 25 sentences at random per MWE, except where there were fewer sentences in the corpus. However, we also included a naive contextindependent implementation in our study, consistent with the other models, following the intuition that the relative compositionality of even a novel compound can often be predicted from its component words alone (e.g. giraffe potato having the plausible compositional interpretation of a potato shaped like a giraffe vs. couch intelli"
W19-2004,W11-1304,0,0.0361857,"nsidered computing the maximum instead of the average (as we report here) of the similarity scores between each paraphrase and its MWE, following the intuition that an MWE would be similar to at least one reported paraphrase, rather than all of them. However, the results for the average similarity were empirically higher across models. 3.3 Dataset Overall 59.7 29.0 Table 1: Mean (µ) and standard deviation (σ) of the compositionality scores for the three datasets used in this research, over a normalised range [0, 100]. Combined Metric D I SC OADJ The English dataset from the DiSCo shared task (Biemann and Giesbrecht, 2011) containing a total of 348 binary phrases, comprising adjective–noun, verb–nounsubj , and verb–nounobj pairs, along with their overall compositionality rating ranging from 0 to 100. The phrases were extracted semi-automatically and their relations were assigned by patterns and checked manually. The compositionality scores were collected from Amazon Mechanical Turk, where workers were presented 4–5 randomly sampled sentences from the UK English WACKy corpora. We focus on the 144 adjective–noun pairs in this study. (1 − β) max Para first, Para allpre ,  Para allpost , where β ∈ [0, 1] is a scal"
W19-2004,Q17-1010,0,0.046614,"(OOV) words, as well new words or misspelled words. It tokenises text at the character level. infersent We used two versions of infersent (Conneau et al., 2017b): infersentGloVe and infersentfastText . Each generates a representation of 300 dimensions, trained over the 1,000,000 most popular English words using GloVe (Pennington et al., 2014) and fastText, respectively. fastText We used the 300-dimensional fastText model pre-trained on Common Crawl and Wikipedia using CBOW (fastTextpre ), as well as one trained over the same Wikipedia corpus4 using skip-gram (fastText). Again, since fastText (Bojanowski et al., 2017) assumes all words to be whitespace delimited, we preprocess our MWE and paraphrases the same way as above (removing the space between them so that armchair critic becomes armchaircritic, say). doc2vec We used the gensim implementation of doc2vec (Le and Mikolov, 2014; Lau and Baldwin, 2016) pretrained on Wikipedia data using the word2vec skip-gram models pretrained on Wikipedia and AP News.6 Contextualised Embeddings Unlike classical embedding techniques, contextualised embed4 Document-level 5 https://github.com/zalandoresearch/ flair 6 https://github.com/jhlau/doc2vec Dated 07-Jan-2019 30 Em"
W19-2004,D17-1070,0,0.365537,"o train a model (i.e. all occurrences of sitting duck need to be pre-tokenised to a single token, such as sitting duck). This is not ideal, as it means the model will need to be retrained for a new set of MWEs (as the tokenisation will necessarily change). It also requires “complete” knowledge of the MWEs before the training step, which is impractical in most cases. Character-level embedding models (Hakimi Parizi and Cook, 2018) are one possible solution to the fixed-vocabulary problem, in being able to handle an unbounded vocabulary, including MWEs. Document embeddings (Le and Mikolov, 2014; Conneau et al., 2017a) are also highly relevant to dynamically generating embeddings for MWEs, as they generate representations of arbitrary spans of text, which are potentially able to capture the context of use of the MWE. 3 Directpre = cos(mwe, w1 + w2 ) Directpost =α cos(mwe, w1 )+ (1 − α) cos(mwe, w2 ) , where: mwe, w1 , and w2 are the embeddings for the combined MWE, first component and second component, respectively;3 w1 +w2 is the elementwise sum of the vectors of each of the component words of the MWE; and α ∈ [0, 1] is a scalar which allows us to vary the weight of the respective components in predictin"
W19-2004,J19-1001,0,0.0566207,"tics combination of the scores obtained by individually comparing the component vectors with that of the MWE via a weighted sum (Directpost ). Formally: therefore, eliminate the need for manual annotation (which is expensive and time-consuming). Salehi et al. (2015) were the first to apply word embeddings to the task of predicting the compositionality of MWEs. The assumption is that the compositionality of an MWE is proportional to the relative similarity between each of the components and the overall MWE, represented by their respective embeddings. This method was recently tuned variously by Cordeiro et al. (2019) and remains state-of-the-art for the task of MWE compositionality prediction, but has the downside that it requires automatic token-level pre-identification of each MWE in the training corpus in order to train a model (i.e. all occurrences of sitting duck need to be pre-tokenised to a single token, such as sitting duck). This is not ideal, as it means the model will need to be retrained for a new set of MWEs (as the tokenisation will necessarily change). It also requires “complete” knowledge of the MWEs before the training step, which is impractical in most cases. Character-level embedding mo"
W19-2004,W18-4920,0,0.119661,"est, followed by fastText and infersent. Moreover, we find that recently-proposed contextualised embedding models such as BERT and ELMo are not adept at handling noncompositionality in multiword expressions. 1 Introduction Modern embedding models, including contextual embeddings, have been shown to work impressively well across a range of tasks (Peters et al., 2018; Devlin et al., 2018). However, study of their performance on data with a mix of compositionality levels, whose meaning is often not easily predicted from that of its constituent words, has been limited (Salehi et al., 2015; Hakimi Parizi and Cook, 2018; Nandakumar et al., 2018). At present, there exists no definitive metric to measure the modelling capabilities of an embedding technique across a spectrum of noncompositionality, especially in the case of newer, contextualised representations, such as ELMo and BERT. In this study, we apply various embedding methods to the task of determining the compositionality of English multiword expressions (“MWEs”), specifically noun–noun and adjective– noun pairs, to test their performance on data representing a range of compositionality (Sag et al., 2002). Compositionality prediction can be modeled as"
W19-2004,W16-1609,1,0.819583,"nglish words using GloVe (Pennington et al., 2014) and fastText, respectively. fastText We used the 300-dimensional fastText model pre-trained on Common Crawl and Wikipedia using CBOW (fastTextpre ), as well as one trained over the same Wikipedia corpus4 using skip-gram (fastText). Again, since fastText (Bojanowski et al., 2017) assumes all words to be whitespace delimited, we preprocess our MWE and paraphrases the same way as above (removing the space between them so that armchair critic becomes armchaircritic, say). doc2vec We used the gensim implementation of doc2vec (Le and Mikolov, 2014; Lau and Baldwin, 2016) pretrained on Wikipedia data using the word2vec skip-gram models pretrained on Wikipedia and AP News.6 Contextualised Embeddings Unlike classical embedding techniques, contextualised embed4 Document-level 5 https://github.com/zalandoresearch/ flair 6 https://github.com/jhlau/doc2vec Dated 07-Jan-2019 30 Emb. method Directpre Directpost Flair 0.165 Flaircontext 0.181 fastTextpre 0.395 fastText 0.464 BERT 0.071 BERTcontext 0.089 ELMo 0.420 ELMocontext 0.461 word2vec 0.581 infersentGloVe 0.321 infersentfastText 0.169 doc2vec −0.157 Para first Para allpre Para allpost 0.295 (α = 0.1) 0.314 (α = 0"
W19-2004,N15-1099,1,0.78707,"Missing"
W19-2004,N18-2035,0,0.0178072,"ve components in predicting the compositionality of the compound. This helps us effectively capture the compositionality of the MWE with regards to each of its individual constituents. We do not perform any tuning of α over heldout data and are, as such, overfitting as we select the best-performing α post hoc. We do, however, present analysis of hyper-parameter sensitivity in Section 5. 3.2 Paraphrase Similarity Assuming access to paraphrases of an MWE, another intuition is that if the MWE appears in similar contexts to the component words of its paraphrases, it is likely to be compositional (Shwartz and Waterson, 2018). Each paraphrase provides an interpretation of the semantics of the MWE, e.g. ancient history is “in the past”, “old news” or “forever ago” (note how each paraphrase brings out a slightly different interpretation). The R AMISCH MWE dataset (described in Section 4.1) provides one or more paraphrases for each MWE contained in it. We calculate the similarity of the embeddings of the MWE and its paraphrases using the following three formulae: Methodology Following Salehi et al. (2015) and Nandakumar et al. (2018), we compute the overall compositionality of an MWE with three broad metrics: direct"
W19-2004,U18-1009,1,0.859928,"xt and infersent. Moreover, we find that recently-proposed contextualised embedding models such as BERT and ELMo are not adept at handling noncompositionality in multiword expressions. 1 Introduction Modern embedding models, including contextual embeddings, have been shown to work impressively well across a range of tasks (Peters et al., 2018; Devlin et al., 2018). However, study of their performance on data with a mix of compositionality levels, whose meaning is often not easily predicted from that of its constituent words, has been limited (Salehi et al., 2015; Hakimi Parizi and Cook, 2018; Nandakumar et al., 2018). At present, there exists no definitive metric to measure the modelling capabilities of an embedding technique across a spectrum of noncompositionality, especially in the case of newer, contextualised representations, such as ELMo and BERT. In this study, we apply various embedding methods to the task of determining the compositionality of English multiword expressions (“MWEs”), specifically noun–noun and adjective– noun pairs, to test their performance on data representing a range of compositionality (Sag et al., 2002). Compositionality prediction can be modeled as a regression task (Baldwin"
W19-2004,D14-1162,0,0.103463,"o remove space). We treat each constituent word as a single word document to generate embeddings. 4.2.2 Character-level Character-level embeddings can generate vectors for words based on n-gram character aggregations. This means they can generate embeddings for out-of-vocabulary (OOV) words, as well new words or misspelled words. It tokenises text at the character level. infersent We used two versions of infersent (Conneau et al., 2017b): infersentGloVe and infersentfastText . Each generates a representation of 300 dimensions, trained over the 1,000,000 most popular English words using GloVe (Pennington et al., 2014) and fastText, respectively. fastText We used the 300-dimensional fastText model pre-trained on Common Crawl and Wikipedia using CBOW (fastTextpre ), as well as one trained over the same Wikipedia corpus4 using skip-gram (fastText). Again, since fastText (Bojanowski et al., 2017) assumes all words to be whitespace delimited, we preprocess our MWE and paraphrases the same way as above (removing the space between them so that armchair critic becomes armchaircritic, say). doc2vec We used the gensim implementation of doc2vec (Le and Mikolov, 2014; Lau and Baldwin, 2016) pretrained on Wikipedia dat"
W19-2004,N18-1202,0,0.287058,"is paper, we apply various embedding methods to multiword expressions to study how well they capture the nuances of noncompositional data. Our results from a range of word-, character-, and document-level embbedings suggest that word2vec performs the best, followed by fastText and infersent. Moreover, we find that recently-proposed contextualised embedding models such as BERT and ELMo are not adept at handling noncompositionality in multiword expressions. 1 Introduction Modern embedding models, including contextual embeddings, have been shown to work impressively well across a range of tasks (Peters et al., 2018; Devlin et al., 2018). However, study of their performance on data with a mix of compositionality levels, whose meaning is often not easily predicted from that of its constituent words, has been limited (Salehi et al., 2015; Hakimi Parizi and Cook, 2018; Nandakumar et al., 2018). At present, there exists no definitive metric to measure the modelling capabilities of an embedding technique across a spectrum of noncompositionality, especially in the case of newer, contextualised representations, such as ELMo and BERT. In this study, we apply various embedding methods to the task of determining t"
W19-2004,P16-2026,0,0.100247,"s a description of each model along with how they are trained. Where available, we made use of pretrained models as is standard practice in NLP. As the different models were trained on different corpora, we are not attempting to perform a controlled comparative evaluation of the different models, so much as a comparison of the standard pre-trained versions of each. If we were to retrain our own models over a standard dataset such as English Wikipedia, we would expect the results for the document-level embedding methods in particular to drop. R AMISCH Similar to R EDDY , the English dataset of Ramisch et al. (2016) contains 90 binary noun compounds with annotated scores of compositionality ranging from 0 to 5, both overall and component-specific (of which we use only the former). It also contains a list of paraphrases for each NC, presented in decreasing order of popularity among the annotators. 29 4.2.1 Word-level A word embedding captures the context of a word in a document (in relation to other words) in the form of a vector representation. It tokenises text at the word level. dings capture the semantics of a word or phrase in a manner which is sensitised to the context of usage. We used the pretrain"
W99-0902,J94-4003,0,0.0265373,"nment paradigm between the two input segments, it is possible to apply the scoring and learning methods proposed herein in their existing forms. Note, however, that in the case of translation pair extraction, there is a real possibility of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily. In fact, it may occur t h a t there is a residue of unaligned segments in either or both languages, as could easily occur if one language included zero anaphora. It may, therefore, be desirable to apply a dynamic threshold on the discriminative ratio (cf. (Dagan and Itai, 1994)) to accept only those translation pairs with sufficiently high statistical confidence, for example. 6 Conclusion In this paper, we proposed an adaptation of the TF-IDF model to Japanese grapheme-phoneme alignment. We then went on to extend the basic statistical method to devise a fully unsupervised learning method, by way of a two discrimination-based metrics and incremental refinement of the statistical model. Experimentation suggested t h a t the proposed learning method marginally outperforms both a baseline rule-based method and the nonincremental statistical method. Items of future resea"
W99-0902,H91-1026,0,0.0124554,"s monotonically decreasing when averaged over the given corridor, in practice local maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the m a x i m u m discriminative value. 5 Other a p p l i c a t i o n s of this research Other than the constraints described in Section 2 and frequency determination techniques, the proposed methodology is theoretically scalable to any domain where two streams of chunked information require alignment. This suggests applications to the extraction of translation pairs from aligned bilingual corpora (Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996), where the system input would be made up of aligned strings (generally sentences) in the two languages. Given t h a t we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scoring and learning methods proposed herein in their existing forms. Note, however, that in the case of translation pair extraction, there is a real possibility of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily. In fact, it may occur t h a t there is a residue of unaligne"
W99-0902,P93-1003,0,0.0168673,"ing when averaged over the given corridor, in practice local maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the m a x i m u m discriminative value. 5 Other a p p l i c a t i o n s of this research Other than the constraints described in Section 2 and frequency determination techniques, the proposed methodology is theoretically scalable to any domain where two streams of chunked information require alignment. This suggests applications to the extraction of translation pairs from aligned bilingual corpora (Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996), where the system input would be made up of aligned strings (generally sentences) in the two languages. Given t h a t we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scoring and learning methods proposed herein in their existing forms. Note, however, that in the case of translation pair extraction, there is a real possibility of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily. In fact, it may occur t h a t there is a residue of unaligned segments in"
W99-0902,J96-1001,0,0.0147695,"ged over the given corridor, in practice local maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the m a x i m u m discriminative value. 5 Other a p p l i c a t i o n s of this research Other than the constraints described in Section 2 and frequency determination techniques, the proposed methodology is theoretically scalable to any domain where two streams of chunked information require alignment. This suggests applications to the extraction of translation pairs from aligned bilingual corpora (Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996), where the system input would be made up of aligned strings (generally sentences) in the two languages. Given t h a t we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scoring and learning methods proposed herein in their existing forms. Note, however, that in the case of translation pair extraction, there is a real possibility of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily. In fact, it may occur t h a t there is a residue of unaligned segments in either or both languag"
W99-0902,J97-4001,0,\N,Missing
Y00-1002,1999.tmi-1.21,1,0.891206,"Missing"
Y00-1002,1999.tmi-1.20,1,0.705777,"reater cohesion that those in the other two regions. If selectional restrictions are given this degree of specificity, it must mean that the associated case slot is highly specialised in its usage and that the lexicographer encoding the selectional restrictions is confident as to the demarkation of use of that case slot. A match at this high level of specialisation tends to have greater credibility than a match at a higher level, and point to genuine case slot correspondence. Figure 1: Semantic density We model semantic density-based match quality according to case slot restrictiveness (CSR) (Baldwin & Tanaka 1999). The degree of CSR of a given node x subsuming nodes 1 1 ,12 ,...1,, is estimated as: CSR(x) = (4) 2_,i =i tree_depth(x, where tree depth is defined as the number of nodes between the subtree root x and subsumed leaf inclusive. This produces the desired ranking for the above figure of 0 < CSR(R i ) < CSR(R2) < CSR(R3 ) < 1. We balance up the degree of semantic density against the degree of backing-off required to achieve that semantic density, using the mq function as for (m2 ). The combined score for case slots a 11 and b with selectional restrictions a l , a 2 , ...ax , ...ak and b 1 , b 2"
Y00-1002,H92-1086,0,0.0373678,"e processes are sufficient to cluster together analytic, but not synthetic and lexical alternations. The next step (step 2) is to collapse together all combined alternations from step 1, for which the SUFF component is covered by a single lexical/conjugational paradigm. Lexical paradigms are a classification of transitive/intransitive and causative/non-causative verb pairs according to derivational affix. An example of such a derivational affix pair is -e-/-ar-, as seen for such verbs as haz&quot;// &quot;star tTRANS&quot; and sonaerul sonawaru &quot;to provide&quot; / &quot;be endowed with&quot; imerul hazimaru &quot;to start (see Jacobsen (1992:pp 258-68) for a thorough listing of such affix pairs). The only conjugational paradigms currently considered are the passive and synthetic causative. All alternations governed by a common lexical/conjugational paradigm are clustered together into one common alternation, with SUFF describing the paradigm applied in the clustering process. Note that there is no overlap between the particular paradigms currently targeted, such that ambiguity as to the applicable paradigm type can never occur. In the final step of alternation clustering (step 3), we score up &quot;sub-alternations&quot; based on the outpu"
Y00-1002,P98-1099,0,0.0213682,"Missing"
Y00-1002,P98-2247,0,0.064881,"Missing"
Y00-1002,C98-2242,0,\N,Missing
Y00-1002,C98-1096,0,\N,Missing
Y04-1012,J96-1002,0,0.00709068,"ammatical relation between a noun and determiner, dobj denotes a grammatical relation between a predicate and its direct object, and ncsubj denotes the grammatical relation between a predicate and its subject; the POS tag AT denotes an article, VH denotes the verb have, and PP denotes a pronoun. Combined, this feature vector is a positive training exemplar used in estimating the relative goodness of other noun–verb pairs with respect to the telic role, as read is a positive telic role instance for book. We used training exemplars such as this to learn a token-level maximum entropy classifier (Berger et al., 1996) for each of the 30 nouns. This performed in the manner of cross validation, whereby, for each noun, we take exemplars from the remaining 29 nouns as training data. Additionally, for the test nouns, we use only sentences which do not include a training exemplar. To get a sense for the effectiveness of this classifier architecture at identifying agentive and telic role data, the values for ) . . ) in the two sample sentences I always had books to read and Complete books have been written on this subject are:           - 120 - PACLIC 18, December 8th-10th, 2004, Waseda Universi"
Y04-1012,bouillon-etal-2002-acquisition,0,0.258621,"Missing"
Y04-1012,briscoe-carroll-2002-robust,0,0.0345024,"s from corpus data. These are in the same vein as Hearst’s template-based strategy (Hearst, 1992), whereby we identify highly precise (generally low-recall) syntactic constructions that are indicative of a verb constituting the agentive or telic role of a given noun. An example of such a template is an N’ modified by an infinitival relative clause, such as (a) book to read, wherein read represents the purpose of book and is thus a candidate for the telic role. We estimate the occurrence of different verbs with a given noun in these constructional templates by running a dependency parser (RASP Briscoe and Carroll (2002)) over the British National Corpus (BNC: Burnard (2000)). The first method uses hand-generated templates for each role type. The second employs a maximum entropy-based supervised learning technique which dynamically learns constructional and lexical preferences from the dependency data. In evaluation, we took a sample of 30 nouns, independently selected 50 verbs for each, and generated a ranked list of verbs for a given noun. We then evaluated the results using a variant of Spearman’s rank correlation. In the remainder of this paper, we first introduce the resources used in this research ( 2)."
Y04-1012,P89-1010,0,0.140796,"occur as the deep object (ARG1) of a transitive verb. We recognize that this is an oversimplification, as evidenced by knife and its telic role cut, which we would not expect to occur as cut (the) knife. The principal motivation for making this simplifying assumption is empirical, in that our primary concern is with high precision, potentially at the cost of low recall. In order to normalize for the effects of the independent noun and verb word probabilities in calculating the frequencies of occurrence of a given noun–verb pair over the set of templates, we use point-wise mutual information (Church and Hanks, 1989). If is the probability of occurrence of word , the mutual information between noun and verb is defined as follows.                We combine this with the corpus frequency for a given noun–verb pair within each template to derive a single score: - 118 - PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo Template N (be  ) (worth deserving meriting) (V[+ing] V[+nom]) N BE worthy of V[+nom] N (deserves merits) V[+nom] Adverb-V[+en] N Adverb V[+en] N N BE Adverb-V[ed] V[+ing] Noun N to V Example (a) book worth reading (the) book is worthy of readin"
Y04-1012,N03-1011,0,0.0508188,"s for. Agentive role: the origin of the entity, or its coming into being e.g., creator, artifact, natural kind, or casual chain For example, for the noun book, publication is a formal role noun, text is a constitutive role noun, read is a telic role verb, and write is an agentive role verb. Research has been done on extracting the formal and constitutive roles of nouns. Hearst (1992), Widdows and Dorow (2002), and others developed methods of automatically acquiring noun hyponyms— corresponding to the formal role—by identifying a set of frequently used and unambiguous lexicosyntactic patterns. Girju et al. (2003) proposed a method of learning part–whole relations, which correspond to the constitutive role. It is also possible to use lexical resources such as WordNet (Fellbaum, 1998) to determine formal (through hypernym links) and constitutive role data (through meronym links). Telic and agentive roles, on the other hand, have received relatively little attention in terms of automatic acquisition and are not available from any large-scale lexical resources. The only work we are aware of which directly targets the task of learning telic and agentive qualia data is that of Bouillon et al. (2002), - 115"
Y04-1012,C92-2082,0,0.359732,"shape, dimensionality, color, or position. Constitutive role: the internal constitution of the entity. e.g., material, weight, parts, or component elements. Telic role: the typical function of the entity. i.e. what the entity is for. Agentive role: the origin of the entity, or its coming into being e.g., creator, artifact, natural kind, or casual chain For example, for the noun book, publication is a formal role noun, text is a constitutive role noun, read is a telic role verb, and write is an agentive role verb. Research has been done on extracting the formal and constitutive roles of nouns. Hearst (1992), Widdows and Dorow (2002), and others developed methods of automatically acquiring noun hyponyms— corresponding to the formal role—by identifying a set of frequently used and unambiguous lexicosyntactic patterns. Girju et al. (2003) proposed a method of learning part–whole relations, which correspond to the constitutive role. It is also possible to use lexical resources such as WordNet (Fellbaum, 1998) to determine formal (through hypernym links) and constitutive role data (through meronym links). Telic and agentive roles, on the other hand, have received relatively little attention in terms"
Y04-1012,E03-1073,0,0.0394967,"work we are aware of which directly targets the task of learning telic and agentive qualia data is that of Bouillon et al. (2002), - 115 - who use symbolic learning to identify “qualia pairs”—token instances of noun–verb pairs which correspond to a some qualia role—in corpus data. Our work differs in that we can identify the qualia roles of an arbitrary noun, as suitable for the development of a lexical resource, and sub-classify noun–verb pairs according to the specific qualia role they constitute. An example application of telic and agentive roles is the interpretation of logical metonymy (Lapata and Lascarides, 2003), such as in Mary finished her beer. Under the standard interpretation of logical metonymy, finish here predicates over an unexpressed verb, which takes her beer as object. By accessing the qualia structure of beer, it is possible to resolve the unexpressed verb by way of telic and agentive roles, resulting, e.g., in the interpretation finished drinking her beer (from the telic role – although in other cases the agentive role may be more appropriate). Busa and Johnston (1996) proposed an interpretation-based method of translating complex nominals from English to Italian, interpreting the relat"
Y04-1012,J93-2005,0,0.181596,"resses the relative “goodness” of each noun–verb pair for the qualia role in question. A highly-ranked verb can then be considered as a candidate for inclusion in the qualia structure of that noun. Below, we outline the two methods, the first of which uses hand-generated templates to identify candidates for each role type, while the second employs a supervised learning technique to dynamically identify templates predictive of the different role types. 3.1 Template-based ranking method Verbs which form part of the qualia structure of a noun tend to occur frequently in particular constructions (Pustejovsky et al., 1993; Bouillon et al., 2002). For example, verbs which readily allow passivization of a given noun tend to be good candidates for the agentive role of that noun, i.e. frequent occurrence of sentences such as This book was written (by him) is evidence for write being a candidate for the agentive role of book. Similarly, the a N worth V+ing construction, e.g. a book worth reading, indicates that V (e.g. read) is a candidate for the telic role of N (e.g. book). We identified several constructional templates for the telic role and one for the agentive role, and counted the raw frequency of occurrence"
Y04-1012,C02-1114,0,0.0209483,"nality, color, or position. Constitutive role: the internal constitution of the entity. e.g., material, weight, parts, or component elements. Telic role: the typical function of the entity. i.e. what the entity is for. Agentive role: the origin of the entity, or its coming into being e.g., creator, artifact, natural kind, or casual chain For example, for the noun book, publication is a formal role noun, text is a constitutive role noun, read is a telic role verb, and write is an agentive role verb. Research has been done on extracting the formal and constitutive roles of nouns. Hearst (1992), Widdows and Dorow (2002), and others developed methods of automatically acquiring noun hyponyms— corresponding to the formal role—by identifying a set of frequently used and unambiguous lexicosyntactic patterns. Girju et al. (2003) proposed a method of learning part–whole relations, which correspond to the constitutive role. It is also possible to use lexical resources such as WordNet (Fellbaum, 1998) to determine formal (through hypernym links) and constitutive role data (through meronym links). Telic and agentive roles, on the other hand, have received relatively little attention in terms of automatic acquisition a"
Y04-1012,J90-1003,0,\N,Missing
Y04-1012,W03-1122,0,\N,Missing
Y07-1001,C04-1041,0,0.0228999,"y show to be applicable in a range of DLA tasks relating to English verbs. For token-level DLA tasks, general-purpose DLA can take the form of supertagging over a sufficiently general feature set. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical categorie(s) associated with each word in an input string, relative to a given DLR. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). Baldwin (2005c) used supertagging to learn token-level lexical types for the ERG, based on the full template set of the FN TBL 1.0 English tagger (Ngai and Florian, 2001). Blunsom and Baldwin (2006) revisited these experiments with a generalised feature set, and performed the lexical type prediction task relative to both the ERG and the JACY grammar of Japanese (Siegel and Bender, 2002). A third form of general-purpose DLA is resource alignment, which can be as general as simply assuming that the two DLRs (i.e. system designs) correspond to a common tree or graph data structure. One common e"
Y07-1001,copestake-flickinger-2000-open,0,0.0181605,"system designs) correspond to a common tree or graph data structure. One common example of resource alignment is mining lexical items directly from a DLR (e.g. a machine-readable dictionary (Sanfilippo and Pozna´nski, 1992) or WordNet (Daud´e et al., 2000)). 3.2. Targeted DLA With targeted DLA, a specialised methodology is proposed to (automatically) learn a particular linguistic property. The outputs from a suite of targeted DLA methods are then combined to form fully-specified lexical items. For example, in the following lexical entry for the noun admission in the English Resource Grammar (Copestake and Flickinger, 2000; Flickinger, 2002): admission_n1 := n_pp_mc-of_le & [ STEM < &quot;admission&quot; >, SYNSEM [ LKEYS.KEYREL.PRED 5 &quot;_admission_n_of_rel&quot;, PHON.ONSET voc ] ]. the key values are the lexical type (n_pp_mc-of_le) and the onset (voc).2 The lexical type states that the word is a noun (n) which optionally subcategorises for a PP (pp), is either countable or uncountable (mc), and that the selected PP is headed by of (of). The onset value states that the word has a vowel (rather than consonant) onset. Given separate targeted DLA classifiers for subcategorisation frames and countability, we could construct the"
Y07-1001,N03-1006,0,0.155342,"s is highly tuned to countability classification, and not immediately reusable for any other DLA task. This same general approach can also be used to learn the lexemes themselves in the case of multiword expressions. For example, Baldwin (2005b) learns which verbs combine with intransitive prepositions to form verb particle constructions in English, and at the same time predicts the lexical type(s) of each such verb particle. Other examples of targeted DLA include ontology alignment (Knight and Luk, 1994; Atserias et al., 1997; Daud´e et al., 2000; Klein, 2001), grammatical gender prediction (Cucerzan and Yarowsky, 2003; Nicholson et al., 2006), classifying the semantics of derivational affixes (Light, 1996), and verb aspect learning (Siegel and McKeown, 2000). Note that it is possible to hybridise targeted and general-purpose DLA, e.g. in building or reusing a targeted DLA system to learn lexical properties in one format and then mapping this onto the DLR of choice via a general-purpose alignment method (e.g. Carroll and Fang (2004) for verb subcategorisation learning). 4. Reliance on Secondary DLRs DLA systems tend to be made up of a variety of preprocessors, feature extractors and machine learners. To spe"
Y07-1001,P00-1064,0,0.0651771,"Missing"
Y07-1001,C94-1042,0,0.0613556,"arly from the viewpoint of DLA for precision grammars. 1. Introduction Over recent years, computational linguistics has benefitted considerably from advances in statistical modelling and machine learning, culminating in methods capable of deeper, more accurate automatic analysis, over a wider range of languages. Implicit in much of this work, however, has been the existence of deep language resources (DLR hereafter), that is resources which encode precise symbolic linguistic knowledge. DLRs aim either to capture a particular feature of a language, such as verb argument structure (e.g. COMLEX (Grishman et al., 1994) or PropBank (Palmer et al., 2005)) or lexical semantics (e.g. WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998)), or alternatively to model a language in its entirety, in the form of a precision grammar (e.g. the English Resource Grammar (Flickinger, 2002), various ParGram grammars (Butt et al., 2002) or CCGBank (Hockenmaier and Steedman, 2007)). In line with Baldwin (2005a), we consider the development of DLRs to be made up of two basic tasks: (1) design of a data representation to systematically capture the generalisations and idiosyncracies of the dataset of interest (system design"
Y07-1001,C92-2082,0,0.00729406,"evel unknown word lexical type prediction for precision grammars. Another example is multiword expression identification (Lapata and Lascarides, 2003; Kim and Baldwin, 2006), 5.2. Type-level Classification Once again, we have discussed a number of type-level DLA tasks above, including: subcategorisation learning, countability learning, type-level unknown word lexical type prediction for precision grammars, grammatical gender prediction, and ontology induction and alignment. Other examples include word sense discrimination (Sch¨utze, 1998; Agirre and Soroa, 2007), semantic relation harvesting (Hearst, 1992; Pantel and Pennacchiotti, 2006; Snow et al., 2006), learning qualia structure (Bouillon et al., 2002; Yamada and Baldwin, 2004), and extraction of multiword expressions (Baldwin, 2005b; Baldwin, 2005d; Villada Moir´on, 2005). 6. Conclusion This paper has presented a broad survey of research on deep lexical acquisition, particularly in the context of precision grammars. We developed a classification of DLA methods along three broad axes: general-purpose vs. targeted, in vitro vs. in vivo, and token- vs. type-level classification. We hope this will provide a framework for continued cross-ferti"
Y07-1001,J07-3004,0,0.0477766,"owever, has been the existence of deep language resources (DLR hereafter), that is resources which encode precise symbolic linguistic knowledge. DLRs aim either to capture a particular feature of a language, such as verb argument structure (e.g. COMLEX (Grishman et al., 1994) or PropBank (Palmer et al., 2005)) or lexical semantics (e.g. WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998)), or alternatively to model a language in its entirety, in the form of a precision grammar (e.g. the English Resource Grammar (Flickinger, 2002), various ParGram grammars (Butt et al., 2002) or CCGBank (Hockenmaier and Steedman, 2007)). In line with Baldwin (2005a), we consider the development of DLRs to be made up of two basic tasks: (1) design of a data representation to systematically capture the generalisations and idiosyncracies of the dataset of interest (system design); and (2) classification of data items according to the predefined data representation (data classification). In the case of a deep grammar, for example, system design encompasses the construction of the system of lexical types, templates, and/or phrase structure rules, and data classification corresponds to the determination of the lexical type(s) eac"
Y07-1001,E03-1040,0,0.0215921,"to any DLR, and generally employ a combination of type- and token-level features. For example, Baldwin (2005a) uses a set of character-level n-grams based on the lexeme and derivational features from a derivational lexicon, in addition to token-level contextual features from the simple word context, a POS tagger, a chunk tagger and a full parser. Pantel and Pennacchiotti (2006) and Snow et al. (2006) independently proposed methods of automatically inducing templates for use in learning semantic relations, but in a manner which could equally be applied to any other DLA task. In this same vein, Joanis and Stevenson (2003) propose a general-purpose verb feature set which they show to be applicable in a range of DLA tasks relating to English verbs. For token-level DLA tasks, general-purpose DLA can take the form of supertagging over a sufficiently general feature set. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical categorie(s) associated with each word in an input string, relative to a given DLR. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been ap"
Y07-1001,W06-2110,1,0.749306,", and combining this into a type-level prediction (see Section 3.2.). Equivalently for (token-level) sensebanking—that is, the annotation of lexical items with word sense information in a given context—it is possible to have type-level sense induction inform the sensebanking (Agirre and Soroa, 2007). 5.1. Token-level Classification We have seen a number of token-level classification tasks in our discussion above, namely: supertagging and token-level unknown word lexical type prediction for precision grammars. Another example is multiword expression identification (Lapata and Lascarides, 2003; Kim and Baldwin, 2006), 5.2. Type-level Classification Once again, we have discussed a number of type-level DLA tasks above, including: subcategorisation learning, countability learning, type-level unknown word lexical type prediction for precision grammars, grammatical gender prediction, and ontology induction and alignment. Other examples include word sense discrimination (Sch¨utze, 1998; Agirre and Soroa, 2007), semantic relation harvesting (Hearst, 1992; Pantel and Pennacchiotti, 2006; Snow et al., 2006), learning qualia structure (Bouillon et al., 2002; Yamada and Baldwin, 2004), and extraction of multiword ex"
Y07-1001,J98-1006,0,0.0195539,"r and parse selection module as before, the second step would remain in vivo. 5. Data Point Granularity Finally, DLA methods can either be applied to token-level instances of a given lexical item, or alternatively perform type-level classification. It is important to note that DLRs can similarly be type-based, such as wordnets or precision grammars, or token-based, such as treebanks or sensebanks. That is not to say, however, that typebased DLRs are exclusively associated with type-based DLA methods, or token-based DLRs with 3 Noting the possibility of learning from monosemous relatives, c.f. Leacock et al. (1998) and Martinez et al. (2006). 7 token-based DLA methods. For example, subcategorisation lexicons are type-level, but subcategorisation learning is conventionally performed by performing token-level analysis of the subcategorisation properties of individual token instances of a given lexical item, and combining this into a type-level prediction (see Section 3.2.). Equivalently for (token-level) sensebanking—that is, the annotation of lexical items with word sense information in a given context—it is possible to have type-level sense induction inform the sensebanking (Agirre and Soroa, 2007). 5.1"
Y07-1001,P96-1004,0,0.219077,"is same general approach can also be used to learn the lexemes themselves in the case of multiword expressions. For example, Baldwin (2005b) learns which verbs combine with intransitive prepositions to form verb particle constructions in English, and at the same time predicts the lexical type(s) of each such verb particle. Other examples of targeted DLA include ontology alignment (Knight and Luk, 1994; Atserias et al., 1997; Daud´e et al., 2000; Klein, 2001), grammatical gender prediction (Cucerzan and Yarowsky, 2003; Nicholson et al., 2006), classifying the semantics of derivational affixes (Light, 1996), and verb aspect learning (Siegel and McKeown, 2000). Note that it is possible to hybridise targeted and general-purpose DLA, e.g. in building or reusing a targeted DLA system to learn lexical properties in one format and then mapping this onto the DLR of choice via a general-purpose alignment method (e.g. Carroll and Fang (2004) for verb subcategorisation learning). 4. Reliance on Secondary DLRs DLA systems tend to be made up of a variety of preprocessors, feature extractors and machine learners. To spell out the reliance of a given method on external resources, and its relative separation f"
Y07-1001,J93-2004,0,0.0306684,"R Development and Statistical NLP This paper is certainly not novel in proposing that machine learning and statistical natural language processing (NLP) enter the fray of DLR development. While the conventional view is that there exists an unbridgable gulf between DLR development and statistical NLP, in practice an increasingly symbiotic relationship has developed between the two (Baldwin et al., 2007). Indeed, statistical NLP has provided a strong consumer base for DLRs, as illustrated by the “core” English language technologies of: • POS tagging (developed primarily using the Penn Treebank (Marcus et al., 1993)) • treebank parsing (based almost exclusively on the Penn Treebank (Marcus et al., 1993)) • word sense disambiguation (based on SemCor (Landes et al., 1998) and the Senseval tasks) 1 Although there have been notable successes in porting system designs across languages for a given resource type, e.g. in ontology induction (Maedche, 2002; Nichols et al., 2005). 4 • semantic role labelling (based on PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998)) That is not to say that it has been a one-way street, however, and modern-day DLR developers are calling upon statistical NLP techniq"
Y07-1001,U06-1008,0,0.0160455,"le as before, the second step would remain in vivo. 5. Data Point Granularity Finally, DLA methods can either be applied to token-level instances of a given lexical item, or alternatively perform type-level classification. It is important to note that DLRs can similarly be type-based, such as wordnets or precision grammars, or token-based, such as treebanks or sensebanks. That is not to say, however, that typebased DLRs are exclusively associated with type-based DLA methods, or token-based DLRs with 3 Noting the possibility of learning from monosemous relatives, c.f. Leacock et al. (1998) and Martinez et al. (2006). 7 token-based DLA methods. For example, subcategorisation lexicons are type-level, but subcategorisation learning is conventionally performed by performing token-level analysis of the subcategorisation properties of individual token instances of a given lexical item, and combining this into a type-level prediction (see Section 3.2.). Equivalently for (token-level) sensebanking—that is, the annotation of lexical items with word sense information in a given context—it is possible to have type-level sense induction inform the sensebanking (Agirre and Soroa, 2007). 5.1. Token-level Classificatio"
Y07-1001,P06-2077,0,0.0137774,"lte im Walde (2003), inter alia). Systems generally employ tagging and/or parsing to form token-level subcategorisation frame hypotheses, based on manually-constructed templates. Various statistical filters are applied to these hypotheses to form type-level predictions. Corpus-based noun countability learning operates in a similar fashion, in tagging and/or parsing sentences containing a noun of interest, and running extraction templates over the output of the preprocessors. These are then pooled together as inputs to a classifier which returns a type-level prediction (Baldwin and Bond, 2003; Nagata et al., 2006). Similarly to subcategorisation learning, the filtering of the tagger/parser output analysis is highly tuned to countability classification, and not immediately reusable for any other DLA task. This same general approach can also be used to learn the lexemes themselves in the case of multiword expressions. For example, Baldwin (2005b) learns which verbs combine with intransitive prepositions to form verb particle constructions in English, and at the same time predicts the lexical type(s) of each such verb particle. Other examples of targeted DLA include ontology alignment (Knight and Luk, 199"
Y07-1001,N01-1006,0,0.0171984,"tly general feature set. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical categorie(s) associated with each word in an input string, relative to a given DLR. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). Baldwin (2005c) used supertagging to learn token-level lexical types for the ERG, based on the full template set of the FN TBL 1.0 English tagger (Ngai and Florian, 2001). Blunsom and Baldwin (2006) revisited these experiments with a generalised feature set, and performed the lexical type prediction task relative to both the ERG and the JACY grammar of Japanese (Siegel and Bender, 2002). A third form of general-purpose DLA is resource alignment, which can be as general as simply assuming that the two DLRs (i.e. system designs) correspond to a common tree or graph data structure. One common example of resource alignment is mining lexical items directly from a DLR (e.g. a machine-readable dictionary (Sanfilippo and Pozna´nski, 1992) or WordNet (Daud´e et al., 20"
Y07-1001,U06-1011,1,0.825104,"lity classification, and not immediately reusable for any other DLA task. This same general approach can also be used to learn the lexemes themselves in the case of multiword expressions. For example, Baldwin (2005b) learns which verbs combine with intransitive prepositions to form verb particle constructions in English, and at the same time predicts the lexical type(s) of each such verb particle. Other examples of targeted DLA include ontology alignment (Knight and Luk, 1994; Atserias et al., 1997; Daud´e et al., 2000; Klein, 2001), grammatical gender prediction (Cucerzan and Yarowsky, 2003; Nicholson et al., 2006), classifying the semantics of derivational affixes (Light, 1996), and verb aspect learning (Siegel and McKeown, 2000). Note that it is possible to hybridise targeted and general-purpose DLA, e.g. in building or reusing a targeted DLA system to learn lexical properties in one format and then mapping this onto the DLR of choice via a general-purpose alignment method (e.g. Carroll and Fang (2004) for verb subcategorisation learning). 4. Reliance on Secondary DLRs DLA systems tend to be made up of a variety of preprocessors, feature extractors and machine learners. To spell out the reliance of a"
Y07-1001,C02-2025,0,0.0162455,"given sentence context via the constraints of syntacticallyinteracting words, and combined to form a consolidated lexical entry for that word. That is, rather than relying on indirect feature signatures to perform lexical acquisition, the DLR itself drives the incremental learning process. Supertagging is also a classic instance of in vivo DLA, as all of its feature extraction and optimisation over label sequences is based on the target DLR, with the qualification that we classify a precision grammar and its associated treebank, e.g., as a single DLR due to the tight linkage between the two (Oepen et al., 2002; Hockenmaier and Steedman, 2007). In an intriguing piece of research, Zhang and Kordoni (2005) combine these two in a two-stage in vivo token-level DLA system for unknown words in precision grammars. First, a targeted DLA system is run over each unknown word in a given sentence to predict an N -best list of lexical type hypotheses (similarly to a supertagger, but without the sequential learning element). These are passed onto the target precision grammar, which generates a parse forest for the given sentence. The final selection of lexical type is then made by the grammar proper in unpacking"
Y07-1001,J05-1004,0,0.0860335,"recision grammars. 1. Introduction Over recent years, computational linguistics has benefitted considerably from advances in statistical modelling and machine learning, culminating in methods capable of deeper, more accurate automatic analysis, over a wider range of languages. Implicit in much of this work, however, has been the existence of deep language resources (DLR hereafter), that is resources which encode precise symbolic linguistic knowledge. DLRs aim either to capture a particular feature of a language, such as verb argument structure (e.g. COMLEX (Grishman et al., 1994) or PropBank (Palmer et al., 2005)) or lexical semantics (e.g. WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998)), or alternatively to model a language in its entirety, in the form of a precision grammar (e.g. the English Resource Grammar (Flickinger, 2002), various ParGram grammars (Butt et al., 2002) or CCGBank (Hockenmaier and Steedman, 2007)). In line with Baldwin (2005a), we consider the development of DLRs to be made up of two basic tasks: (1) design of a data representation to systematically capture the generalisations and idiosyncracies of the dataset of interest (system design); and (2) classification of data"
Y07-1001,P06-1015,0,0.0560881,"eir relative portability to different DLRs, with general-purpose DLA being applicable to any arbitrary DLA task, and targeted DLA being tailored to a specific subtask. 3.1. General-purpose DLA General-purpose DLA methods are designed to be applicable to any DLR, and generally employ a combination of type- and token-level features. For example, Baldwin (2005a) uses a set of character-level n-grams based on the lexeme and derivational features from a derivational lexicon, in addition to token-level contextual features from the simple word context, a POS tagger, a chunk tagger and a full parser. Pantel and Pennacchiotti (2006) and Snow et al. (2006) independently proposed methods of automatically inducing templates for use in learning semantic relations, but in a manner which could equally be applied to any other DLA task. In this same vein, Joanis and Stevenson (2003) propose a general-purpose verb feature set which they show to be applicable in a range of DLA tasks relating to English verbs. For token-level DLA tasks, general-purpose DLA can take the form of supertagging over a sufficiently general feature set. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting th"
Y07-1001,A92-1011,0,0.111211,"Missing"
Y07-1001,J98-1004,0,0.0110432,"Missing"
Y07-1001,W02-1210,0,0.0306069,"LR. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). Baldwin (2005c) used supertagging to learn token-level lexical types for the ERG, based on the full template set of the FN TBL 1.0 English tagger (Ngai and Florian, 2001). Blunsom and Baldwin (2006) revisited these experiments with a generalised feature set, and performed the lexical type prediction task relative to both the ERG and the JACY grammar of Japanese (Siegel and Bender, 2002). A third form of general-purpose DLA is resource alignment, which can be as general as simply assuming that the two DLRs (i.e. system designs) correspond to a common tree or graph data structure. One common example of resource alignment is mining lexical items directly from a DLR (e.g. a machine-readable dictionary (Sanfilippo and Pozna´nski, 1992) or WordNet (Daud´e et al., 2000)). 3.2. Targeted DLA With targeted DLA, a specialised methodology is proposed to (automatically) learn a particular linguistic property. The outputs from a suite of targeted DLA methods are then combined to form full"
Y07-1001,J00-4004,0,0.0111878,"d to learn the lexemes themselves in the case of multiword expressions. For example, Baldwin (2005b) learns which verbs combine with intransitive prepositions to form verb particle constructions in English, and at the same time predicts the lexical type(s) of each such verb particle. Other examples of targeted DLA include ontology alignment (Knight and Luk, 1994; Atserias et al., 1997; Daud´e et al., 2000; Klein, 2001), grammatical gender prediction (Cucerzan and Yarowsky, 2003; Nicholson et al., 2006), classifying the semantics of derivational affixes (Light, 1996), and verb aspect learning (Siegel and McKeown, 2000). Note that it is possible to hybridise targeted and general-purpose DLA, e.g. in building or reusing a targeted DLA system to learn lexical properties in one format and then mapping this onto the DLR of choice via a general-purpose alignment method (e.g. Carroll and Fang (2004) for verb subcategorisation learning). 4. Reliance on Secondary DLRs DLA systems tend to be made up of a variety of preprocessors, feature extractors and machine learners. To spell out the reliance of a given method on external resources, and its relative separation from the target DLR, we differentiate between in vitro"
Y07-1001,P06-1101,0,0.0627726,"t DLRs, with general-purpose DLA being applicable to any arbitrary DLA task, and targeted DLA being tailored to a specific subtask. 3.1. General-purpose DLA General-purpose DLA methods are designed to be applicable to any DLR, and generally employ a combination of type- and token-level features. For example, Baldwin (2005a) uses a set of character-level n-grams based on the lexeme and derivational features from a derivational lexicon, in addition to token-level contextual features from the simple word context, a POS tagger, a chunk tagger and a full parser. Pantel and Pennacchiotti (2006) and Snow et al. (2006) independently proposed methods of automatically inducing templates for use in learning semantic relations, but in a manner which could equally be applied to any other DLA task. In this same vein, Joanis and Stevenson (2003) propose a general-purpose verb feature set which they show to be applicable in a range of DLA tasks relating to English verbs. For token-level DLA tasks, general-purpose DLA can take the form of supertagging over a sufficiently general feature set. Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical categorie(s)"
Y07-1001,Y04-1012,1,0.810459,"tion (Lapata and Lascarides, 2003; Kim and Baldwin, 2006), 5.2. Type-level Classification Once again, we have discussed a number of type-level DLA tasks above, including: subcategorisation learning, countability learning, type-level unknown word lexical type prediction for precision grammars, grammatical gender prediction, and ontology induction and alignment. Other examples include word sense discrimination (Sch¨utze, 1998; Agirre and Soroa, 2007), semantic relation harvesting (Hearst, 1992; Pantel and Pennacchiotti, 2006; Snow et al., 2006), learning qualia structure (Bouillon et al., 2002; Yamada and Baldwin, 2004), and extraction of multiword expressions (Baldwin, 2005b; Baldwin, 2005d; Villada Moir´on, 2005). 6. Conclusion This paper has presented a broad survey of research on deep lexical acquisition, particularly in the context of precision grammars. We developed a classification of DLA methods along three broad axes: general-purpose vs. targeted, in vitro vs. in vivo, and token- vs. type-level classification. We hope this will provide a framework for continued cross-fertilisation across tasks and methodologies in the area. Acknowledgements This paper builds off research with a number of collaborato"
Y07-1001,U05-1006,0,0.0191992,"ed to form a consolidated lexical entry for that word. That is, rather than relying on indirect feature signatures to perform lexical acquisition, the DLR itself drives the incremental learning process. Supertagging is also a classic instance of in vivo DLA, as all of its feature extraction and optimisation over label sequences is based on the target DLR, with the qualification that we classify a precision grammar and its associated treebank, e.g., as a single DLR due to the tight linkage between the two (Oepen et al., 2002; Hockenmaier and Steedman, 2007). In an intriguing piece of research, Zhang and Kordoni (2005) combine these two in a two-stage in vivo token-level DLA system for unknown words in precision grammars. First, a targeted DLA system is run over each unknown word in a given sentence to predict an N -best list of lexical type hypotheses (similarly to a supertagger, but without the sequential learning element). These are passed onto the target precision grammar, which generates a parse forest for the given sentence. The final selection of lexical type is then made by the grammar proper in unpacking the preferred parse from the parse tree, based on the parse selection module. Once again, it is"
Y07-1001,J99-2004,0,\N,Missing
Y07-1001,S07-1002,0,\N,Missing
Y07-1001,W02-1503,0,\N,Missing
Y07-1001,J06-2001,0,\N,Missing
Y07-1001,E03-1073,0,\N,Missing
Y07-1001,W06-1620,1,\N,Missing
Y07-1001,A97-1052,0,\N,Missing
Y07-1001,J93-2002,0,\N,Missing
Y07-1001,P98-1013,0,\N,Missing
Y07-1001,C98-1013,0,\N,Missing
Y07-1001,P93-1032,0,\N,Missing
Y07-1001,W05-1008,1,\N,Missing
Y07-1001,P03-1059,1,\N,Missing
Y07-1001,W07-2205,1,\N,Missing
Y07-1001,bouillon-etal-2002-acquisition,0,\N,Missing
Y11-1031,W06-3812,0,0.11262,"ey of systems developed for part-of-speech (POS) induction by Christodoulopoulos et al. (2010) shows that these systems are notoriously difficult to evaluate due to the nature of the task. Rather than focusing on the individual learners developed in these systems, for our purposes, we observe that two main feature types were utilised in all the systems that were surveyed: morphological, and collocational (or pseudo-syntactic). Of the seven systems surveyed, five developed systems based on solely on collocational or distributional properties of tokens in a text, while two systems (Clark, 2003; Biemann, 2006) additionally included morphological features. Relevant to the morphological features we develop in this study is the research of Goldsmith (2001), and his use of the term signatures to mean a collection of morphological patterns relevant to a stem. He employs the MDL (minimum description length) algorithm as a way of discovering and grouping verbs that inflect in the same way. Although this field focuses on collocational or distributional features that emulate syntactic position, Goldsmith’s work focuses solely on morphological patterns as a way of grouping inflectional categories. Rather tha"
Y11-1031,D10-1056,0,0.0153076,"gument and predicate) must be attributable to the function of that position.” III . B IDIRECTIONALITY “[T]o establish that there is just a single word class, it is not enough for Xs to be usable as Ys without modification: it must also be the case that Ys are usable as Xs.” Figure 1: Criteria for determining word classes by Evans and Osada (2005) otherwise known as clustering, and then assigning labels after the fact. However in this study we use linguistic features in order to induce linguistically motivated clusters. A recent survey of systems developed for part-of-speech (POS) induction by Christodoulopoulos et al. (2010) shows that these systems are notoriously difficult to evaluate due to the nature of the task. Rather than focusing on the individual learners developed in these systems, for our purposes, we observe that two main feature types were utilised in all the systems that were surveyed: morphological, and collocational (or pseudo-syntactic). Of the seven systems surveyed, five developed systems based on solely on collocational or distributional properties of tokens in a text, while two systems (Clark, 2003; Biemann, 2006) additionally included morphological features. Relevant to the morphological fea"
Y11-1031,E03-1009,0,0.266581,"A recent survey of systems developed for part-of-speech (POS) induction by Christodoulopoulos et al. (2010) shows that these systems are notoriously difficult to evaluate due to the nature of the task. Rather than focusing on the individual learners developed in these systems, for our purposes, we observe that two main feature types were utilised in all the systems that were surveyed: morphological, and collocational (or pseudo-syntactic). Of the seven systems surveyed, five developed systems based on solely on collocational or distributional properties of tokens in a text, while two systems (Clark, 2003; Biemann, 2006) additionally included morphological features. Relevant to the morphological features we develop in this study is the research of Goldsmith (2001), and his use of the term signatures to mean a collection of morphological patterns relevant to a stem. He employs the MDL (minimum description length) algorithm as a way of discovering and grouping verbs that inflect in the same way. Although this field focuses on collocational or distributional features that emulate syntactic position, Goldsmith’s work focuses solely on morphological patterns as a way of grouping inflectional catego"
Y11-1031,U09-1007,1,0.7156,"signature as possible for each lexeme. For this, we used the same morphological analyser as provided the source of the stem lexicon, but altered the analyser to remove all distinctions between word classes, to avoid biasing the clustering results. For example, we defined the (verbal) circumfix peN+..+an (e.g. such as pem+bakar+an peN+burn+an, meaning “the process of incinerating”) to be able to flank any stem. This leads to uninhibited over-generation, but it also enables the possibility of analysing all classes of stems in a uniform manner. We obtained the original morphological analyser by Mistica et al. (2009), which was built using X FST (Beesley and Karttunen, 2003) and includes the 10,687 lexicalised stems from Table 1. We then merged all stems in the lexicon into a single undifferentiated word class. The set of affixes the morphological analyser can process is outlined in Figure 2. 5 Method 5.1 Feature Engineering We perform two kinds of experiments, based on two distinct feature representations: multinomial and binomial. Multinomial features take into account the number of occurrences of a morphological pattern, and the binary features record the existence of a particular morphological pattern"
Y11-1031,C00-2137,0,0.126131,"Missing"
Y11-1031,J01-2001,0,\N,Missing
Y11-1039,N03-1020,0,0.0738015,"80. To expose each participant to as many summarisation configurations as possible, over their visit, we performed random selection without replacement over the 20 summarisation configurations. Additionally, for each exhibit area, we randomly varied the order in which the “correct” length summary vs. the short and long summaries were presented to the visitor. 4 Results 4.1 Clustering of Summarisation Configurations To determine the relative differentiation in content between the pre-generated summaries for each exhibit area, we performed a pairwise summary comparison using the ROUGE-2 metric (Lin and Hovy, 2003). For each pairing of the 20 summarisation configurations, we averaged across the different summary lengths and exhibit areas to generate an overall similarity. Based on these similarity values, we clustered the summarisation configurations using “oblivious” hierarchical agglomerative clustering over the three attributes of summarisation algorithm (binarised into the individual algorithms), pronoun filtering and content diversification. That is, we calculated the single attribute which leads to the (weighted) purest partitioning of the data at each level of the dendrogram in a bottom-up fashio"
Y11-1039,O01-1003,0,0.026584,"length was not strongly evident in our results. 5 Related Work While multi-document summarisation has been a highly active research area, personalised summarisation over single documents has received considerably less attention. 379 There has been work on personalized document search and summarisation in the medical domain for clinicians and patients, based on semantically-enhanced extraction of snippets and terminology standardisation (McKeown et al., 2001; McKeown et al., 2003). Here, however, the personalisation was at the level of two discrete user profiles, and not truly individualised. Radev et al. (2001) present the design of a search engine which supports recommendation, clustering, and personalised summarisation, but do not include any technical details or evaluation. Goren-Bar and Prete (2005) report on a preliminary situated experiment on content delivery in a museum, but found that the simple static text delivery method was preferred to the adaptive method. Berkovsky et al. (2008) present a method for generating summaries of different length, and demonstrate a correlation between the level of user interest in a topic area, and the preferred summary length. However, their method relies on"
Y11-1039,J02-4001,0,0.0509272,"s generated for different exhibit areas. A primary interest in this research is the determination of the utility of established multi-document summarisation algorithms for our novel summarisation task. In the following sections, we outline the summarisation algorithms trialled in this research, describe how we personalise summary length, and outline a simple method for content diversification, to avoid repetition in the summaries for individual exhibit areas. 3.1 Summarisation Algorithms For our experiments, we implemented five standard extractive summarisation algorithms from the literature (Radev et al., 2002): • First-N sentence algorithm (F IRST N): select the first N sentences of each document. • Lead-based algorithm (L EAD): select the first N sentences of each paragraph. • Centroid algorithm (C ENTROID): cluster the document collection using a variant of TF·IDF, and rank sentences through a weighted sum of token weights based on the cluster centroid, a sentence positional weight, and similarity with the first sentence (Radev et al., 2000). • LexRank algorithm (L EX R ANK): cluster the document collection, and rank sentences using a variant of PageRank (Brin and Page, 1998) over the component w"
Y11-1039,W00-0403,0,0.0749099,"vidual exhibit areas. 3.1 Summarisation Algorithms For our experiments, we implemented five standard extractive summarisation algorithms from the literature (Radev et al., 2002): • First-N sentence algorithm (F IRST N): select the first N sentences of each document. • Lead-based algorithm (L EAD): select the first N sentences of each paragraph. • Centroid algorithm (C ENTROID): cluster the document collection using a variant of TF·IDF, and rank sentences through a weighted sum of token weights based on the cluster centroid, a sentence positional weight, and similarity with the first sentence (Radev et al., 2000). • LexRank algorithm (L EX R ANK): cluster the document collection, and rank sentences using a variant of PageRank (Brin and Page, 1998) over the component words (Erkan and Radev, 2004). • Manifold-ranking algorithm (M ANIFOLD): score each sentence based on a manifold-ranking process, and rerank sentences based on a diversity penalty (Wan et al., 2007). Each algorithm was run over the primary and secondary document for a given exhibit area. All sentences were ranked, but only sentences from the primary document (that authored by Melbourne Museum) were candidates for selection in the final sum"
Y12-1005,I11-1100,0,0.0290552,"Missing"
Y12-1005,P11-2008,0,0.0354247,"Missing"
Y12-1005,P12-3005,1,0.788094,"g inconsistencies, the free-form adoption of new terms, and regular violations of English grammar norms. Unsurprisingly, when NLP 58 tools are applied directly to social media data, the results tend to be miserable when compared to data sets such as the Wall Street Journal component of the Penn Treebank. However, there have been recent successes in adapting parsers and POS taggers to social media data (Foster et al., 2011; Gimpel et al., 2011). Additionally, lexical normalisation and other preprocessing strategies have been shown to enhance the performance of NLP tools over social media data (Lui and Baldwin, 2012; Han et al., to appear). Furthermore, social media posts tend to be short and the content highly varied, meaning it is difficult to adapt a tool to the domain, or harness textual context to disambiguate the content. There is also the engineering challenge of real-time processing of the text stream, as much of NLP research is carried out offline with only secondary concern for throughput. As such, we might conclude that social media data is a foe of NLP, in that it challenges traditional assumptions made in NLP research on the nature of the target text and the requirements for real-time respon"
Y12-1005,D12-1039,1,\N,Missing
Y12-1021,W03-1028,0,0.942919,"in the form of either simplex nouns (e.g. library) or noun phrases (e.g. social issue). They have been studied in the past to provide topic-related information for many applications such as text summarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automat"
Y12-1021,W98-0319,0,0.141465,"Congress. The live chats contain 33 online discussions that the Library’s Educational Outreach team hosted for teachers between 2002 and 2006. To define dialogue acts that suit this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many can be found in both spoken and written dialogues based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). For live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000). Ivanovic (2008) proposed 12 dialogue acts applying DAMSL for customer service chats. Given the fact that our live chat forum data is closer to customer service chats in terms of the nature of the data (e.g. question, request, gratitude etc.), we decided to adopt the set from Ivanovic 201 (2008) and added two more dialogue acts – BACKGROUND and OTHER. The list of dialogue acts and examples can be found in Table 1. We selected 15 forums containing at least 200 utterances. The data was first segmented into discourse units, and sentence tokenized. Then, we cleaned the data"
Y12-1021,D10-1084,1,0.745951,"ummarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automatically extract keywords for multi-party live chats. Live chats are essentially text-based dialogues, with less disfluencies than spoken dialogues but greater scope for overlapping utterances and out-of"
Y12-1021,S10-1004,1,0.815205,"ummarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automatically extract keywords for multi-party live chats. Live chats are essentially text-based dialogues, with less disfluencies than spoken dialogues but greater scope for overlapping utterances and out-of"
Y12-1021,P10-2055,0,0.0297937,"ion that POS patterns such as (NN NN) and (JJ NN) are more frequent among keywords. Nguyen and Kan (2007) extracted keywords using structural information such as the document title and section headings derived from scientific articles. Wan and Xiao (2008) used a document clustering method to extract salient words, then utilized those to rank the candidates. Liu et al. (2009) developed an unsupervised method using TF·IDF and variants thereof. The main approach is to cluster the terms with respect to the sub-topics, rank candidates in each cluster, then select top-ranked candidates as keywords. Li et al. (2010) proposed a method based on semantic similarity among n-ary phrases, based on Wikipedia entities and links, and used the weighted GirvanNewman algorithm for candidate ranking. More recently, Kim et al. (2010b) proposed a keyword extraction shared task over scientific articles. Participants used a broad range of features based on document structure, semantic similarity and various document and term heuristics. Keyword extraction has also been carried out on various types of documents. Scientific articles and news articles are often the target of keyword extraction (Hulth, 2003; Nguyen and Kan,"
Y12-1021,N09-1070,0,0.208282,"Missing"
Y12-1021,P98-2188,0,0.13797,"Missing"
Y12-1021,J00-3003,0,0.291157,"Missing"
Y12-1021,C08-1122,0,0.64964,"ent. Hereafter, we will refer to this term as first appearance. The GenEx system (Turney, 1999) employed nine heuristic features based exclusively on morphosyntax, such as word length and phrase frequency. Hulth (2003) used TF·IDF, first appearance and keyphraseness2 as the basis of his method, and added POS tags assigned to candidate terms based on the observation that POS patterns such as (NN NN) and (JJ NN) are more frequent among keywords. Nguyen and Kan (2007) extracted keywords using structural information such as the document title and section headings derived from scientific articles. Wan and Xiao (2008) used a document clustering method to extract salient words, then utilized those to rank the candidates. Liu et al. (2009) developed an unsupervised method using TF·IDF and variants thereof. The main approach is to cluster the terms with respect to the sub-topics, rank candidates in each cluster, then select top-ranked candidates as keywords. Li et al. (2010) proposed a method based on semantic similarity among n-ary phrases, based on Wikipedia entities and links, and used the weighted GirvanNewman algorithm for candidate ranking. More recently, Kim et al. (2010b) proposed a keyword extraction"
Y12-1021,W97-0703,0,\N,Missing
Y12-1021,C98-2183,0,\N,Missing
Y12-1050,P06-1026,0,0.29257,"Missing"
Y12-1050,W04-3240,0,0.672503,"Missing"
Y12-1050,P08-1095,0,0.0280354,"th maximum TF. 3.4 percentage a dialogue act Interaction among Utterances Finally, we investigated the interaction between features proposed in Bangalore et al. (2006) and Kim et al. (2010a). Bangalore et al. (2006) used sentences to provide dialogue act information of previous utterances over spoken dialogues; Kim et al. (2010a) used predicted dialogue acts directly. A major point of difference for us is that our data contains multiple participants; thus, the interactions among utterances tend to be more indirect. Moreover, due to difficulties in utterance disentanglement similarily shown in Elsner and Charniak (2008)), we expect reduced effectiveness over our data of such information (although some degree of interaction exists). However, to partly help with disentanglement, we noticed that some users mentioned the user name(s) of the users they are responding to in their posts, which allows us to identify the utterances they link to. Based on these observations, we tested the five interaction features listed below: • Prev1, Prev2, Prev3: dialogue act(s) or sentence(s) from 1 ∼ 3 previous utterances; • User: a dialogue act or sentence from 1 previous utterance in which the user is the same as the author in"
Y12-1050,W98-0319,0,0.150135,"section describes the data and dialogue act categories in detail. 2.1 Dataset 1: Live Forum Chats tween 2002 and 2006. To define dialogue acts suitable for this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many have been based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997), initially applied to the TRAIN corpus of transcribed spoken task-oriented dialogues. In live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000) and characteristics of conversations. Ivanovic (2008) proposed 12 dialogue acts applying DAMSL to customer service chats. We found that forum chats are not dissimilar to customer service chats in terms of the nature of conversations (e.g. question, request, thanking, etc.), and so decided to adopt the DA set defined by Ivanovic (2008). To the 12 dialogue acts from Ivanovic (2008), we added two further dialogue acts — BACKGROUND and OTHER. BACKGROUND is designed to cover contributions containing information about the participants themselves, which often occurs before dis"
Y12-1050,N09-1072,0,0.0138524,"ialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difficult to reuse existing methods for live chats. For example, spoken dialogue introduces difficulties due to errors"
Y12-1050,D10-1084,1,0.888648,"g online forums from the USA Library of Congress. We found that, for multi-party dialogues, features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats. 1 Introduction Dialogue Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has rece"
Y12-1050,W10-2923,1,0.644224,"g online forums from the USA Library of Congress. We found that, for multi-party dialogues, features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats. 1 Introduction Dialogue Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has rece"
Y12-1050,N06-1047,0,0.0320268,"e Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have"
Y12-1050,P98-2188,0,0.357409,"he remainder of this section describes the data and dialogue act categories in detail. 2.1 Dataset 1: Live Forum Chats tween 2002 and 2006. To define dialogue acts suitable for this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many have been based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997), initially applied to the TRAIN corpus of transcribed spoken task-oriented dialogues. In live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000) and characteristics of conversations. Ivanovic (2008) proposed 12 dialogue acts applying DAMSL to customer service chats. We found that forum chats are not dissimilar to customer service chats in terms of the nature of conversations (e.g. question, request, thanking, etc.), and so decided to adopt the DA set defined by Ivanovic (2008). To the 12 dialogue acts from Ivanovic (2008), we added two further dialogue acts — BACKGROUND and OTHER. BACKGROUND is designed to cover contributions containing information about the participants themselves, which"
Y12-1050,J00-3003,0,0.357584,"Missing"
Y12-1050,D11-1002,1,0.846245,"et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difficult to reuse existing methods for live chats. For example, spoken dialogue introduces difficulties due to errors inherent in speech recognition output, but allows acoustic and prosodic features to be leveraged (e.g. Stolck"
Y12-1050,C00-2137,0,0.0120644,"ine are bold-faced. MENT) caused confusion. Tables 7 and 8 show the the performance of each label produced by stemmed unigram, keywords, TextUserL features. We observed that some dialogue acts, such as EXPRESSION, OPENING, THANKING, are relatively easy to detect; others, such as NO-ANSWER, REQUEST, RESPONSE-ACK are hard to predict accurately. We also noticed that the lower recall produced the lower F-score for those dialogue acts which are hard to detect. Finally, we conducted randomized estimation to calculate whether any performance differences between methods are statistically significant (Yeh, 2000). We found that the keyword features led to statistically significant improvements over the base470 line system (p < 0.05). 6 Conclusion We have investigated the task of classifying dialogue acts in multi-party chats, and proposed features to automatically classify dialogue acts based on context, structure, keyword, and interactions among utterances. We found that the system using contextual and keyword features performed the best. Further, we have shown that features from structure and interactions did not perform well, unlike their effectiveness over 1-on-1 live chats in Kim et al. (2010a)."
Y12-1050,C98-2183,0,\N,Missing
Y12-1052,P03-1059,1,0.784249,"nd Brill, 2001), selecting between target candidates for machine translation (Grefenstette, 1998), and determining the semantic gender of nouns in context (Bergsma et al., 2009). All of these were based on English data. Lapata and Keller (2005) examine a range of English tasks whereby frequencies of events can be used as evidence for the disambiguation. They assert that using Web page counts as a de facto corpus is a model that is generally as good as, or better than, established results in the field. Lapata and Keller also examine a type-level task: that of the countability of English nouns (Baldwin and Bond, 2003). In this type of task, the token context is not available, and context must instead be generated to observe evidence. They construct a set of surface cues — much and many to disambiguate mass and count nouns respectively — and extract evidence from these. The performance is good, but not as high as that which Baldwin and Bond observe by using more sophisticated tools such as chunkers. A similar experiment was performed by Nicholson and Baldwin (2009), for a set of about 50 count classifiers in Malay; again, the Web was observed to be a strong performer for observing useful evidence. 483 As fo"
Y12-1052,W05-1008,1,0.811457,"nimelb.edu.au Abstract We present a case study on applying common methods for the prediction of lexical properties to a low-resource language, namely Wambaya. Leveraging a small corpus leads to a typical high-precision, low-recall system; using the Web as a corpus has no utility for this language, but a machine learning approach seems to utilise the available resources most effectively. This motivates a semi-supervised approach to lexicon extension. 1 Introduction Deep lexical acquisition (DLA) is the process of (semi-)automatically creating or extending linguistically-rich lexical resources (Baldwin, 2005a; Baldwin, 2005b; Baldwin, 2007). Conventionally, DLA has been applied to high-resourced languages such as English, German or Japanese to broaden the coverage of a medium-coverage resource, or enrich existing linguistic annotation in resources. However, it also has tremendous potential in accelerating the documentation of low-density languages, a fact that is often discussed but very rarely delivered on in the literature. This paper attempts to deliver on this promise, and asks the question: how well do standard approaches to DLA perform over low-density languages? For example, one of the sta"
Y12-1052,Y07-1001,1,0.858533,"t a case study on applying common methods for the prediction of lexical properties to a low-resource language, namely Wambaya. Leveraging a small corpus leads to a typical high-precision, low-recall system; using the Web as a corpus has no utility for this language, but a machine learning approach seems to utilise the available resources most effectively. This motivates a semi-supervised approach to lexicon extension. 1 Introduction Deep lexical acquisition (DLA) is the process of (semi-)automatically creating or extending linguistically-rich lexical resources (Baldwin, 2005a; Baldwin, 2005b; Baldwin, 2007). Conventionally, DLA has been applied to high-resourced languages such as English, German or Japanese to broaden the coverage of a medium-coverage resource, or enrich existing linguistic annotation in resources. However, it also has tremendous potential in accelerating the documentation of low-density languages, a fact that is often discussed but very rarely delivered on in the literature. This paper attempts to deliver on this promise, and asks the question: how well do standard approaches to DLA perform over low-density languages? For example, one of the standard approaches to DLA is to ext"
Y12-1052,P01-1005,0,0.0124139,"cur in Nordlinger’s descriptive grammar, combining the inline linguistic examples and eight provided transcribed texts. These amount to 1131 unique sentences (many of the sentences from the text were also used as linguistic examples): about a third of these were from the texts. We used these sentences — without the syntactico-semantic annotation from the treebank — as a raw corpus of Wambaya. 2.2 Lexical properties The analysis of lexical information is often done on individual tokens, often under the banner of “lexical disambiguation”. Some examples are context-sensitive spelling correction (Banko and Brill, 2001), selecting between target candidates for machine translation (Grefenstette, 1998), and determining the semantic gender of nouns in context (Bergsma et al., 2009). All of these were based on English data. Lapata and Keller (2005) examine a range of English tasks whereby frequencies of events can be used as evidence for the disambiguation. They assert that using Web page counts as a de facto corpus is a model that is generally as good as, or better than, established results in the field. Lapata and Keller also examine a type-level task: that of the countability of English nouns (Baldwin and Bon"
Y12-1052,I05-2035,0,0.0154998,"lural. In this work, we examine the four grammatical genders: semantically, class I and class II loosely correspond to masculine and feminine animates, class III to non-flesh food items and some round body parts, and class IV to the semantic residue. Gender morphology in Wambaya is mostly regular, but this is less true in other Australian languages, often because of vowel harmony, so we focus primarily on morphosyntax. Nordlinger’s grammar has been implented in a Head-driven Phrase Structure Grammar (HPSG; Bender (2008)) as part of an analysis of the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005; Drellishak and Bender, 2005; Bender et al., 2010). We use the lexical items from the HPSG lexicon to construct a set of nominal types marked for gender. There are 786 class assignments for 724 distinct nominals; their distribution is shown in Table 1. I II III IV 233 199 51 303 Table 1: Distribution of classes for Wambaya nominals. Most of the multi-class items are animates (humans and animals) that belong to both class I and class II (masculine and feminine). These pairs have the same stem, but different forms in the absolutive, which is the unmarked case from which the lemma is derived. Fo"
Y12-1052,W02-1502,0,0.0151166,"ular, a dual, and a plural. In this work, we examine the four grammatical genders: semantically, class I and class II loosely correspond to masculine and feminine animates, class III to non-flesh food items and some round body parts, and class IV to the semantic residue. Gender morphology in Wambaya is mostly regular, but this is less true in other Australian languages, often because of vowel harmony, so we focus primarily on morphosyntax. Nordlinger’s grammar has been implented in a Head-driven Phrase Structure Grammar (HPSG; Bender (2008)) as part of an analysis of the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005; Drellishak and Bender, 2005; Bender et al., 2010). We use the lexical items from the HPSG lexicon to construct a set of nominal types marked for gender. There are 786 class assignments for 724 distinct nominals; their distribution is shown in Table 1. I II III IV 233 199 51 303 Table 1: Distribution of classes for Wambaya nominals. Most of the multi-class items are animates (humans and animals) that belong to both class I and class II (masculine and feminine). These pairs have the same stem, but different forms in the absolutive, which is the unmarked case from w"
Y12-1052,P08-1111,0,0.109722,"ability of this style of approach. In this work, we take Wambaya as a real-world chronically low-density language, and examine the task of predicting the grammatical gender of nominal lexical items. Wambaya is a nearly extinct language (Gordon, 2005) from the Mirndi group of Australian languages. Like many languages from the Australian family, its complicated syntax and rich morphology makes natural language processing of Wambaya difficult. Unlike many of its neighbours, however, it has been well documented in a descriptive grammar (Nordlinger, 1998) and a Headdriven Phrase Structure Grammar (Bender, 2008). While resources for Wambaya are of little intrinsic value as it is doubtful that new text will be generated in the language, developing these resources is still instructive for parallel development in comparable languages (Warlpiri, for example, has a notable speech community (Gordon, 2005)). Additionally, it provides an invaluable test bed for DLA research, to test the potential of methods over similarly lowdensity languages, and truly test the bounds of DLA for the purposes of language preservation. Lexicon extension for Wambaya is a task comparable to the state of many resources: the avai"
Y12-1052,W09-1116,0,0.0176235,"of the sentences from the text were also used as linguistic examples): about a third of these were from the texts. We used these sentences — without the syntactico-semantic annotation from the treebank — as a raw corpus of Wambaya. 2.2 Lexical properties The analysis of lexical information is often done on individual tokens, often under the banner of “lexical disambiguation”. Some examples are context-sensitive spelling correction (Banko and Brill, 2001), selecting between target candidates for machine translation (Grefenstette, 1998), and determining the semantic gender of nouns in context (Bergsma et al., 2009). All of these were based on English data. Lapata and Keller (2005) examine a range of English tasks whereby frequencies of events can be used as evidence for the disambiguation. They assert that using Web page counts as a de facto corpus is a model that is generally as good as, or better than, established results in the field. Lapata and Keller also examine a type-level task: that of the countability of English nouns (Baldwin and Bond, 2003). In this type of task, the token context is not available, and context must instead be generated to observe evidence. They construct a set of surface cue"
Y12-1052,chrupala-etal-2008-learning,0,0.019853,"the Yahoo! API2 ; and • machine learning using context windows around token instances identified from the corpus. As stated in Section 1, the selection of methods at this level is not intended to reflect any keen insights into Wambaya so much as a standard inventory of DLA methods, which we apply to the task. Note that these features attempt to leverage token–level observations into type–level information; if we were examining token predictions in a tagging framework, then the feature engineering approaches for POS tagging as performed by Hajiˇc and Hladk´a (1997) or morphological analysis in Chrupała et al. (2008) could provide further sophistication. 2 http://developer.yahoo.com/search/ 3.1 Corpus Frequency Corpus frequency-based methods involve identifying lexical cues in the given language, and using observation of the relative frequency of each cue to classify instances. The frequencies are based on a monolingual corpus of the language, in our case, the small set of 1131 unique sentences of Wambaya. We use cues observed from the Wambaya corpus as evidence for the gender of a lexical item. This is based on the intuition that a given nominal will only co-occur with demonstratives that agree in gender"
Y12-1052,N03-1006,0,0.0179978,"483 As for grammatical gender, research has tended to focus on Indo-European languages. Hajiˇc and Hladk´a (1997) examine grammatical gender in Czech as part of the part-of-speech tagging process. In Czech, morphological surface cues on a noun token give a strong indication of gender; more so in a stream of tokens where modifier inflection can also be taken into account. This method would probably also be effective for Wambaya, due to its mostly regular gender morphology. Morphological surface cues were also motivated for lexical semantics of derivational morphology by Light (1996). Finally, Cucerzan and Yarowsky (2003) explore a minimally-supervised approach for the prediction of grammatical gender of a mixture of tokens and types by extracting contextual cues from a seed set of nouns and bootstrapping to morphological cues. Token-level performance is high for the five languages they examine. 3 Methodology Based on standard DLA methodology, we examine three prediction methods for Wambaya nominals: • co-occurrence frequencies with demonstratives from a Wambaya corpus; • co-occurrence frequencies with demonstratives from Web page counts estimated using the Yahoo! API2 ; and • machine learning using context wi"
Y12-1052,A97-1017,0,0.012932,"Missing"
Y12-1052,J03-3005,0,0.0490757,"ne where any non-zero count is treated as a positive classification: predictably, a small boost in recall is traded off with a small drop in precision. Across the Wambaya corpus, these differences are not statistically significant at the 0.05 level, and are not reported in detail. 3.2 Web-as-Corpus Frequency The methodology for using the Web as a corpus is very similar to the corpus frequency approach, except that page count estimates returned by a search engine are used in place of actual observed instances. The assumption that these values are strongly correlated was found to be accurate by Keller and Lapata (2003) for a range of classification tasks. At first glance, using the Web to estimate corpus counts for a language close to extinction is patently absurd, as there is no speech community generat(3) Gulug-ardi ng-u ini alaji. sleep-CAUS ( NF ) 1SG . A - FUT this.I.SG . ABS boy.I.ABS “I’m going to put this boy to bed.” (4) Garnguji nyi-n yabu alaji. many.I.ABS 2SG . A . PR - PROG have boy.I.ABS “You have a lot of kids.” (5) Alangi-nka yana jalyu. boy.I-DAT this.IV.SG . ABS bed.IV.ABS “This is the boy’s bed.” (6) Jawaranya ng-u yidanyi ngaba ng-u yardi yaniya cool drink billycan.II.ABS 1SG . A - FUT g"
Y12-1052,E09-2011,0,0.0304832,"of the class I nominal alaji t−4 cool t−3 gulugardi garnguji drink t−2 ngu nyin ninaka t−1 ini yabu nanga t+1 yana t+2 jalyu t+3 t+4 Table 4: Machine learning features based on the fully-inflected (INFL) forms of alaji, from examples (3)–(6) p1 a p2 al p3 ala p4 alaj s1 i s2 ji s3 aji s4 laji Table 5: Machine learning features based on the prefixes and suffixes of the absolutive form of alaji 485 ing Web documents in that language. However, it may be the case that we actually observe documents which are linguistic descriptions of the target language, and not simply noise3 . The ODIN project (Lewis and Xia, 2009) is an attempt to leverage such linguistic data into resources automatically. An additional reason for performing the experiment is that it is a standard DLA method which is used for higher-density languages, but there is no indication in the literature of how well to expect it to perform over low-density languages. In our case, we experiment with the Yahoo! search engine API. Since the API rate-limits queries, we chose to only examine the ABS nominal set with the four proximal absolutive demonstratives, and the STEM set with the four demonstratives. We continued to contrast the surface cues i"
Y12-1052,P96-1004,0,0.105779,"erving useful evidence. 483 As for grammatical gender, research has tended to focus on Indo-European languages. Hajiˇc and Hladk´a (1997) examine grammatical gender in Czech as part of the part-of-speech tagging process. In Czech, morphological surface cues on a noun token give a strong indication of gender; more so in a stream of tokens where modifier inflection can also be taken into account. This method would probably also be effective for Wambaya, due to its mostly regular gender morphology. Morphological surface cues were also motivated for lexical semantics of derivational morphology by Light (1996). Finally, Cucerzan and Yarowsky (2003) explore a minimally-supervised approach for the prediction of grammatical gender of a mixture of tokens and types by extracting contextual cues from a seed set of nouns and bootstrapping to morphological cues. Token-level performance is high for the five languages they examine. 3 Methodology Based on standard DLA methodology, we examine three prediction methods for Wambaya nominals: • co-occurrence frequencies with demonstratives from a Wambaya corpus; • co-occurrence frequencies with demonstratives from Web page counts estimated using the Yahoo! API2 ;"
Y12-1052,N09-2018,1,0.81528,"od as, or better than, established results in the field. Lapata and Keller also examine a type-level task: that of the countability of English nouns (Baldwin and Bond, 2003). In this type of task, the token context is not available, and context must instead be generated to observe evidence. They construct a set of surface cues — much and many to disambiguate mass and count nouns respectively — and extract evidence from these. The performance is good, but not as high as that which Baldwin and Bond observe by using more sophisticated tools such as chunkers. A similar experiment was performed by Nicholson and Baldwin (2009), for a set of about 50 count classifiers in Malay; again, the Web was observed to be a strong performer for observing useful evidence. 483 As for grammatical gender, research has tended to focus on Indo-European languages. Hajiˇc and Hladk´a (1997) examine grammatical gender in Czech as part of the part-of-speech tagging process. In Czech, morphological surface cues on a noun token give a strong indication of gender; more so in a stream of tokens where modifier inflection can also be taken into account. This method would probably also be effective for Wambaya, due to its mostly regular gender"
Y12-1052,1999.tc-1.8,0,\N,Missing
