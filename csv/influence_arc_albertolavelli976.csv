1995.iwpt-1.18,C90-2034,0,0.0430278,"Missing"
2000.iwpt-1.31,E99-1014,1,0.789194,"ach ment. The approach has been developed as part of LE-FACILE, a successfully completed EU project for multilingual text classification and IE [4]. The approach is currently used in Pinocchio, an environment for developing and running IE applications [2]. The proposed approach has been tested mainly for Italian, but proved to work also for English and partially for Russian. Due to space limitations, in this paper we concentrate exclusively on the issues connected with grammar organization. For further details on the adopted formalism, the parsing approach, and some experimental results, see [3] . Rules are grouped into cascades that are finite , ordered sequences of rules. Cascades represent elementary logical units, in the sense that all the rules in a cascade deal with some specific construction (e.g., subcategorization of verbs) . From a functional point of view a cascade is composed of three segments: s1 contains rules that deal with idiosyncratic cases for the construction at hand; s2 contains rules dealing with the regular cases; s3 contains default rules that fire only when no other rule can be successfully applied. The initial generic grammars for chunking and clause recogni"
2020.lrec-1.259,S12-1051,0,0.0864548,"Missing"
2020.lrec-1.259,S14-2004,0,0.116637,"Missing"
2020.lrec-1.259,Q16-1026,0,0.0214798,"the output vectors of BLSTM into a CRF layer, as it is depicted in Figure 1. For each token in the input sequence, first a character-level representation is computed by a CNN with character embeddings as inputs. Then the characterlevel representation vector is concatenated with the word embedding vector to feed the BLSTM network. The CNN for Character-level Representation is an effective approach to extract morphological information (like the prefix or suffix of a word) from characters of words and encode it into neural representations. In NeuroNLP2 the CNN is similar to the one proposed in (Chiu and Nichols, 2016), except that it uses only character embeddings as inputs, without character type. At the second layer each input sequence is presented both forwards and backwards to a bidirectional LSTM, whose output allows to capture past and future information. LSTMs (Hochreiter and Schmidhuber, 1997) are variants of recurrent neural networks (RNNs) designed to cope with gradient vanishing problems. A LSTM unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwar"
2020.lrec-1.259,doddington-etal-2004-automatic,0,0.253517,"to generate new names (e.g., for food names, salmon tacos is a potential food name given the existence of salmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format ("
2020.lrec-1.259,I05-5002,0,0.0337384,"Missing"
2020.lrec-1.259,P15-1033,0,0.0184464,"M unit is composed of three multiplicative gates which control the proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwards to two separate LSTMs and then to concatenate the output to capture past and future information, respectively. The LSTM’s hidden state takes information only from the past, knowing nothing about the future. However, for many tasks it is beneficial to have access to both past (left) and fu2111 ture (right) contexts. A possible solution, whose effectiveness has been proven by previous work (Dyer et al., 2015), is provided by bi-directional LSTMs (BLSTM). (Ma and Hovy, 2016) apply a dropout layer on both the input and output vectors of the BLSTM. Finally, the third layer implemented in NeuroNLP2 is a Conditional Random Fields (CRF) based decoder, which considers dependencies between entity labels in their context and then jointly decodes the best chain of labels for a given input sentence. For example, in NER with standard IOB annotation, an I-token can not follow an O, a constraint which is captured by the CFR layer. Conditional Random Fields (Lafferty et al., 2001) offer several advantages over h"
2020.lrec-1.259,W07-1401,1,0.279345,"5 1] [1 0 0] 0.345 0.398 0.259 0.174 0.396 0.274 0.357 0.111 - M RR1,3 [-1 0 1] -0.213 -0.028 -0.184 -0.015 -0.018 Test M RR2,3 [0 0.5 1] 0.346 0.231 0.356 0.298 0.318 M RR1 [1 0 0] 0.407 0.179 0.357 0.165 0.132 Table 6: Results on the development and test sets for compatibility relation detection. 6.1. Datasets used for Textual Entailment We have tested the performance of a neural approach, based on BERT, on two RTE datasets available for Italian. RTE3 Italian. This is the Italian translation of the RTE-3 dataset carried out during the EU project EXCITEMENT 7 . The RTE-3 dataset for English (Giampiccolo et al., 2007) consists of 1600 text-hypothesis pairs, equally divided into a development set and a test set. While the length of the hypotheses (h) was the same as in the RTE1a and RTE2 datasets, a certain number of texts (t) were longer than in previous datasets, up to a paragraph. Four applications – namely IE, IR, QA and SUM – were considered as settings or contexts for the pairs generation, and 200 pairs were selected for each application in each dataset. RTE Evalita 2009. This is the dataset developed for Evalita 2009 (Bos et al., 2009) tasks. Pairs of texts have be taken from Italian Wikipedia articl"
2020.lrec-1.259,N10-1061,0,0.0133615,"lmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belonging to the categories in C. As an example, using the IOB format (Inside-Outside-Beginning, (Ramshaw and Marcus, 1995)), the sentence “I would like to order a salami pizza and"
2020.lrec-1.259,N15-1097,0,0.153153,"the results on the DPD dataset of NeuroNLP2 + NNg , compared to the others, show that NNg correctly generalizes nominal entities from the gazetteer, improving both Recall and Precision with respect to the multi-token approach. 5. Lexical Relations among Words This section addresses the capacity of neural models to detect semantic relations (e.g., synonymy, semantic similarity, entailment, compatibility) between words (or phrases, like the nominal expressions described in Section 4.2). We focus our experiments on the compatibility relation, and adopt the definition of compatibility proposed by Kruszewski and Baroni (2015): two linguistic expressions w1 and w2 are compatible iff, in a reasonably normal state of affairs, they can both truthfully refer to the same thing. If they cannot, then they are incompatible. Under this definition compatibility is a symmetric relation, which is different both from subsumption, which in not symmetric, from semantic similarity (Agirre et al., 2012) (two expressions can be compatible although not semantically similar, like aperitif and chips, and from textual entailment (Dagan et al., 2005), as entailment is not a symmetric relation. 5.1. Task definition The task is defined as"
2020.lrec-1.259,N16-1030,0,0.0338423,"Italian. 2.3. A Sequence Labeling Neural Architecture: NeuroNLP2 In this section we introduce NeuroNLP2 (Ma and Hovy, 2016), a reference neural architecture for sequence labeling in NLP that achieved state-of-the-art performance for named entity recognition for English on the ConLL-2003 dataset. Specifically, we describe the most recent implementation of the system in Pytorch distributed by the authors2 . We selected this system not only for its state-ofthe-art performance and for code availability, but also for the peculiar structure of the network, which is common to other works, including (Lample et al., 2016). The system is composed of three layers (Figure 1): (i) a CNN that allows to extract information from the input text without any pre-processing; (ii) a bidirectional LSTM layer that presents each sequence forwards and backwards to two sep1 https://github.com/google-research/bert/ blob/master/multilingual.md 2 https://github.com/XuezheMax/NeuroNLP2 Figure 1: The main NeuroNLP2 structure. Dashed arrows indicate dropout layers applied on both the input and output vectors of BLSTM. arate LSTMs; (iii) a CRF layer that decodes the best label sequence. NeuroNLP2 constructs a neural network model by"
2020.lrec-1.259,P16-1101,0,0.0307357,"proportions of information to forget and to pass on to the next time step. The basic idea is to present each sequence forwards and backwards to two separate LSTMs and then to concatenate the output to capture past and future information, respectively. The LSTM’s hidden state takes information only from the past, knowing nothing about the future. However, for many tasks it is beneficial to have access to both past (left) and fu2111 ture (right) contexts. A possible solution, whose effectiveness has been proven by previous work (Dyer et al., 2015), is provided by bi-directional LSTMs (BLSTM). (Ma and Hovy, 2016) apply a dropout layer on both the input and output vectors of the BLSTM. Finally, the third layer implemented in NeuroNLP2 is a Conditional Random Fields (CRF) based decoder, which considers dependencies between entity labels in their context and then jointly decodes the best chain of labels for a given input sentence. For example, in NER with standard IOB annotation, an I-token can not follow an O, a constraint which is captured by the CFR layer. Conditional Random Fields (Lafferty et al., 2001) offer several advantages over hidden Markov models and stochastic grammars for such tasks, includ"
2020.lrec-1.259,W19-5807,1,0.855983,"Missing"
2020.lrec-1.259,D14-1162,0,0.0826463,"Missing"
2020.lrec-1.259,N18-1202,0,0.0365747,"tc. Three families of word embeddings can be identified: • Bag of words based. The original word order independent models like Word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). • Attention (Transformer) based. Embeddings generated by BERT (Devlin et al., 2018), which has produced state-of-the-art results to date in downstream 2110 tasks like NER, Q&A, classification etc. BERT takes into account the order of words in a sentence but is based on attention mechanism as opposed to sequence models like ELMo. • RNN family based. Sequence models (ELMo) that produce word embeddings (Peters et al., 2018). ELMo uses stacked bidirectional LSTMs to generate word embeddings that have different properties based on the layer that generates them. 2.2. BERT BERT (Devlin et al., 2018) is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pretrained on Wikipedia and BooksCorpus and requires taskspecific fine-tuning. BERT is available pre-trained on domain-specific corpora. E.g., Clinical BERT (BERT pre-trained on a corpus of clinical notes) and sciBERT (Pr"
2020.lrec-1.259,S10-1036,0,0.00948324,"ine tokens of one entity name with tokens of another entity name to generate new names (e.g., for food names, salmon tacos is a potential food name given the existence of salmon and tacos). 3 4 2112 20/04/2018 https://github.com/kyzhouhzau/BERT-NER I O would O like O to O order O a O salami B-FOOD pizza I-FOOD and O two O mozzarella B-FOOD cheese I-FOOD sandwiches I-FOOD Table 2: Example of IOB annotation of food nominal entities. Nominal entity recognition has been approached with systems based on linguistic knowledge, including morphosyntactic information, chunking, and head identification (Pianta and Tonelli, 2010). In the framework of the ACE program (Doddington et al., 2004) there has been several attempts to develop supervised systems for nominal entities (Haghighi and Klein, 2010), which, however, had to face the problem of the scarcity of annotated data, and, for this reason, were developed for few entity types. Similarly to what is done for named entities, nominal entity recognition has been approached as a sequence labeling task. Given an utterance U = {t1 , t2 , ..., tn } and a set of entity categories C = {c1 , c2 , ..., cm }, the task is to label the tokens in U that refer to entities belongin"
2020.lrec-1.259,P19-1493,0,0.0162728,"ted in Devlin et al. (2018). In particular, some aspects of the SENTIPOLC 2016 dataset are difficult to address with BERT. For example, the fact that the dataset is strongly unbalanced, usually an important aspect to take into account with a supervised system like BERT. To reduce this effect we down-sample the most common polarity, but even in this case, the result is not competitive with the state of the art. On the other hand, is important to notice that in both cases (SENTIPOLC 2016 and ABSITA 2018), the models were not fine-tuned on Italian, but only on the task. According to the paper by Pires et al. (2019), multilingual BERT is able to perform some cross-lingual adaptation but it is reasonable to think that in a task more related to semantic a deeper process of fine-tuning is needed. dataset SENTIPOLC 2016 - Task 2 ABSITA 2018 - Task ACD ABSITA 2018 - Task ACP BERT 52.17 74.05 68.13 SotA 66.38 81.08 76.73 Table 8: Application of BERT to Sentiment Analysis. 8. Text Classification Finally, we focus on text classification applied to radiological reports in Italian. Radiological reporting generates a large amount of free-text clinical narratives, a potentially valuable source of information for imp"
2020.lrec-1.259,W18-5446,0,0.0181501,"rmation extraction from Italian texts. We carried on experiments using available datasets on both sequence tagging (i.e., named entity recognition, nominal entity recognition) and classification tasks (i.e., lexical relations among words, semantic relations among sentences, sentiment analysis, text classification). We consider this paper as a contribution in the direction of developing benchmarks encompassing a variety of tasks in order to favour models that share general linguistic knowledge across tasks. This is very much in the spirit of GLUE, the General Language Understanding Evaluation (Wang et al., 2018), a collection of resources for training, evaluating, and analyzing natural language understanding systems. The paper is structured as follows. Section 2 reports basic notions about deep learning for NLP that will be used for our experiments. Sections 3 and 4 focus on sequence tagging tasks, named entity recognition and nominal entity recognition, respectively. Sections 4-7 report on classification tasks: lexical relations, textual entailment, sentiment analysis and text classification. Finally, Section 9 discusses our achievement and proposes work for the future. 2. Deep Learning for NLP This"
2020.smm4h-1.15,N19-1423,0,0.0935494,"Missing"
2020.smm4h-1.15,W18-5904,0,0.185888,"Missing"
A92-1003,P90-1029,0,0.0312712,"Missing"
A92-1003,J87-1005,0,0.0801603,"Missing"
A92-1003,J89-1001,0,0.0407361,"Missing"
A92-1003,P89-1024,0,0.0335843,"Missing"
A92-1003,P88-1003,0,\N,Missing
alicante-etal-2012-treebank,W04-1911,0,\N,Missing
alicante-etal-2012-treebank,E12-1006,0,\N,Missing
alicante-etal-2012-treebank,bosco-etal-2000-building,1,\N,Missing
alicante-etal-2012-treebank,E95-1034,0,\N,Missing
alicante-etal-2012-treebank,W10-1401,0,\N,Missing
alicante-etal-2012-treebank,C00-1006,0,\N,Missing
alicante-etal-2012-treebank,C94-1008,0,\N,Missing
alicante-etal-2012-treebank,P03-1056,0,\N,Missing
alicante-etal-2012-treebank,C10-1045,0,\N,Missing
alicante-etal-2012-treebank,P99-1065,0,\N,Missing
alicante-etal-2012-treebank,D07-1096,0,\N,Missing
alicante-etal-2012-treebank,W03-3017,0,\N,Missing
alicante-etal-2012-treebank,W10-1406,0,\N,Missing
bongelli-etal-2012-corpus,rosenthal-etal-2010-towards,0,\N,Missing
bongelli-etal-2012-corpus,W08-0606,0,\N,Missing
bongelli-etal-2012-corpus,W09-1401,0,\N,Missing
bongelli-etal-2012-corpus,W10-3001,0,\N,Missing
bosco-etal-2008-comparing,W00-1903,0,\N,Missing
bosco-etal-2008-comparing,D07-1099,0,\N,Missing
bosco-etal-2008-comparing,D07-1097,0,\N,Missing
bosco-etal-2008-comparing,W07-1526,1,\N,Missing
bosco-etal-2008-comparing,J04-4004,0,\N,Missing
bosco-etal-2008-comparing,W01-0521,0,\N,Missing
bosco-etal-2008-comparing,W06-2920,0,\N,Missing
bosco-etal-2008-comparing,J03-4003,0,\N,Missing
bosco-etal-2008-comparing,H05-1066,0,\N,Missing
bosco-etal-2008-comparing,P06-3004,0,\N,Missing
bosco-etal-2008-comparing,P03-1056,0,\N,Missing
bosco-etal-2008-comparing,P99-1065,0,\N,Missing
bosco-etal-2008-comparing,P03-1013,0,\N,Missing
bosco-etal-2008-comparing,W04-1501,1,\N,Missing
bosco-etal-2008-comparing,bosco-lombardo-2006-comparing,1,\N,Missing
bosco-etal-2008-comparing,D07-1096,0,\N,Missing
bosco-etal-2008-comparing,H91-1060,0,\N,Missing
bosco-etal-2010-comparing,W06-2920,0,\N,Missing
bosco-etal-2010-comparing,W08-1004,0,\N,Missing
bosco-etal-2010-comparing,P07-1122,1,\N,Missing
bosco-etal-2010-comparing,P09-1008,0,\N,Missing
bosco-etal-2010-comparing,D07-1096,1,\N,Missing
C12-2021,chowdhury-lavelli-2012-evaluation,1,0.884647,"Missing"
C12-2021,E12-1043,1,0.673608,"Missing"
C12-2021,E06-1051,1,0.84618,"Missing"
C12-2021,W06-2202,1,0.856329,"Missing"
C12-2021,D09-1013,0,0.031038,"Missing"
C12-2021,P11-1053,0,0.060374,"Missing"
C12-2021,P10-1013,0,0.0670754,"Missing"
C90-3033,J89-2001,0,0.0571597,"Missing"
C90-3033,J83-3001,0,0.0502944,"Missing"
C90-3033,J83-2002,0,0.0586267,"Missing"
C90-3033,J81-4004,0,0.0432375,"Missing"
C90-3033,E89-1033,0,0.0524865,"Missing"
C90-3033,J81-2002,0,0.0963191,"Missing"
C90-3033,P85-1023,0,0.0573876,"Missing"
C90-3033,P89-1013,0,0.0411504,"Missing"
C90-3033,P82-1016,0,0.0307987,"on two phenomena: intersentential ellipsis and coordination (possibly with gaps). Ellipsis is a very common phenomenon and is frequently encountered in dialogues between persons. Up to the present, studies on natural language interaction with computers generally highlight the frequency of this phenomenon, (see, for example, [Eastman and McLean, 1981]). For this reason, ellipsis has received much attention and different solutions have been proposed based on the mechanism used to analyze the sentence: semantic grammars (the LIFER/LADDER system [Hendrix, 1977]), ATN [Kwasny and Sondheimer, 1981, Weischedel and Sondheimer, 1982], or caseframe instantiation (the XCALIBUR system [Carbonell et al., 1983]). As far as coordination is concerned, it is also frequently considered a phenomenon of ill-fomaedness for the following reasons: - since every pair of constituents of the same syntactic category may be coordinated, if the grammar specifically included all these possibilities it would greatly increase the size of the grammar itself; - a constituent inside a coordination may have gaps (that is, missing elements) that, in general, are not allowed in constituents of the same type. Even in the most purely linguistic area,"
C90-3033,J83-3003,0,0.0389962,"Missing"
C90-3033,J89-1001,1,\N,Missing
C90-3033,P85-1014,0,\N,Missing
C90-3033,C88-1061,0,\N,Missing
C90-3033,P86-1013,0,\N,Missing
C90-3033,J74-3000,0,\N,Missing
C90-3033,J74-2000,0,\N,Missing
C90-3033,J74-1000,0,\N,Missing
chowdhury-lavelli-2012-evaluation,I05-2038,0,\N,Missing
chowdhury-lavelli-2012-evaluation,de-marneffe-etal-2006-generating,0,\N,Missing
chowdhury-lavelli-2012-evaluation,W10-1905,0,\N,Missing
chowdhury-lavelli-2012-evaluation,P03-1054,0,\N,Missing
chowdhury-lavelli-2012-evaluation,P04-1043,0,\N,Missing
chowdhury-lavelli-2012-evaluation,P05-1022,0,\N,Missing
chowdhury-lavelli-2012-evaluation,E12-1043,1,\N,Missing
chowdhury-lavelli-2012-evaluation,W11-0216,1,\N,Missing
E06-1051,H05-1091,0,0.771553,"c treebanks (such as the Genia treebank1 ) 1 401 http://www-tsujii.is.s.u-tokyo.ac.jp/ can be successfully exploited to overcome this problem. Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features. In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b). Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002). The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference. We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data se"
E06-1051,P04-1054,0,0.829939,", such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approaches based on syntactic featur"
E06-1051,P05-1050,1,0.413193,"n general, different from the input space. Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998). In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows K(x1 , x2 ) = hφ(x1 ), φ(x2 )i , kφ(x1 )kkφ(x2 )k (1) where φ(·) is the embedding vector and k · k is the 2-norm. The ker"
E06-1051,C02-1151,0,0.0119689,"clusively on simple linguistic features. In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b). Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002). The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference. We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4). The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and, consequently, from the gro"
E06-1051,P05-1052,0,0.57511,"ence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approaches based on syntactic features from producing any resu"
E06-1052,fillmore-etal-2002-seeing,0,0.0101591,"tifies the template occurrences in sentences. The set of entailing templates may be collected either manually or automatically. We propose this configuration both as an algorithm for RE and as an evaluation scheme for paraphrase acquisition. The role of the syntactic matcher is to identify the different syntactic variations in which templates occur in sentences. Table 1 presents a list of generic syntactic phenomena that are known in the literature to relate to linguistic variability. A phenomenon which deserves a few words of explanation is the “transparent head noun” (Grishman et al., 1986; Fillmore et al., 2002). A transparent noun N1 typically occurs in constructs of the form ‘N1 preposition N2’ for which the syntactic relation involving N1, which is the head of the NP, applies to N2, the modifier. In the example in Table 1, ‘fragment’ is the transparent head noun while the relation ‘activate’ applies to Y as object. 4 4.1 Manual Data Analysis Protein Interaction Dataset Bunescu et al. (2005) proposed a set of tasks regarding protein name and protein interaction extraction, for which they manually tagged about 200 Medline abstracts previously known to contain human protein interactions (a binary sym"
E06-1052,J86-3002,0,0.263055,"ctic matcher which identifies the template occurrences in sentences. The set of entailing templates may be collected either manually or automatically. We propose this configuration both as an algorithm for RE and as an evaluation scheme for paraphrase acquisition. The role of the syntactic matcher is to identify the different syntactic variations in which templates occur in sentences. Table 1 presents a list of generic syntactic phenomena that are known in the literature to relate to linguistic variability. A phenomenon which deserves a few words of explanation is the “transparent head noun” (Grishman et al., 1986; Fillmore et al., 2002). A transparent noun N1 typically occurs in constructs of the form ‘N1 preposition N2’ for which the syntactic relation involving N1, which is the head of the NP, applies to N2, the modifier. In the example in Table 1, ‘fragment’ is the transparent head noun while the relation ‘activate’ applies to Y as object. 4 4.1 Manual Data Analysis Protein Interaction Dataset Bunescu et al. (2005) proposed a set of tasks regarding protein name and protein interaction extraction, for which they manually tagged about 200 Medline abstracts previously known to contain human protein in"
E06-1052,P04-1053,0,0.0120801,"Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE patterns. The goal of the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly. Hasegawa et al. (2004) performs unsupervised hierarchical clustering over a simple set of features. The algorithm does not extract entity pairs for a given relation from a set of documents but rather classifies all relations in a large corpus. This approach is more similar to text mining tasks than to classic IE problems. To conclude, several unsupervised approaches learn relevant IE templates for a complete scenario, but without identifying their relevance to each specific relation within the scenario. Accordingly, the evaluations of these works either did not address the direct applicability for RE or evaluated i"
E06-1052,P02-1006,0,0.0430257,"s sufficient to identify an entailing template since it implies that the target relation holds as well. Under this notion, paraphrases are bidirectional entailment relations. Several methods extract atomic paraphrases by exhaustively processing local corpora (Lin and Pantel, 2001; Shinyama et al., 2002). Learning from a local corpus is bounded by the corpus scope, which is usually domain specific (both works above processed news domain corpora). To cover a broader range of domains several works utilized the Web, while requiring several manually provided examples for each input relation, e.g. (Ravichandran and Hovy, 2002). Taking a step further, the TEASE algorithm (Szpektor et al., 2004) provides a completely unsupervised method for acquiring entailment relations from the Web for a given input relation (see Section 5.1). Most of these works did not evaluate their results in terms of application coverage. Lin and Pantel (2001) compared their results to humangenerated paraphrases. Shinyama et al. (2002) measured the coverage of their learning algorithm relative to the paraphrases present in a given corpus. Szpektor et al. (2004) measured “yield”, the number of correct rules learned for an input re1 See the 3rd"
E06-1052,P05-1047,0,0.00703089,"standard RE setting. Our findings are encouraging for both goals, particularly relative to their early maturity level, and reveal constructive evidence for the remaining room for improvement. 2 2.1 Background Unsupervised Information Extraction Information Extraction (IE) and its subfield Relation Extraction (RE) are traditionally performed in a supervised manner, identifying the different ways to express a specific information or relation. Given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE"
E06-1052,P03-1029,0,0.0139967,"osed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE patterns. The goal of the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly. Hasegawa et al. (2004) performs unsupervised hierarchical clustering over a simple set of features. The algorithm does not extract entity pairs for a given relation from a set of documents but rather classifies all relations in a large corpus. This approach is more similar to text mining tasks than to classic IE problems. To conclude, several unsupervised approaches learn relevant"
E06-1052,W04-3206,1,0.897468,"tion, we require a syntactic matching module that identifies template instances in text. First, we manually analyzed the proteininteraction dataset and identified all cases in which protein interaction is expressed by an entailing template. This set a very high idealized upper bound for the recall of the paraphrase-based approach for this dataset. Yet, obtaining high coverage in practice would require effective paraphrase acquisition and lexical-syntactic template matching. Next, we implemented a prototype that utilizes a state-of-the-art method for learning entailment relations from the web (Szpektor et al., 2004), the Minipar dependency parser (Lin, 1998) and a syntactic matching module. As expected, the performance of the implemented system was much lower than the ideal upper bound, yet obtaining quite reasonable practical results given its unsupervised nature. The contributions of our investigation follow 409 the dual goal set above. To the best of our knowledge, this is the first comprehensive evaluation that measures directly the performance of unsupervised paraphrase acquisition relative to a standard application dataset. It is also the first evaluation of a generic paraphrase-based approach for"
E06-1052,C00-2136,0,0.0273469,"rase-based approach for the standard RE setting. Our findings are encouraging for both goals, particularly relative to their early maturity level, and reveal constructive evidence for the remaining room for improvement. 2 2.1 Background Unsupervised Information Extraction Information Extraction (IE) and its subfield Relation Extraction (RE) are traditionally performed in a supervised manner, identifying the different ways to express a specific information or relation. Given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtr"
E12-1043,P05-1022,0,0.0335084,"evaluation by Chowdhury et al. (2011a) reported that uPT kernels achieve better results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings 6 We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4 . The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips so"
E12-1043,W11-0216,1,0.812019,"1 , R2 ) where KT P W F stands for the new feature based kernel (henceforth, TPWF kernel) computed using flat features collected by exploiting patterns, trigger words, negative cues and walk features. KSL and KP ET stand for the Shallow Linguistic (SL) kernel and the Path-enclosed Tree Proposed TPWF Kernel Figure 1 shows an example of a reduced graph. A reduced graph is an extension of the smallest common subgraph of the dependency graph that aims at overcoming its limitations. It is a known issue that the smallest common subgraph (or subtree) sometimes does not contain cue words. Previously, Chowdhury et al. (2011a) proposed a linguistically motivated extension of the minimal (i.e. smallest) common subtree (which includes the candidate entity pairs), known as Mildly Extended Dependency Tree (MEDT). However, the rules used for MEDT are too constrained. Our objective in constructing the reduced graph is to include any potential modifier(s) or cue word(s) that describes the relation between the given pair of entities. Sometimes such modifiers or cue words are not directly dependent (syntactically) on any 422 BioInfer AIMed IEPA HPRD50 LLL P R F P R F P R F P R F P R F Only walk features 51.8 71.2 60.0 48."
E12-1043,E06-1051,1,0.430288,"walk features (i.e. e-walks and v-walks)2 . 2. The syntactic dependency patterns are automatically collected from a type of dependency subgraph (we call it reduced graph, more details in Section 4.1.1) during runtime. 3. We only use the regex patterns, trigger words and negative cues mentioned in the literature (Ono et al., 2001; Fundel et al., 2007; Bui et al., 2010). The objective is to verify whether we can exploit knowledge which is already known and used. 4. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). The aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models. 2 The syntactic dependencies of the words of a sentence create a dependency graph. A v-walk feature consists of (wordi − dependency typei,i+1 − wordi+1 ), and an ewalk feature is"
E12-1043,P03-1054,0,0.0027712,"orpora used by Tikk et al. (2010) for benchmarking various kernel methods4 . The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules to add some of such dependencies: To measure the contribution of the features collected from the reduced graphs (using dependency patterns, trigger words and negative cues) and regex patterns, we have applied the new TPWF kernel on the 5 PPI corpora before and after using these features. Results shown in Table 2 clearly indicate that usage of these features improve the performance. The improvement of performan"
E12-1043,N10-1004,0,0.00492886,"results for PPI extraction than the other techniques used for tree kernel computation. 425 5 Experimental Settings 6 We have followed the same criteria commonly used for the PPI extraction tasks, i.e. abstractwise 10-fold cross validation on individual corpus and one-answer-per-occurrence criterion. In fact, we have used exactly the same (abstract-wise) fold splitting of the 5 benchmark (converted) corpora used by Tikk et al. (2010) for benchmarking various kernel methods4 . The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Before parsing the sentences, all the entities are blinded by assigning names as EntityX where X is the entity index. In each example, the POS tags of the two candidate entities are changed to EntityX. The parse trees produced by the Charniak-Johnson reranking parser are then processed by the Stanford parser5 (Klein and Manning, 2003) to obtain syntactic dependencies according to the Stanford Typed Dependency format. The Stanford parser often skips some syntactic dependencies in output. We use the following two rules t"
E12-1043,D09-1013,0,0.209536,"PPI task, most of which are kernel based methods. Tikk et al. (2010) reported a benchmark evaluation of various kernels on PPI extraction. An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting con"
E12-1043,P04-1043,0,0.469173,"endency patterns are automatically collected from a type of dependency subgraph (we call it reduced graph, more details in Section 4.1.1) during runtime. 3. We only use the regex patterns, trigger words and negative cues mentioned in the literature (Ono et al., 2001; Fundel et al., 2007; Bui et al., 2010). The objective is to verify whether we can exploit knowledge which is already known and used. 4. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). The aim of our work is to take advantage of different types of information (i.e., dependency patterns, regex patterns, trigger words, negative cues, syntactic dependencies among words and constituent parse trees) and their different representations (i.e. flat features, tree structures and graphs) which can complement each other to learn more accurate models. 2 The syntactic dependencies of the words of a sentence create a dependency graph. A v-walk feature consists of (wordi − dependency typei,i+1 − wordi+1 ), and an ewalk feature is composed of (dependency typei−1,i − wordi − dependency typ"
E12-1043,E06-1015,0,0.497111,"the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. Kim et al. (2010) proposed walk-weighted subsequence kernel using e-walks, partial matches, non-contiguous paths, and different weights for different sub-structures (which are used to capture structural similarities during kernel computation). Miwa et al. (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al., 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). They used multiple parser inputs. The system is regarded as the current state-of-theart PPI extraction system because of its high results on different PPI corpora (see the results in Table 4). As an extension of their work, they boosted system performance by training on multiple PPI corpora instead of on a single corpus and adopting a corpus weighting concept with support vector machine (SVM) which they call SVM-CW (Miwa et al., 2009b). Since most of their results are reported by training on the combination of multiple c"
E12-1043,E06-1052,1,0.792459,"only the X–Y pair should be considered. So, the patterns should be constrained to reduce the number of unwanted matches. For example, they could be applied on smaller linguistic units than full sentences. Secondly, different techniques could be used to identify less-informative syntactic dependencies inside dependency patterns to make them more accurate and effective. Thirdly, usage of automatically collected paraphrases of regular expression patterns instead of the patterns directly could be also helpful. Weakly supervised collection of paraphrases for RE has been already investigated (e.g. Romano et al. (2006)) and, hence, can be tried for improving the TPWF kernel (which is a component of the proposed hybrid kernel). Acknowledgments 7 Conclusion In this paper, we have proposed a new hybrid kernel for RE that combines two vector based kernels and a tree kernel. The proposed kernel outperforms any of the exiting approaches by a wide margin on the BioInfer corpus, the largest PPI benchmark corpus available. On the other four smaller benchmark corpora, it performs either better or almost as good as the existing stateof-the art approaches. We have also proposed a novel feature based kernel, called TPWF"
E12-1043,I05-1034,0,0.0252687,"directly quoted from their respective original papers. where KSL , KGC and KLC correspond to SL, global context (GC) and local context (LC) kernels respectively. The GC kernel exploits contextual information of the words occurring before, between and after the pair of entities (to be investigated for RE) in the corresponding sentence; while the LC kernel exploits contextual information surrounding individual entities. 4.3 Path-enclosed tree (PET) Kernel The path-enclosed tree (PET) kernel3 was first proposed by Moschitti (2004) for semantic role labeling. It was later successfully adapted by Zhang et al. (2005) and other works for relation extraction on general texts (such as newspaper do3 Also known as shortest path-enclosed tree (SPT) kernel. main). A PET is the smallest common subtree of a phrase structure tree that includes the two entities involved in a relation. A tree kernel calculates the similarity between two input trees by counting the number of common sub-structures. Different techniques have been proposed to measure such similarity. We use the Unlexicalized Partial Tree (uPT) kernel (Severyn and Moschitti, 2010) for the computation of the PET kernel since a comparative evaluation by Cho"
E91-1006,W90-0207,0,0.013045,"TAG G and a string w as input, and decides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indicates that t"
E91-1006,W89-0205,1,0.884896,"the input string are selected and in the second step the input string is parsed with respect to this set of trees. Another paper by Schabes and Joshi (1989) shows how parsing strategies can take advantage of lexicalization in order to improve parsers' performance. Two major advantages have been discussed in the cited work: grammar filtering (the parser can use only a subset of the entire grammar) and bottom-up information (further constraints are imposed on the way trees can be combined). Given these premises and starting from an already known method for bidirectional CF language recognition (Satta and Stock, 1989), it seems quite natural to propose an anchor-driven bidirectional parser for Lexicalized TAGs that tries to make more direct use of the information contained within the anchors. The algorithm employs a mixed strategy: it works bottom-up from the lexical anchors and then expands (partial) analyses making top-down predictions. Abstract In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented. The algorithm takes advantage of a peculiar characteristic of Lexicalized TAGs, i.e. that each elementary tree is associated with a lexical item, called its anchor. Th"
E91-1006,C88-2121,0,0.0592882,"Missing"
E91-1006,P88-1032,0,0.0318566,"ides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indicates that the part of a n dominated by the nodes between ldot an"
E91-1006,W89-0235,0,0.0650283,"Missing"
E91-1006,P85-1011,0,0.0877865,"a tabular method that accepts a TAG G and a string w as input, and decides whether w e L ( G ) . This is done by recovering (partial) analyses for substrings of w and by combining them. More precisely, the algorithm factorizes analyses of derived trees by employing a specific structure called state. Each state retains a pointer to a node n in some tree ae l u A , along with two additional pointers (called Idol and rdot) to n itself or to Various parsing algorithms for TAGs have been proposed in the literature: the worst-case time complexity varies from O(n 4 log n) (Harbusch, 1990) to O(n 6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and O(n 9) (Schabes and Joshi, 1988). *Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would also like to thank Anne Abeill6. All errors are of course our own. - 27 - its children in a. Let an be a tree obtained from the maximal subtree of a with root n, by means of some adjoining operations. Informally speaking and with a little bit of simplification, the two following cases are possible. First, ff ldot, rdo~n, state s indi"
E91-1006,P90-1036,0,\N,Missing
E99-1014,M98-1030,0,0.0205145,"hod. The lower recall shows difficulties of building complete foreground lexica, a well known fact in IE. Concerning the effectiveness of the IE process, in the Italian application on bond issues the system reached P=80, R--72, F(1)=76 on the 95 Proceedings of EACL '99 texts used for development (33 ANSA agency news, 20 ""II Sole 24 ore"" newspaper articles, 42 Radiocor agency news; 10,472 words in all). Table 4 shows the kind of template used for this application. Effectiveness was automatically calculated by comparing the system results against a user-defined tagged corpus via the MUC scorer (Douthat, 1998). The development cycle of the template application was organised as follows: resources (grammars, lexicon and knowledge base) were developed by carefully inspecting the first 33 texts of the corpus. Then the system was compared against the whole corpus (95 texts) with the following results: Recall=51, Precision=74, F(1)=60. Note that the corpus used for training was composed only by ANSA news, while the test corpus included 20 ""I1 Sole 24 ore"" newspaper articles and 42 Radiocor agency news (i.e., texts quite different from ANSA's in both terminology and length). Finally resources were tuned o"
E99-1014,M95-1014,0,0.0283948,"to extract (and to represent) syntactic relations among elements in the sentence, i.e. grammatical functions and thematic roles. Scenario Template recognition needs the correct treatment of syntactic relations at both sentence and text level (Aone et al., 1998). Full parsing systems are generally able to correctly model syntactic relations, 102 but they tend to be slow (because of huge search spaces) and brittle (because of gaps in the grammar). The use of big grammars partially solves the problem of gaps but worsens the problem of huge search spaces and makes grammar modifications difficult (Grishman, 1995). Grammar modifications are always to be taken into account. Many domain-specific texts present idiosyncratic phenomena that require non-standard rules. Often such phenomena are limited to some cases only (e.g., some types of coordinations are applied to people only and not to organizations). Inserting generic rules for such structures introduces (useless) extra complexity into the search space and - when applied indiscriminately (e.g., on classes other than people) - can worsen the system results. It is not clear how semantic restrictions can be introduced into (big) generic grammars. In this"
E99-1014,W97-1307,0,0.0429923,"the A-structure recognition Tsent is the token associated with [has decided']. [Tsent]dep is integrated with the search space for each unattached modifier (see Section 3.3). The way A-structures are produced is interesting for a number of reasons. First of all generic grammars are used to cope with generic linguistic phenomena at sentence level. Secondly we represent syntactic relations in the sentence (i.e., grammatical functions and thematic roles); such relations allow a better treatment of linguistic phenomena than possible in shallow approaches (Aone et ah, 1998; Proceedings of EACL '99 Kameyama, 1997). The initial generic grammar is designed to cover the most frequent phenomena in a restrictive sense. Additional rules can be added to the grammar (when necessary) for coping with the uncovered phenomena, especially domain-specific idiosyncratic forms. The limited size of the grammar makes modifications simple (the A-structure grammar for Italian contains 66 rules). The deterministic approach combined with the use of sequences, cascades and segments makes grammar modifications simple, as changes in a cascade (e.g., rule addition/modification) influence only the following part of the cascade o"
E99-1014,M98-1012,0,\N,Missing
L18-1279,P16-1231,0,0.0551007,"Missing"
L18-1279,D15-1041,0,0.0606067,"Missing"
L18-1279,P16-1070,0,0.0250876,"ctic function are explicitly selected. Another attempt to properly annotate Web data was made in the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various media, also available in UD format. In this resource, the treatment of Internet-related phenomena mainly entailed the revision of the inventory of dependency relations; in particular, new labels were introduced, that since then became an integral part of the UD scheme2 . Other examples of non-canonical texts annotated in compliance with UD specifications are the Treebank of Learner English (Berzak et al., 2016) and the Singlish treebank (Wang et al., 2017). The first one is a collection of English as a Second Language (ESL) sentences, which thus contains a large number of non-standard syntactic structures due to grammatical errors made by the non-native English speakers. As regards their annotation, the main guiding principle prescribes to follow the literal meaning, emphasizing a syntactic analysis that is more faithful to the observed language usage. This is reflected, for example, in the annotation of a direct object as a non-core predicate dependent, if (wrongly) preceded by a preposition, or co"
L18-1279,E12-1009,0,0.0309641,"pers.plur of avere, ’to have’), we manually inserted the lemma of their standard counterpart; for other out-of-vocabulary words, such as dialectal and foreign terms or unintelligible forms, the lemma remained the same as the word form. Note also that for abbreviations of multiple words we kept the abbreviation in the lemma field as well. Syntactic analysis: this step has been performed first automatically, by training three parsers on Italian standard texts, namely those included in UD Italian v2. The tools used were the graph-based (Bohnet, 2010) and transitionbased (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) MATE parsers, and RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a). The parsing step was performed on the entire resource and relied on the previouslyannotated layers. A first set of 300 tweets was then revised by two independent annotators in order to calculate the inter-annotator agreement (a Cohen’s k = 0.92) and to test the parsers results. Finally, the output that gained the best results (i.e. the one from the transition-based MATE parser) was chosen as the final version, and the two annotators completely revised it. The first part (about 3,500 tweets) of the manuallycorr"
L18-1279,D12-1133,0,0.0504435,"no instead of hanno, 3rd pers.plur of avere, ’to have’), we manually inserted the lemma of their standard counterpart; for other out-of-vocabulary words, such as dialectal and foreign terms or unintelligible forms, the lemma remained the same as the word form. Note also that for abbreviations of multiple words we kept the abbreviation in the lemma field as well. Syntactic analysis: this step has been performed first automatically, by training three parsers on Italian standard texts, namely those included in UD Italian v2. The tools used were the graph-based (Bohnet, 2010) and transitionbased (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) MATE parsers, and RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a). The parsing step was performed on the entire resource and relied on the previouslyannotated layers. A first set of 300 tweets was then revised by two independent annotators in order to calculate the inter-annotator agreement (a Cohen’s k = 0.92) and to test the parsers results. Finally, the output that gained the best results (i.e. the one from the transition-based MATE parser) was chosen as the final version, and the two annotators completely revised it. The first part (about 3,500 twe"
L18-1279,C10-1011,0,0.046825,"or typos and grammatical errors (anno instead of hanno, 3rd pers.plur of avere, ’to have’), we manually inserted the lemma of their standard counterpart; for other out-of-vocabulary words, such as dialectal and foreign terms or unintelligible forms, the lemma remained the same as the word form. Note also that for abbreviations of multiple words we kept the abbreviation in the lemma field as well. Syntactic analysis: this step has been performed first automatically, by training three parsers on Italian standard texts, namely those included in UD Italian v2. The tools used were the graph-based (Bohnet, 2010) and transitionbased (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) MATE parsers, and RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a). The parsing step was performed on the entire resource and relied on the previouslyannotated layers. A first set of 300 tweets was then revised by two independent annotators in order to calculate the inter-annotator agreement (a Cohen’s k = 0.92) and to test the parsers results. Finally, the output that gained the best results (i.e. the one from the transition-based MATE parser) was chosen as the final version, and the two annotators completely"
L18-1279,bosco-etal-2008-comparing,1,0.729799,"ia corpora we are aware of are those of Bosco et al. (2016) (i.e. the PoSTWITA corpus) and Rei et al. (2016), both annotated at PoS level only. The contribution of the work hereby presented aims at filling this gap, by creating a treebank of Italian non-canonical texts retrieved from Twitter. The treebank has been built using as starting data the PoSTWITA corpus (mentioned above), and it has been syntactically annotated in compliance with the Universal Dependencies format. Several goals motivate this work, among these: a) to provide a resource that can be used for parser training on standard (Bosco et al., 2008; Bosco et al., 2010; Bosco and Mazzei, 2013) and non-standard texts (whose results, in turn, can be exploited in sentiment analysis applications1 ), as well as for systematic linguistic analysis related to social media language (similar to what proposed in Hu et al. (2013)); b) in a long-term perspective, to encourage the creation of similar resources in languages other than Italian, supported by the availability of a shared representation format, namely the Universal Dependencies (possibly extended to cover social media linguistic phenomena). As for the second point, that is the choice of th"
L18-1279,bosco-etal-2010-comparing,1,0.862828,"Missing"
L18-1279,W13-2308,1,0.846559,"59 tokens), annotated at PoS level only. The format of the resource, also shown in Figure 1, appears as a two-column text file with tweets identified by their IDs (in the header) and separated by blank lines; each word in the tweet has its own line, which in turn contains two tab-separated fields, for the word form and its Part of Speech respectively. Figure 1: Example of PoSTWITA original format. The corpus was automatically tokenized with an adapted version of the Tweet-NLP tokenizer(Gimpel et al., 2011), PoS-tagged with the TnT tagger (Brants, 2000) trained on the UD Italian treebank v1.3 (Bosco et al., 2013), and then manually corrected. The whole process of conversion into UD and its annotation has been described in Sanguinetti et al. (2017), but for the sake of clarity, we summarize here the main steps we followed in order to get a fully UD-compliant resource. Tokenization: no particular changes have been made in this sense from PoSTWITA to PoSTWITA-UD, except for preposition-article and verb-clitic contractions, that were left as single tokens in PoSTWITA and then splitted into the corresponding syntactic words during conversion into UD. All other tokenization choices remained unchanged in PoS"
L18-1279,A00-1031,0,0.381176,"et (114,967 tokens) and 300 tweets in the test set (4,759 tokens), annotated at PoS level only. The format of the resource, also shown in Figure 1, appears as a two-column text file with tweets identified by their IDs (in the header) and separated by blank lines; each word in the tweet has its own line, which in turn contains two tab-separated fields, for the word form and its Part of Speech respectively. Figure 1: Example of PoSTWITA original format. The corpus was automatically tokenized with an adapted version of the Tweet-NLP tokenizer(Gimpel et al., 2011), PoS-tagged with the TnT tagger (Brants, 2000) trained on the UD Italian treebank v1.3 (Bosco et al., 2013), and then manually corrected. The whole process of conversion into UD and its annotation has been described in Sanguinetti et al. (2017), but for the sake of clarity, we summarize here the main steps we followed in order to get a fully UD-compliant resource. Tokenization: no particular changes have been made in this sense from PoSTWITA to PoSTWITA-UD, except for preposition-article and verb-clitic contractions, that were left as single tokens in PoSTWITA and then splitted into the corresponding syntactic words during conversion into"
L18-1279,D14-1082,0,0.0493399,"Missing"
L18-1279,D16-1238,0,0.035454,"Missing"
L18-1279,W17-6514,0,0.0360764,"Missing"
L18-1279,L16-1248,0,0.119222,"Missing"
L18-1279,W16-1715,0,0.0425409,"Hu et al. (2013)); b) in a long-term perspective, to encourage the creation of similar resources in languages other than Italian, supported by the availability of a shared representation format, namely the Universal Dependencies (possibly extended to cover social media linguistic phenomena). As for the second point, that is the choice of the annotation format, Universal Dependencies is a recent project that has gained broad consensus over the last few years, becoming the reference framework for dependency annotation. Despite the critical points raised, for example, on some annotation choices (Gerdes and Kahane, 2016), or on the crossresource consistency problem (de Marneffe et al., 2017), an ever increasing number of languages and resources have been (and are about to be) made available in this format; also two CoNLL Shared Task have been organized in 2017 and 2018, using UD treebanks as datasets. This highlights the need for a widely recognized standard to refer to, either in the process of creating a resource (from scratch or by conversion) or in the evaluation of NLP tools whose training is based on such a resource. Finally, one more factor that made us lean towards using the UD format is the possibili"
L18-1279,P11-2008,0,0.31957,"Missing"
L18-1279,Q16-1023,0,0.101404,"Missing"
L18-1279,D14-1108,0,0.0344158,"novel resource, while Section 6. closes the paper with few remarks on how we intend to follow up on this work. 2. Related Work Social media texts fall under the broader language variety often referred to as non-canonical, or non-standard, language; its automatic processing and analysis is challenged namely by all those linguistic phenomena that deviate from what is conventionally conceived as the ”norm”, i.e. the standard language. In this section, we mention some of the attempts made in other related resources to tackle these challenges, especially as regards syntactic annotation. Tweebank (Kong et al., 2014) is a corpus that presents a simplified, though linguistically-grounded, dependencybased scheme; the resource consists of unlabeled dependency graphs that allow multiple roots in case a tweet contains more than one utterance, and where just nodes with a syntactic function are explicitly selected. Another attempt to properly annotate Web data was made in the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various media, also available in UD format. In this resource, the treatment of Internet-related phenomena mainly entailed the revision of the i"
L18-1279,P14-1130,0,0.0286619,"nserted the lemma of their standard counterpart; for other out-of-vocabulary words, such as dialectal and foreign terms or unintelligible forms, the lemma remained the same as the word form. Note also that for abbreviations of multiple words we kept the abbreviation in the lemma field as well. Syntactic analysis: this step has been performed first automatically, by training three parsers on Italian standard texts, namely those included in UD Italian v2. The tools used were the graph-based (Bohnet, 2010) and transitionbased (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) MATE parsers, and RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a). The parsing step was performed on the entire resource and relied on the previouslyannotated layers. A first set of 300 tweets was then revised by two independent annotators in order to calculate the inter-annotator agreement (a Cohen’s k = 0.92) and to test the parsers results. Finally, the output that gained the best results (i.e. the one from the transition-based MATE parser) was chosen as the final version, and the two annotators completely revised it. The first part (about 3,500 tweets) of the manuallycorrected corpus was made available in Novem"
L18-1279,W15-4301,0,0.047828,"l media language, Twitter, Italian, Universal Dependencies 1. Introduction The increasing reliance on the popularity of the Internet and social media in every-day life has led, among other things, to the proliferation of the so-called user-generated contents. As one of the most popular social media, Twitter is among the main providers of this type of contents, the usage of which in scientific research ranges from data analysis, sentiment analysis and opinion mining, to language technologies. Often, though, Twitter user-generated contents are not edited and/or revised for grammatical accuracy (Lynn et al., 2015). Furthermore, the limited number of characters for each tweet can stimulate creativity and encourage an innovative and non-standard usage of language conventions. As regards NLP, dealing with this kind of linguistic data presents a series of challenges, which are reflected in the lower output quality of various automatic tools and at different linguistic levels (see, e.g., Gimpel et al.(2011), Foster et al. (2011) and Ritter et al. (2011)). These considerations highlight the need for properly annotated resources to provide adequate coverage of such a phenomenon. This is especially true consid"
L18-1279,W16-3905,0,0.307736,"a presents a series of challenges, which are reflected in the lower output quality of various automatic tools and at different linguistic levels (see, e.g., Gimpel et al.(2011), Foster et al. (2011) and Ritter et al. (2011)). These considerations highlight the need for properly annotated resources to provide adequate coverage of such a phenomenon. This is especially true considering the (relatively) little progress made in this field: for the Italian language in particular, at the time of writing, the only linguistically-annotated social media corpora we are aware of are those of Bosco et al. (2016) (i.e. the PoSTWITA corpus) and Rei et al. (2016), both annotated at PoS level only. The contribution of the work hereby presented aims at filling this gap, by creating a treebank of Italian non-canonical texts retrieved from Twitter. The treebank has been built using as starting data the PoSTWITA corpus (mentioned above), and it has been syntactically annotated in compliance with the Universal Dependencies format. Several goals motivate this work, among these: a) to provide a resource that can be used for parser training on standard (Bosco et al., 2008; Bosco et al., 2010; Bosco and Mazzei, 2"
L18-1279,K17-3014,0,0.0339046,"Missing"
L18-1279,D11-1141,0,0.067368,"analysis and opinion mining, to language technologies. Often, though, Twitter user-generated contents are not edited and/or revised for grammatical accuracy (Lynn et al., 2015). Furthermore, the limited number of characters for each tweet can stimulate creativity and encourage an innovative and non-standard usage of language conventions. As regards NLP, dealing with this kind of linguistic data presents a series of challenges, which are reflected in the lower output quality of various automatic tools and at different linguistic levels (see, e.g., Gimpel et al.(2011), Foster et al. (2011) and Ritter et al. (2011)). These considerations highlight the need for properly annotated resources to provide adequate coverage of such a phenomenon. This is especially true considering the (relatively) little progress made in this field: for the Italian language in particular, at the time of writing, the only linguistically-annotated social media corpora we are aware of are those of Bosco et al. (2016) (i.e. the PoSTWITA corpus) and Rei et al. (2016), both annotated at PoS level only. The contribution of the work hereby presented aims at filling this gap, by creating a treebank of Italian non-canonical texts retrie"
L18-1279,W17-6526,1,0.943934,"tweets identified by their IDs (in the header) and separated by blank lines; each word in the tweet has its own line, which in turn contains two tab-separated fields, for the word form and its Part of Speech respectively. Figure 1: Example of PoSTWITA original format. The corpus was automatically tokenized with an adapted version of the Tweet-NLP tokenizer(Gimpel et al., 2011), PoS-tagged with the TnT tagger (Brants, 2000) trained on the UD Italian treebank v1.3 (Bosco et al., 2013), and then manually corrected. The whole process of conversion into UD and its annotation has been described in Sanguinetti et al. (2017), but for the sake of clarity, we summarize here the main steps we followed in order to get a fully UD-compliant resource. Tokenization: no particular changes have been made in this sense from PoSTWITA to PoSTWITA-UD, except for preposition-article and verb-clitic contractions, that were left as single tokens in PoSTWITA and then splitted into the corresponding syntactic words during conversion into UD. All other tokenization choices remained unchanged in PoSTWITA-UD; this also entailed the occurrence of cases where multiple tokens were kept as a single one, whether mistakenly or on purpose, i"
L18-1279,D17-1002,0,0.0328246,"Missing"
L18-1279,K17-3003,0,0.0302758,"Missing"
L18-1279,K17-3001,1,0.879882,"res due to grammatical errors made by the non-native English speakers. As regards their annotation, the main guiding principle prescribes to follow the literal meaning, emphasizing a syntactic analysis that is more faithful to the observed language usage. This is reflected, for example, in the annotation of a direct object as a non-core predicate dependent, if (wrongly) preceded by a preposition, or conversely, a non-core dependent annotated as predicate argument because of an elided preposition. The second example of UD format applied to non-standard texts is the one presented in Wang et al. (2017) and regarding the syntactically-annotated resource of Colloquial Singapore English (or Singlish)3 , an English-based creole language, frequently used in written forms of Web media. Most of the problems encountered in the annotation of such texts had to do with the treatment of terms and expressions imported from local languages, whose annotation is mainly based on the conventions of such languages rather than English, as well as on topic-prominence phenomena, copula 2 These are discourse, goeswith, list and vocative. The resource is not available in the official UD repository, but here: https"
L18-1279,silveira-etal-2014-gold,0,0.0893837,"”norm”, i.e. the standard language. In this section, we mention some of the attempts made in other related resources to tackle these challenges, especially as regards syntactic annotation. Tweebank (Kong et al., 2014) is a corpus that presents a simplified, though linguistically-grounded, dependencybased scheme; the resource consists of unlabeled dependency graphs that allow multiple roots in case a tweet contains more than one utterance, and where just nodes with a syntactic function are explicitly selected. Another attempt to properly annotate Web data was made in the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various media, also available in UD format. In this resource, the treatment of Internet-related phenomena mainly entailed the revision of the inventory of dependency relations; in particular, new labels were introduced, that since then became an integral part of the UD scheme2 . Other examples of non-canonical texts annotated in compliance with UD specifications are the Treebank of Learner English (Berzak et al., 2016) and the Singlish treebank (Wang et al., 2017). The first one is a collection of English as a Second Language (ESL) sentences"
L18-1279,tamburini-melandri-2012-anita,1,0.797235,"ctively), b) a number of other new labels for nonstandard elements typically found in tweets, such as URLs (URL), email addresses (EMAIL), pictograms (EMO), hashtags (HASHTAG) and mentions (MENTION). In PoSTWITA-UD, ADP A and VERB CLIT were completely removed, because of the splitting of such contractions, while the other Internet-specific tags all conflated into SYM, the tag used for symbols. Any other unconventional token whose tagging was not possible for some reason was assigned a X tag. Lemmatization and morphological analysis: lemmas and mophological features were retrieved using AnIta (Tamburini and Melandri, 2012). However, as expected, the corpus also contains a whole host of non-standard word forms that were not recognized by the lemmatizer. In the spirit of leaving the texts as much intact as possible, we decided not to normalize such forms, which still appear in the resource as they do in the original tweet. On the other hand, in case of abbreviations (ke⇒ che, ’that’), word lengthening (pizzaaaaaaa⇒pizza), capitalization (GOVERNO⇒governo, ’government’), minor typos and grammatical errors (anno instead of hanno, 3rd pers.plur of avere, ’to have’), we manually inserted the lemma of their standard co"
L18-1279,P17-1159,0,0.100772,"ttempt to properly annotate Web data was made in the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various media, also available in UD format. In this resource, the treatment of Internet-related phenomena mainly entailed the revision of the inventory of dependency relations; in particular, new labels were introduced, that since then became an integral part of the UD scheme2 . Other examples of non-canonical texts annotated in compliance with UD specifications are the Treebank of Learner English (Berzak et al., 2016) and the Singlish treebank (Wang et al., 2017). The first one is a collection of English as a Second Language (ESL) sentences, which thus contains a large number of non-standard syntactic structures due to grammatical errors made by the non-native English speakers. As regards their annotation, the main guiding principle prescribes to follow the literal meaning, emphasizing a syntactic analysis that is more faithful to the observed language usage. This is reflected, for example, in the annotation of a direct object as a non-core predicate dependent, if (wrongly) preceded by a preposition, or conversely, a non-core dependent annotated as pr"
L18-1279,D14-1109,0,0.0360955,"Missing"
L18-1279,P14-1019,0,0.016673,"of their standard counterpart; for other out-of-vocabulary words, such as dialectal and foreign terms or unintelligible forms, the lemma remained the same as the word form. Note also that for abbreviations of multiple words we kept the abbreviation in the lemma field as well. Syntactic analysis: this step has been performed first automatically, by training three parsers on Italian standard texts, namely those included in UD Italian v2. The tools used were the graph-based (Bohnet, 2010) and transitionbased (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) MATE parsers, and RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a). The parsing step was performed on the entire resource and relied on the previouslyannotated layers. A first set of 300 tweets was then revised by two independent annotators in order to calculate the inter-annotator agreement (a Cohen’s k = 0.92) and to test the parsers results. Finally, the output that gained the best results (i.e. the one from the transition-based MATE parser) was chosen as the final version, and the two annotators completely revised it. The first part (about 3,500 tweets) of the manuallycorrected corpus was made available in November 2017, as part of"
lavelli-etal-2002-sissa,ide-etal-2000-xces,0,\N,Missing
lavelli-etal-2002-sissa,bird-etal-2000-atlas,0,\N,Missing
lavelli-etal-2002-sissa,A97-1035,0,\N,Missing
lavelli-etal-2002-sissa,prodanof-etal-2000-reusability,1,\N,Missing
lavelli-etal-2002-sissa,macleod-etal-2000-american,0,\N,Missing
N13-1093,W10-1911,1,0.806027,"only the root of the sub-graph. Due to space limitation we refer the readers to the corresponding papers for the description of the above mentioned features and the definition of reduced graph. 767 We further extend the heterogeneous feature set by adding features related to relevant non-target entities (with respect to the relation of interest; henceforth, HF v3). For the purpose of DDI extraction, we deem the presence of DISEASE mentions (which might result as a consequence of a DDI) can provide some clues. So, we use a publicly available state-of-the-art disease NER system called BioEnEx (Chowdhury and Lavelli, 2010) to annotate the DDIExtraction-2011 challenge corpus. For 8 The RE system developed for this work and the created list of trigger words for DDI can be downloaded from https://github.com/fmchowdhury/HyREX . 9 No, not, neither, without, lack, fail, unable, abrogate, absence, prevent, unlikely, unchanged, rarely. 10 For example, “interested” from “... not interested ...”, and “confused” from “... not to be confused ...”. each candidate (drug) mention pair, we add the following features in HF v3: • NTEMinsideSentence: Whether the corresponding sentence contains important non-target entity mention("
N13-1093,E12-1043,1,0.930993,"cue is either “no”, “n’t” or “not”, then we add additional features related to negation scope: • bothEntDependOnImmediateGovernor: Whether the immediate governor (if any) of the negation cue is also governor of a dependency sub-tree (of the dependency graph of the corresponding sentence) that includes both of the candidate mentions. • immediateGovernorIsVerbGovernor: Whether the immediate governor of the negation cue is a verb. • nearestVerbGovernor: The closest verb governor (i.e. parent or grandparent inside the dependency graph), if any, of the negation cue. The latter is the TPWF kernel (Chowdhury and Lavelli, 2012a) from which we use following features: HasTriggerWord, Trigger-X, DepPattern-i, ewalk, v-walk The TPWF kernel extracts the HasTriggerWord, Trigger-X and DepPattern-i features from a subgraph called reduced graph. We also follow this approach with one minor difference. Unlike Chowdhury and Lavelli (2012a), we look for trigger words in the whole reduced graph instead of using only the root of the sub-graph. Due to space limitation we refer the readers to the corresponding papers for the description of the above mentioned features and the definition of reduced graph. 767 We further extend the h"
N13-1093,C12-2021,1,0.547328,"cue is either “no”, “n’t” or “not”, then we add additional features related to negation scope: • bothEntDependOnImmediateGovernor: Whether the immediate governor (if any) of the negation cue is also governor of a dependency sub-tree (of the dependency graph of the corresponding sentence) that includes both of the candidate mentions. • immediateGovernorIsVerbGovernor: Whether the immediate governor of the negation cue is a verb. • nearestVerbGovernor: The closest verb governor (i.e. parent or grandparent inside the dependency graph), if any, of the negation cue. The latter is the TPWF kernel (Chowdhury and Lavelli, 2012a) from which we use following features: HasTriggerWord, Trigger-X, DepPattern-i, ewalk, v-walk The TPWF kernel extracts the HasTriggerWord, Trigger-X and DepPattern-i features from a subgraph called reduced graph. We also follow this approach with one minor difference. Unlike Chowdhury and Lavelli (2012a), we look for trigger words in the whole reduced graph instead of using only the root of the sub-graph. Due to space limitation we refer the readers to the corresponding papers for the description of the above mentioned features and the definition of reduced graph. 767 We further extend the h"
N13-1093,W10-3001,0,0.0573437,"Missing"
N13-1093,E06-1051,1,0.873962,"In this section, we propose a new hybrid kernel, KHybrid , for this purpose. It is defined as follows: KHybrid (R1 , R2 ) = KHF (R1 , R2 ) + KSL (R1 , R2 ) + w * KP ET (R1 , R2 ) 6 These cues usually occur more frequently and generally have larger negation scope than other negation cues. 7 These expressions often provide clues that one of the bioentity mentions negatively influences the level of activity of the other. Here, KHF stands for a new feature based kernel (proposed in this paper) that uses a heterogeneous set of features. KSL stands for the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006). KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant used for the PET kernel. It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. The proposed kernel composition is valid according to the closure properties of kernels. We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. In Stage 2, each candidate drug mention pair represents an instance. 2.2.1 Proposed KHF kernel As mentioned earlier, this proposed kernel uses heterogen"
N13-1093,W09-1401,0,0.182329,"Missing"
N13-1093,W11-1802,0,0.0249213,"Missing"
N13-1093,S12-1035,0,0.0356635,"s to be no empirical evidence supporting its exploitation for the purpose of RE. Even if we could manage to obtain highly accurate automatically detected 1 In the context of event extraction (a closely related task of RE), there have been efforts in BioNLP shared tasks of 2009 and 2011 for (non-mandatory sub-task of) event negation detection (3 participants in 2009; 2 in 2011) (Kim et al., 2009; Kim et al., 2011). The participants approached the sub-task using either pre-defined patterns or some heuristics. 2 This task is popularized by various recently held shared tasks (Farkas et al., 2010; Morante and Blanco, 2012). negation scopes, it is not clear how to feed this information inside the RE approach. Simply considering whether a pair of candidate mentions falls under the scope of a negation cue might not be helpful. In this paper, we propose that the scope of negations can be exploited at two different levels. Firstly, the system would check whether all the target entity3 mentions inside a sentence along with possible relation clues (or trigger words), if any, fall (directly or indirectly) under the scope of a negation cue. If such a sentence is found, then it should be discarded (i.e. candidate mention"
N13-1093,morante-2010-descriptive,0,0.0624409,"s state of the art). 4 Conclusion A major flexibility in the proposed approach is that it does not require a separate dataset (which needs to match the genre of the text to be used for RE) annotated with negation scopes. Instead, the proposed Stage 1 classifier uses the RE training data (which do not have negation scope annotations) to self-supervise itself. Various new features have been exploited (both in stages 1 and 2) that can provide strong indications of the scope of negation cues with respect to the relation to be extracted. The only thing needed is the list of possible negation cues (Morante (2010) includes such a comprehensive list). Our proposed kernel, which has a component that exploits a heterogeneous set of features including negation scope and presence of non-target entities, already obtains better results than previous studies. 769 P R F-score (Thomas et al., 2011) 60.5 71.9 65.7 (Chowdhury et al., 2011) 58.6 70.5 64.0 (Chowdhury and Lavelli, 2011) 58.4 70.1 63.7 (Bjorne et al., 2011) 58.0 68.9 63.0 Proposed KHybrid 60.0 74.3 66.4 KHybrid + Stage 1 baseline 61.8 68.9 65.1 KHybrid + proposed Stage 1 60.0 74.2 66.4 (only training sentences are filtered) KHybrid + proposed Stage 1"
N13-1093,P04-1043,0,0.22965,"t is defined as follows: KHybrid (R1 , R2 ) = KHF (R1 , R2 ) + KSL (R1 , R2 ) + w * KP ET (R1 , R2 ) 6 These cues usually occur more frequently and generally have larger negation scope than other negation cues. 7 These expressions often provide clues that one of the bioentity mentions negatively influences the level of activity of the other. Here, KHF stands for a new feature based kernel (proposed in this paper) that uses a heterogeneous set of features. KSL stands for the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006). KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant used for the PET kernel. It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. The proposed kernel composition is valid according to the closure properties of kernels. We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. In Stage 2, each candidate drug mention pair represents an instance. 2.2.1 Proposed KHF kernel As mentioned earlier, this proposed kernel uses heterogeneous features. The first version of the heterogeneous feature set (hence"
N13-1093,E06-1015,0,0.0421041,"e other. Here, KHF stands for a new feature based kernel (proposed in this paper) that uses a heterogeneous set of features. KSL stands for the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006). KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant used for the PET kernel. It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. The proposed kernel composition is valid according to the closure properties of kernels. We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. In Stage 2, each candidate drug mention pair represents an instance. 2.2.1 Proposed KHF kernel As mentioned earlier, this proposed kernel uses heterogeneous features. The first version of the heterogeneous feature set (henceforth, HF v1) combines features proposed by two previous RE works. The former is Zhou et al. (2005), which uses 51 different features. We select the following 27 of their features for our feature set: WBNULL, WBFL, WBF, WBL, WBO, BM1F, BM1L, AM2F, AM2L, #MB, #WB, CPHBNULL, CPHBFL, CPHBF, CPHBL, CPHBO, CPHBM1F, CPHBM1L, CPHAM2F, CPHA"
N13-1093,P05-1053,0,0.265215,"rnel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. The proposed kernel composition is valid according to the closure properties of kernels. We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. In Stage 2, each candidate drug mention pair represents an instance. 2.2.1 Proposed KHF kernel As mentioned earlier, this proposed kernel uses heterogeneous features. The first version of the heterogeneous feature set (henceforth, HF v1) combines features proposed by two previous RE works. The former is Zhou et al. (2005), which uses 51 different features. We select the following 27 of their features for our feature set: WBNULL, WBFL, WBF, WBL, WBO, BM1F, BM1L, AM2F, AM2L, #MB, #WB, CPHBNULL, CPHBFL, CPHBF, CPHBL, CPHBO, CPHBM1F, CPHBM1L, CPHAM2F, CPHAM2F, CPP, CPPH, ET12SameNP, ET12SamePP, ET12SameVP, PTP, PTPH In addition, HF v1 also includes surrounding tokens within the window of {-2,+2} for each candidate mention. We are unaware of any available list of trigger words for drug-drug interaction. So, we created such a list.8 We extend the heterogeneous feature set by adding features related to the scope of n"
S07-1028,A00-2018,0,0.0155255,"Missing"
S07-1028,W06-1670,0,0.0312251,"Missing"
S07-1028,E06-1051,1,0.719815,"grate information from heterogeneous knowledge sources. All basic kernels, but the tree kernel (see Section 2.1.3), are explicitly calculated as follows Ki (x1 , x2 ) = hφ(x1 ), φ(x2 )i, (2) where φ(·) is the embedding vector. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 2 can be carried out explicitly since the input representations defined below are extremely sparse. 2.1 Syntactic Kernels Syntactic kernels are defined over the whole sentence where the candidate nominals appear. 2.1.1 Global Context Kernel Bunescu and Mooney (2005) and Giuliano et al. (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. Fore-Between Tokens before and between the two entities, e.g. “the head of [ORG], Dr. [P ER]”. Between Only tokens between the two entities, e.g. “[ORG] spokesman [P ER]”. Between-After Tokens between and after the two entities, e.g. “[P ER], a [ORG] professor”. Here, we investigate whether this assumption is also correct for semantic relations between nominals. Our global context kernel operates on the contexts defin"
S07-1028,W06-2909,1,0.834692,"man2 In the literature, it is also called n-spectrum kernel. ual design of attribute-value syntactic features (Moschitti, 2004). A tree kernel KT (t1 , t2 ) evaluates the similarity between two trees t1 and t2 in terms of the number of fragments they have in common. Let Nt be the set of nodes of a tree t and F = {f1 , f2 , . . . , f|F |} be the fragment space of t1 and t2 . Then KT (t1 , t2 ) = P ni ∈Nt1 P nj ∈Nt2 ∆(ni , nj ) , (5) P|F| where ∆(ni , nj ) = k=1 Ik (ni ) × IK (nj ) and Ik (n) = 1 if k is rooted in n, 0 otherwise. For this task, we defined an ad-hoc class of structured features (Moschitti et al., 2006), the Reduced Tree (RT), which can be derived from a sentence parse tree t by the following steps: (1) remove all the terminal nodes but those labeled as relation entities and those POS tagged as verbs, auxiliaries, prepositions, modals or adverbs; (2) remove all the internal nodes not covering any remaining terminal; (3) replace the entity words with placeholders that indicate the direction in which the relation should hold. Figure 1 shows a parse tree and the resulting RT structure. 2.2 Semantic Kernels In (Giuliano et al., 2006), we used the local context kernel to infer semantic informatio"
S07-1028,P04-1043,0,0.20728,", we obtained state-of-the-art results on two biomedical data sets. In addition, promising results have been recently obtained for relations such as work for and org based in in the news domain1 . In this paper, we investigate the use of the above approach to discover semantic relations between nominals. In addition to the original feature representation, we have integrated deep syntactic processing of the global context and semantic information for each candidate nominals using WordNet as external knowledge source. Each source of information is represented by kernel functions. A tree kernel (Moschitti, 2004) is used to exploit the deep syntactic processing obtained using the Charniak parser (Charniak, 2000). On the other hand, bag of synonyms and hypernyms is used to enhance the representation of the candidate nominals. The final system is based on five basic kernel functions (bag-ofwords kernel, global context kernel, tree kernel, supersense kernel, bag of synonyms and hypernyms kernel) linearly combined and weighted under different conditions. The experiments were carried out using support vector machines (Vapnik, 1998) as classifier. We present results on the Classification of Semantic Relatio"
S13-2057,P05-1022,0,0.0194682,"e Vs all) to predict the class label of the extracted DDIs. During this training, all the negative instances from the training data are removed. The filtering techniques described in Sections 2.1 and 2.2 are not used in this stage. The extracted DDIs are assigned a default DDI class label. Once the above models are trained, they are applied on the extracted DDIs from the test data. The class label of the model which has the highest confidence score for an extracted DDI instance is assigned to such instance. 4 Data Pre-processing and Experimental Settings The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Then the parse trees are processed by the Stanford parser (Klein and Manning, 2003) to obtain syntactic dependencies. The Stanford parser often skips some syntactic dependencies in output. We use the rules proposed in Chowdhury and Lavelli (2012a) to recover some of such dependencies. We use the same techniques for unknown characters (if any) as described in Chowdhury and Lavelli (2011). Our system uses the SVM-Light-TK toolkit3 (Moschitti, 2006; Joac"
S13-2057,W10-1911,1,0.315741,"2012a) to recover some of such dependencies. We use the same techniques for unknown characters (if any) as described in Chowdhury and Lavelli (2011). Our system uses the SVM-Light-TK toolkit3 (Moschitti, 2006; Joachims, 1999) for computation of the hybrid kernels. The ratio of negative and positive examples has been used as the value of the costratio-factor parameter. The SL kernel is computed using the jSRE tool4 . The KHF kernel can exploit non-target entities to extract important clues (Chowdhury and Lavelli, 2013). So, we use a publicly available state-of-theart NER system called BioEnEx (Chowdhury and Lavelli, 2010) to automatically annotate both the training and the test data with disease mentions. The DDIExtraction 2013 shared task data include two types of texts: texts taken from the DrugBank database and texts taken from MedLine abstracts. During training we used both types together. 5 Experimental Results Table 1 shows the results of 5-fold cross validation for DDI detection on the training data. As we can see, the usage of the LIS and LII filtering techniques improves both precision and recall. We submitted three runs for the DDIExtraction 2013 shared task. The only difference between the three run"
S13-2057,E12-1043,1,0.684025,"eractions from biomedical literature. The dataset of the shared task is composed by texts from the DrugBank database as well as MedLine abstracts in order to deal with different type of texts and language styles. Participants were asked to not only extract DDIs but also classify them into one of four predefined classes: advise, effect, mechanism and int. A detailed description of the task settings and data can be found in Segura-Bedmar et al. (2013). The system that we used in this shared task combines various techniques proposed in our recent research activities for relation extraction (RE) (Chowdhury and Lavelli, 2012a; Chowdhury and Lavelli, 2012b; Chowdhury and Lavelli, 2013).1 This paper presents the multi-phase relation extraction (RE) approach which was used for the DDI Extraction task of SemEval 2013. As a preliminary step, the proposed approach indirectly (and automatically) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. Then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not fil"
S13-2057,C12-2021,1,0.784144,"eractions from biomedical literature. The dataset of the shared task is composed by texts from the DrugBank database as well as MedLine abstracts in order to deal with different type of texts and language styles. Participants were asked to not only extract DDIs but also classify them into one of four predefined classes: advise, effect, mechanism and int. A detailed description of the task settings and data can be found in Segura-Bedmar et al. (2013). The system that we used in this shared task combines various techniques proposed in our recent research activities for relation extraction (RE) (Chowdhury and Lavelli, 2012a; Chowdhury and Lavelli, 2012b; Chowdhury and Lavelli, 2013).1 This paper presents the multi-phase relation extraction (RE) approach which was used for the DDI Extraction task of SemEval 2013. As a preliminary step, the proposed approach indirectly (and automatically) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. Then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not fil"
S13-2057,N13-1093,1,0.801239,"red task is composed by texts from the DrugBank database as well as MedLine abstracts in order to deal with different type of texts and language styles. Participants were asked to not only extract DDIs but also classify them into one of four predefined classes: advise, effect, mechanism and int. A detailed description of the task settings and data can be found in Segura-Bedmar et al. (2013). The system that we used in this shared task combines various techniques proposed in our recent research activities for relation extraction (RE) (Chowdhury and Lavelli, 2012a; Chowdhury and Lavelli, 2012b; Chowdhury and Lavelli, 2013).1 This paper presents the multi-phase relation extraction (RE) approach which was used for the DDI Extraction task of SemEval 2013. As a preliminary step, the proposed approach indirectly (and automatically) exploits the scope of negation cues and the semantic roles of involved entities for reducing the skewness in the training data as well as discarding possible negative instances from the test data. Then, a state-of-the-art hybrid kernel is used to train a classifier which is later applied on the instances of the test data not filtered out by the previous step. The official results of the t"
S13-2057,E06-1051,1,0.382345,"advance. Interested readers are referred to Chowdhury and Lavelli (2012b) for example and description of how anti-positive governors are automatically collected from the training data. 353 2.3 Hybrid Kernel based RE Classifier As RE classifier we use the following hybrid kernel that has been proposed in Chowdhury and Lavelli (2013). It is defined as follows: KHybrid (R1 , R2 ) = KHF (R1 , R2 ) + KSL (R1 , R2 ) + w * KP ET (R1 , R2 ) where KHF is a feature based kernel (Chowdhury and Lavelli, 2013) that uses a heterogeneous set of features, KSL is the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006), and KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant that allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. We exploit the SVMLight-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. The parameters are tuned by doing 5-fold cross validation on the training data. 3 DDI Type Classification The next step is to classify the extracted DDIs into different categories. We train 4 separate models for each of the DDI types (one Vs all) to predict the"
S13-2057,P03-1054,0,0.026624,"The extracted DDIs are assigned a default DDI class label. Once the above models are trained, they are applied on the extracted DDIs from the test data. The class label of the model which has the highest confidence score for an extracted DDI instance is assigned to such instance. 4 Data Pre-processing and Experimental Settings The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Then the parse trees are processed by the Stanford parser (Klein and Manning, 2003) to obtain syntactic dependencies. The Stanford parser often skips some syntactic dependencies in output. We use the rules proposed in Chowdhury and Lavelli (2012a) to recover some of such dependencies. We use the same techniques for unknown characters (if any) as described in Chowdhury and Lavelli (2011). Our system uses the SVM-Light-TK toolkit3 (Moschitti, 2006; Joachims, 1999) for computation of the hybrid kernels. The ratio of negative and positive examples has been used as the value of the costratio-factor parameter. The SL kernel is computed using the jSRE tool4 . The KHF kernel can exp"
S13-2057,N10-1004,0,0.018039,"all the negative instances from the training data are removed. The filtering techniques described in Sections 2.1 and 2.2 are not used in this stage. The extracted DDIs are assigned a default DDI class label. Once the above models are trained, they are applied on the extracted DDIs from the test data. The class label of the model which has the highest confidence score for an extracted DDI instance is assigned to such instance. 4 Data Pre-processing and Experimental Settings The Charniak-Johnson reranking parser (Charniak and Johnson, 2005), along with a self-trained biomedical parsing model (McClosky, 2010), has been used for tokenization, POS-tagging and parsing of the sentences. Then the parse trees are processed by the Stanford parser (Klein and Manning, 2003) to obtain syntactic dependencies. The Stanford parser often skips some syntactic dependencies in output. We use the rules proposed in Chowdhury and Lavelli (2012a) to recover some of such dependencies. We use the same techniques for unknown characters (if any) as described in Chowdhury and Lavelli (2011). Our system uses the SVM-Light-TK toolkit3 (Moschitti, 2006; Joachims, 1999) for computation of the hybrid kernels. The ratio of negat"
S13-2057,P04-1043,0,0.0116531,"ple and description of how anti-positive governors are automatically collected from the training data. 353 2.3 Hybrid Kernel based RE Classifier As RE classifier we use the following hybrid kernel that has been proposed in Chowdhury and Lavelli (2013). It is defined as follows: KHybrid (R1 , R2 ) = KHF (R1 , R2 ) + KSL (R1 , R2 ) + w * KP ET (R1 , R2 ) where KHF is a feature based kernel (Chowdhury and Lavelli, 2013) that uses a heterogeneous set of features, KSL is the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006), and KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant that allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. We exploit the SVMLight-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. The parameters are tuned by doing 5-fold cross validation on the training data. 3 DDI Type Classification The next step is to classify the extracted DDIs into different categories. We train 4 separate models for each of the DDI types (one Vs all) to predict the class label of the extracted DDIs. During this training, all the negative i"
S13-2057,E06-1015,0,0.0250974,"Chowdhury and Lavelli (2013). It is defined as follows: KHybrid (R1 , R2 ) = KHF (R1 , R2 ) + KSL (R1 , R2 ) + w * KP ET (R1 , R2 ) where KHF is a feature based kernel (Chowdhury and Lavelli, 2013) that uses a heterogeneous set of features, KSL is the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006), and KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004). w is a multiplicative constant that allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus. We exploit the SVMLight-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation. The parameters are tuned by doing 5-fold cross validation on the training data. 3 DDI Type Classification The next step is to classify the extracted DDIs into different categories. We train 4 separate models for each of the DDI types (one Vs all) to predict the class label of the extracted DDIs. During this training, all the negative instances from the training data are removed. The filtering techniques described in Sections 2.1 and 2.2 are not used in this stage. The extracted DDIs are assigned a default DDI class label. Once the above models are trai"
S13-2057,S13-2056,0,0.0558983,"Missing"
S13-2077,esuli-sebastiani-2006-sentiwordnet,0,0.112993,"dalities are possible: (1) Constrained (using the provided training data only; other resources, such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint"
S13-2077,C12-2036,1,0.507685,"itional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 466–470, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics and Lin, 2001) for training SVM models and testing. Moreover, in the preprocessing phase we used TweetNL"
S13-2077,D09-1063,0,0.0989609,"such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International W"
S13-2077,N13-1039,0,0.071406,"Missing"
S13-2077,S13-2052,0,0.0804079,"Missing"
W01-1505,W97-1503,1,0.756719,"coming from the processors are shown. The bottom bar shows which of the processors/filters is currently active (using the IStateMonitor interface described in Section 2.2). 3.2 Integration of processors Differently from the activity of creation and editing of projects, only the final part of the work involved in the integration of processors is accomplished via the SISSA graphical interface (more written in C and running under Windows: (Prodanof et al., 1998; Prodanof et al., 2000)) and the preprocessor and the parser of G EPPETTO (ITC-irst, written in Common Lisp and running under Solaris: (Ciravegna et al., 1997; Ciravegna et al., 1998)). Figure 2: The starting page of SiSSA. precisely, the registration in the Processor Repository of the availability of the processors). In order to make a processor SiSSA-compliant, the following steps are necessary: to provide it with a wrapper so that it communicates via the CORBA IDLs of SISSA; to make a translation between the processor’s native input/output and the corresponding linguistic representation specified by process-data; to register the processor in the Processor Repository using the SiSSA graphical interface; during this step the class of the processor"
W01-1505,A97-1035,0,0.0301259,"iSSA; (3) checking that the chosen architectural hypothesis corresponds to the functional specifications of the given application. 1 Introduction In recent years there has been a growing interest in the commercial deployment of NLP technologies and in infrastructures for sharing NLP tools and resources. Such interest makes more and more urgent the availability of toolsets that allow an easy and quick integration of linguistic resources and modules and the rapid prototyping of NLP applications. An example of the efforts in such a direction is GATE (a General Architecture for Text Engineering, (Cunningham et al., 1997)), which provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. L. Dini and G. Mazzini CELI corso Moncalieri 21 10131 Torino I TALY dini@celi.it mazzini@celi.it This paper presents SiSSA (Sistema integrato di Supporto allo Sviluppo di Applicazioni - Integrated System of Support to Application Development), a project with a twofold aim: the definition of a common metaformalism (called FIST) for the unification of different formalisms for grammar description, and the im"
W01-1505,prodanof-etal-2000-reusability,0,0.031205,"ocessors (left), the connections between processors (middle), and the XSL filters (right). In the lower part of the window the messages coming from the processors are shown. The bottom bar shows which of the processors/filters is currently active (using the IStateMonitor interface described in Section 2.2). 3.2 Integration of processors Differently from the activity of creation and editing of projects, only the final part of the work involved in the integration of processors is accomplished via the SISSA graphical interface (more written in C and running under Windows: (Prodanof et al., 1998; Prodanof et al., 2000)) and the preprocessor and the parser of G EPPETTO (ITC-irst, written in Common Lisp and running under Solaris: (Ciravegna et al., 1997; Ciravegna et al., 1998)). Figure 2: The starting page of SiSSA. precisely, the registration in the Processor Repository of the availability of the processors). In order to make a processor SiSSA-compliant, the following steps are necessary: to provide it with a wrapper so that it communicates via the CORBA IDLs of SISSA; to make a translation between the processor’s native input/output and the corresponding linguistic representation specified by process-data;"
W06-2202,W02-2004,0,0.0757621,"Missing"
W06-2202,W03-0425,0,0.0552721,"Missing"
W06-2202,W04-1213,0,0.0160737,"Extraction (IE) systems based on supervised machine learning techniques, there is usually a tradeoff between carefully tuning the system to specific tasks and domains and having a ”generic” IE system able to obtain good (even if not the topmost) performance when applied to different tasks and domains (requiring a very reduced porting time). Usually, the former alternative is chosen and system performance is often shown only for a very limited number of tasks (sometimes even only for a single task), after a careful tuning. For example, in the Bio-entity Recognition Shared Task at JNLPBA 2004 (Kim et al., 2004) the best performing system obtained a considerable performance improvement adopting domain specific hacks. A second important issue in designing IE systems concerns the fact that usually IE data sets are 9 ever, a key difference between the two algorithms is the capability of SIE to drastically reduce the computation time by exploiting Instance Filtering (Gliozzo et al., 2005a). This characteristic allows scaling from toy problems to real-world data sets making SIE attractive in applicative fields, such as bioinformatics, where very large amounts of data have to be analyzed. 2 A Simple IE sys"
W06-2202,W02-2024,0,0.0816992,"Missing"
W06-2202,W04-1219,0,0.0618768,"Missing"
W06-2202,W03-0419,0,\N,Missing
W10-1911,W04-3111,0,0.0529507,"Missing"
W10-1911,W09-1319,0,0.0224626,"ges in our approach – pre-processing, feature extraction and model training, and post-processing. 3.1 Pre-processing At first, the system uses GeniaTagger7 to tokenize texts and provide PoS tagging. After that, it corrects some common inconsistencies introduced by GeniaTagger inside the tokenized data (e.g. GeniaTagger replaces double inverted commas with 4 However, there are some work on disease recognition in biomedical literature using other techniques such as morphosyntactic heuristic based approach (e.g. MetaMap (Aronson, 2001)), dictionary look-up method and statistical approach (N´ ev´ eol et al., 2009; Jimeno et al., 2008; Leaman et al., 2009). 5 As mentioned in http://banner.sourceforge.net/ 2 6 http://www.computationalmedicine.org/challenge/index.php http://biotext.berkeley.edu/data/dis treat data.html 7 3 https://www.i2b2.org/NLP/Relations/Main.php http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/ 84 two single inverted commas). These PoS tagged tokized data are parsed using Stanford parser8 . The dependency relations provided as output by the parser are used later as features. The tokens are further processed using the following generalization and normalization steps: are derived usin"
W10-1911,P04-1055,0,0.0312669,"isease annotated corpora have been released in the last few years, they have been annotated primarily to serve the purpose of relation extraction and, for different reasons, they 1 The source code of our system is available for download at http://hlt.fbk.eu/people/chowdhury/research 83 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 83–90, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics are not suitable for the development of ML based disease mention recognition systems (Leaman et al., 2009). For example, the BioText (Rosario and Hearst, 2004) corpus has no specific annotation guideline and contains several inconsistencies, while PennBioIE (Kulick et al., 2004) is very specific to a particular sub-domain of diseases. Among other disease annotated corpora, EBI disease corpus (Jimeno et al., 2008) is not annotated with disease mention boundaries which makes it unsuitable for BNER evaluation for diseases. Recently, an annotated corpus, named as Arizona Disease Corpus (AZDC) (Leaman et al., 2009), has been released which has adequate and suitable annotation of disease mentions following specific annotation guidelines. There has been so"
W10-1911,W04-1221,0,0.0171325,"taglucosidase GBA”). Only 28 of them are correctly annotated by our system. The major source of errors, however, concerns abbreviated disease names (e.g. “PNH”). We believe one way to reduce this specific error type is to generate a list of possible abbreviated disease names from the long forms of disease names available in databases such as UMLS Metathesaurus. 6 Why Features for Diseases and Genes/Proteins are not the Same Many of the existing BNER systems, which are mainly tuned for gene/protein identification, use features such as token shape (also known as word class and brief word class (Settles, 2004)), Greek alphabet matching, Roman number matching and so forth. As mentioned earlier, we have done extensive experiments with various feature combinations for the selection of disease specific features. We have observed that many of the features used for gene/protein identification are not equally effective for disease identification. Table 7 shows some of the results of those experiments. This observation is reasonable because gene/protein names are much more complex than entities such as diseases. For example, they often contain punctuation characters (such as parentheses or hyphen), Greek a"
W11-0216,W08-0601,0,0.0674736,"Missing"
W11-0216,H05-1091,0,0.274085,"PPI task, which adopt different PPI annotations. Consequently the experimental results obtained by different approaches are often difficult to compare. Pyysalo et al. (2008) put together these corpora (including the AIMed corpus used in this paper) in a common format for comparative evaluation. Each of these corpora is known as converted corpus of the corresponding original corpus. Several kernel-based RE approaches have been reported to date for the PPI task. These are based on various methods such as subsequence kernel (Lodhi et al., 2002; Bunescu and Mooney, 2006), dependency graph kernel (Bunescu and Mooney, 2005), etc. Different work exploited dependency analyses with different kernel approaches such as bag-of3 http://projects.ldc.upenn.edu/ace/ Also known as shortest path-enclosed tree or SPT (Zhou et al., 2007). 4 words kernel (e.g. Miwa et al. (2009)), graph based kernel (e.g. Kim et al. (2010)), etc. However, there are only few researches that attempted the exploitation of tree kernels on dependency tree structures. Sætre et al. (2007) used DT kernels on AIMed corpus and achieved an F-score of 37.1. The results were far better when they combined the output of the dependency parser with that of a H"
W11-0216,P04-1054,0,0.231621,"RE have drawn a lot of interest in recent years since they can exploit a huge amount of features without an explicit representation. Some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures. Other kernels simply use techniques such as bag-of-words, subsequences, etc. to map the syntactic and contextual information to flat features, and later compute similarity. One variation of tree kernels is the dependency tree (DT) kernel (Culotta and Sorensen, 2004; Nguyen et al., 2009). A DT kernel (DTK) is a tree kernel that is computed on a dependency tree (or subtree). A dependency tree encodes grammatical relations between words in a sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges. The main advantage of a DT in comparison with phrase structure tree (PST) is that the former allows for relating two words directly (and in more compact substructures than PST) even if they are far apart in the corresponding sentence according to their lexical word order. Seve"
W11-0216,D07-1024,0,0.0572078,"e is only one node for each individual word whereas in their constructed trees (please refer to Fig. 6 of Miwa et al. (2009)), a word (that belongs to the shortest path) has as many node representations as the number of dependency relations with other words (those belonging to the shortest path). Perhaps, this redundancy of information might be the reason their approach achieved higher result. In addition to work on PPI pair extraction, there has been some approaches that exploited dependency parse analyses along with kernel methods for identifying sentences that might contain PPI pairs (e.g. Erkan et al. (2007)). In this paper, we focus on finding the best representation based on a single structure. We speculate that this can be helpful to improve the state-of-theart using several combinations of structures and features. As a first step, we decided to use uPTK, which is more robust to overfitting as the description in the next section unveil. 3 Unlexicalized Partial Tree Kernel (uPTK) The uPTK was firstly proposed in (Severyn and Moschitti, 2010) and experimented with semantic role labeling (SRL). The results showed no improvement for such task but it is well known that in SRL lexical information is"
W11-0216,E06-1051,1,0.41627,"our and their results. A possible explanation could be related to parameter settings. Another source of uncertainty is given by the tool for tree kernel computation, which in their case is not mentioned. Moreover, their description of PT and SST (in Figure 1 of their paper) appears to be imprecise: for example, in (partial or complete) phrase structure trees, words can only appear as leaves but in their figure they appear as nonterminal nodes. The comparison with other kernel approaches (i.e. not necessarily tree kernels on DT or PST) shows that there are model achieving higher results (e.g. Giuliano et al. (2006), Kim et al. (2010), Airola et al. (2008), etc). State-of-the-art results on most of the PPI data sets are obtained by the hybrid kernel presented in Miwa et al. (2009). As noted earlier, our work focuses on the design of an effective DTK 131 for PPI that can be combined with others and that can hopefully be used to design state-of-the-art hybrid kernels. 6 Conclusion In this paper, we have proposed a study of PPI extraction from specific biomedical data based on tree kernels. We have modeled and experimented with new kernels and DT structures, which can be exploited for RE tasks in other doma"
W11-0216,P04-1043,1,0.893934,"llowing three dependency structures to be exploited by convolution tree kernels: • Dependency Words (DW) tree: a DW tree is the minimal subtree of a DT, which includes e1 and e2. An extra node is inserted as parent of the corresponding NE, labeled with the NE category. Only words are considered in this tree. • Grammatical Relation (GR) tree: a GR tree is similar to a DW tree except that words are replaced by their grammatical functions, e.g. prep, nsubj, etc. 2 Convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features (Moschitti, 2004). 126 • Grammatical Relation and Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two"
W11-0216,E06-1015,1,0.809134,"produce better results than the DTK on minimal subtrees. The second is that previously proposed DT structures can be further improved by introducing simplified representation of the entities as well as augmenting nodes in the DT tree structure with relevant features. This paper presents an evaluation of the above assumptions. More specifically, the contributions of this paper are the following: • We propose the use of new DT structures, which are improvement on the structures defined in Nguyen et al. (2009) with the most general (in terms of substructures) DTK, i.e. Partial Tree Kernel (PTK) (Moschitti, 2006). • We firstly propose the use of the Unlexicalized PTK (Severyn and Moschitti, 2010) with our dependency structures, which significantly improves PTK. • We compare the performance of the proposed DTKs on PPI with the one of PST kernels and 125 • Finally, we introduce a novel approach (called mildly extended dependency tree (MEDT) kernel1 , which achieves the best performance among various (both DT and PST) tree kernels. The remainder of the paper is organized as follows. In Section 2, we introduce tree kernels and relation extraction and we also review previous work. Section 3 describes the u"
W11-0216,D09-1143,1,0.146382,"est in recent years since they can exploit a huge amount of features without an explicit representation. Some of these approaches are structure kernels (e.g. tree kernels), which carry out structural similarities between instances of relations, represented as phrase structures or dependency trees, in terms of common substructures. Other kernels simply use techniques such as bag-of-words, subsequences, etc. to map the syntactic and contextual information to flat features, and later compute similarity. One variation of tree kernels is the dependency tree (DT) kernel (Culotta and Sorensen, 2004; Nguyen et al., 2009). A DT kernel (DTK) is a tree kernel that is computed on a dependency tree (or subtree). A dependency tree encodes grammatical relations between words in a sentence where the words are nodes, and dependency types (i.e. grammatical functions of children nodes with respect to their parents) are edges. The main advantage of a DT in comparison with phrase structure tree (PST) is that the former allows for relating two words directly (and in more compact substructures than PST) even if they are far apart in the corresponding sentence according to their lexical word order. Several kernel approaches"
W11-0216,I05-1034,0,0.0447003,"structural information in term of sub-structures, providing a viable alternative to flat features (Moschitti, 2004). 126 • Grammatical Relation and Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two entities involved in a relation. Zhou et al. (2007) proposed the so called contextsensitive tree kernel approach based on PST, which expands PET to include necessary contextual information. The expansion is carried out by some heuristics tuned on the target RE task. Nguyen et al. (2009) improved the PET representation by inserting extra nodes for denoting the NE category of the entities inside the subtree. They also used sequence kernels from tree paths, which provided higher accuracy. 2.3 Relation"
W11-0216,D07-1076,0,0.582512,"Words (GRW) tree: a GRW tree is the minimal subtree that uses both words and grammatical functions, where the latter are inserted as parent nodes of the former. Using PTK for the above dependency tree structures, the authors achieved an F-measure of 56.3 (for DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE 2004 corpus3 . Moschitti (2004) proposed the so called pathenclosed tree (PET)4 of a PST for Semantic Role Labeling. This was later adapted by Zhang et al. (2005) for relation extraction. A PET is the smallest common subtree of a PST, which includes the two entities involved in a relation. Zhou et al. (2007) proposed the so called contextsensitive tree kernel approach based on PST, which expands PET to include necessary contextual information. The expansion is carried out by some heuristics tuned on the target RE task. Nguyen et al. (2009) improved the PET representation by inserting extra nodes for denoting the NE category of the entities inside the subtree. They also used sequence kernels from tree paths, which provided higher accuracy. 2.3 Relation Extraction in the biomedical domain There are several benchmarks for the PPI task, which adopt different PPI annotations. Consequently the experime"
W11-0412,W10-1911,1,0.878521,"Missing"
W11-0412,W03-0407,0,0.0781774,"raining GSC with adequate size in several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (Ng and Cardie, 2003; Pierce and Cardie, 2004; McClosky et al., 2006; He and Gildea, 2006). According to Ng and Cardie (2003), self-training is the procedure where a committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently. Only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained. This procedure repeats until a stop condition is met. According to Clark et al. (2003), self-training is a procedure in which “a tagger is retrained on its own labeled cache at each round”. In other words, a single classifier is trained on the initially (gold) annotated data and then applied on a set of unannotated data. Those examples meeting a selection criterion are added to the annotated dataset and the classifier is retrained on this new data set. This procedure can continue for several rounds as required. Co-training is another weakly supervised approach (Blum and Mitchell, 1998). It applies for those tasks where each of the two (or more) sets of features from the initial"
W11-0412,W10-1838,0,0.0301208,"on guidelines. Rebholz-Schuhmann and Hahn (2010c) did an intrinsic evaluation of the SSC where they created an 4 http://jura.wi.mit.edu/entrez gene/ http://www.uniprot.org/ 6 See proceedings of the 1st CALBC Workshop, 2010, Editors: Dietrich Rebholz-Schuhmann and Udo Hahn (http://www.ebi.ac.uk/Rebholzsrv/CALBC/docs/FirstProceedings.pdf) for details. 5 103 SSC and a GSC on a dataset of 3,236 Medline7 abstracts. They were not able to make any specific conclusion whether the SSC is approaching to the GSC. They were of the opinion that SSC annotations are more similar to terminological resources. Hahn et al. (2010) proposed a policy where silver standards can be dynamically optimized and customized on demand (given a specific goal function) using a gold standard as an oracle. The gold standard is used for optimization only, not for training for the purpose of SSC annotation. They argued that the nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an SSC to customize one on their own and not stick to something that is already prefabricated without concrete application in mind. Self-training and co-t"
W11-0412,P06-1043,0,0.0371871,"gued that the nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an SSC to customize one on their own and not stick to something that is already prefabricated without concrete application in mind. Self-training and co-training are two of the existing approaches that have been used for compensating the lack of a training GSC with adequate size in several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (Ng and Cardie, 2003; Pierce and Cardie, 2004; McClosky et al., 2006; He and Gildea, 2006). According to Ng and Cardie (2003), self-training is the procedure where a committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently. Only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained. This procedure repeats until a stop condition is met. According to Clark et al. (2003), self-training is a procedure in which “a tagger is retrained on its own labeled cache at each round”. In other words, a single classifier is trained on the initially (gold) annot"
W11-0412,N03-1023,0,0.0213519,"ing for the purpose of SSC annotation. They argued that the nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an SSC to customize one on their own and not stick to something that is already prefabricated without concrete application in mind. Self-training and co-training are two of the existing approaches that have been used for compensating the lack of a training GSC with adequate size in several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (Ng and Cardie, 2003; Pierce and Cardie, 2004; McClosky et al., 2006; He and Gildea, 2006). According to Ng and Cardie (2003), self-training is the procedure where a committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently. Only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained. This procedure repeats until a stop condition is met. According to Clark et al. (2003), self-training is a procedure in which “a tagger is retrained on its own labeled cache at each round”. In other words, a single cla"
W11-0412,W01-0501,0,0.0165736,"each round”. In other words, a single classifier is trained on the initially (gold) annotated data and then applied on a set of unannotated data. Those examples meeting a selection criterion are added to the annotated dataset and the classifier is retrained on this new data set. This procedure can continue for several rounds as required. Co-training is another weakly supervised approach (Blum and Mitchell, 1998). It applies for those tasks where each of the two (or more) sets of features from the initially (gold) annotated training data is sufficient to classify/annotate the unannotated data (Pierce and Cardie, 2001; Pierce and Cardie, 7 http://www.nlm.nih.gov/databases/databases medline.html 2004; He and Gildea, 2006). As with SSC annotation and self-training, it also attempts to increase the amount of annotated data by making use of unannotated data. The main idea of co-training is to represent the initially annotated data using two (or more) separate feature sets, each called a “view”. Then, two (or more) classifiers are trained on those views of the data which are then used to tag new unannotated data. From this newly annotated data, the most confident predictions are added to the previously annotate"
W11-0412,W04-2405,0,0.0218747,"f SSC annotation. They argued that the nature of diverging tasks to be solved, the levels of specificity to be reached, the sort of guidelines being preferred, etc should allow prospective users of an SSC to customize one on their own and not stick to something that is already prefabricated without concrete application in mind. Self-training and co-training are two of the existing approaches that have been used for compensating the lack of a training GSC with adequate size in several different tasks such as word sense disambiguation, semantic role labelling, parsing, etc (Ng and Cardie, 2003; Pierce and Cardie, 2004; McClosky et al., 2006; He and Gildea, 2006). According to Ng and Cardie (2003), self-training is the procedure where a committee of classifiers are trained on the (gold) annotated examples to tag unannotated examples independently. Only those new annotations to which all the classifiers agree are added to the training set and classifiers are retrained. This procedure repeats until a stop condition is met. According to Clark et al. (2003), self-training is a procedure in which “a tagger is retrained on its own labeled cache at each round”. In other words, a single classifier is trained on the"
W11-0412,rebholz-schuhmann-etal-2010-calbc,0,0.0609581,"Missing"
W17-6526,E12-1009,0,0.0732217,"Missing"
W17-6526,N13-1037,0,0.0633806,"release of Universal Dependencies, with a further revision and/or extension of the annotation manual, if necessary. (12) Then we aim to train statistical parsers using this root newly-created gold standard and compare their redep sults with the ones obtained in other similar experiments (see, e.g. Petrov and McDonald (2012)). We are aware of the debate on the nature of il programma dettagliato : http://t.co/OJq7hBcH NLP results obtained with Twitter-based datasets ( the program detailed : http://t.co/OJq7hBcH ) and their poor generalization with other social media texts (Darling et al., 2012; Eisenstein, 2013). Therefore, in the future we could also attempt On the other hand, a URL may also happen to incorporate texts from different social media to occur within the sentence, as a syntacticallysources and provide a more balanced resource. integrated element. Although we have not Finally, we would also like to widen the debate encountered similar cases in our treebank yet, we on social media text processing by opening this consider the URL as a proper noun and apply work to a multilingual comparison, which would the same annotation criteria described above for be made possible by the UD format, speci"
W17-6526,D12-1133,0,0.0468169,"ewspapers, Wikipedia and legal Italian and European Community sources. Therefore, we performed an outof-domain parsing experiment, by training different systems on this treebank, though being aware that the result would be undermined by the deep differences between the text types included in such resources. For the automatic annotation we used some of the parsers that obtained the best performance in a recent comparative study concerning an Italian dependency treebank (Lavelli, 2016), in particular: • the MATE tools, that include both a graphbased (Bohnet, 2010) and a transition-based parser (Bohnet and Nivre, 2012; Bohnet and Parser MATE graph-based MATE transition-based RBG full -LX 62.53 64.92 64.36 -F 67.05 66.65 67.07 -UD 91.26 91.44 90.16 Table 1: Results of the parsers after the different annotation stages, i.e. with lemmas and languagespecific PoS tags (-LX), and with morphological features as well (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 201"
W17-6526,C10-1011,0,0.0602924,"3a)4 , version 2, which includes texts from newspapers, Wikipedia and legal Italian and European Community sources. Therefore, we performed an outof-domain parsing experiment, by training different systems on this treebank, though being aware that the result would be undermined by the deep differences between the text types included in such resources. For the automatic annotation we used some of the parsers that obtained the best performance in a recent comparative study concerning an Italian dependency treebank (Lavelli, 2016), in particular: • the MATE tools, that include both a graphbased (Bohnet, 2010) and a transition-based parser (Bohnet and Nivre, 2012; Bohnet and Parser MATE graph-based MATE transition-based RBG full -LX 62.53 64.92 64.36 -F 67.05 66.65 67.07 -UD 91.26 91.44 90.16 Table 1: Results of the parsers after the different annotation stages, i.e. with lemmas and languagespecific PoS tags (-LX), and with morphological features as well (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Le"
W17-6526,W13-2308,1,0.944636,"ssing, for Italian in particular, which can be exploited for training NLP systems in order to enhance their performance on social media texts. On the other hand, it may also contribute to the wider debate about social media texts and their analysis, for example by showing how much syntactic information can be helpful for a given NLP task or downstream application; we refer in particular to phenomena such as negation and coordination scope, which, if not correctly detected, can strongly undermine the results obtained e.g. by a sentiment analysis engine in classifying the polarity of a message (Bosco et al., 2013b). From a methodological point of view, our choice to adopt the UD scheme stems from the interest in a dependency-based representation for2 http://universaldependencies.org/ 229 Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages 229-239, Pisa, Italy, September 18-20 2017 mat that has gained full acceptance from the research community over a few years, especially regarding Italian resources. The goal of creating this resource goes hand in hand with that of sharing it and validating its annotation according to a shared standard, such as the one UD"
W17-6526,J96-2004,0,0.441897,"s tree editor. Although their work proceeded independently, some particularly critical phenomena were previously discussed. This allowed to come up with shared guidelines (see Section 7.2). In order to take into account the fact that the outputs of the different parsers can be affected by different errors, the two annotators used as starting dataset the output files from two (of the three used) different parsers, randomly selected. As a result of the first correction phase, the degree of inter-annotator agreement (on relations alone) was calculated, using Cohen’s kappa as the reference index (Carletta, 1996). The agreement at this stage was k = 0.83. Based on this result, and in particular on cases with higher disagreement, a consistency check on the application of the guidelines and a further revision were made (after which the agreement went up to k = 0.92); finally, the corrections of both annotators were merged into a single final file. 7.2 Annotation Guidelines Several phenomena featuring social media texts are poorly treated by existing morphological analyzers and parsing systems. In fact, it can be quite difficult to decide their collocation within a single layer of analysis (syntax, seman"
W17-6526,W12-0601,0,0.0195963,"be ready for the next release of Universal Dependencies, with a further revision and/or extension of the annotation manual, if necessary. (12) Then we aim to train statistical parsers using this root newly-created gold standard and compare their redep sults with the ones obtained in other similar experiments (see, e.g. Petrov and McDonald (2012)). We are aware of the debate on the nature of il programma dettagliato : http://t.co/OJq7hBcH NLP results obtained with Twitter-based datasets ( the program detailed : http://t.co/OJq7hBcH ) and their poor generalization with other social media texts (Darling et al., 2012; Eisenstein, 2013). Therefore, in the future we could also attempt On the other hand, a URL may also happen to incorporate texts from different social media to occur within the sentence, as a syntacticallysources and provide a more balanced resource. integrated element. Although we have not Finally, we would also like to widen the debate encountered similar cases in our treebank yet, we on social media text processing by opening this consider the URL as a proper noun and apply work to a multilingual comparison, which would the same annotation criteria described above for be made possible by t"
W17-6526,L16-1248,0,0.0465829,"ween what should or should not be annotated on the syntactic layer. In fact, emoticons and emojis are typically used to express feelings and emotions, reproduce facial expressions or even convey the intonation of spoken language. Although performing on a more pragmatic, than merely syntactic level, they seem to function in a language-like fashion12 . In this sense, they could then be compared to interjections and other discourse particles. Bearing in mind what UD guidelines suggest for 9 Regarding, in particular, UD-based speech treebanks, we mention here the resource available for Slovenian (Dobrovoljc and Nivre, 2016), and that for French (upcoming) (Gerdes and Kahane, 2017). 10 For the sake of readability, we kept just the more relevant dependency edges and the corresponding relations. 11 See, for example, the study on emojis in Italian language (Chiusaroli, 2015) and the EmojitalianoBot and EmojiWorldBot experiments (Monti et al., 2016) 12 http://blog.oxforddictionaries.com/ 2015/11/emoji-language/ such particles13 , we labelled also emoticons and emojis as discourse items, as in example (1). (3) vocative (1) discourse @ChiaZe93 io non sono d’ accordo ( @ChiaZe93 I disagree ) Hashtags are key words or ph"
W17-6526,P11-2008,0,0.369791,"Missing"
W17-6526,D14-1108,0,0.194708,"ch works is that of Foster et al. (2011), who built a dataset containing 1,000 sentences including tweets and forum posts, with the specific aim of investigating the problems of parsing social media texts. Later on, other works attempted to overcome such limits by creating ah hoc resources to be used as training data for parsing. This is the case of the French Social Media Bank (Seddah et al., 2012), a set of 1,700 sentences from various types of usergenerated content (among those, tweets), annotated using an adapted version of the French Treebank (Abeill´e et al., 2003) scheme, and TWEEBANK (Kong et al., 2014), built by manually adding dependency parses to tweets drawn from the PoS-tagged Twitter corpus of Owoputi et al. (2013). Finally, it is worth mentioning the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various Web media, including blogs, emails, reviews and Yahoo! answers, and also available in UD format. To the best of our knowledge, however, the one presented here is the first work devoted to create a Twitter treebank annotated according to UD specifications, and is almost certainly the first resource of this kind created for Italian. 3 Th"
W17-6526,P14-1130,0,0.0296817,"0) and a transition-based parser (Bohnet and Nivre, 2012; Bohnet and Parser MATE graph-based MATE transition-based RBG full -LX 62.53 64.92 64.36 -F 67.05 66.65 67.07 -UD 91.26 91.44 90.16 Table 1: Results of the parsers after the different annotation stages, i.e. with lemmas and languagespecific PoS tags (-LX), and with morphological features as well (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a), which is based on a lowrank factorization method that enables to map high dimensional feature vectors into low dimensional representations; the full model was chosen. For the near future, we also plan to extend the experiment to other state-of-the-art parsers as well (namely TurboParser (Martins et al., 2013) and ZPar (Zhang and Nivre, 2011)), and to combine all the outputs produced to obtain an improved parsing quality (Hall et al., 2010). In order to get an overall picture of the parsing results after each of the steps described in Section 5, we p"
W17-6526,W15-4301,0,0.0947569,"eir increasing importance in NLP, several efforts have been made to annotate, manually or semi-automatically, social media texts. However, the use of typical NLP tools and techniques has proved critical, essentially by virtue of the unconventional use of the language norms at all levels (orthography, lexicon, morphology and syntax) and the amount of noise such non-standard linguistic behaviors and meta-textual elements can bring about. Although various attempts to produce such kind of specialized resources and tools are described in literature (e.g. (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Rei et al., 2016)), most of these attempts mainly focus on 3 http://www.evalita.it 230 PoS-tagged corpora, while few of them deal with syntactic annotation as well. One of such works is that of Foster et al. (2011), who built a dataset containing 1,000 sentences including tweets and forum posts, with the specific aim of investigating the problems of parsing social media texts. Later on, other works attempted to overcome such limits by creating ah hoc resources to be used as training data for parsing. This is the case of the French Social Media Bank (Seddah et al., 2012), a set of 1,700 sente"
W17-6526,P13-2109,0,0.0149933,"ell (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a), which is based on a lowrank factorization method that enables to map high dimensional feature vectors into low dimensional representations; the full model was chosen. For the near future, we also plan to extend the experiment to other state-of-the-art parsers as well (namely TurboParser (Martins et al., 2013) and ZPar (Zhang and Nivre, 2011)), and to combine all the outputs produced to obtain an improved parsing quality (Hall et al., 2010). In order to get an overall picture of the parsing results after each of the steps described in Section 5, we parsed both development and test set a) after the insertion of lemmas and language-specific PoS tags, and b) after the morphological features were also added. To get a measure of how much parsing quality differs between standard and Twitter texts, in Table 1 we report also the results of the parser on the UD Italian test set (489 sentences). For the eval"
W17-6526,L16-1262,0,0.0452412,"Missing"
W17-6526,N13-1039,0,0.0897427,"ed Work Considering their increasing importance in NLP, several efforts have been made to annotate, manually or semi-automatically, social media texts. However, the use of typical NLP tools and techniques has proved critical, essentially by virtue of the unconventional use of the language norms at all levels (orthography, lexicon, morphology and syntax) and the amount of noise such non-standard linguistic behaviors and meta-textual elements can bring about. Although various attempts to produce such kind of specialized resources and tools are described in literature (e.g. (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Rei et al., 2016)), most of these attempts mainly focus on 3 http://www.evalita.it 230 PoS-tagged corpora, while few of them deal with syntactic annotation as well. One of such works is that of Foster et al. (2011), who built a dataset containing 1,000 sentences including tweets and forum posts, with the specific aim of investigating the problems of parsing social media texts. Later on, other works attempted to overcome such limits by creating ah hoc resources to be used as training data for parsing. This is the case of the French Social Media Bank (Seddah et al., 2012), a"
W17-6526,C12-1149,0,0.0239685,"1; Owoputi et al., 2013; Lynn et al., 2015; Rei et al., 2016)), most of these attempts mainly focus on 3 http://www.evalita.it 230 PoS-tagged corpora, while few of them deal with syntactic annotation as well. One of such works is that of Foster et al. (2011), who built a dataset containing 1,000 sentences including tweets and forum posts, with the specific aim of investigating the problems of parsing social media texts. Later on, other works attempted to overcome such limits by creating ah hoc resources to be used as training data for parsing. This is the case of the French Social Media Bank (Seddah et al., 2012), a set of 1,700 sentences from various types of usergenerated content (among those, tweets), annotated using an adapted version of the French Treebank (Abeill´e et al., 2003) scheme, and TWEEBANK (Kong et al., 2014), built by manually adding dependency parses to tweets drawn from the PoS-tagged Twitter corpus of Owoputi et al. (2013). Finally, it is worth mentioning the English Web Treebank (Silveira et al., 2014), a collection of more than 16k sentences taken from various Web media, including blogs, emails, reviews and Yahoo! answers, and also available in UD format. To the best of our knowl"
W17-6526,silveira-etal-2014-gold,0,0.112482,"Missing"
W17-6526,tamburini-melandri-2012-anita,1,0.757039,"fic tokens that, according to UD specifications, should be classified as SYM (symbol) were further specified based on the token type. As a result, all the categories that typically occur in social media texts, like emoticons, Internet addresses, email addresses, hashtags and Twitter mentions had their own tag, i.e. EMO, URL, EMAIL, HASHTAG and MENTION. 231 Lemmas and Morphological Features In order to produce a correctly formatted corpus in CoNLL-U format, we also inserted information about lemmas and morphological features associated to each word. To speed up the process, we relied on AnIta (Tamburini and Melandri, 2012), an Italian morphological analyzer based on a large lexicon (about 110,000 lemmas) able to analyze the various word forms and produce all the possible lemmas and morphological features for these forms. A two-step semi-automatic conversion between the different annotation schemes ensured a full compatibility with the UD specifications. In the first step we added lemmas and languagespecific PoS tags (xpos). As mentioned above, the insertion was done partly with a script that converts AnIta output into a UD-compatible form, and matches the word forms on the PoSTWITA-UD side with the lemmas provi"
W17-6526,P11-2033,0,0.034247,"e evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a), which is based on a lowrank factorization method that enables to map high dimensional feature vectors into low dimensional representations; the full model was chosen. For the near future, we also plan to extend the experiment to other state-of-the-art parsers as well (namely TurboParser (Martins et al., 2013) and ZPar (Zhang and Nivre, 2011)), and to combine all the outputs produced to obtain an improved parsing quality (Hall et al., 2010). In order to get an overall picture of the parsing results after each of the steps described in Section 5, we parsed both development and test set a) after the insertion of lemmas and language-specific PoS tags, and b) after the morphological features were also added. To get a measure of how much parsing quality differs between standard and Twitter texts, in Table 1 we report also the results of the parser on the UD Italian test set (489 sentences). For the evaluation step we used the script ma"
W17-6526,D14-1109,0,0.0501377,"n-based parser (Bohnet and Nivre, 2012; Bohnet and Parser MATE graph-based MATE transition-based RBG full -LX 62.53 64.92 64.36 -F 67.05 66.65 67.07 -UD 91.26 91.44 90.16 Table 1: Results of the parsers after the different annotation stages, i.e. with lemmas and languagespecific PoS tags (-LX), and with morphological features as well (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a), which is based on a lowrank factorization method that enables to map high dimensional feature vectors into low dimensional representations; the full model was chosen. For the near future, we also plan to extend the experiment to other state-of-the-art parsers as well (namely TurboParser (Martins et al., 2013) and ZPar (Zhang and Nivre, 2011)), and to combine all the outputs produced to obtain an improved parsing quality (Hall et al., 2010). In order to get an overall picture of the parsing results after each of the steps described in Section 5, we parsed both developme"
W17-6526,P14-1019,0,0.0680733,"n-based parser (Bohnet and Nivre, 2012; Bohnet and Parser MATE graph-based MATE transition-based RBG full -LX 62.53 64.92 64.36 -F 67.05 66.65 67.07 -UD 91.26 91.44 90.16 Table 1: Results of the parsers after the different annotation stages, i.e. with lemmas and languagespecific PoS tags (-LX), and with morphological features as well (-F). The parser outputs were evaluated against the gold standard of the test set (300 tweets, -LX and -F columns) but also against the UD Italian test set (489 sentences, -UD column). Kuhn, 2012); they were run using standard parameters; • RBG (Lei et al., 2014; Zhang et al., 2014b; Zhang et al., 2014a), which is based on a lowrank factorization method that enables to map high dimensional feature vectors into low dimensional representations; the full model was chosen. For the near future, we also plan to extend the experiment to other state-of-the-art parsers as well (namely TurboParser (Martins et al., 2013) and ZPar (Zhang and Nivre, 2011)), and to combine all the outputs produced to obtain an improved parsing quality (Hall et al., 2010). In order to get an overall picture of the parsing results after each of the steps described in Section 5, we parsed both developme"
W97-1503,C90-2039,0,0.0229265,"of a single environment for the whole development cycle: different users have dedicated facilities for development, but a common environment for integrating and testing. on. Via API, it is also possible to interface LEAs with other kinds of external modules, e.g. modules which make available functionalities not provided by the environment (e.g. Knowledge Bases or morphological analyzers). PM can also choose among different unification algorithms that have been designed to: * carefully control and minimize the amount of copying needed with non-deterministic parsing schemata (Wroblewski, 1987) (Kogure, 1990); • provide a better match between the characteristics of the unifiers and those of the linguistic processors. Indeed, different linguistic processors may profit of different unification algorithms. The availability of different unification algorithms allows the user to choose the one which best fits the needs of the particular linguistic processor at hand. 5.3 Supporting the Computational Linguist A considerable amount of effort has been devoted to create suitable (specialized) graphical tools for CL. Recall that CL main task is to build a linguistic system satisfying the application requirem"
W97-1503,P91-1031,0,0.0187017,"nd processors are seen by the LER as black boxes that can be combined by means of a graphical interface. When the architecture meets the requirements, a delivery system can be produced. It contains the selected linguistic system and processor(s), and excludes the GEPPETTO development environment. • External constraints providing ,explicit links to external modules, e.g. morphological processors, independent KBs, etc.; • Directives for the unifier. For instance, it is possible to force the unifier to consider in the first place the paths that have been observed to cause more frequent failures (Uszkoreit, 1991). 5.2 • Macros. PM task is to identify the processors that can satisfy the architectural requirements. She/he can choose among the processors made available by GEPPETTO 5 or link external ones to the environment. In the latter case, an API is provided to connect the external processor to the GEPPETTO world. Once a new processor has been properly linked, it is completely identical to the other default processors: it can be selected via the graphical interface, it can take advantage of the debugging/testing facilities, and so Declaration statements and external constraints greatly enhance the mo"
