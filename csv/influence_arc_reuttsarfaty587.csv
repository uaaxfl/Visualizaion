2020.acl-demos.7,P13-1023,0,0.0511692,"Missing"
2020.acl-demos.7,W13-2322,0,0.0778147,"Missing"
2020.acl-demos.7,Q16-1010,0,0.0328475,"Missing"
2020.acl-demos.7,W17-6507,0,0.633359,"et al., 2013; Palmer et al., 2010; Abend and Rappoport, 2013; Oepen et al., 2014) are harder to predict with sufficient accuracy, calling for a middle ground. Indeed, De Marneffe and Manning (2008) introduced collapsed and propagated dependencies, in an attempt to make some semantic-like relations more apparent. The Universal Dependencies (UD) project1 similarly embraces the concept of Enhanced Dependencies (Nivre et al., 2018)), adding explicit relations that are otherwise left implicit. Schuster and Manning (2016) provide further enhancements targeted specifically at English (Enhanced UD).2 Candito et al. (2017) suggest further enhancements to address diathesis alternations.3 In this work we continue this line of thought, and take it a step further. We present pyBART, an Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for do"
2020.acl-demos.7,D17-1009,0,0.0750282,"he Universal Enhanced UD and Schuster and Manning (2016)’s Enhanced++ English UD. We refer to their union on English as Enhanced UD. 3 Efforts such as PropS (Stanovsky et al., 2016) and PredPatt (White et al., 2016), share our motivation of extracting predicate-argument structures from treebank-trainable trees, though outside of the UD framework. Efforts such as KNext (Durme and Schubert, 2008) automatically extract logic-based forms by converting treebank-trainable trees, for consumption by further processing. HLF (Rudinger and Van Durme, 2014), DepLambda (Reddy et al., 2016) and UDepLambda (Reddy et al., 2017) attempt to provide a formal semantic representation by converting dependency structures to logical forms. While they share a high-level goal with ours — exposing functional relations in a sentence in a unified way — their end result, logical forms, is substantially different from pyBART structures. While providing substantial benefits for semantic parsing applications, logical forms are less readable for non-experts than labeled relations between content words. As these efforts rely on dependency trees as a backbone, they could potentially benefit from pyBART’s focus on syntactic enhancements"
2020.acl-demos.7,W14-2908,0,0.0676178,"Missing"
2020.acl-demos.7,L16-1376,0,0.156296,"Missing"
2020.acl-demos.7,W08-2219,0,0.0229511,"n a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns. 1 1 universaldepdenencies.org In this paper we do not distinguish between the Universal Enhanced UD and Schuster and Manning (2016)’s Enhanced++ English UD. We refer to their union on English as Enhanced UD. 3 Efforts such as PropS (Stanovsky et al., 2016) and PredPatt (White et al., 2016), share our motivation of extracting predicate-argument structures from treebank-trainable trees, though outside of the UD framework. Efforts such as KNext (Durme and Schubert, 2008) automatically extract logic-based forms by converting treebank-trainable trees, for consumption by further processing. HLF (Rudinger and Van Durme, 2014), DepLambda (Reddy et al., 2016) and UDepLambda (Reddy et al., 2017) attempt to provide a formal semantic representation by converting dependency structures to logical forms. While they share a high-level goal with ours — exposing functional relations in a sentence in a unified way — their end result, logical forms, is substantially different from pyBART structures. While providing substantial benefits for semantic parsing applications, logic"
2020.acl-demos.7,L18-1169,0,0.110953,"Missing"
2020.acl-demos.7,N18-2035,0,0.0264068,"adding a node to represent the STATE and have tense, aspect, modality and evidentiality directly modifying it.11 Copular Sentences and Stative Predicates: We added to all copula constructions new node named STATE, which represents the stative event introduced by the copular clause. This node becomes the root, and we rewire the entire clause around this STATE. By doing so we unify it with the structures of clauses with finite predicative. Once we added the STATE node, we form a new relation, termed ev, to mark event/state modifications. The resulting structure is as follows: advmod Compounds: Shwartz and Waterson (2018) show that in many cases, compounds can be seen as having a multiple-head. Therefore, we share the existing relations across the compound parts. (9) I see dead people ev company (14) amod xcomp Tomorrow is STATE another day cop Adjectival modifiers: Adjectival modification can be viewed as capturing the same information as a predicative copular sentence conveying the same meaning (so, “a green apple” implies that “an apple is green”). To explicitly capture this productive implication, we add a subject relation from each adjectival modifier to its corresponding modified noun. nsubj Evidential r"
2020.acl-demos.7,D15-1076,0,0.0505789,"departure point is the English EUD representation (Schuster and Manning, 2016) and related Some preserved UD relations are omitted for readability. https://spacy.io 48 2 efforts discussed above, which we seek to extend in a way which is useful to information seeking applications. To identify relevant constructions that are not covered by current representations, we use a data-driven process. We consider concrete relations that are expressed in annotated task-based corpora: a relation extraction dataset (ACE05, (Walker et al., 2006)), which annotates relations and events, and a QA-SRL dataset (He et al., 2015) which connects predicates to sentence segments that are perceived by people as their (possibly implied) arguments. For each of these corpora, we consider the dependency paths between the annotated elements, looking for cases where a direct relation in the corpus corresponds to an indirect dependency path in the syntactic graph. We identify recurring cases that we think can be shortened, and which can be justified linguistically and empirically. We then come up with proposed enhancements and modifications, and verify them empirically against a larger corpus by extracting cases that match the c"
2020.acl-demos.7,D16-1177,0,0.111441,"Missing"
2020.acl-demos.7,D17-1004,0,0.116335,"Missing"
2020.acl-demos.7,W18-6012,0,\N,Missing
2020.acl-main.660,P06-1084,0,0.108832,"ed our own COPYNET, a sequence-to-sequence pointernetwork where the input consists of complete morphological lattices for each token, and a copy-attention mechanism is trained to jointly select morphological segments and tag associations from within the lattice, to construct the complete multi-tag analyses. Data and Metrics. We use the Hebrew section of the SPMRL shared task (Seddah et al., 2013b) using the standard split, training on 5000 sentences and evaluating on 500 sentences. For generating the lattices we rely on a rule-based algorithm we devised on top of the wide-coverage lexicon of (Adler and Elhadad, 2006), the same lexicon employed in previous work on Hebrew (More and Tsarfaty, 2016; More et al., 2019; Tsarfaty et al., 2019). We report the F-Scores on Seg/POS as defined in More and Tsarfaty (2016); More et al. (2019). Results. Table 1 shows the multi-tagging results for the different models. The pre-neural model obtains 95.5 F1 on joint Seg+POS prediction on the standard dev set. As for the neural models, in an oracle segmentation scenario, where the gold morphological segmentation is known in advance, both BERT and the LSTM-CRF get close to the pre-neural model results. However, they solve an"
2020.acl-main.660,D15-1159,0,0.0198423,"e units, discourse relations, as well as rhetoric and pragmatic structure associated with complete narratives. Since natural language exhibits ambiguity at all levels of analysis, statistical parsers aim to learn how to pick the best analysis from multiple suitable candidates (Smith, 2011). The introduction of Deep Learning has revolutionized all areas of Artificial Intelligence, and NLP research is no exception (Goldberg, 2016). Neuralnetwork models now demonstrate an all-times peak in the performance of various NLP tasks, from conventional tasks in the NLP pipeline like tagging and parsing (Alberti et al., 2015; Nguyen et al., 2017; Zhou et al., 2019) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully desi"
2020.acl-main.660,N16-1181,0,0.0258904,"parsers aim to learn how to pick the best analysis from multiple suitable candidates (Smith, 2011). The introduction of Deep Learning has revolutionized all areas of Artificial Intelligence, and NLP research is no exception (Goldberg, 2016). Neuralnetwork models now demonstrate an all-times peak in the performance of various NLP tasks, from conventional tasks in the NLP pipeline like tagging and parsing (Alberti et al., 2015; Nguyen et al., 2017; Zhou et al., 2019) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-ext"
2020.acl-main.660,E17-2067,0,0.122701,"parsing task (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Bohnet et al., 2013a; Seeker and Centinoglu, 2015; More et al., 2019). In contrast, pre-trained embeddings select a single vector for each input token — prior to any further analysis. Finally, pre-trained embeddings trained on words cannot assign vectors to unseen words. The use of unsupervised char-based or sub-word units (Bojanowski et al., 2017) to remedy this situation shows mixed results; while these models learn orthographic similarities between seen and unseen words, they fail to learn the functions of sub-word units (Avraham and Goldberg (2017); Vania and Lopez (2017) and references therein). This paper aims to underscore the challenges of processing MRLs, reiterate the lessons learned in the pre-neural era, and establish their relevance to MRL processing in neural terms. On the one hand, technical proposals as pre-trained embeddings, finetuning, and end-to-end modeling, have advanced NLP greatly. On the other hand, neural advances often overlook MRL complexities, and disregard strategies that were proven useful for MRLs in the past. We argue that breakthroughs in Neural Models for MRLs (NMRL) can be obtained by incorporating symbol"
2020.acl-main.660,D19-1090,0,0.022255,"re et al., 2016). The solutions proposed to the above challenges included: (i) parsing morphemes rather than words, (ii) joint modeling of local morphology and global structures, and (iii) exploiting external knowledge to analyze the long tail of unattested word-forms. Upon the introduction of Neural Network models into NLP (Goldberg, 2016), it was hoped that we could dispense with the need to model different languages differently. Curiously though, this has not been the case. Languages with rich morphology typically require careful treatment, and often the design of additional resources (cf. Czarnowska et al. (2019)). Moreover, current modeling strategies for neural NLP appear to stand in contrast with the pre-neural proposals for processing MRLs. 7396 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7396–7408 c July 5 - 10, 2020. 2020 Association for Computational Linguistics First, unsupervised pre-training techniques employing language modeling objectives (LM, MLM) are applied nowadays to raw words rather than morphemes, and deliver word-embeddings agnostic to internal structure. While some morphological structure may be implicitly encoded in these vectors"
2020.acl-main.660,P18-2077,0,0.0166793,"2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2018). That is, as long as the input object can be represented as a vector, the neural model will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words. Initially, word embeddings were noncontextualized (Mikolov et al., 2013; Pennington et al., 2014), i.e., they assigned the same vector to the occurrences of a word in differ"
2020.acl-main.660,N09-1037,0,0.0441274,"Missing"
2020.acl-main.660,E09-1038,1,0.69339,"el and Manning, 2009) or parsing and SRL (Johansson and Nugues, 2008). 7399 Finally, the LEXICAL challenge refers to the problem of out-of-vocabulary items. Supervised training successfully analyzes attested forms, but fails to analyze the long tail of morphological forms in the language, not yet attested during training. Pre-neural models for MRLs thus benefit from additional symbolic information beyond the supervised data. It can be in the form of online dictionaries, wide-coverage lexica, or a-priori knowledge of the structure of morphological paradigms in the language (Sagot et al., 2006; Goldberg et al., 2009). Where We’re At Upon the introduction of neural models into NLP the hope was that we could dispense with the need to develop language-specific modeling strategies, and that models will seamlessly carry over from any one language (type) to another. Curiously, this was not yet shown to be the case. NLP advances in MRLs still lag behind those for English, with lower empirical results on classical tasks (Straka et al., 2016), and very scarce results for applications as question answering and natural language inference (Hu et al., 2020). More fundamentally, NLP researchers nowadays successfully pr"
2020.acl-main.660,W19-3622,0,0.0132158,"required for end-to-end models in MRLs. The key strategies we propose in order to address NMRL include transitionining to (i) morphologicalembeddings, (ii) joint lattice-based modeling, and (iii) paradigm cell-filling (Blevins, 2016; Ackerman et al., 2009), as we detail shortly. Research Questions. To instigate research on NMRL, let us define the three overarching D EEP challenges of MRLs in the spirit of (Tsarfaty et al., 2010). For these challenges, the aim is to devise solutions that respect the linguistic complexities while employing the most recent deep learning advances. 5 Furthermore, Gonen et al. (2019) have recently shown that one needs to know the explicit morphological analyses in order to effectively ignore or neutralize certain morphemes, for instance discarding gender for reducing bias in the data. 7401 • T HE D EEP A RCHITECTURAL C HALLENGE: The ‘classical’ architectural challenge aimed to define optimal input and output units adequate for processing MRLs. In neural terms, this challenge boils down to a question concerning the units that should enter pre-training. Are they words? Word-pieces? Segmented morphemes? Lemmas? Lattices? Furthermore, should these units be predicted from exis"
2020.acl-main.660,C10-1045,0,0.13625,"ives (LM, MLM) are applied nowadays to raw words rather than morphemes, and deliver word-embeddings agnostic to internal structure. While some morphological structure may be implicitly encoded in these vectors, the morphemes themselves remain un-accessible (Vania et al., 2018; Cotterell and Sch¨utze, 2015). Second, pre-neural models for parsing MRLs call for joint inference over local and global structures, tasking multiple, ambiguous, morphological analyses (a.k.a. lattices) as input, and disambiguating these morphological structure jointly with the parsing task (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Bohnet et al., 2013a; Seeker and Centinoglu, 2015; More et al., 2019). In contrast, pre-trained embeddings select a single vector for each input token — prior to any further analysis. Finally, pre-trained embeddings trained on words cannot assign vectors to unseen words. The use of unsupervised char-based or sub-word units (Bojanowski et al., 2017) to remedy this situation shows mixed results; while these models learn orthographic similarities between seen and unseen words, they fail to learn the functions of sub-word units (Avraham and Goldberg (2017); Vania and Lopez (2017) and references"
2020.acl-main.660,N18-1108,0,0.0198601,"could dispense with the need to develop language-specific modeling strategies, and that models will seamlessly carry over from any one language (type) to another. Curiously, this was not yet shown to be the case. NLP advances in MRLs still lag behind those for English, with lower empirical results on classical tasks (Straka et al., 2016), and very scarce results for applications as question answering and natural language inference (Hu et al., 2020). More fundamentally, NLP researchers nowadays successfully predict linguistic properties of English via neural models as in Linzen et al. (2016); Gulordava et al. (2018), but they are less successful in doing so for languages that differ from English, as in Ravfogel et al. (2018). It is high time for the MRL community to shed light on the methodological and empirical gaps between neural models for English and for MRLs, and to bridge this gap. 4 The Research Objective: NLP for MRLs in the Deep Learning Era The point of departure of this paper is the claim that neural modeling practices employed in NLP nowadays are suboptimal in the face of properties of MRLs. In what follows we illuminate this claim for the four neural methodological constructs that we termed"
2020.acl-main.660,D18-1111,0,0.0309167,"sis from multiple suitable candidates (Smith, 2011). The introduction of Deep Learning has revolutionized all areas of Artificial Intelligence, and NLP research is no exception (Goldberg, 2016). Neuralnetwork models now demonstrate an all-times peak in the performance of various NLP tasks, from conventional tasks in the NLP pipeline like tagging and parsing (Alberti et al., 2015; Nguyen et al., 2017; Zhou et al., 2019) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat"
2020.acl-main.660,N19-1419,0,0.020652,"ically increased the performance of any NLP task they were applied to. Third, working with contextualized embeddings has been so successful, that it shifted the focus of NLP practitioners from training models from scratch to fine-tuning (Liu et al., 2019a) pre-trained embeddings. That is, instead of tailoring hugely complex models for specific tasks and training them from scratch, a huge effort is invested in learning a general language model (LM) that can assign contextualized embeddings to words. These vectors are often argued to capture, or encode, various aspects of structure and meaning (Hewitt and Manning, 2019), and then, a relatively small amount of task-specific data may be used to fine-tune the pretrained embeddings, so that the model can solve a particular task at hand. Finally, traditional NLP tasks, such as the parsing layers mentioned earlier, were typically organized into a pipeline turning unstructured texts gradually into more complex structures by gradually increasing the complexity of analysis. Eventually, complex semantic structures formed the basis for the design of dialogue systems, question answering systems, etc. Nowadays, NN models for complex semantic tasks are often designed and"
2020.acl-main.660,Q19-1003,1,0.892298,"deliver word-embeddings agnostic to internal structure. While some morphological structure may be implicitly encoded in these vectors, the morphemes themselves remain un-accessible (Vania et al., 2018; Cotterell and Sch¨utze, 2015). Second, pre-neural models for parsing MRLs call for joint inference over local and global structures, tasking multiple, ambiguous, morphological analyses (a.k.a. lattices) as input, and disambiguating these morphological structure jointly with the parsing task (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Bohnet et al., 2013a; Seeker and Centinoglu, 2015; More et al., 2019). In contrast, pre-trained embeddings select a single vector for each input token — prior to any further analysis. Finally, pre-trained embeddings trained on words cannot assign vectors to unseen words. The use of unsupervised char-based or sub-word units (Bojanowski et al., 2017) to remedy this situation shows mixed results; while these models learn orthographic similarities between seen and unseen words, they fail to learn the functions of sub-word units (Avraham and Goldberg (2017); Vania and Lopez (2017) and references therein). This paper aims to underscore the challenges of processing MR"
2020.acl-main.660,D08-1008,0,0.0241314,"Missing"
2020.acl-main.660,C16-1033,1,0.89633,"ts of complete morphological lattices for each token, and a copy-attention mechanism is trained to jointly select morphological segments and tag associations from within the lattice, to construct the complete multi-tag analyses. Data and Metrics. We use the Hebrew section of the SPMRL shared task (Seddah et al., 2013b) using the standard split, training on 5000 sentences and evaluating on 500 sentences. For generating the lattices we rely on a rule-based algorithm we devised on top of the wide-coverage lexicon of (Adler and Elhadad, 2006), the same lexicon employed in previous work on Hebrew (More and Tsarfaty, 2016; More et al., 2019; Tsarfaty et al., 2019). We report the F-Scores on Seg/POS as defined in More and Tsarfaty (2016); More et al. (2019). Results. Table 1 shows the multi-tagging results for the different models. The pre-neural model obtains 95.5 F1 on joint Seg+POS prediction on the standard dev set. As for the neural models, in an oracle segmentation scenario, where the gold morphological segmentation is known in advance, both BERT and the LSTM-CRF get close to the pre-neural model results. However, they solve an easier and unrealistic task, since in realistic scenarios the gold segmentatio"
2020.acl-main.660,Q16-1023,0,0.0220262,"-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2018). That is, as long as the input object can be represented as a vector, the neural model will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words. Initially, word embeddings were noncontextualized (Mikolov et al., 2013; Pennington et al., 2014), i.e., they assigned the same vector to the occur"
2020.acl-main.660,Q16-1037,0,0.0847022,"P the hope was that we could dispense with the need to develop language-specific modeling strategies, and that models will seamlessly carry over from any one language (type) to another. Curiously, this was not yet shown to be the case. NLP advances in MRLs still lag behind those for English, with lower empirical results on classical tasks (Straka et al., 2016), and very scarce results for applications as question answering and natural language inference (Hu et al., 2020). More fundamentally, NLP researchers nowadays successfully predict linguistic properties of English via neural models as in Linzen et al. (2016); Gulordava et al. (2018), but they are less successful in doing so for languages that differ from English, as in Ravfogel et al. (2018). It is high time for the MRL community to shed light on the methodological and empirical gaps between neural models for English and for MRLs, and to bridge this gap. 4 The Research Objective: NLP for MRLs in the Deep Learning Era The point of departure of this paper is the claim that neural modeling practices employed in NLP nowadays are suboptimal in the face of properties of MRLs. In what follows we illuminate this claim for the four neural methodological c"
2020.acl-main.660,N19-1225,0,0.171816,"ropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words. Initially, word embeddings were noncontextualized (Mikolov et al., 2013; Pennington et al., 2014), i.e., they assigned the same vector to the occurrences of a word in different contexts. Later models present contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019b), they assign different vectors to the occurrences of the same word in different contexts. Embeddings in general, and contextualized ones in particular, dramatically increased the performance of any NLP task they were applied to. Third, working with contextualized embeddings has been so successful, that it shifted the focus of NLP practitioners from training models from scratch to fine-tuning (Liu et al., 2019a) pre-trained embeddings. That is, instead of tailoring hugely complex models for specific tasks and training them from scratch, a huge effort is invested in learning a general languag"
2020.acl-main.660,2021.ccl-1.108,0,0.0606333,"Missing"
2020.acl-main.660,D15-1166,0,0.0536514,"ty at all levels of analysis, statistical parsers aim to learn how to pick the best analysis from multiple suitable candidates (Smith, 2011). The introduction of Deep Learning has revolutionized all areas of Artificial Intelligence, and NLP research is no exception (Goldberg, 2016). Neuralnetwork models now demonstrate an all-times peak in the performance of various NLP tasks, from conventional tasks in the NLP pipeline like tagging and parsing (Alberti et al., 2015; Nguyen et al., 2017; Zhou et al., 2019) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning"
2020.acl-main.660,K17-3014,0,0.0201523,"ations, as well as rhetoric and pragmatic structure associated with complete narratives. Since natural language exhibits ambiguity at all levels of analysis, statistical parsers aim to learn how to pick the best analysis from multiple suitable candidates (Smith, 2011). The introduction of Deep Learning has revolutionized all areas of Artificial Intelligence, and NLP research is no exception (Goldberg, 2016). Neuralnetwork models now demonstrate an all-times peak in the performance of various NLP tasks, from conventional tasks in the NLP pipeline like tagging and parsing (Alberti et al., 2015; Nguyen et al., 2017; Zhou et al., 2019) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or fea"
2020.acl-main.660,L16-1262,1,0.797013,"Missing"
2020.acl-main.660,D14-1162,0,0.0891791,"be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2018). That is, as long as the input object can be represented as a vector, the neural model will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words. Initially, word embeddings were noncontextualized (Mikolov et al., 2013; Pennington et al., 2014), i.e., they assigned the same vector to the occurrences of a word in different contexts. Later models present contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019b), they assign different vectors to the occurrences of the same word in different contexts. Embeddings in general, and contextualized ones in particular, dramatically increased the performance of any NLP task they were applied to. Third, working with contextualized embeddings has been so successful, that it shifted the focus of NLP practitioners from training models from scratch to"
2020.acl-main.660,N18-1202,0,0.307025,"odel will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words. Initially, word embeddings were noncontextualized (Mikolov et al., 2013; Pennington et al., 2014), i.e., they assigned the same vector to the occurrences of a word in different contexts. Later models present contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019b), they assign different vectors to the occurrences of the same word in different contexts. Embeddings in general, and contextualized ones in particular, dramatically increased the performance of any NLP task they were applied to. Third, working with contextualized embeddings has been so successful, that it shifted the focus of NLP practitioners from training models from scratch to fine-tuning (Liu et al., 2019a) pre-trained embeddings. That is, instead of tailoring hugely complex models for specific tasks and training them from scratch, a huge effort is i"
2020.acl-main.660,W18-5412,0,0.0139863,"carry over from any one language (type) to another. Curiously, this was not yet shown to be the case. NLP advances in MRLs still lag behind those for English, with lower empirical results on classical tasks (Straka et al., 2016), and very scarce results for applications as question answering and natural language inference (Hu et al., 2020). More fundamentally, NLP researchers nowadays successfully predict linguistic properties of English via neural models as in Linzen et al. (2016); Gulordava et al. (2018), but they are less successful in doing so for languages that differ from English, as in Ravfogel et al. (2018). It is high time for the MRL community to shed light on the methodological and empirical gaps between neural models for English and for MRLs, and to bridge this gap. 4 The Research Objective: NLP for MRLs in the Deep Learning Era The point of departure of this paper is the claim that neural modeling practices employed in NLP nowadays are suboptimal in the face of properties of MRLs. In what follows we illuminate this claim for the four neural methodological constructs that we termed pre-training, fine-tuning, feature-extraction and end-to-end modeling. Pre-training of word embeddings presuppo"
2020.acl-main.660,W14-6111,1,0.900276,"Missing"
2020.acl-main.660,Q15-1026,0,0.0651512,"s rather than morphemes, and deliver word-embeddings agnostic to internal structure. While some morphological structure may be implicitly encoded in these vectors, the morphemes themselves remain un-accessible (Vania et al., 2018; Cotterell and Sch¨utze, 2015). Second, pre-neural models for parsing MRLs call for joint inference over local and global structures, tasking multiple, ambiguous, morphological analyses (a.k.a. lattices) as input, and disambiguating these morphological structure jointly with the parsing task (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Bohnet et al., 2013a; Seeker and Centinoglu, 2015; More et al., 2019). In contrast, pre-trained embeddings select a single vector for each input token — prior to any further analysis. Finally, pre-trained embeddings trained on words cannot assign vectors to unseen words. The use of unsupervised char-based or sub-word units (Bojanowski et al., 2017) to remedy this situation shows mixed results; while these models learn orthographic similarities between seen and unseen words, they fail to learn the functions of sub-word units (Avraham and Goldberg (2017); Vania and Lopez (2017) and references therein). This paper aims to underscore the challen"
2020.acl-main.660,Q18-1030,0,0.0563428,"gs of lemmas rather than words (Avraham and Goldberg, 2017). Also, dependency parsing for MRLs requires access to morphological segments, according to the UD scheme (Straka et al., 2016). A reasonable solution might be to morphologically analyze and segment all input words prior to pre-training. Unfortunately, this solution does not fit the bill for MRLs either. First, current neural segmentors and taggers for MRLs are not accurate enough, and errors in the analyses propagate through the pre-training to contaminate the trained embeddings and later tasks. In the universal segmentation work of (Shao et al., 2018), for instance, neural segmentation for languages which are high on both the synthesis and the fusion index, such as Arabic and Hebrew, lags far behind. Beyond that, there is the technical matter of resources. Pre-training models as Devlin et al. (2018); Liu et al. (2019b); Yang et al. (2019) requires massive amounts of data and computing resources, and such training often takes place outside of academia. Training morphological embeddings rather than word embeddings was not taken up by any commercial partner.4 Next, let us turn to the notion of fine-tuning, widely used today in all sorts of NL"
2020.acl-main.660,L16-1680,0,0.0271858,"Missing"
2020.acl-main.660,P06-3009,1,0.593453,"s, since on the one hand they represent the explicit decomposition of words into morphemes, and on the other hand retain the morphological ambiguity of the input stream, to be disambiguated downstream, when information from later phases, syntactic or semantic, becomes available. Lattice-based processing has led to re-thinking the MODELING architectures for MRLs, and to propose JOINT models, where multiple levels of information are represented during training, and are jointly predicted at inference time. Such joint models have been developed for MRLs in the context of phrase-structure parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010) and dependency parsing (Bohnet et al., 2013b; Seeker and C¸etino˘glu, 2015; More et al., 2019). In all cases, it has been shown that joint models obtain better results than their morphological or syntactic standalone counterparts.3 3 Joint models are shown to be effective for other tasks and languages, such as parsing and NER (Finkel and Manning, 2009) or parsing and SRL (Johansson and Nugues, 2008). 7399 Finally, the LEXICAL challenge refers to the problem of out-of-vocabulary items. Supervised training successfully analyzes attested for"
2020.acl-main.660,P13-2103,1,0.900976,"av Klein Amit Seker Bar Ilan University, Ramat-Gan, Israel reut.tsarfaty@biu.ac.il {dbareket,klein.stav,aseker00}@gmail.com Abstract The term Morphologically-Rich Languages (MRLs) refers to languages such as Arabic, Hebrew, Turkish or Maltese, in which significant information is expressed morphologically, e.g., via word-level variation, rather than syntactically, e.g., via fixed word-order and periphrastic constructions, as in English. These properties lead to diverse and ambiguous structures, accompanied with huge lexica, which in turn make MRLs notoriously hard to parse (Nivre et al., 2007; Tsarfaty, 2013). A decade ago, Tsarfaty et al. (2010) put forth three overarching challenges for the MRLs research community: It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs). Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs. We then aim to"
2020.acl-main.660,W10-1401,1,0.913205,"Missing"
2020.acl-main.660,D19-3044,1,0.813236,"ch token, and a copy-attention mechanism is trained to jointly select morphological segments and tag associations from within the lattice, to construct the complete multi-tag analyses. Data and Metrics. We use the Hebrew section of the SPMRL shared task (Seddah et al., 2013b) using the standard split, training on 5000 sentences and evaluating on 500 sentences. For generating the lattices we rely on a rule-based algorithm we devised on top of the wide-coverage lexicon of (Adler and Elhadad, 2006), the same lexicon employed in previous work on Hebrew (More and Tsarfaty, 2016; More et al., 2019; Tsarfaty et al., 2019). We report the F-Scores on Seg/POS as defined in More and Tsarfaty (2016); More et al. (2019). Results. Table 1 shows the multi-tagging results for the different models. The pre-neural model obtains 95.5 F1 on joint Seg+POS prediction on the standard dev set. As for the neural models, in an oracle segmentation scenario, where the gold morphological segmentation is known in advance, both BERT and the LSTM-CRF get close to the pre-neural model results. However, they solve an easier and unrealistic task, since in realistic scenarios the gold segmentation is never known in advance. In the more re"
2020.acl-main.660,D18-1278,0,0.0459848,"Missing"
2020.acl-main.660,P17-1184,0,0.14164,"sarfaty, 2008; Green and Manning, 2010; Bohnet et al., 2013a; Seeker and Centinoglu, 2015; More et al., 2019). In contrast, pre-trained embeddings select a single vector for each input token — prior to any further analysis. Finally, pre-trained embeddings trained on words cannot assign vectors to unseen words. The use of unsupervised char-based or sub-word units (Bojanowski et al., 2017) to remedy this situation shows mixed results; while these models learn orthographic similarities between seen and unseen words, they fail to learn the functions of sub-word units (Avraham and Goldberg (2017); Vania and Lopez (2017) and references therein). This paper aims to underscore the challenges of processing MRLs, reiterate the lessons learned in the pre-neural era, and establish their relevance to MRL processing in neural terms. On the one hand, technical proposals as pre-trained embeddings, finetuning, and end-to-end modeling, have advanced NLP greatly. On the other hand, neural advances often overlook MRL complexities, and disregard strategies that were proven useful for MRLs in the past. We argue that breakthroughs in Neural Models for MRLs (NMRL) can be obtained by incorporating symbolic knowledge and pre-neu"
2020.acl-main.660,W18-5446,0,0.060244,"Missing"
2020.acl-main.660,J11-1005,0,0.433937,"am applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2018). That is, as long as the input object can be represented as a vector, the neural model will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input, instead of actual words"
2020.acl-main.660,P11-2033,0,0.0221838,"19) to diverse downstream applications, such as machine translation (Bahdanau et al., 2014; Luong et al., 2015), question answering (Andreas et al., 2016), text-to-code generation (Hayati et al., 2018) and natural language navigation (Mei et al., 2016). In addition to revolutionizing empirical NLP, neural models have also altered the methodology of conducting NLP research, in various ways, which we review here in turn. First, while state-of-the-art models for structure prediction in NLP used to rely heavily on intricate formal structures and carefully designed features (or feature-templates) (Zhang and Nivre, 2011; Zhang and Clark, 2011a), current neural models provide a form of representation learning and may be viewed as automatic feature-extractors (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2018). That is, as long as the input object can be represented as a vector, the neural model will learn how to map it to the appropriate set of structural decisions, without having to write features or feature-templates by hand. Second, most neural models for NLP rely on pre-training, the process of acquiring word-level vector representations termed word-embeddings. 7397 These vectors are used as input,"
2020.acl-main.660,P16-1162,0,\N,Missing
2020.acl-main.660,N15-1140,0,\N,Missing
2020.acl-main.660,K19-1043,0,\N,Missing
2020.emnlp-main.224,W15-4612,0,0.178323,"d that might or might not be answered in the following discourse. Previous Discourse Parsing Efforts Most of the recent work on models for (shallow) discourse parsing focuses on specific subtasks, for example on argument identification (Knaebel et al., 2019), or discourse sense classification (Dai and Huang, 2019; Shi and Demberg, 2019; Van Ngo et al., 2019). Full (shallow) discourse parsers tend to use a pipeline approach, for example by having separate classifiers for implicit and explicit relations (Lin et al., 2014), or by building different models for intra- vs. inter-sentence relations (Biran and McKeown, 2015). We also adopt the pipeline approach for our baseline model (Section 6), which performs both relation classification and argument identification, since our QA pairs jointly represent arguments and relations. Previous Discourse Crowdsourcing Efforts There has been research on how to crowd-source discourse relation annotations. Kawahara et al. (2014) crowd-source Japanese discourse relations and simplify the task by reducing the tagset and extracting the argument spans automatically. A follow-up paper found that the data quality of these Japanese annotations was lacking compared to expert annot"
2020.emnlp-main.224,W15-2707,0,0.0445097,"Missing"
2020.emnlp-main.224,D16-1264,0,0.14151,"Missing"
2020.emnlp-main.224,D19-1586,0,0.0278389,"rmation through QAs, solicited from laymen speakers. 2805 The main difference lies in the propositions captured: we collect questions that have an answer in the sentence, targeting specific relation types. In the QUD annotations (Westera et al., 2020) any type of question can be asked that might or might not be answered in the following discourse. Previous Discourse Parsing Efforts Most of the recent work on models for (shallow) discourse parsing focuses on specific subtasks, for example on argument identification (Knaebel et al., 2019), or discourse sense classification (Dai and Huang, 2019; Shi and Demberg, 2019; Van Ngo et al., 2019). Full (shallow) discourse parsers tend to use a pipeline approach, for example by having separate classifiers for implicit and explicit relations (Lin et al., 2014), or by building different models for intra- vs. inter-sentence relations (Biran and McKeown, 2015). We also adopt the pipeline approach for our baseline model (Section 6), which performs both relation classification and argument identification, since our QA pairs jointly represent arguments and relations. Previous Discourse Crowdsourcing Efforts There has been research on how to crowd-source discourse relati"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.297,P06-1084,0,0.0850331,"lanced homographs are magnified in morphologically rich languages (MRLs), including many Semitic languages, where distinct morphemes are often affixed to the word itself, resulting in additional ambiguity (Fabri et al., 2014; Habash et al., 2009). Furthermore, in many Semitic MRLs, the letters are almost entirely consonantal, omitting vowels. This results in a particularly high number of homographs, each with a different pronunciation and meaning. In this paper, we focus upon unbalanced homographs in Hebrew, a highly ambiguous MRL in which vowels are generally omitted (Itai and Wintner, 2008; Adler and Elhadad, 2006). Take for example the Hebrew word !מדינה. This frequent word is generally read as a single nominal morpheme, !הÉי£מְד, meaning “country”. However, it can also be read as !ּהÉי£מִּד, “from the law/judgment of her”, wherein the initial and final letters both serve as distinct morphemes. This last usage is far less common, and, in an overall distribution, it would be relegated to the long tail, with very few attestations in any given corpus. Hebrew is a low-resource language, and as such, the problem of unbalanced homographs is particularly acute. Existing tagged corpora of Hebrew"
2020.findings-emnlp.297,J99-2004,0,0.512516,"embeddings, based on a corpus of 400M words of Hebrew text. To be sure, BERT might seem the more obvious choice rather than word2vec. However, BERT has been shown to be somewhat ineffective for morphologically rich languages such as Hebrew (Tsarfaty et al., 2020). BERTbased models underperform YAP and perform at the same level as BILSTM-based models, and BERT fails to capture internal morphological complexity (Klein and Tsarfaty, 2020). 7 For verbs only, we add a morphosyntactic valence feature indicating the transitivity of the general usage of the verb. This is reminiscent of supertagging (Bangalore and Joshi, 1999) and shows non-negligible empirical contribution on our data. See Appendix Table 2 for a comparison of results with and without the valence feature. 3319 4 YAP does considerably better here: all F1 scores are above .5, and four of the cases are above .8. The weakest cases are those in which YAP has to differentiate between an unsegmented noun and a case of a noun plus possessive suffix (cases 14,20). In both of these cases, YAP scores an F1 of approximately .56 (which, interestingly, is precisely on par with the analogous unbalanced case [10]). In Table 3, we display results regarding our spec"
2020.findings-emnlp.297,D19-5229,0,0.0146919,"morphological characteristics, whereas word2vec embeddings have the benefit of pretraining on over 100M words. The combination of the latter two methods overall outperforms each one of them individually; thus, although word2vec succeeds in encoding most of what is needed to differentiate between the options, the information provided by the morph lattice sometimes helps to make the correct call. In Table 4, we compare the results of our composite-method with those of YAP. Our specialized classifiers set a new SOTA for all the cases. 5 (2014); Belinkov and Glass (2015); Neme and Paumier (2019); Fadel et al. (2019a,b); Darwish et al. (2020). However, these studies all perform evaluations on standard Arabic textual datasets, and do not evaluate accuracy regarding minority options of unbalanced homographs. We believe that these models would likely benefit from specialized challenge sets of the sort presented here to overcome the specific hurdle of unbalanced homographs. 6 Conclusion Due to high morphological ambiguity, as well as the lack of diacritics, Semitic languages pose a particularly difficult disambiguation task, especially when it comes to unbalanced homographs. For such cases, specialized contr"
2020.findings-emnlp.297,D15-1274,0,0.0253558,"ntly large to optimally train the embeddings of the morphological characteristics, whereas word2vec embeddings have the benefit of pretraining on over 100M words. The combination of the latter two methods overall outperforms each one of them individually; thus, although word2vec succeeds in encoding most of what is needed to differentiate between the options, the information provided by the morph lattice sometimes helps to make the correct call. In Table 4, we compare the results of our composite-method with those of YAP. Our specialized classifiers set a new SOTA for all the cases. 5 (2014); Belinkov and Glass (2015); Neme and Paumier (2019); Fadel et al. (2019a,b); Darwish et al. (2020). However, these studies all perform evaluations on standard Arabic textual datasets, and do not evaluate accuracy regarding minority options of unbalanced homographs. We believe that these models would likely benefit from specialized challenge sets of the sort presented here to overcome the specific hurdle of unbalanced homographs. 6 Conclusion Due to high morphological ambiguity, as well as the lack of diacritics, Semitic languages pose a particularly difficult disambiguation task, especially when it comes to unbalanced"
2020.findings-emnlp.297,bird-etal-2008-acl,0,0.0586462,"rein a relatively small number of words appear frequently, and a much larger number of items appear in a long tail of words, as rare events (Czarnowska et al., 2019). Significantly, this also applies to the distribution of analyses of a given homograph. Take for instance the simple POS-tag ambiguity in English between noun and verb (Elkahky et al., 2018). The word “fair” can be used as an adjective (“a fair price”) or as a noun (“she went to the fair”). Yet, the distribution of these two analyses is certainly not fair; the adjectival usage is far more frequent than the nominal usage (e.g., in Bird et al. (2008) the latter is six times more frequent than the former). We will call such cases “unbalanced homographs”. Cases of unbalanced homographs pose a formidable challenge for automated morphological parsers and segmenters. In tagged training corpora, the frequent option will naturally dominate the overwhelming majority of the occurrences. If the training corpus is not sufficiently large, then the sparsity of the minority analysis will prevent generalization by machine-learning models. By the same token, it can be difficult to evaluate the performance of tagging systems regarding unbalanced homograph"
2020.findings-emnlp.297,N09-1004,0,0.0143477,"ssifiers and leverage them within a pipeline architecture. We envision the classifiers positioned at the beginning of the pipeline, disambiguating frequent forms from the get-go, and yielding improvement down the line, ultimately improving results for downstream tasks (e.g. NMT). Indeed, as we have demonstrated, neural classifiers trained on our contrast sets handily achieve a new SOTA for all of the homographs in the corpus. Related Work Many recent papers have proposed global or unsupervised methods for homograph disambiguation in English (e.g. Liu et al. (2018); Wilks and Stevenson (1997); Chen et al. (2009)). While such methods have obvious advantages, they have limited applicability to Hebrew. As noted, in Hebrew the majority of the words are ambiguous, including the core building blocks of the language; without these anchors, global approaches tend to result in poor performance regarding unbalanced homographs. The problem of Hebrew diacritization is analogous to that of Arabic diacritization; Arabic, like Hebrew, is a morphologically-rich language written without diacritics, resulting in high ambiguity. Many recent studies have proposed machinelearning approaches for the prediction of Arabic d"
2020.findings-emnlp.297,D19-1090,0,0.0283845,"t SOTA of Hebrew disambiguation performs poorly on cases of unbalanced ambiguity. Leveraging our new dataset, we achieve a new state-of-the-art for all 21 words, improving the overall average F1 score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research. 1 Introduction It is a known phenomenon that the distribution of linguistic units, or words, in a language follows a Zipf law distribution (Zipf, 1949), wherein a relatively small number of words appear frequently, and a much larger number of items appear in a long tail of words, as rare events (Czarnowska et al., 2019). Significantly, this also applies to the distribution of analyses of a given homograph. Take for instance the simple POS-tag ambiguity in English between noun and verb (Elkahky et al., 2018). The word “fair” can be used as an adjective (“a fair price”) or as a noun (“she went to the fair”). Yet, the distribution of these two analyses is certainly not fair; the adjectival usage is far more frequent than the nominal usage (e.g., in Bird et al. (2008) the latter is six times more frequent than the former). We will call such cases “unbalanced homographs”. Cases of unbalanced homographs pose a for"
2020.findings-emnlp.297,2020.sigmorphon-1.24,1,0.661224,"signan (1985). Regarding short-context disambiguation methods in general, see Hearst (1991); Yarowsky (1994). 6 We use word2vecf (Levy and Goldberg, 2014) to build syntax-sensitive word embeddings, based on a corpus of 400M words of Hebrew text. To be sure, BERT might seem the more obvious choice rather than word2vec. However, BERT has been shown to be somewhat ineffective for morphologically rich languages such as Hebrew (Tsarfaty et al., 2020). BERTbased models underperform YAP and perform at the same level as BILSTM-based models, and BERT fails to capture internal morphological complexity (Klein and Tsarfaty, 2020). 7 For verbs only, we add a morphosyntactic valence feature indicating the transitivity of the general usage of the verb. This is reminiscent of supertagging (Bangalore and Joshi, 1999) and shows non-negligible empirical contribution on our data. See Appendix Table 2 for a comparison of results with and without the valence feature. 3319 4 YAP does considerably better here: all F1 scores are above .5, and four of the cases are above .8. The weakest cases are those in which YAP has to differentiate between an unsegmented noun and a case of a noun plus possessive suffix (cases 14,20). In both of"
2020.findings-emnlp.297,D18-1277,0,0.0679408,"score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research. 1 Introduction It is a known phenomenon that the distribution of linguistic units, or words, in a language follows a Zipf law distribution (Zipf, 1949), wherein a relatively small number of words appear frequently, and a much larger number of items appear in a long tail of words, as rare events (Czarnowska et al., 2019). Significantly, this also applies to the distribution of analyses of a given homograph. Take for instance the simple POS-tag ambiguity in English between noun and verb (Elkahky et al., 2018). The word “fair” can be used as an adjective (“a fair price”) or as a noun (“she went to the fair”). Yet, the distribution of these two analyses is certainly not fair; the adjectival usage is far more frequent than the nominal usage (e.g., in Bird et al. (2008) the latter is six times more frequent than the former). We will call such cases “unbalanced homographs”. Cases of unbalanced homographs pose a formidable challenge for automated morphological parsers and segmenters. In tagged training corpora, the frequent option will naturally dominate the overwhelming majority of the occurrences. If"
2020.findings-emnlp.297,P14-2050,0,0.00940289,"raphs. The unbalanced cases are shown in the top half of the table (1-12). YAP’s F1 score is below .8 for all but one of the cases, and it is below .6 for 9 out of the 12 cases. In the two cases of Pronoun vs. Suffixed Preposition (2,4), YAP performs particularly poorly, scoring .4 and .1. In contrast, the bottom half of the table (13-21) details nine cases of balanced homographs. As expected, short contexts was demonstrated by Fraenkel et al. (1979); Choueka and Lusignan (1985). Regarding short-context disambiguation methods in general, see Hearst (1991); Yarowsky (1994). 6 We use word2vecf (Levy and Goldberg, 2014) to build syntax-sensitive word embeddings, based on a corpus of 400M words of Hebrew text. To be sure, BERT might seem the more obvious choice rather than word2vec. However, BERT has been shown to be somewhat ineffective for morphologically rich languages such as Hebrew (Tsarfaty et al., 2020). BERTbased models underperform YAP and perform at the same level as BILSTM-based models, and BERT fails to capture internal morphological complexity (Klein and Tsarfaty, 2020). 7 For verbs only, we add a morphosyntactic valence feature indicating the transitivity of the general usage of the verb. This i"
2020.findings-emnlp.391,P06-1084,0,0.181325,"the basic processing units for NLP tasks down the pipeline (Mueller et al., 2013; More and Tsarfaty, 2016). As opposed to the commonly known scenario of morphological tagging (Bohnet et al., 2013), where every input token is assigned a single morphological signature (containing its lemma, part-of-speech tag, and morphological features such as gender, number, person, tense, etc.), in the MD scenario internally-complex input tokens may consist of multiple distinct units, each of which gets assigned its own morphological signature. Pre-neural statistical approaches for MD (Barhaim et al., 2008; Adler and Elhadad, 2006a; Lee et al., 2011; Habash et al., 2013) typically used weighted finite-state machines to unravel the possible morphological decompositions, and classic machine learning models to select the most likely decomposition. Current neural models, however, take radically different paths. One neural approach to MD employs pipeline, where a predicted segmentation of words into morphemes is passed on to sequence labeling component that performs tagging of each segment in context. This segmentation-first scenario employs sequence tagging to assign a single morphological tag to each segment similar to PO"
2020.findings-emnlp.391,Q13-1034,0,0.0717364,"Missing"
2020.findings-emnlp.391,Q17-1010,0,0.0096484,"embedded vector of a fixed size. The entire MA&D process is depicted in Figure 3. 4 Experimental Setup The Data The PtrNetMD architecture we propose does not depend on any specific definition of morphological signature. To showcase this, we experiment with data from two different languages and two different annotation schemes. We use the Universal Dependencies v2.2 dataset (Nivre et al., 4370 Lattice Embedding We use pre-trained FastText models to embed the forms and lemmas. FastText models generate vectors for any word using character ngrams, thus handling Out-of-Vocabulary forms and lemmas (Bojanowski et al., 2017). For POS tags and features we instantiate and train from scratch two embedding modules. Together, these 4 embedded properties are combined to produce a single morphological analysis vector. Figure 2: Morphological Embedding Layer Architecture. An analysis composed of 3 morphemes is transformed into a single embedded vector. Figure 3: Our Proposed MA&D Architecture. A sequence of tokens is transformed into a sequence of analyses while preserving the token order. The sequence of analyses is embedded and fed into an encoder. Then at each decoding step the entire encoded representation along with"
2020.findings-emnlp.391,N10-1115,0,0.0398996,"ly with the segmentation and tags. The syntactic information contributes to the MD performance as can be seen in the Infused columns. However, our PtrNetMD handles incomplete morphological information better than MoreMD-DEP, as can be seen in the Uninfused columns. 6 Related Work Initial work on MD viewed it as a special case of POS tagging and applied generative probabilistic frameworks such as Hidden Markov Models (Barhaim et al., 2008) as well as discriminative featurebased models (Sak et al., 2009; Lee et al., 2011; Bohnet et al., 2013; Habash et al., 2013). When used as input to parsing, Goldberg and Elhadad (2010) showed that consuming the predicted MD output of Adler and Elhadad (2006b) as input to dependency parsing significantly reduced parsing performance on Hebrew. To address this error propagation inherent in the pipeline approach, More et al. (2019) and Seeker and C¸etino˘glu (2015) proposed joint morphosyntactic frameworks which enable interaction between the morphological and syntactic layers. While proving to be state-of-the-art for both MD and dependency parsing, on Hebrew and Turkish respectively, these solutions involved massive handcrafted feature engineering. MA&D on Arabic was addressed"
2020.findings-emnlp.391,P05-1071,0,0.142595,"wed that consuming the predicted MD output of Adler and Elhadad (2006b) as input to dependency parsing significantly reduced parsing performance on Hebrew. To address this error propagation inherent in the pipeline approach, More et al. (2019) and Seeker and C¸etino˘glu (2015) proposed joint morphosyntactic frameworks which enable interaction between the morphological and syntactic layers. While proving to be state-of-the-art for both MD and dependency parsing, on Hebrew and Turkish respectively, these solutions involved massive handcrafted feature engineering. MA&D on Arabic was addressed by Habash and Rambow (2005); Roth et al. (2008) using MA output and applying a set of classification and language models to make grammatical and lexical predictions. A ranking component then scored the analyses produced by the MA using a weighted sum of matched predicted features. Zalmout and 4374 Habash (2017) presented a neural version of the above system using LSTM networks in several configurations and embedding levels to model the various morphological features and use them to score and rank the MA analyses. In addition, they incorporated features based on the space of possible analyses from the MA into the MD comp"
2020.findings-emnlp.391,N13-1044,0,0.191251,"wn the pipeline (Mueller et al., 2013; More and Tsarfaty, 2016). As opposed to the commonly known scenario of morphological tagging (Bohnet et al., 2013), where every input token is assigned a single morphological signature (containing its lemma, part-of-speech tag, and morphological features such as gender, number, person, tense, etc.), in the MD scenario internally-complex input tokens may consist of multiple distinct units, each of which gets assigned its own morphological signature. Pre-neural statistical approaches for MD (Barhaim et al., 2008; Adler and Elhadad, 2006a; Lee et al., 2011; Habash et al., 2013) typically used weighted finite-state machines to unravel the possible morphological decompositions, and classic machine learning models to select the most likely decomposition. Current neural models, however, take radically different paths. One neural approach to MD employs pipeline, where a predicted segmentation of words into morphemes is passed on to sequence labeling component that performs tagging of each segment in context. This segmentation-first scenario employs sequence tagging to assign a single morphological tag to each segment similar to POS tagging in English, where each token in"
2020.findings-emnlp.391,2020.lrec-1.480,0,0.0159568,"oose the best one. In addition, our neural MD component is language agnostic and doesn’t depend on any language-specific properties, and as a result can be easily applied to any language. Yildiz et al. (2016) proposed a MA&D framework with a neural MD model, however their MD component was implemented as a binary classifier predicting whether or not a current property value is correct, and was trained in a semi-supervised fashion. Such simple topology is focused on predicting POS tags and morphological feature but is inappropriate for the general case that includes segmentation. Most recently, Khalifa et al. (2020) provided further validation of the hypothesis that in lowresource settings, morphological analyzers help boost the performance of the full morphological disambiguation task. We support this claim as well with our results on Hebrew and Turkish, which are considered low-resource languages, at least in terms of the resources the UD treebank collection provides. In the same vein, incorporating symbolic morphological information in MRLs has long shown to improve NLP tasks; see for instance Marton et al. (2013) for the contribution of morphological knowledge on parsing quality on Arabic. End-to-end"
2020.findings-emnlp.391,2020.sigmorphon-1.24,1,0.772133,"ent in context. This segmentation-first scenario employs sequence tagging to assign a single morphological tag to each segment similar to POS tagging in English, where each token in the input sequence is assigned a single label by the tagger. This method might be expected to work for MRLs just as well as standard NLP models do for English tagging, however, in actuality, such pipeline architectures are prone to error propagation, which undermines the accuracy of almost any task down the NLP pipeline (tagging, parsing, named entity recognition, relation extraction, etc.) (Tsarfaty et al., 2020; Klein and Tsarfaty, 2020; Bareket and Tsarfaty, 2020). A second conceivable approach is an end-to-end sequence-to-sequence model that consumes a sequence of tokens (or characters) and produces a 4368 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4368–4378 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Hebrew Token bbit hlbn Morphological Analysis b/ADP bit/NOUN b/ADP h/DET bit/NOUN h/DET lbn/NOUN h/DET lbn/ADJ hlbn/VERB English Translation in a house in the house the buttermilk the white whitened Table 1: Partial list of Morphological Analyses for the Hebrew to"
2020.findings-emnlp.391,P11-1089,0,0.196902,"s for NLP tasks down the pipeline (Mueller et al., 2013; More and Tsarfaty, 2016). As opposed to the commonly known scenario of morphological tagging (Bohnet et al., 2013), where every input token is assigned a single morphological signature (containing its lemma, part-of-speech tag, and morphological features such as gender, number, person, tense, etc.), in the MD scenario internally-complex input tokens may consist of multiple distinct units, each of which gets assigned its own morphological signature. Pre-neural statistical approaches for MD (Barhaim et al., 2008; Adler and Elhadad, 2006a; Lee et al., 2011; Habash et al., 2013) typically used weighted finite-state machines to unravel the possible morphological decompositions, and classic machine learning models to select the most likely decomposition. Current neural models, however, take radically different paths. One neural approach to MD employs pipeline, where a predicted segmentation of words into morphemes is passed on to sequence labeling component that performs tagging of each segment in context. This segmentation-first scenario employs sequence tagging to assign a single morphological tag to each segment similar to POS tagging in Englis"
2020.findings-emnlp.391,D15-1176,0,0.0886532,"Missing"
2020.findings-emnlp.391,D15-1166,0,0.312478,"sequence which may differ in length and vocabulary. PtrNet in addition can handle output vocabulary depending on the input sequence which can be variable in length. Seq2Seq is composed of an encoder and a decoder. The encoder consumes and encodes the entire (embedded) input sequence. Then, the decoder is fed the entire encoded input representation and step by step produces discrete outputs which are fed back as input to the next decoding step. PtrNets have an additional Copy Attention layer. The attention layer focuses on specific elements of the encoded input sequence at each decoding step (Luong et al., 2015). Copy Attention is a special case where the attention weights determine which input element the decoder’s state is most aligned with, which can then be copied to the output. Pointer Networks for MD (PtrNetMD) The PtrNet architecture is designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence (Vinyals et al., 2015). Our goal is then to encode the morphological lattice as a sequence, and then feed it to the PtrNet so that the individual analyses in the lattice can be pointed, selected and copied in"
2020.findings-emnlp.391,J13-1008,0,0.0189388,"re but is inappropriate for the general case that includes segmentation. Most recently, Khalifa et al. (2020) provided further validation of the hypothesis that in lowresource settings, morphological analyzers help boost the performance of the full morphological disambiguation task. We support this claim as well with our results on Hebrew and Turkish, which are considered low-resource languages, at least in terms of the resources the UD treebank collection provides. In the same vein, incorporating symbolic morphological information in MRLs has long shown to improve NLP tasks; see for instance Marton et al. (2013) for the contribution of morphological knowledge on parsing quality on Arabic. End-to-end neural modeling for word segmentation was addressed by Shao et al. (2018) who modeled segmentation as character-level sequence labeling, and applied it to the UD data collection. While improving the results averaged over the entire UD set, Hebrew and Arabic accuracy remained low. Wang et al. (2016) tackled the segmentation challenge by taking an unsupervised approach for learning segment boundaries, but did not address POS and morphological features assignments. A pre-requisite for our proposed approach i"
2020.findings-emnlp.391,Q19-1003,1,0.910164,"P total /(T P total + F N total ) recision×Recall) F 1 = 2×P P recision+Recall P UDPipe Oracle UDPipe Predicted Shared Task Leader PtrNetMD Infused PtrNetMD Uninfused Having morphemes available even if out of order or partially, has merit to downstream tasks that consume and further process them. Aligned mset accounts for this quality. Furthermore, both our multi-tagging and sequence-to-sequence tagging baseline models produce a tag sequence without segmentation boundaries, and aligned mset can be used to compare them against our PtrNetMD model. Finally since this computation was also used by More et al. (2019) we are able to compare our results to their non-neural MA&D framework applied to the Hebrew SPRML treebank, which is so far considered the current state-of-the-art for Hebrew segmentation and tagging. Ideal vs Realistic Analysis Scenarios Following More et al. (2019) we distinguish between two evaluation scenarios. An Infused scenario is an idealised scenario in which the input lattice to our model has complete lexical coverage, and is guaranteed to include the correct analysis as one of its many internal paths. An Uninfused scenario is a realistic case in which the lexical coverage might be"
2020.findings-emnlp.391,C16-1033,1,0.833753,"nk, our model outperforms all previously reported results for Hebrew MD in realistic scenarios. 1 Introduction In Morphologically Rich Languages (MRLs) (Tsarfaty et al., 2010), raw tokens are morphologically ambiguous, complex, and consist of sub-token units referred to as morphemes.1 Morphological Disambiguation (MD) is the task of decomposing the tokens into their constituent morphemes, 1 In Universal Dependencies terms, these are called syntactic words, to be distinguished from raw input tokens. to be used as the basic processing units for NLP tasks down the pipeline (Mueller et al., 2013; More and Tsarfaty, 2016). As opposed to the commonly known scenario of morphological tagging (Bohnet et al., 2013), where every input token is assigned a single morphological signature (containing its lemma, part-of-speech tag, and morphological features such as gender, number, person, tense, etc.), in the MD scenario internally-complex input tokens may consist of multiple distinct units, each of which gets assigned its own morphological signature. Pre-neural statistical approaches for MD (Barhaim et al., 2008; Adler and Elhadad, 2006a; Lee et al., 2011; Habash et al., 2013) typically used weighted finite-state machi"
2020.findings-emnlp.391,D13-1032,0,0.0694775,"Missing"
2020.findings-emnlp.391,L16-1262,1,0.848173,"Missing"
2020.findings-emnlp.391,P08-2030,0,0.0490214,"dicted MD output of Adler and Elhadad (2006b) as input to dependency parsing significantly reduced parsing performance on Hebrew. To address this error propagation inherent in the pipeline approach, More et al. (2019) and Seeker and C¸etino˘glu (2015) proposed joint morphosyntactic frameworks which enable interaction between the morphological and syntactic layers. While proving to be state-of-the-art for both MD and dependency parsing, on Hebrew and Turkish respectively, these solutions involved massive handcrafted feature engineering. MA&D on Arabic was addressed by Habash and Rambow (2005); Roth et al. (2008) using MA output and applying a set of classification and language models to make grammatical and lexical predictions. A ranking component then scored the analyses produced by the MA using a weighted sum of matched predicted features. Zalmout and 4374 Habash (2017) presented a neural version of the above system using LSTM networks in several configurations and embedding levels to model the various morphological features and use them to score and rank the MA analyses. In addition, they incorporated features based on the space of possible analyses from the MA into the MD component. By enriching"
2020.findings-emnlp.391,L18-1292,0,0.0127721,"POS and morphological features assignments. A pre-requisite for our proposed approach is the availabilty of a morphological analyzer (MA) component. Over the past years several MA resources have been published and are available for MA&D research. The CoNLL-UL project (More et al., 2018) provides static lattice files generated for the CoNLL18 UD shared task (Zeman et al., 2018). Other MA resources are available for specific languages, for example: HEBLEX (Adler and Elhadad, 2006a), TRMorph2 (C¸a˘grı C¸o¨ ltekin, 2014), and Calima-Star (Taji et al., 2018). To facilitate MA for the UD treebanks, Sagot (2018) produced a collection of multilingual lexicons in the CoNLL-UL format covering many of the UD languages. The Universal Morphology (UniMorph) project contains morphological data annotated in a canonical schema for many languages, which has been shown to improve, e.g., low-resource machine translation (Shearing et al., 2018). Encoding complete lattices into vector representations was previously achieved by modifying the implementation of the LSTM cells to keep track of the history of multiple node children (Ladhak et al., 2016; Su et al., 2017; Sperber et al., 2017). More recently, Sperber et a"
2020.findings-emnlp.391,Q15-1026,0,0.0472475,"Missing"
2020.findings-emnlp.391,Q18-1030,0,0.01905,"owresource settings, morphological analyzers help boost the performance of the full morphological disambiguation task. We support this claim as well with our results on Hebrew and Turkish, which are considered low-resource languages, at least in terms of the resources the UD treebank collection provides. In the same vein, incorporating symbolic morphological information in MRLs has long shown to improve NLP tasks; see for instance Marton et al. (2013) for the contribution of morphological knowledge on parsing quality on Arabic. End-to-end neural modeling for word segmentation was addressed by Shao et al. (2018) who modeled segmentation as character-level sequence labeling, and applied it to the UD data collection. While improving the results averaged over the entire UD set, Hebrew and Arabic accuracy remained low. Wang et al. (2016) tackled the segmentation challenge by taking an unsupervised approach for learning segment boundaries, but did not address POS and morphological features assignments. A pre-requisite for our proposed approach is the availabilty of a morphological analyzer (MA) component. Over the past years several MA resources have been published and are available for MA&D research. The"
2020.findings-emnlp.391,W18-1813,0,0.0145549,"es generated for the CoNLL18 UD shared task (Zeman et al., 2018). Other MA resources are available for specific languages, for example: HEBLEX (Adler and Elhadad, 2006a), TRMorph2 (C¸a˘grı C¸o¨ ltekin, 2014), and Calima-Star (Taji et al., 2018). To facilitate MA for the UD treebanks, Sagot (2018) produced a collection of multilingual lexicons in the CoNLL-UL format covering many of the UD languages. The Universal Morphology (UniMorph) project contains morphological data annotated in a canonical schema for many languages, which has been shown to improve, e.g., low-resource machine translation (Shearing et al., 2018). Encoding complete lattices into vector representations was previously achieved by modifying the implementation of the LSTM cells to keep track of the history of multiple node children (Ladhak et al., 2016; Su et al., 2017; Sperber et al., 2017). More recently, Sperber et al. (2019) applied selfattention layers coupled with reachability masks and positional embedding to efficiently handle lattice inputs. All of these lattice-aware networks were applied to speech recognition tasks, where the segmentation of the input stream refers only to overt elements, with no covert elements as in morpholog"
2020.findings-emnlp.391,D17-1145,0,0.0208145,"o facilitate MA for the UD treebanks, Sagot (2018) produced a collection of multilingual lexicons in the CoNLL-UL format covering many of the UD languages. The Universal Morphology (UniMorph) project contains morphological data annotated in a canonical schema for many languages, which has been shown to improve, e.g., low-resource machine translation (Shearing et al., 2018). Encoding complete lattices into vector representations was previously achieved by modifying the implementation of the LSTM cells to keep track of the history of multiple node children (Ladhak et al., 2016; Su et al., 2017; Sperber et al., 2017). More recently, Sperber et al. (2019) applied selfattention layers coupled with reachability masks and positional embedding to efficiently handle lattice inputs. All of these lattice-aware networks were applied to speech recognition tasks, where the segmentation of the input stream refers only to overt elements, with no covert elements as in morphology. In this work, in contrast, we cope with non-concatenative morphological phenomena where not all segments are covert. Finally, our system is simple to apply and easy to comprehend. In contrast with the non-trivial modification to the internals"
2020.findings-emnlp.391,P19-1115,0,0.1059,"ple, the analysis of the token bbit contains three morphological segments b, h, bit in the chosen path, yet the h segment is not visible in the input token bbit (Figure 1). 3 Proposed Method The Task The input to our MA&D framework is a sequence of tokens and the output is a sequence of disambiguated morphological analyses, one per token. We assume a symbolic MA that generates ambiguous lattices containing all possible morphological analyses per token, based on a broad-coverage lexicon and/or symbolic rules of the language. Given an input lattice, we frame MD as a lattice disambiguation task. Sperber et al. (2019) approached this task by constructing a specific architecture that captures the lattice representation. We, in contrast, choose to modify the lattice representation and feed it to an existing network architecture. The key idea, in a nutshell, is to linearize the lattice into a sequence of partially-ordered analyses, and feed this partial order to a pointer network. For each token, the network will then learn to point to (select) the most likely analysis, preserving the linear constraints captured in the lattice structure. Pointer Network (PtrNet) Pointer networks (Vinyals et al., 2015) are des"
2020.findings-emnlp.391,K17-3009,0,0.0564982,"Missing"
2020.findings-emnlp.391,D17-1073,0,0.047323,"Missing"
2020.findings-emnlp.391,K18-2001,0,0.0316057,"Missing"
2020.findings-emnlp.391,coltekin-2014-set,0,0.0299495,"ge by taking an unsupervised approach for learning segment boundaries, but did not address POS and morphological features assignments. A pre-requisite for our proposed approach is the availabilty of a morphological analyzer (MA) component. Over the past years several MA resources have been published and are available for MA&D research. The CoNLL-UL project (More et al., 2018) provides static lattice files generated for the CoNLL18 UD shared task (Zeman et al., 2018). Other MA resources are available for specific languages, for example: HEBLEX (Adler and Elhadad, 2006a), TRMorph2 (C¸a˘grı C¸o¨ ltekin, 2014), and Calima-Star (Taji et al., 2018). To facilitate MA for the UD treebanks, Sagot (2018) produced a collection of multilingual lexicons in the CoNLL-UL format covering many of the UD languages. The Universal Morphology (UniMorph) project contains morphological data annotated in a canonical schema for many languages, which has been shown to improve, e.g., low-resource machine translation (Shearing et al., 2018). Encoding complete lattices into vector representations was previously achieved by modifying the implementation of the LSTM cells to keep track of the history of multiple node children"
2020.findings-emnlp.391,W18-5816,0,0.0162733,"roach for learning segment boundaries, but did not address POS and morphological features assignments. A pre-requisite for our proposed approach is the availabilty of a morphological analyzer (MA) component. Over the past years several MA resources have been published and are available for MA&D research. The CoNLL-UL project (More et al., 2018) provides static lattice files generated for the CoNLL18 UD shared task (Zeman et al., 2018). Other MA resources are available for specific languages, for example: HEBLEX (Adler and Elhadad, 2006a), TRMorph2 (C¸a˘grı C¸o¨ ltekin, 2014), and Calima-Star (Taji et al., 2018). To facilitate MA for the UD treebanks, Sagot (2018) produced a collection of multilingual lexicons in the CoNLL-UL format covering many of the UD languages. The Universal Morphology (UniMorph) project contains morphological data annotated in a canonical schema for many languages, which has been shown to improve, e.g., low-resource machine translation (Shearing et al., 2018). Encoding complete lattices into vector representations was previously achieved by modifying the implementation of the LSTM cells to keep track of the history of multiple node children (Ladhak et al., 2016; Su et al., 201"
2020.findings-emnlp.391,2020.acl-main.660,1,0.50494,"ms tagging of each segment in context. This segmentation-first scenario employs sequence tagging to assign a single morphological tag to each segment similar to POS tagging in English, where each token in the input sequence is assigned a single label by the tagger. This method might be expected to work for MRLs just as well as standard NLP models do for English tagging, however, in actuality, such pipeline architectures are prone to error propagation, which undermines the accuracy of almost any task down the NLP pipeline (tagging, parsing, named entity recognition, relation extraction, etc.) (Tsarfaty et al., 2020; Klein and Tsarfaty, 2020; Bareket and Tsarfaty, 2020). A second conceivable approach is an end-to-end sequence-to-sequence model that consumes a sequence of tokens (or characters) and produces a 4368 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4368–4378 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Hebrew Token bbit hlbn Morphological Analysis b/ADP bit/NOUN b/ADP h/DET bit/NOUN h/DET lbn/NOUN h/DET lbn/ADJ hlbn/VERB English Translation in a house in the house the buttermilk the white whitened Table 1: Partial list of Morphological"
2020.findings-emnlp.391,W10-1401,1,0.740029,"ut is a morphological lattice and the output is a sequence of indices pointing at a single disambiguated path of morphemes. We demonstrate the efficacy of the model on segmentation and tagging, for Hebrew and Turkish texts, based on their respective Universal Dependencies (UD) treebanks. Our experiments show that with complete lattices, our model outperforms all shared-task results on segmenting and tagging these languages. On the SPMRL treebank, our model outperforms all previously reported results for Hebrew MD in realistic scenarios. 1 Introduction In Morphologically Rich Languages (MRLs) (Tsarfaty et al., 2010), raw tokens are morphologically ambiguous, complex, and consist of sub-token units referred to as morphemes.1 Morphological Disambiguation (MD) is the task of decomposing the tokens into their constituent morphemes, 1 In Universal Dependencies terms, these are called syntactic words, to be distinguished from raw input tokens. to be used as the basic processing units for NLP tasks down the pipeline (Mueller et al., 2013; More and Tsarfaty, 2016). As opposed to the commonly known scenario of morphological tagging (Bohnet et al., 2013), where every input token is assigned a single morphological"
2020.findings-emnlp.50,N19-1423,0,0.0166581,"ains descriptions that can be seen in a bird image. For each document, we propose to calculate the pairwise similarity between captions and sentences in the Wikipedia description, and based on this similarity, assign a VRS-score to each sentence. We calculate the VRS-score of a sentence sj to a caption by computing the cosine similarity of the embeddings of both the captions (c0:L ) and sentences (s0:M ) in the document. For a fixed-size sentence embeddings, we use a pre-trained siameseand-triplet network (Reimers and Gurevych, 2019; Schroff et al., 2015) on top of a pre-trained BERT network (Devlin et al., 2019). The VRS-score of sentence sj with respect to all available captions c1:L is thus defined to be: score(sj ) = L X ci · sj ksj kkci k (3) i=1 We then take the highest k scoring sentences from s0:K to be the visually relevant extractive summary of the document. We can then concatenate the similarity embedding to the VRS summary of the text, and perform the multiplicative attention on this revised encoding of the documents and the same image encoding as before. A bird’s eye overview of our overall architecture is presented in Figure 2. The text that enters the similarity (clustering) component i"
2020.findings-emnlp.50,D15-1166,0,0.0601039,"and are not optimal for classifying long textual descriptions. In this work, we proceed in a different, yet complementary, direction to previous work, aiming to purposefully model the contribution of the textual modality to ZSL. We aim to establish the importance of adequately processing the text into a sound representation of visually salient features, in order to increase the vision-and-language compatibility, which can then be effectively learned in an end-toend manner. 3 Strong Baseline Model The basic architecture, which term ZESTvanilla , is a simple multiplicative attention mechanism (Luong et al., 2015) inspired by Romera-Paredes and Torr (2015). We model the problem using an attention-based model, where the image is queried against a set of candidate documents. Formally, let xS1 , . . . , xSM be image feature vectors from a training-set, where xSi ∈ Rm . The set of M training images corresponds to a set of L seen classes. Each class has a single “class description” which is a document written by experts in free language (e.g. Wikipedia). We denote dS1 , . . . , dSL as a ˆ. set of L document feature vectors, where dSi ∈ Rm U U Likewise, let x1 , . . . , xN be the image feature vectors from a"
2020.findings-emnlp.50,D19-1410,0,0.0174755,", the classifier will predict whether the sentence is relevant, that is, whether it contains descriptions that can be seen in a bird image. For each document, we propose to calculate the pairwise similarity between captions and sentences in the Wikipedia description, and based on this similarity, assign a VRS-score to each sentence. We calculate the VRS-score of a sentence sj to a caption by computing the cosine similarity of the embeddings of both the captions (c0:L ) and sentences (s0:M ) in the document. For a fixed-size sentence embeddings, we use a pre-trained siameseand-triplet network (Reimers and Gurevych, 2019; Schroff et al., 2015) on top of a pre-trained BERT network (Devlin et al., 2019). The VRS-score of sentence sj with respect to all available captions c1:L is thus defined to be: score(sj ) = L X ci · sj ksj kkci k (3) i=1 We then take the highest k scoring sentences from s0:K to be the visually relevant extractive summary of the document. We can then concatenate the similarity embedding to the VRS summary of the text, and perform the multiplicative attention on this revised encoding of the documents and the same image encoding as before. A bird’s eye overview of our overall architecture is p"
2020.findings-emnlp.50,D19-1514,0,0.0139003,"e assume the latter scenario. ZSL studies that rely on Wikipedia articles as auxiliary information improve the visual representation and the compatibility function, and use text representations such as Bag-of-Words and TF-IDF, without further text processing. (Lei Ba et al., 2015; Elhoseiny et al., 2013, 2016, 2017; Zhu et al., 2018). Qiao et al. (2016) used a simple BOW and a L1,2 -norm objective to suppress the noisy signal in the text. However, this basic treatment of the text is problematic, as it misses crucial information for detecting the correct class. Recent studies (Lu et al., 2019; Tan and Bansal, 2019) have shown improved performance on multiple vision-and-language tasks using pre-trained BERT-based models that jointly learn a representation for vision and language. However, they are tuned on relatively short texts and are not optimal for classifying long textual descriptions. In this work, we proceed in a different, yet complementary, direction to previous work, aiming to purposefully model the contribution of the textual modality to ZSL. We aim to establish the importance of adequately processing the text into a sound representation of visually salient features, in order to increase the v"
2020.findings-emnlp.50,W16-3213,0,0.0243028,"emphasize the parts that are different, both in the image and the text — and these are typically their most salient visual features. 4.2.1 Visually Relevant Summaries (VRS) Our method for enhancing the textual description is based on visually relevant extractive summaries. Extractive summarization is the task of extracting a small number of sentences that summarize a given document. In this work, we define visually relevant extractive summarization (VRS) as the task of extracting only sentences that represent visually relevant language. The term visually relevant language (VRL) was coined by Winn et al. (2016) to indicate sentences which are visually descriptive with respect to the object (i.e., bird species). A na¨ıve approach for VRS would be to extract sentences with parts that we know are visually salient in our domain (e.g., the 7 parts employed by the vision recognition representation). However, this na¨ıve approach has several drawbacks. First, bird parts can be described using many different terms and paraphrases; additionally, a bird can be described by its property values (e.g., black), without any mention of the attribute (e.g., beak). Instead, we propose to use the similarity of sentenc"
2020.insights-1.17,2020.acl-main.774,0,0.0776033,"Missing"
2020.insights-1.17,D19-1606,0,0.0279413,"t (and thus the context suggests an implied event). The second modeling follows the NLI scheme, a standard approach for evaluating language understanding. The ENT, NEU and CON labels refer to the entail, neutral and contradict labels accordingly. Introduction Crowdsourcing has become extremely popular in recent years for annotating datasets. Many works use frameworks like Amazon Mechanical Turk (AMT) by converting complex linguistic tasks into easy-to-grasp presentations which make it possible to crowdsource linguistically-annotated data at scale (Bowman et al., 2015; FitzGerald et al., 2018; Dasigi et al., 2019; Wolfson et al., 2020). In this work, we attempt to use existing methodologies for crowdsourcing linguistic annotations in order to collect annotations for complement coercion (Pustejovsky, 1991, 1995), a phenomenon involving an implied action triggered by an eventselecting verb. Specifically, certain verb classes require an event-denoting complement, as in: “I started reading a book”, “I finished eating the cake”, etc. However, such event-denoting complements might remain implicit, not appearing in the surface form. Consider for instance, the sentence “I started a new book.” Here the event t"
2020.insights-1.17,P18-2103,1,0.842896,"ese sentences are used as the hypotheses. To construct the premises, we remove the dependent verb (e.g. ‘read’), as well as all the words between the anchor and the dependent verb (e.g. ‘to’ in the infinitive form: “to read”). Additional examples are provided in Appendix D. Note that this procedure sometimes generates ungrammatical or implausible sentences, which are flagged by the annotators. Crowdsourcing Procedure We follow the standard procedure of collecting NLI data with crowdsourcing and collect annotations from Amazon Mechanical Turk (AMT). Specifically, we follow the instruction from Glockner et al. (2018), which involves three questions: 1. Do the sentences describe the same event? 2. Does the new sentence add new information to the original sentence? 3. Is the new sentence incorrect/ungrammatical? We discard any example which at least one worker marked as incorrect/ungrammatical. If the answer 3 We follow Bowman et al. (2015), who modeled entailment based on event coreference. 108 4 These are frequent verbs that often appear in complement coercion constructions (McGregor et al., 2017). 5 We use spaCy’s parser (Honnibal and Johnson, 2015; Honnibal and Montani, 2017). to the first question was"
2020.insights-1.17,S10-1005,0,0.0314834,"on, termination, or continuation of an activity” (Levin, 1993) — such as: ‘start’, ‘begin’, ‘continue’ and ‘finish’ (McGregor et al., 2017). This set of verbs is the focus of our work. Note however, that such verbs may appear in similar constructions that do not imply any covert action or event. For instance, in the following sentence: 3. I started a new company. Here, the verb ‘start’ is used as an entity-selecting (and not event-selecting) verb, a synonym of ‘found’ or ‘establish’. See more examples of similar non-coercive constructions in Appendix B. Annotated data for complement coercion (Pustejovsky et al., 2010) was collected in the past, based on a tailor-made annotation methodology (Pustejovsky et al., 2009), consisting of a multi-step process that includes word-sense disambiguation by experts. The annotation focused on coercion detection (as well as labeling the arguments type) and did not involve identifying the implied action. Here, we aim to collect complement coercion data via non-expert annotation, at scale, to test whether models can recover the implicit events and resolve the emerging ambiguities. Crowdsourcing NLI NLI, originally framed as Recognizing Textual Entailment (RTE), has become a"
2020.insights-1.17,L18-1058,0,0.0281906,"Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Sluicing (Hansen and Søgaard, 2020). However, when attempting to crowdsource and label complement coercion instances, we reach very low agreement scores in the first step: determining whether there is an implied event or not. We discuss this experiment in greater detail in Appendix C. 3.2 NLI for Complement Coercion In light of the low agreements on explicit modeling of the task of complement coercion, we turn to a different crowdsourcing approach which was proven successful for many linguistic phenomena – using NLI as discussed above (§2). NLI was used to collect data"
2020.insights-1.17,2020.acl-main.626,0,0.0249531,", we also reach an understanding that the datasets at hand do not reflect the full capacity of language, and specific linguistic phenomena, which may posses specific challenges, are lost in 7 Due to large scale annotations, ‘marginal’ phenomena might be ignored to keep the instructions clear and concise. the crowds. Some phenomena turn out to be more complex, and require specific solutions. In this work we show that, like we do with algorithmic solutions we need to reconsider the data collection process. We hold that data collection for these phenomena also require training of the annotators (Roit et al., 2020; Pyatkin et al., 2020), whether experts or crowdsourcing workers, and may also require coming up with novel annotation protocols. Another potential solution is to use deliberation between the workers as a mean to improve agreement (Schaekermann et al., 2018). With respect to the disagreements we observed, a deliberation between workers would allow them to share the construals each individual had imagined, thus reaching a consensus on the labels. It would also serve as a training for recovering more construals, allowing them to better identify the neutral cases. 5 Conclusions In this work, we"
2020.insights-1.17,D19-1228,0,0.0139234,"specific focus on lexical and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Slui"
2020.insights-1.17,2020.acl-main.462,0,0.0375187,"ing his case.” ENT NEU CON 6. “We start the interviews later today.” ; “We start shooting the interviews later today.” NEU CON CON Example 4 was labeled by all three annotators as entail. However, annotators were in disagreement on examples 5, 6. Example 5 was annotated with all three possible labels (entail, contradict and neutral). Indeed, different readings of this phrase are possible — more formally, different readers construe the meaning of the utterance differently; “[Construal] is a dynamic process of meaning construction, in which speakers and hearers encode and decode, respectively” (Trott et al., 2020). An annotator who understands the word ‘case’ as a legal case, will choose entail, while an annotator 6 We stopped at 76 examples since we did not see fit to annotate more data with the low agreements we obtained. who interprets ‘case’ as a bag and imagines a different background story (for example, a young man packing a brief-case), will choose contradict. Finally, an annotator who thinks of both scenarios will choose neutral, which can be argued to be the correct answer. However, we find that for a human hearer, holding both scenarios in mind at the same time is hard, which we attribute to"
2020.insights-1.17,2020.tacl-1.25,0,0.0187519,"his format, along with the different labels we employ, are shown in Table 2. Example Label I started a book I bought last week. ; I started reading a book I bought last week. ENT I started a book. ; I started reading a book. I started eating a book. NEU CON Table 2: Examples for NLI pairs with a complement coercion structure. The ENT, NEU and CON labels refers to entail, neutral and contradict accordingly. Corpus Candidates In order to keep the task simple, we avoid complexities of lexical, semantic and grammatical differences. Each example is composed of a minimal-pair (Kaushik et al., 2019; Warstadt et al., 2020; Gardner et al., 2020) consisting of two sentences; one as the premise and the other as the hypothesis. We construct minimal pairs as follows: First, we extract dependencyparsed sentences from the Book Corpus (Zhu et al., 2015) containing the lemma of one of the verbs: ‘start’, ‘begin’, ‘continue’ and ‘finish’.4 Then, we keep sentences where the anchor verb is attached to another verb with an ‘xcomp’ dependency5 (e.g. ‘started’ in “started reading”). These sentences are used as the hypotheses. To construct the premises, we remove the dependent verb (e.g. ‘read’), as well as all the words betw"
2020.insights-1.17,I17-1100,0,0.0542104,"Missing"
2020.insights-1.17,N18-1101,0,0.0348949,"t coercion data via non-expert annotation, at scale, to test whether models can recover the implicit events and resolve the emerging ambiguities. Crowdsourcing NLI NLI, originally framed as Recognizing Textual Entailment (RTE), has become a standard framework for testing reasoning capabilities of models. It originated from the work by Dagan et al. (2005), where a small dataset was curated by experts using precise guidelines with a specific focus on lexical and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences c"
2020.insights-1.17,2020.acl-main.543,0,0.0501549,"al and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Sluicing (Hansen and Søgaa"
2020.sigmorphon-1.24,P17-1080,0,0.0261069,"label-space that needs to be learned per word. Also, unlike the previous scenario, the model is able to generate unseen multitags (to some extent) by creating previously unseen Prefix-Host compositions. 1 Since Hebrew can stack prefixes before a host, the prefixes require a multi-tag. Similarly, hosts with pronominal clitics may also be assigned a multi-tag rather than one tag. 207 especially in the field on Neural Machine Translation. In 2010 Luong et al. (2010) explicitly showed that incorporating morphological knowledge in the translation process significantly improves translation. In 2017 Belinkov et al. (2017) found that for learning morphology it is better to use character based representation rather than word-based ones. They also found that neural networks encode morphology in the lower layers of the network, which might explain why mere finetuning is insufficient to capture morphological complexity. Later, Straka et al. (2019) achieved SoTA on POS tagging on 54 languages, including Heberew, but was using BERT embeddings along with character level embeddings and Fasttext (Bojanowski et al., 2017) word embeddings on gold morphology, which strengthen our claim that word pieces by themselves don’t"
2020.sigmorphon-1.24,H92-1086,0,0.165329,"tactic relations by inflection or agglutination at word level. In NLP, MRLs often require segmentation into sub-word units called morphemes as part of the pre-processing in the NLP pipelines. The term morphological fusion, or simply fusion, refers to the degree to which morphemes are connected to a word host or stem (Bickel and Nichols, 2013). There are three values for the degree of fusion: isolating (low), concatenative (mild) and non-concatenative (high). MRLs thus belong to the mild- and high-fusion language groups. In concatenative MRLs like Turkish (Swift, 1963) and Russian (Wade, 1992; Shevelov, 1957) morphemes are linearly connected to the stem, and so a concatenated word-form can easily be segmented back into its composing morphemes. Segmenting highly fusional MRLs (henceforth fMRL), like Hebrew (Berman and Bolozky, 1978), is not as simple, since words can be affixed in such a way that makes the stem and/or affix undergo morpho-phonological changes resulting in ambiguous, syncretic word-forms. These changes cannot be restored without morphological disambiguation of the word in context of the whole sentence. Furthermore, word-forms may involve a combination of a root and a template which"
2020.sigmorphon-1.24,P81-1022,0,0.233393,"Missing"
2020.sigmorphon-1.24,Q17-1010,0,0.0677569,"orating morphological knowledge in the translation process significantly improves translation. In 2017 Belinkov et al. (2017) found that for learning morphology it is better to use character based representation rather than word-based ones. They also found that neural networks encode morphology in the lower layers of the network, which might explain why mere finetuning is insufficient to capture morphological complexity. Later, Straka et al. (2019) achieved SoTA on POS tagging on 54 languages, including Heberew, but was using BERT embeddings along with character level embeddings and Fasttext (Bojanowski et al., 2017) word embeddings on gold morphology, which strengthen our claim that word pieces by themselves don’t capture morphology well. This was also supported by Mielke and Eisner (2019), that explicitly mentioned the non-concatenativity of Hebrew and Arabic as the major drawback of sub word tokenization systems. WP gets assigned the multi-tag of the actual morphemes it contains. At inference time we provide BERT-tokenized words as input, and each WP gets assigned an informed multi-tag as observed during fine-tuning. For evaluation, we combine the prediction made on all WPs of a word to a single ordere"
2020.sigmorphon-1.24,W10-1401,1,0.906224,"Missing"
2020.sigmorphon-1.24,N19-1423,0,0.0105013,"be determined in the context of the utterance, making explicit the contribution of each linguistic sub-word unit (a.k.a., morpheme) to the global meaning. In this study we aim to investigate how well morphological information is captured by contextualized embedding models, or, more specifically, by their underlying word-pieces. We hypothesize that the word-pieces tokenization scheme in these models, which is not reflective of the actual morphology, will decrease the models ability to predict morphological functions on sub-word units. In order to test this hypothesis we use Multilingual BERT (Devlin et al., 2019) on the task of multi-tagging raw words in a morphologically rich and ambiguous language, Modern Hebrew. Pre-neural studies on Hebrew found that explicitly modeling sub-word morphological information, substantially improves results on tagging and parsing down the NLP pipeline (More and Tsarfaty, 2016; More et al., 2019). Here our results show a significant drop in multi-tagging accuracy in word-level settings compared to settings where we aim to tag the distinct WPs. Nevertheless, when we purposefully incorporate morphological knowledge that reflect the internal functions of WPs, the tagging o"
2020.sigmorphon-1.24,D10-1015,0,0.0394486,"unambiguously detect which morphemes are relevant for the WP only, and the One technical advantage of this setting is that it substantially limits the label-space that needs to be learned per word. Also, unlike the previous scenario, the model is able to generate unseen multitags (to some extent) by creating previously unseen Prefix-Host compositions. 1 Since Hebrew can stack prefixes before a host, the prefixes require a multi-tag. Similarly, hosts with pronominal clitics may also be assigned a multi-tag rather than one tag. 207 especially in the field on Neural Machine Translation. In 2010 Luong et al. (2010) explicitly showed that incorporating morphological knowledge in the translation process significantly improves translation. In 2017 Belinkov et al. (2017) found that for learning morphology it is better to use character based representation rather than word-based ones. They also found that neural networks encode morphology in the lower layers of the network, which might explain why mere finetuning is insufficient to capture morphological complexity. Later, Straka et al. (2019) achieved SoTA on POS tagging on 54 languages, including Heberew, but was using BERT embeddings along with character l"
2020.sigmorphon-1.24,Q19-1003,1,0.784684,"pieces. We hypothesize that the word-pieces tokenization scheme in these models, which is not reflective of the actual morphology, will decrease the models ability to predict morphological functions on sub-word units. In order to test this hypothesis we use Multilingual BERT (Devlin et al., 2019) on the task of multi-tagging raw words in a morphologically rich and ambiguous language, Modern Hebrew. Pre-neural studies on Hebrew found that explicitly modeling sub-word morphological information, substantially improves results on tagging and parsing down the NLP pipeline (More and Tsarfaty, 2016; More et al., 2019). Here our results show a significant drop in multi-tagging accuracy in word-level settings compared to settings where we aim to tag the distinct WPs. Nevertheless, when we purposefully incorporate morphological knowledge that reflect the internal functions of WPs, the tagging of WPs substantially improves. Introduction Contextualized word-embedding models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), rely on sub-word units called wordpieces (Johnson et al., 2017), that enable these models to generalize over frequent charactersequences and elegantly handle out-of-vocabular"
2020.sigmorphon-1.24,C16-1033,1,0.872757,"by their underlying word-pieces. We hypothesize that the word-pieces tokenization scheme in these models, which is not reflective of the actual morphology, will decrease the models ability to predict morphological functions on sub-word units. In order to test this hypothesis we use Multilingual BERT (Devlin et al., 2019) on the task of multi-tagging raw words in a morphologically rich and ambiguous language, Modern Hebrew. Pre-neural studies on Hebrew found that explicitly modeling sub-word morphological information, substantially improves results on tagging and parsing down the NLP pipeline (More and Tsarfaty, 2016; More et al., 2019). Here our results show a significant drop in multi-tagging accuracy in word-level settings compared to settings where we aim to tag the distinct WPs. Nevertheless, when we purposefully incorporate morphological knowledge that reflect the internal functions of WPs, the tagging of WPs substantially improves. Introduction Contextualized word-embedding models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), rely on sub-word units called wordpieces (Johnson et al., 2017), that enable these models to generalize over frequent charactersequences and elegantly han"
2021.acl-long.77,J12-2006,0,0.41069,"ce of modal senses that linguists identify. Consequently, modality features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e."
2021.acl-long.77,W18-4912,0,0.0233428,"í and Pustejovsky, 2009; Rudinger et al., 2018). Although these tasks rely on modality features, so far there is no accepted standard for modal concepts and labels, which aligns with the semantic space of modal senses that linguists identify. Consequently, modality features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of defici"
2021.acl-long.77,hendrickx-etal-2012-modality,0,0.0371211,"tures are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e.g., can, might, should, must in English (Ruppenhofer and Rehbein, 2012; Ma"
2021.acl-long.77,2016.lilt-14.4,0,0.153874,"lity features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e.g., can, might, should, must in English (Ruppenho"
2021.acl-long.77,2021.ccl-1.108,0,0.0355769,"Missing"
2021.acl-long.77,W16-1613,0,0.0605052,"Missing"
2021.acl-long.77,2016.lilt-14.3,0,0.0353098,"Missing"
2021.acl-long.77,2016.lilt-14.5,0,0.0829621,"tion for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e.g., can, might, should, must in English (Ruppenhofer and Rehbein, 2012; Marasovi´c et al., 2016; Quaresma et al., 2014)). However, as ackn"
2021.acl-long.77,J12-2001,0,0.0281141,"ntributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, but also improves the detection of modal events in their own right. 1 (2) Modality refers to the linguistic ability to describe alternative ways the world could be.1 Modal expressions aim to identify wishes, rules, beliefs, or norms in texts (Kratzer, 1981; Portner, 2009), which is a crucial part of Natural Language Understanding (NLU) (Morante and Sporleder, 2012). Concretely, events in natural language are often reported in a manner that emphasizes non-actual perspectives on them, rather than their actual propositional content. Consider examples (1a)–(1b): ∗ a. We presented a paper at ACL’19. b. We did not present a paper at ACL’20. The propositional content p =“present a paper at ACL’X” can be easily verified for sentences (1a)(1b) by looking up the proceedings of the conference to (dis)prove the existence of the relevant publication. The same proposition p is still referred to in sentences (2a)–(2d), but now in each one, p is described from a differ"
2021.acl-long.77,W13-0501,0,0.0219487,"of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e.g., can, might, should, must in English (Ruppenhofer and Rehbein, 2012; Marasovi´c et al., 2016;"
2021.acl-long.77,D14-1162,0,0.087985,"with an increasing level of complexity: 6 The processed data is available at https://github. com/OnlpLab/Modality-Corpus. 7 Words like can and right have non-modal meanings in addition to modal meanings. 1. M ODAL S ENSE C LASSIFICATION . Here we aim to classify the modal sense of a trigger, assuming a modal trigger is already known. Specifically, we examine the contribution of the context to the lemma. We perform sense classification with the following variations: (i) Vote: a majority vote, (ii) Token: out of context token-based classification where the trigger token is encoded using GloVe (Pennington et al., 2014)), (iii) Context: Tokenin-context classification, given the whole sentence encoded with RoBERTa (Liu et al., 2019) as input, with a marked trigger position, (iv) Masked: given the sentence encoded with RoBERTa but with the trigger masked, (v) Trigger+Head: only the trigger word and event head are given, encoded with RoBERTa, and finally, (vi) Full+Head: the full sentence is encoded using RoBERTa with both the trigger and the event head marked. 2. M ODALITY D ETECTION AND C LASSIFICA TION . This is a realistic scenario, where we do not assume the trigger is known. We aim to both identify the tr"
2021.acl-long.77,W13-0306,1,0.718248,"n such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more. Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these senses are often analyzed independently of the events that they modify. This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, but also improves the detection of modal events in their own right. 1 (2) Modality refers to the linguistic ability to describe alter"
2021.acl-long.77,N18-1067,0,0.021491,"Missing"
2021.acl-long.77,ruppenhofer-rehbein-2012-yes,0,0.170733,"ich aligns with the semantic space of modal senses that linguists identify. Consequently, modality features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class o"
2021.acl-long.77,W00-0726,0,0.0139479,"ating if the token is a modal trigger, and if so, what coarse-grained sense it has (Plausibility vs. Priority). Fine-Grained: indicating if the token is a modal trigger, and if so, which one of the senses at the lowest level of the hierarchy it has. We conflated Desires/Wishes and Plans/Goals into a single type called Intentions, since both these senses are under-represented in our corpus. See appendix A for the complete label distribution in our data. Evaluation Metrics We report for all experiments BIOSE-chunk Precision, Recall, and (Macro) F1, calculated with the official ConllEval script (Sang and Buchholz, 2000). When evaluating span tagging for event-based modality we report labeled and unlabeled scores. When we report unlabeled F1 for trigger classification, we check whether the token has been correctly identified as modal vs. not-modal, regardless of its sense. in the test set is tagged with its most frequent label in the training set. For detecting modal triggers as well as for event detection, we experiment by fine-tuning a RO BERTA-based classifier (Liu et al., 2019).8 The encoded sequence is fed through a linear layer with a softmax function predicting the appropriate tag for a given token. Fo"
2021.acl-long.77,W08-0606,0,0.149121,"pass, and in fact, such verification is not the goal of this way of reporting. Rather, speakers describe such events in order to indicate PLANS (2a), DESIRES (2b), NORMS (2c), or the assessed PLAUSIBILITY (2d) of the associated propositional content p. Investigating how to classify these perspectives on events has been the focus of extensive research on modality in theoretical linguistics (Kratzer, 1981; Palmer, 1986; Portner, 2009). In terms of NLP technology, modal concepts as expressed in (2) are relevant to many downstream tasks, such as the automatic detection of hedging and speculation (Vincze et al., 2008; Malhotra et al., 2013), uncertainty (Vincze et al., 2008; Miwa et al., 2012; Zerva et al., 2017; Prieto et al., 2020), opinion (Wiebe et al., 2005; Rubin, 2010; Miwa et al., 2012), and factuality (Saurí and Pustejovsky, 2009; Rudinger et al., 2018). Although these tasks rely on modality features, so far there is no accepted standard for modal concepts and labels, which aligns with the semantic space of modal senses that linguists identify. Consequently, modality features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International"
2021.acl-long.77,P18-4013,0,0.0122262,"iary verbs (can, could, may, must, should, and shall) and modal senses from a restricted set of three labels (deontic, dynamic, epistemic). Note that their proposed setup is not designed to separate modal sentences from non-modal ones, as the Marasovi´c and Frank (2016) dataset contains only modal sentences. Second, it cannot directly indicate that a sentence contains multiple modal triggers with different senses. Models Our baseline for modal trigger detection is a simple majority vote baseline where each token 959 8 We also experimented with a PyTorch-based sequence tagging model (NCRF++ by Yang and Zhang (2018)) with GoogleNews-vectors-negative300 embeddings (https://code.google.com/archive/p/word2vec/), but this setting did not outperform our majority vote baseline (and certainly under-performed the model based on contextualized representations), and we didn’t pursue this direction further. 9 The code for data processing, configuration files and training are available at https://github.com/OnlpLab/ Modality. Unlabeled Labeled Baseline RoBERTa Baseline RoBERTa Modal/Not Modal P R F1 75.81 62.07 68.24 70.05 76.68 73.2 NA NA NA NA NA NA Coarse-Grained P R 75.81 62.07 72.07 76.17 71.36 57.92 67.03 70.8"
2021.acl-long.77,W15-2705,0,0.018594,"at linguists identify. Consequently, modality features are 953 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 953–965 August 1–6, 2021. ©2021 Association for Computational Linguistics either treated idiosyncratically or are absent from semantic frameworks (Donatelli et al., 2018, §4.6). In support of such downstream tasks, a different type of NLP investigations targets modality annotation and detection in its own right (Ruppenhofer and Rehbein (2012); Baker et al. (2012); Zhou et al. (2015); Marasovi´c and Frank (2016); Hendrickx et al. (2012); Nissim et al. (2013); Ghia et al. (2016); Mendes et al. (2016); Lavid et al. (2016), and others). However, each of these studies creates its own scheme, and none of these schemes has been picked up as an accepted standard by the community. Moreover, different endeavors suffer from one (or more) of the following types of deficiencies with respect to their expressivity and coverage. First, many studies limit the modal triggers, i.e., the expressions that trigger the modal meaning, to a closed class of auxiliary verbs (e.g., can, might, shou"
2021.emnlp-main.108,bonial-etal-2014-propbank,0,0.017027,"prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing i"
2021.emnlp-main.108,N19-1423,0,0.0254101,"does someone win at? is at once too specific (inappropriate for locations better described with in, e.g., in Texas) and too general (also potentially applying to win.01’s A2 role, the contest being won). To choose the right prototype, we run a consistency check using an off-the-shelf QA model (see Figure 2, Bottom). We sample a set of gold arguments5 for the role from OntoNotes (Weischedel et al., 2017) and instantiate each prototype for each sampled predicate using the question contextualizer described in the next section (§4.2). We then select the prototype for which a BERT-based QA model (Devlin et al., 2019) trained on SQuAD 1.0 (Rajpurkar et al., 2016) achieves the highest tokenwise F1 in recovering the gold argument from the contextualized question. 4.2 Generating Contextualized Questions For our second stage, we introduce a question contextualizer model which takes in a prototype question and passage, and outputs a contextualized ver5 For core roles we sample 50 argument instances, and for adjunct roles we take 100 but select samples from any predicate sense. 1432 Air molecules move a lot and bump into things. QA-SRL: What bumps into something? ,→ Air molecules What does something bump into? ,"
2021.emnlp-main.108,2020.acl-main.69,0,0.036114,"Missing"
2021.emnlp-main.108,doddington-etal-2004-automatic,0,0.2616,"Missing"
2021.emnlp-main.108,2020.emnlp-main.49,0,0.188877,"prehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was SBJ someone something VERB bring arrive s"
2021.emnlp-main.108,P17-1123,0,0.0198036,"tomatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologie"
2021.emnlp-main.108,2020.coling-main.274,1,0.906344,"ive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may o"
2021.emnlp-main.108,N18-1020,0,0.0184749,"does the question correspond to the correct semantic role? For all of these measures, we source our data from existing SRL datasets and use human evaluation by a curated set of trusted workers on Amazon Mechanical Turk.6 Automated metrics like B LEU or ROUGE are not appropriate for our case because our questions’ meanings can be highly dependent on minor lexical choices (such as with prepositions) and because we lack gold references (particularly for questions without answers present). We assess grammaticality and adequacy on a 5point Likert scale, as previous work uses for similar measures (Elsahar et al., 2018; Dhole and Manning, 2020). We measure role correspondence with two metrics: role accuracy, which asks annotators to assign the question a semantic role based on PropBank role glosses, and question answering accuracy, which compares annotators’ answers to the question against the gold SRL argument (or the absence of such an argument).7 5.2 Main Evaluation Data We evaluate our system on a random sample of 400 predicate instances (1210 questions) from Ontonotes 5.0 (Weischedel et al., 2017) and 120 predicate instances (268 questions) from two small implicit SRL datasets: Gerber and Chai (2010, G"
2021.emnlp-main.108,K17-1034,0,0.0188967,"t al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast maj"
2021.emnlp-main.108,P18-1191,1,0.943025,"s allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions"
2021.emnlp-main.108,2020.acl-main.703,0,0.0647819,"Missing"
2021.emnlp-main.108,W18-0530,0,0.016194,"ntial step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found way"
2021.emnlp-main.108,W18-2501,0,0.0210027,"Missing"
2021.emnlp-main.108,P10-1160,0,0.0876674,"roach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is def"
2021.emnlp-main.108,D15-1076,0,0.168872,"e questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 142"
2021.emnlp-main.108,N10-1086,0,0.0565531,"t is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification"
2021.emnlp-main.108,W04-2705,0,0.0418399,"fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can"
2021.emnlp-main.108,2021.findings-acl.389,1,0.848242,"Missing"
2021.emnlp-main.108,W03-0203,0,0.245581,"e ability to exhaustively enumerate a set of questions corresponding to a known, broadcoverage underlying ontology of relations allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a"
2021.emnlp-main.108,W13-0211,0,0.0536231,"Missing"
2021.emnlp-main.108,N19-1236,1,0.88712,"Missing"
2021.emnlp-main.108,J05-1004,0,0.239174,"ogy, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was"
2021.emnlp-main.108,2020.emnlp-main.224,1,0.745168,"and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferr"
2021.emnlp-main.108,2020.findings-emnlp.3,0,0.0188773,", and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions f"
2021.emnlp-main.108,P18-2124,0,0.0554899,"Missing"
2021.emnlp-main.108,D16-1264,0,0.330999,"by asking questions is an es- generate a contextually-appropriate question whose sential communicative ability, and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting cove"
2021.emnlp-main.108,2020.acl-main.626,1,0.831683,"with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic"
2021.emnlp-main.108,W09-2417,0,0.031509,"al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is defined with respect to an ont"
2021.emnlp-main.159,P17-1183,0,0.0173038,"equent and diverse selection (Sec. 4.1). To examine the importance of the selection strategy we altered it to disregard class membership, and we automatically selected lexemes to maximize only frequency. The results of this experiment are in Table 4. The results show that the selection procedure is indeed important as all different algorithms suffered a loss in performance. It is also evident that diversity is particularly crucial for the orthographic algorithm that is based on the different edit scripts and has less evidence when seed examples cover less classes. 5 Related Work et al., 2016; Aharoni and Goldberg, 2017), and the lemma can be either spelled-out, or inputted as an index in a dictionary (Malouf, 2017).13 While most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction"
2021.emnlp-main.159,D19-1091,0,0.0288073,"Missing"
2021.emnlp-main.159,E17-1032,0,0.0163328,"for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the"
2021.emnlp-main.159,K17-2002,0,0.0218059,"tive morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation. The state of the art model for classic low-resourced scenarios, with a diverse but small dataset, is the transition-based model of Makarov and Clematide (2018). In addition, some works deal with other low-resourced scenarios and assume no inflection tables at all, and are focused on paradigm detection/completion in addition to inflection (Dreyer and Eisner, 2011; Elsner et al., 2"
2021.emnlp-main.159,P18-1246,0,0.0461656,"Missing"
2021.emnlp-main.159,K18-3001,0,0.291534,"Missing"
2021.emnlp-main.159,K17-2001,0,0.0217778,"Missing"
2021.emnlp-main.159,W02-0603,0,0.204419,"ry (Malouf, 2017).13 While most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentatio"
2021.emnlp-main.159,D11-1057,0,0.0202477,"os. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation. The state of the art model for classic low-resourced scenarios, with a diverse but small dataset, is the transition-based model of Makarov and Clematide (2018). In addition, some works deal with other low-resourced scenarios and assume no inflection tables at all, and are focused on paradigm detection/completion in addition to inflection (Dreyer and Eisner, 2011; Elsner et al., 2019; Jin et al., 2020). In contract, our scenario provides the knowledge on the paradigmatic structure In recent years, neural sequence-to-sequence models have taken the lead in all forms of morphological tagging and morphological inflection tasks. Morphological tagging is an analysis task where the input is a complete sentence, i.e., a sequence of word forms, and the model aims to assign each word form in-context a morphological signature that consists of its lemma, part-of-speech (POS) tag, and a set of inflectional features (Hajic, 2000; Mueller et al., 2013; Bohnet et al."
2021.emnlp-main.159,P02-1001,0,0.0915663,"e selection procedure is indeed important as all different algorithms suffered a loss in performance. It is also evident that diversity is particularly crucial for the orthographic algorithm that is based on the different edit scripts and has less evidence when seed examples cover less classes. 5 Related Work et al., 2016; Aharoni and Goldberg, 2017), and the lemma can be either spelled-out, or inputted as an index in a dictionary (Malouf, 2017).13 While most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist"
2021.emnlp-main.159,2020.acl-main.695,0,0.016455,"specify whenever we refer to inflection done exclusively from the lemma. table, given a few forms of the same lexeme.2 Two lines of recent work made progress towards less supervision, in different fashions. The first simply provided scenarios with smaller training sets — for example, in SIGMORPHON’s shared tasks (Cotterell et al., 2017, 2018). The second research avenue aims to discover the paradigmatic structure of an unknown language given a large bulk of unlabeled data, either alone (Soricut and Och, 2015; Elsner et al., 2019), accompanied by a list of all relevant forms in the vocabulary (Erdmann et al., 2020), or by a list of lemmas (Jin et al., 2020). The problem with the first kind of attempts is that given the neural nature of the most successful models, their performance on limited supervision is capped, and data augmentation is likely to help only if the initial data is diverse enough. As for the second scenario of no supervision at all, it is somewhat pessimistic and unrealistic. Even if much labeled data for a language does not exist for a low-resourced language, typically there exists knowledge about its paradigm structure that can be employed. UniMorph (Kirov et al., 2018), for example, i"
2021.emnlp-main.159,N16-1077,0,0.0431473,"Missing"
2021.emnlp-main.159,J01-2001,0,0.188056,"dex in a dictionary (Malouf, 2017).13 While most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that foc"
2021.emnlp-main.159,L18-1550,0,0.0208592,"limited us to western languages, and we aimed to include as many non-Indo-European languages as possible. The Unlabeled Data The problem setting we specified in Section 2 designates the use of a bulk of unlabeled text. In actuality, the proposed algorithm makes use of the text for (i) collecting a vocabulary to seek tagging candidates, and (ii) training embeddings to be used for calculating the semantic relations between candidate pairs. In our experiments, we simply employed language-specific pre-trained word embeddings for both purposes. We used the pre-trained FastText vectors provided by Grave et al. (2018), following their reported success over morphological analogies in the Bigger Analogy Test Set (Li et al., 2018). We clipped the 2 million long FastText vocabulary to include only real words, i.e., we keep only lowercased tokens that do not include non-alphabetic characters, and include at least one vowel.6 This procedure downsized the vocabulary size to between about 200k − 500k words per language.7 Additionally, for run time reasons, we capped the size of the vocabulary for the orthographic variant to 200k words per language. Minimal Supervision Source For every language we extracted inflect"
2021.emnlp-main.159,A00-2013,0,0.306626,"addition to inflection (Dreyer and Eisner, 2011; Elsner et al., 2019; Jin et al., 2020). In contract, our scenario provides the knowledge on the paradigmatic structure In recent years, neural sequence-to-sequence models have taken the lead in all forms of morphological tagging and morphological inflection tasks. Morphological tagging is an analysis task where the input is a complete sentence, i.e., a sequence of word forms, and the model aims to assign each word form in-context a morphological signature that consists of its lemma, part-of-speech (POS) tag, and a set of inflectional features (Hajic, 2000; Mueller et al., 2013; Bohnet et al., 2018). Morphological inflection works in the opposite direction, and may be viewed as a generation task. Here, forms of a lexeme are generated from one another given sets of inflectional features of both the input and output. In many implementations the 13 input form is the lemma, in which case the inflecWhen inflection is done from a form other than the lemma, tional features of the input are not given (Faruqui it is sometimes referred to as &quot;reinflection&quot;. 2085 with a small and undiverse supervision set. Another work that made use of semantics is that o"
2021.emnlp-main.159,2020.acl-main.598,0,0.121908,"clusively from the lemma. table, given a few forms of the same lexeme.2 Two lines of recent work made progress towards less supervision, in different fashions. The first simply provided scenarios with smaller training sets — for example, in SIGMORPHON’s shared tasks (Cotterell et al., 2017, 2018). The second research avenue aims to discover the paradigmatic structure of an unknown language given a large bulk of unlabeled data, either alone (Soricut and Och, 2015; Elsner et al., 2019), accompanied by a list of all relevant forms in the vocabulary (Erdmann et al., 2020), or by a list of lemmas (Jin et al., 2020). The problem with the first kind of attempts is that given the neural nature of the most successful models, their performance on limited supervision is capped, and data augmentation is likely to help only if the initial data is diverse enough. As for the second scenario of no supervision at all, it is somewhat pessimistic and unrealistic. Even if much labeled data for a language does not exist for a low-resourced language, typically there exists knowledge about its paradigm structure that can be employed. UniMorph (Kirov et al., 2018), for example, includes small amounts of labeled inflection"
2021.emnlp-main.159,P17-1182,0,0.0255727,"nsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation. The state of the art model for classic low-resourced scenarios, with a diverse but small dataset, is the transition-based model of Makarov and Clematide (2018). In addition, some wo"
2021.emnlp-main.159,J94-3001,0,0.831451,"e results show that the selection procedure is indeed important as all different algorithms suffered a loss in performance. It is also evident that diversity is particularly crucial for the orthographic algorithm that is based on the different edit scripts and has less evidence when seed examples cover less classes. 5 Related Work et al., 2016; Aharoni and Goldberg, 2017), and the lemma can be either spelled-out, or inputted as an index in a dictionary (Malouf, 2017).13 While most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morph"
2021.emnlp-main.159,L18-1293,0,0.136617,"e vocabulary (Erdmann et al., 2020), or by a list of lemmas (Jin et al., 2020). The problem with the first kind of attempts is that given the neural nature of the most successful models, their performance on limited supervision is capped, and data augmentation is likely to help only if the initial data is diverse enough. As for the second scenario of no supervision at all, it is somewhat pessimistic and unrealistic. Even if much labeled data for a language does not exist for a low-resourced language, typically there exists knowledge about its paradigm structure that can be employed. UniMorph (Kirov et al., 2018), for example, includes small amounts of labeled inflection tables for many languages, from obscure ones like Ingrian to national languages with widespread usage that lack global attention like Georgian. In this work we propose a new, low-resourced morphological inflection scenario, which is more optimistic and realistic for those widely-spoken sparsely-annotated languages. We assume a minimal supervision set and a large bulk of unlabeled text, thus balancing both trends of lowering supervision resources. We bootstrap a tiny amount of as little as five inflection tables, that could be eas2 Thr"
2021.emnlp-main.159,W18-1205,0,0.0126427,"beled Data The problem setting we specified in Section 2 designates the use of a bulk of unlabeled text. In actuality, the proposed algorithm makes use of the text for (i) collecting a vocabulary to seek tagging candidates, and (ii) training embeddings to be used for calculating the semantic relations between candidate pairs. In our experiments, we simply employed language-specific pre-trained word embeddings for both purposes. We used the pre-trained FastText vectors provided by Grave et al. (2018), following their reported success over morphological analogies in the Bigger Analogy Test Set (Li et al., 2018). We clipped the 2 million long FastText vocabulary to include only real words, i.e., we keep only lowercased tokens that do not include non-alphabetic characters, and include at least one vowel.6 This procedure downsized the vocabulary size to between about 200k − 500k words per language.7 Additionally, for run time reasons, we capped the size of the vocabulary for the orthographic variant to 200k words per language. Minimal Supervision Source For every language we extracted inflection tables for 5 lexemes from UniMorph (Kirov et al., 2018). We aimed for lexemes that are both frequently used,"
2021.emnlp-main.159,C18-1008,0,0.269474,"nd show that, for the five Indo-European of them, orthographic regularity is enough to train a morphological inflector that achieves reasonable success. We further show that for languages with complicated morphophonological systems, such as Finnish, Turkish and Hungarian, a method combining both orthographic and semantic regularities is needed in order to reach the same level of performance. An error analysis reveals that the closer an inflection is to the verge of disappearance, the poorer our system performs on it, as less examples exist in the data-derived vocabulary. Our models outperform Makarov and Clematide (2018)’s model designed for low-resourced setting, even when equipped with additional hallucinated data (Anastasopoulos and Neubig, 2019). We also outperform the best model of Jin et al. (2020), that didn’t use any inflection tables, and their skyline for most languages. We conclude that bootstrapping datasets for inflectional morphology in low-resourced languages, is a viable strategy to be adopted and explored.3 2 3 The Algorithmic Framework This work suggests utilizing the patterns exhibited in the small supervision seed, and finding words that exhibit similar (or, analogous) patterns. The algori"
2021.emnlp-main.159,W19-4226,0,0.0192565,"orphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation. The state of the art model for classic low-resourced scenarios, with a diverse but small dataset, is the transition-based model of Makarov and Clematide (2018). In addition, some works deal with other low-resour"
2021.emnlp-main.159,D13-1032,0,0.0342116,"inflection (Dreyer and Eisner, 2011; Elsner et al., 2019; Jin et al., 2020). In contract, our scenario provides the knowledge on the paradigmatic structure In recent years, neural sequence-to-sequence models have taken the lead in all forms of morphological tagging and morphological inflection tasks. Morphological tagging is an analysis task where the input is a complete sentence, i.e., a sequence of word forms, and the model aims to assign each word form in-context a morphological signature that consists of its lemma, part-of-speech (POS) tag, and a set of inflectional features (Hajic, 2000; Mueller et al., 2013; Bohnet et al., 2018). Morphological inflection works in the opposite direction, and may be viewed as a generation task. Here, forms of a lexeme are generated from one another given sets of inflectional features of both the input and output. In many implementations the 13 input form is the lemma, in which case the inflecWhen inflection is done from a form other than the lemma, tional features of the input are not given (Faruqui it is sometimes referred to as &quot;reinflection&quot;. 2085 with a small and undiverse supervision set. Another work that made use of semantics is that of Soricut and Och (201"
2021.emnlp-main.159,Q15-1012,0,0.0258951,"le most pioneering models for supervised morphological inflection used statistical models based on finite-state-machines (Kaplan and Kay, 1994; Eisner, 2002), nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and"
2021.emnlp-main.159,oravecz-etal-2014-hungarian,0,0.0350667,"Missing"
2021.emnlp-main.159,D18-1315,0,0.0788751,"e respectable accuracy. Combined orthographic and semantic regularities alleviate difficulties with particularly complex morpho-phonological systems. We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks. 1 Introduction The introduction of neural models into natural language processing in the last decade has led to huge improvements in all supervised generation tasks, including morphological inflection.1 In particular, previous works (Cotterell et al., 2017; Silfverberg and Hulden, 2018) have achieved near-perfect performance over the Paradigm Cell Filling Problem (PCFP) (Ackerman et al., 2009), wherein models are required to provide any form in an inflection 1 In recent years the term reinflection has surfaced as a reference to morphological inflection done not necessarily from the lemma. In this paper we will refer to both inflection and reinflection as &quot;inflection&quot;, and specify whenever we refer to inflection done exclusively from the lemma. table, given a few forms of the same lexeme.2 Two lines of recent work made progress towards less supervision, in different fashions."
2021.emnlp-main.159,N15-1186,0,0.11977,"sarily from the lemma. In this paper we will refer to both inflection and reinflection as &quot;inflection&quot;, and specify whenever we refer to inflection done exclusively from the lemma. table, given a few forms of the same lexeme.2 Two lines of recent work made progress towards less supervision, in different fashions. The first simply provided scenarios with smaller training sets — for example, in SIGMORPHON’s shared tasks (Cotterell et al., 2017, 2018). The second research avenue aims to discover the paradigmatic structure of an unknown language given a large bulk of unlabeled data, either alone (Soricut and Och, 2015; Elsner et al., 2019), accompanied by a list of all relevant forms in the vocabulary (Erdmann et al., 2020), or by a list of lemmas (Jin et al., 2020). The problem with the first kind of attempts is that given the neural nature of the most successful models, their performance on limited supervision is capped, and data augmentation is likely to help only if the initial data is diverse enough. As for the second scenario of no supervision at all, it is somewhat pessimistic and unrealistic. Even if much labeled data for a language does not exist for a low-resourced language, typically there exist"
2021.emnlp-main.159,P00-1027,0,0.471475,"nowadays neural models for morphological inflections are a lot more pervasive (Cotterell et al., 2016, 2017, 2018) (and they go as back as Rumelhart and McClelland, 1986). In the case of unsupervised learning of morphology, a key task is to induce complete paradigms from unlabled texts. Early works on unsupervised morphology induction focused on morpheme segmentation for concatenative morphology (Goldsmith, 2001; Creutz and Lagus, 2002; Narasimhan et al., 2015; Bergmanis and Goldwater, 2017). Notwithstanding, early unsupervised works that are not limited to concatenative morphology do exist (Yarowsky and Wicentowski, 2000). More recent studies on unsupervised morphology include works on knowledge transfer within a genealogical linguistic family well- to lowresourced languages (Kann et al., 2017, 2020; McCarthy et al., 2019), as well as works aimed at modifying the approaches for the supervised problem, to allow better tackling of low resourced scenarios. These include Bergmanis et al. (2017) that focused on data augmentation, and Anastasopoulos and Neubig (2019) that modified the model itself with a separate features encoder and introduced the now commonly-used hallucination method for data augmentation. The st"
2021.mrl-1.23,2020.sigmorphon-1.3,0,0.0632821,"Missing"
2021.mrl-1.23,D18-1315,0,0.0645118,"Missing"
2021.mrl-1.23,2021.naacl-main.41,0,0.018954,"set and the rest are a test set. The lemmas in the train- and test-set are disjoint (Goldman et al., 2021). Models As an initial attempt at sentence inflection we apply SOTA models for word-inflection to both the word- and sentence-level inflection tasks: • LSTM: by Silfverberg and Hulden (2018) • T RANSDUCE: by Makarov and Clematide (2018) Both models handle characters as input and output, treating white-space as yet another character rather than a special word-delimiter. In addition, moving to sentence inflection allows the use of contextualized pretrained language models. We used the M T5 (Xue et al., 2021) finetuned on each language separately. The morphological features were added to the input as new tokens with randomly initialized embeddings, and the rest of the input and output was tokenized using M T5’s own tokenizer. Results Table 1 compares the results for wordand sentence-inflection for the LSTM and TRANS DUCE models. It is clear that sentence inflection is a harder task. It is not surprising as it involves longer character sequences, and more sophisticated edits such as manipulating word order (e.g. S-V inversions English and German). On sentence inflection, the TRANSDUCE performs bett"
2021.mrl-1.23,C18-1008,0,0.0474464,"Missing"
2021.mrl-1.23,W19-4226,0,0.0475317,"Missing"
C08-1112,E03-1005,0,0.0212413,"ubstitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even such models fa"
C08-1112,J03-4003,0,0.17146,"ee Grammar (PCFG) or a Stochastic Tree Substitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns o"
C08-1112,P08-1043,1,0.228262,"(6b) recovered none of them. Both grammars make attachment mistakes internal to complex NPs, but the RR-model is better at identifying higher level constituents that correlate with meaningful grammatical functions. Our qualitative analysis suggests that our model is even more powerful than our quantitative analysis indicates, yet we leave the discussion of better ways to quantify this for future research. A Note on Related Work Studies on parsing MH to date concentrate mostly on spelling out the integration of a PCFG parser with a morphological disambiguation component (e.g., (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008)). On a setup identical to ours (gold segmentation, no PoS) the latter obtained 70pt. (Tsarfaty and Sima’an, 895 PP B.. in.. 2007) examined the contribution of horizontal and vertical conditioning to an unlexicalized MH parser and concluded that head-driven Markovization performs below the level of vertical conditioning enriched with percolated features. We do not know of existing dependency-parsers applied to parsing MH or mildly-context-sensitive broadcoverage parsers applied to parsing a Semitic language.6 To the best of our knowledge, this is the first fully generative probabilistic framew"
C08-1112,J98-4004,0,0.0831449,"ntages of the Relational-Realizational approach and its potential promise for parsing other “exotic” languages. 2 Background Recent decades have seen a surge of interest in statistical models using a body of annotated text for learning the distributions of grammatically meaningful structures, in order to assign the most likely ones to unseen sentences. Probabilistic Context Free Grammars (PCFGs) have become popular in the articulation of such models, and unlexicalized treebank grammars (or representational variations thereof) were shown to perform reasonably well on English benchmark corpora (Johnson, 1998; Klein and Manning, 2003). A major leap in the performance of PCFG-based statistical parsers has been introduced by the move towards a Head-Driven paradigm (Collins, 2003; Charniak, 1997), in which syntactic categories are enriched with head information percolated up the tree. The head-driven generation process allows one to model the relation between the information content of a constituent and the information content of its head-marked sister. At the same time, such models introduce a bias with respect to the positioning of a non-head constituent relative to its head-marked sister. The vast"
C08-1112,P03-1054,0,0.105053,"G) or a Stochastic Tree Substitution Grammar (STSG) at their backbone. The majority of such models belong to a Head-Driven paradigm, in which a head constituent is generated first, providing a positional anchor for subsequent (e.g., Markovian) sisters’ generation. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Constituency-based models, lexicalized and unlexicalized alike, demonstrate state-of-the-art performance for parsing English (Charniak, 1997; Collins, 2003; Klein and Manning, 2003; Bod, 2003), yet a direct application of such models to parsing less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even su"
C08-1112,H05-1066,0,0.0918325,"Missing"
C08-1112,W07-2220,0,0.012011,"less configurational languages often fails to yield comparable results. The parameters of such parsers capture generalizations that are easily stated in structural terms (e.g., subjects linearly precede predicates, VPs dominate objects, etc.) which may not be adequate for parsing languages with less configurational character. A different vein of research explores data-driven dependency-based parsing methods (e.g., (McDonald et al., 2005)) which seem to be intuitively more adequate for the task. It turns out, however, that even such models fail to provide the desired remedy. Recent reports by (Nivre, 2007) delineated a class of richly-inflected languages with relatively free word-order (including Greek, Basque, and Modern Standard Arabic) for which the parsers performed poorly, regardless of the parsing method used. The need for parsing methods that can effectively cope with such phenomena doesn’t seem to have been eliminated by dependency parsing — perhaps quite the contrary. The essential argument we promote here is that in order to deal with the kind of variation that is empirically observed cross-linguistically an alternative view of the generation process is required. Our Relational-Realiz"
C08-1112,W07-2219,1,0.911837,"Missing"
C08-1112,P06-3009,1,0.737153,"ly, the PCFG in (6b) recovered none of them. Both grammars make attachment mistakes internal to complex NPs, but the RR-model is better at identifying higher level constituents that correlate with meaningful grammatical functions. Our qualitative analysis suggests that our model is even more powerful than our quantitative analysis indicates, yet we leave the discussion of better ways to quantify this for future research. A Note on Related Work Studies on parsing MH to date concentrate mostly on spelling out the integration of a PCFG parser with a morphological disambiguation component (e.g., (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008)). On a setup identical to ours (gold segmentation, no PoS) the latter obtained 70pt. (Tsarfaty and Sima’an, 895 PP B.. in.. 2007) examined the contribution of horizontal and vertical conditioning to an unlexicalized MH parser and concluded that head-driven Markovization performs below the level of vertical conditioning enriched with percolated features. We do not know of existing dependency-parsers applied to parsing MH or mildly-context-sensitive broadcoverage parsers applied to parsing a Semitic language.6 To the best of our knowledge, this is the first fully g"
C08-1112,J93-2004,0,\N,Missing
C16-1033,P06-1084,0,0.526738,"neral CRF implementations, such as MarMoT (M¨uller et al., 2013), that can be applied across languages, assume an unrealistic, gold pre-segmented setting (Bjorkelund et al., 2013). For generic morphological segmentation, Morfessor (Smit et al., 2014) uses a max-likelihood in semi-supervised settings, but it cannot handle the rich labeling of morphological segments. In this paper we present a general, language-agnostic solution the for the MA&D task. We target joint morphological segmentation and tagging, as has been advocated in monolingual cases (Zhang and Clark, 2011; Bar-haim et al., 2008; Adler and Elhadad, 2006; Habash and Rambow, 2005), in universal settings. Our technical approach extends the transition-based framework for structured prediction of Zhang and Clark (2011). We define and implement two MD variants: word-based and morpheme-based. We present the best MA&D results to date, and demonstrate that the morpheme-based variant consistently outperforms our word-based one, while providing state-of-the-art results on full-fledge, fine-grained, MD of Hebrew. Furthermore, our MA&D framework is intentionally designed with language independence in mind. Devoid of requiring language-specific resources,"
C16-1033,W13-4916,0,0.0179217,"the surface form. Such fusion results in an ambiguous number of morphosyntactic nodes that participate in the analysis, impacting downstream applications as syntactic and semantic parsing, translation, etc. Previous work on MA&D in MRLs, and in Semitic languages in particular (Adler, 2007; Bar-Haim et al., 2005; Shacham and Wintner, 2007; Pasha et al., 2014), relied on language-specific lexica and cannot be executed cross-linguistically. General CRF implementations, such as MarMoT (M¨uller et al., 2013), that can be applied across languages, assume an unrealistic, gold pre-segmented setting (Bjorkelund et al., 2013). For generic morphological segmentation, Morfessor (Smit et al., 2014) uses a max-likelihood in semi-supervised settings, but it cannot handle the rich labeling of morphological segments. In this paper we present a general, language-agnostic solution the for the MA&D task. We target joint morphological segmentation and tagging, as has been advocated in monolingual cases (Zhang and Clark, 2011; Bar-haim et al., 2008; Adler and Elhadad, 2006; Habash and Rambow, 2005), in universal settings. Our technical approach extends the transition-based framework for structured prediction of Zhang and Clar"
C16-1033,W06-2920,0,0.287045,"study outperform the state of the art, and we show that the morpheme-based MD consistently outperforms our word-based variant. We further illustrate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks. 1 Problem Statement A decade following the emergence of statistical parsers for English (Charniak, 1996; Bod, 1995), the CoNLL data sets presented a new challenge: the development of data-driven statistical parsers that can be trained to parse any language given an appropriately annotated treebank (Buchholz and Marsi, 2006; Nivre et al., 2007). These data sets facilitated the development of accurate, language-agnostic, dependency parsers (Nivre et al. (2006), McDonald (2006) etc.), but not without shortcomings: they require that input tokens be morphologically analyzed and disambiguated in advance. This latter assumption breaks down in realistic parsing scenarios where the morphological analysis of an input token may consist of multiple syntactic words to participate in the parse tree (Tsarfaty et al., 2010). The universal dependencies (UD) initiative aims to remedy this by presenting a harmonized set of treeba"
C16-1033,D07-1022,0,0.306658,"Missing"
C16-1033,P04-1015,0,0.236023,"pattern defined by a feature function φ. The feature vector Φ(y) is defined via a set of d feature functions {φi }di=1 . The way Φ is defined effectively determines the quality of the parser, since the feature model captures linguistic information to which the model learns to assign 338 weights. Given this vector, Score(y) is computed by multiplying Φ(y) with a weights vector ω ~ ∈ Rd . Score(y) = Φ(y) · ω ~ = d XX ωi φi (cj ) (2) cj ∈y i=1 Following Zhang and Clark (2011), our system learns the weights vector ω ~ ∈ Rd via the generalized perceptron, using the early-update averaged variant of Collins and Roark (2004). The algorithm iterates through a gold-annotated corpus, each sentence is disambiguated (decoded) with the last known weights, and if the decoded result differs from the gold standard, the weights are updated. As in Zhang and Clark (2011), decoding is based on the beam search algorithm, where a number of possible parse sequences are evaluated concurrently to mitigate irrecoverable prediction errors. At each step, the transition system applies all valid applicable transitions to all candidates. The B highest scoring expanded candidates are maintained and passed on to the next step. Those that"
C16-1033,C10-1045,0,0.319005,"Missing"
C16-1033,P05-1071,0,0.466632,"ogical analysis and disambiguation (MA&D) of data from typologically different languages. MA&D is particularly challenging in morphologically rich languages (MRLs), where space-delimited input tokens may have multiple analyses, only one relevant in context. This morphological ambiguity of a token is typically represented as a lattice. The term Morphological Disambiguation (MD) refers to selecting a single path through the morphological analysis (MA) lattice. In Semitic languages, MD is particularly challenging (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Pasha et al., 2014; Habash and Rambow, 2005). To illustrate, Figure 1 shows the MA lattice of the Hebrew phrase ‘bclm hneim’1 (literally: in-shadow-of-them the-pleasant, meaning: in their pleasant shadow). Different paths in the lattice represent different disambiguation decisions. In This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 We transliterate as in Sima’an et al. (2001). License details: http:// 337 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 337–348, Osaka, Japan, December 11-17 2016"
C16-1033,Q14-1011,0,0.0237962,"1 0 ωimd φmd i (ck ) + d X X cl ∈yet j=1 ωjet φet j (cl ) (7) While the number of morphemes, and therefore |ymd |, can vary, |yet |is deterministic per lattice. Using this anchor, the features of the ET transition provide a counter-balance to the effects of variedlength sequences by scoring fully disambiguated paths of each word-lattice individually, occuring a fixed amount of times for all paths. ENDTOKEN vs. IDLE transitions Variable-length sequences in beam search also exist in the structured prediction of constituency trees. Zhu et al. (2013) introduced an IDLE transition (also adopted in Honnibal and Johnson (2014) and Zhang et al. (2014)) that, like ET, has no effect on configuration, but unlike ET, occurs only at the end of the parsing sequence, an arbitrary number of times, until all parsing sequences are complete. While IDLE transitions make sense when applied after a complete hierarchical structure is predicted — where they may learn to rerank candidates based on features that are visible at the top of the structure (the root) — it is futile to use last-seen features that arbitrarily exist at the end of a morphological disambiguation (linear) sequence, to rerank candidates again and again. This is"
C16-1033,D13-1032,0,0.105958,"Missing"
C16-1033,nivre-etal-2006-maltparser,0,0.0345272,"trate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks. 1 Problem Statement A decade following the emergence of statistical parsers for English (Charniak, 1996; Bod, 1995), the CoNLL data sets presented a new challenge: the development of data-driven statistical parsers that can be trained to parse any language given an appropriately annotated treebank (Buchholz and Marsi, 2006; Nivre et al., 2007). These data sets facilitated the development of accurate, language-agnostic, dependency parsers (Nivre et al. (2006), McDonald (2006) etc.), but not without shortcomings: they require that input tokens be morphologically analyzed and disambiguated in advance. This latter assumption breaks down in realistic parsing scenarios where the morphological analysis of an input token may consist of multiple syntactic words to participate in the parse tree (Tsarfaty et al., 2010). The universal dependencies (UD) initiative aims to remedy this by presenting a harmonized set of treebanks, now 54 and counting, with a unified annotation scheme and multilayered annotation. Specifically, UD data distinguishes the input spac"
C16-1033,pasha-etal-2014-madamira,0,0.0925635,"Missing"
C16-1033,W14-6111,1,0.90364,"Missing"
C16-1033,D07-1046,0,0.767215,"geagnostic infrastructure for automatic morphological analysis and disambiguation (MA&D) of data from typologically different languages. MA&D is particularly challenging in morphologically rich languages (MRLs), where space-delimited input tokens may have multiple analyses, only one relevant in context. This morphological ambiguity of a token is typically represented as a lattice. The term Morphological Disambiguation (MD) refers to selecting a single path through the morphological analysis (MA) lattice. In Semitic languages, MD is particularly challenging (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Pasha et al., 2014; Habash and Rambow, 2005). To illustrate, Figure 1 shows the MA lattice of the Hebrew phrase ‘bclm hneim’1 (literally: in-shadow-of-them the-pleasant, meaning: in their pleasant shadow). Different paths in the lattice represent different disambiguation decisions. In This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 We transliterate as in Sima’an et al. (2001). License details: http:// 337 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pa"
C16-1033,E14-2006,0,0.047009,"Missing"
C16-1033,L16-1680,0,0.111913,"Missing"
C16-1033,tsarfaty-goldberg-2008-word,1,0.891455,"a word-lattice in L, and M is a set of disambiguated morphemes (i.e., selected arcs). The terminal configuration is defined to be Ct = {(L, top(L), tokens(L), M )} for any L, M , where tokens(L) is the number of word-lattices that form L. The initial configuration function cs concatenates the Li lattices of the tokens into a single structure L = M A(x1 ) + ... + M A(xk ), and sets n = bottom(L), i = 0 and M = ∅. Defining Transitions. There are two conceivable ways to make morphological disambiguation decisions, in a word-based (WB), and in a morpheme-based (MB), fashion, in the terminology of Tsarfaty and Goldberg (2008). In WB models (a.k.a token-level in the UD terminology), the disambiguation decision determines a complete path of morphemes between token-boundaries. In the lattice, this refers to selecting a path between two token-boundary nodes (double circles). MB disambiguation decisions (also termed lexical-level, or word-level in UD) occur at any node in the lattice indicating a morpheme boundary, with more than one outgoing arc, choosing a specific arc m among them. Interim: Word-Based or Morpheme-Based? WB and MB strategies face contradicting, and complementary, challenges. In WB models, disambiguat"
C16-1033,W10-1401,1,0.938993,"atistical parsers that can be trained to parse any language given an appropriately annotated treebank (Buchholz and Marsi, 2006; Nivre et al., 2007). These data sets facilitated the development of accurate, language-agnostic, dependency parsers (Nivre et al. (2006), McDonald (2006) etc.), but not without shortcomings: they require that input tokens be morphologically analyzed and disambiguated in advance. This latter assumption breaks down in realistic parsing scenarios where the morphological analysis of an input token may consist of multiple syntactic words to participate in the parse tree (Tsarfaty et al., 2010). The universal dependencies (UD) initiative aims to remedy this by presenting a harmonized set of treebanks, now 54 and counting, with a unified annotation scheme and multilayered annotation. Specifically, UD data distinguishes the input space-delimited tokens from the (morpho)syntactic words that participate in the parse tree (Nivre et al., 2016). Efforts towards parsing texts into universal dependencies in realistic scenarios thus require languageagnostic infrastructure for automatic morphological analysis and disambiguation (MA&D) of data from typologically different languages. MA&D is par"
C16-1033,P06-3009,1,0.929491,"Missing"
C16-1033,P13-2103,1,0.838406,"e F1 metric comparing the MSRs of predicted vs. gold lattice arcs, for full morphological disambiguation (segmentation, POS tags, and all morphological properties), and for segmentation and POS tags only. For comparison with previous work, we also report token-level accuracy (while F1 awards partial success on word-lattices, token-level accuracy requires exact match on a whole path per token). 4.1 The Case for Modern Hebrew Setup We evaluate MA&D performance on the Modern Hebrew section of the SPMRL 2014 Shared Task (Seddah et al., 2014), which has been derived from the Unified-SD treebank of Tsarfaty (2013). We updated the treebank to provide consistent theories for the treebank annotation and lexicographic resources (Itai and Wintner, 2008), a consistency that we found lacking in the SPMRL 2014 Hebrew section. We use the standard split, and train on the standard train set (5k sentences). Here we provide results and in-depth analysis on dev and confirm our findings on test. A pre-condition for the execution of our MD models is an M A(x) function that generates word-lattices for x (§2). We start off with a morphological analyzer that we implemented, called HEBLEX, which relies on the Ben-Gurion H"
C16-1033,J11-1005,0,0.419005,"nd cannot be executed cross-linguistically. General CRF implementations, such as MarMoT (M¨uller et al., 2013), that can be applied across languages, assume an unrealistic, gold pre-segmented setting (Bjorkelund et al., 2013). For generic morphological segmentation, Morfessor (Smit et al., 2014) uses a max-likelihood in semi-supervised settings, but it cannot handle the rich labeling of morphological segments. In this paper we present a general, language-agnostic solution the for the MA&D task. We target joint morphological segmentation and tagging, as has been advocated in monolingual cases (Zhang and Clark, 2011; Bar-haim et al., 2008; Adler and Elhadad, 2006; Habash and Rambow, 2005), in universal settings. Our technical approach extends the transition-based framework for structured prediction of Zhang and Clark (2011). We define and implement two MD variants: word-based and morpheme-based. We present the best MA&D results to date, and demonstrate that the morpheme-based variant consistently outperforms our word-based one, while providing state-of-the-art results on full-fledge, fine-grained, MD of Hebrew. Furthermore, our MA&D framework is intentionally designed with language independence in mind."
C16-1033,P14-1125,0,0.10565,"∈yet j=1 ωjet φet j (cl ) (7) While the number of morphemes, and therefore |ymd |, can vary, |yet |is deterministic per lattice. Using this anchor, the features of the ET transition provide a counter-balance to the effects of variedlength sequences by scoring fully disambiguated paths of each word-lattice individually, occuring a fixed amount of times for all paths. ENDTOKEN vs. IDLE transitions Variable-length sequences in beam search also exist in the structured prediction of constituency trees. Zhu et al. (2013) introduced an IDLE transition (also adopted in Honnibal and Johnson (2014) and Zhang et al. (2014)) that, like ET, has no effect on configuration, but unlike ET, occurs only at the end of the parsing sequence, an arbitrary number of times, until all parsing sequences are complete. While IDLE transitions make sense when applied after a complete hierarchical structure is predicted — where they may learn to rerank candidates based on features that are visible at the top of the structure (the root) — it is futile to use last-seen features that arbitrarily exist at the end of a morphological disambiguation (linear) sequence, to rerank candidates again and again. This is because at the end of th"
C16-1033,P13-1043,0,0.0256577,"d φmd i (ymd ) + i=1 d X j=1 ωjet φet j (yet ) = d X X ck ∈ymd i=1 0 ωimd φmd i (ck ) + d X X cl ∈yet j=1 ωjet φet j (cl ) (7) While the number of morphemes, and therefore |ymd |, can vary, |yet |is deterministic per lattice. Using this anchor, the features of the ET transition provide a counter-balance to the effects of variedlength sequences by scoring fully disambiguated paths of each word-lattice individually, occuring a fixed amount of times for all paths. ENDTOKEN vs. IDLE transitions Variable-length sequences in beam search also exist in the structured prediction of constituency trees. Zhu et al. (2013) introduced an IDLE transition (also adopted in Honnibal and Johnson (2014) and Zhang et al. (2014)) that, like ET, has no effect on configuration, but unlike ET, occurs only at the end of the parsing sequence, an arbitrary number of times, until all parsing sequences are complete. While IDLE transitions make sense when applied after a complete hierarchical structure is predicted — where they may learn to rerank candidates based on features that are visible at the top of the structure (the root) — it is futile to use last-seen features that arbitrarily exist at the end of a morphological disam"
C18-1190,D15-1041,0,0.0208876,"s. In this paper we consider two dimensions of representational choices: • Input Items: Token-Based vs. Morpheme-Based. First, we compare NN models trained on a signal consisting of the raw tokens with ones trained on sequences of morphological segments that represent standalone lexical and functional units. The hypothesis is that models trained on morphologically segmented input will provide better generalization capacity for the network and will thus improve the prediction accuracy. • Vocabulary Encoding: String-Based vs. Char-Based. It has been proposed in previous work (Ling et al., 2015; Ballesteros et al., 2015) that combating the complexity of word structure in NN architectures may benefit from encoding the vocabulary using sub-words or character sequences. We thus contrast models that encode vocabulary items as complete strings to ones that encode a vocabulary of characters that are used to construct each of the strings. The empirical investigation we conduct aims to empirically answer two questions: (i) would the choice of representation affect prediction accuracy? and, (ii) would the different representation choices affect different neural network architectures in different ways? 4 Data Preparati"
C18-1190,D14-1082,0,0.042432,"Missing"
C18-1190,P15-1017,0,0.0837831,"Missing"
C18-1190,P14-2009,0,0.191837,"rformance. 1 Introduction Deep learning (Goodfellow et al., 2016) has seen a surge of interest in recent years, transforming all application domains of machine learning. In particular, neural network (NN) architectures currently dominate the development of models and applications in NLP, including syntactic parsing (Chen and Manning, 2014; Dyer et al., 2015; Durrett and Klein, 2015), sequence labeling (Grave, 2008), information extraction (Chen et al., 2015; dos Santos et al., 2015), text classification (Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Dos Santos and Gatti, 2014; Dong et al., 2014). Much NN work in NLP has been conducted on English, which in turn raises the question whether these methods will be equally effective when applied to languages with different linguistic characteristics than English, and in particular, Morphologically rich languages (MRLs) (Tsarfaty et al., 2010). MRLs are languages where word structure holds substantial information, corresponding to multiple meaning-bearing units (morphemes) per space-delimited token. Furthermore, in MRLs word-order may be flexible. These properties may pose challenges to NN models which rely heavily on the distributional cha"
C18-1190,C14-1008,0,0.0496783,"neural networks’ task performance. 1 Introduction Deep learning (Goodfellow et al., 2016) has seen a surge of interest in recent years, transforming all application domains of machine learning. In particular, neural network (NN) architectures currently dominate the development of models and applications in NLP, including syntactic parsing (Chen and Manning, 2014; Dyer et al., 2015; Durrett and Klein, 2015), sequence labeling (Grave, 2008), information extraction (Chen et al., 2015; dos Santos et al., 2015), text classification (Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Dos Santos and Gatti, 2014; Dong et al., 2014). Much NN work in NLP has been conducted on English, which in turn raises the question whether these methods will be equally effective when applied to languages with different linguistic characteristics than English, and in particular, Morphologically rich languages (MRLs) (Tsarfaty et al., 2010). MRLs are languages where word structure holds substantial information, corresponding to multiple meaning-bearing units (morphemes) per space-delimited token. Furthermore, in MRLs word-order may be flexible. These properties may pose challenges to NN models which rely heavily on th"
C18-1190,P15-1061,0,0.109794,"acy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks’ task performance. 1 Introduction Deep learning (Goodfellow et al., 2016) has seen a surge of interest in recent years, transforming all application domains of machine learning. In particular, neural network (NN) architectures currently dominate the development of models and applications in NLP, including syntactic parsing (Chen and Manning, 2014; Dyer et al., 2015; Durrett and Klein, 2015), sequence labeling (Grave, 2008), information extraction (Chen et al., 2015; dos Santos et al., 2015), text classification (Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Dos Santos and Gatti, 2014; Dong et al., 2014). Much NN work in NLP has been conducted on English, which in turn raises the question whether these methods will be equally effective when applied to languages with different linguistic characteristics than English, and in particular, Morphologically rich languages (MRLs) (Tsarfaty et al., 2010). MRLs are languages where word structure holds substantial information, corresponding to multiple meaning-bearing units (morphemes) per space-delimited token. Furthermore,"
C18-1190,P15-1033,0,0.0260415,"Missing"
C18-1190,C16-1033,1,0.892187,"Missing"
C18-1190,W10-1401,1,0.83224,"Missing"
D09-1088,P99-1065,0,0.201526,"Missing"
D09-1088,J03-4003,0,0.919227,"ve grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 3 4 Such clauses are defined formally as exocentric in formal theories of syntax, and are used to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S VP-P RD natan gave NP-SBJ Dani ADVP etmol yesterday (3b) NP+D+ACC -OBJ et-hamatana the-present PP-COM le-dina to-Dina S VP-P RD natan gave NP+D+ACC -OBJ et-ha-matana the-present ADVP etmol yesterday NP-SBJ Dani Dani PP-COM le-dina to-Dina Figure 1: The State-Splits Approach for Ex. (3) (3a) S V P @S HEAD,V P @"
D09-1088,J98-4004,0,0.765139,"affordable alternative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclud"
D09-1088,P03-1054,0,0.642196,"ernative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function s"
D09-1088,C08-1112,1,0.886132,"Missing"
D09-1088,W08-1008,0,0.0606741,"parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD model"
D09-1088,P06-3009,1,0.852998,"ding off the parameters described in table 3, in accordance with the trees depicted in figures 1–3.8 For all models, we use relative frequency estimates. For lexical parameters, we use a simple smoothing procedure assigning probability to unknown words using the per-tag distribution of rare words (“rare” threshold set to &lt; 2). The input to our parser consists of morphologically segmented surface forms, and the parser has to as9 This setup is more difficult than, e.g., the Arabic parsing setup of (Bikel, 2002), as they assume gold-standard pos-tags as input. Yet it is easier than the setup of (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008) which uses unsegmented surface forms as input. The decision to use segmented and untagged forms was made to retain a realistic scenario. Morphological analysis is known to be ambiguous, and we do not assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for which accuracy results cannot be faithfully compared. See (Tsarfaty, 2006) for discussion. 10 The fla"
D09-1088,P95-1037,0,0.156278,"multiple surface exponents. The question this paper addresses is therefore what kind of modeling approach would be adequate for modeling the interplay between syntax and morphology in marking grammatical relations in Hebrew, as reflected by the sentence-pair (3). They both mean, roughly, “Dani gave the present to Dina yesterday; their word-order vary, but the pattern of object marking is retained. (3) 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unle"
D09-1088,J93-2004,0,0.0313799,"that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F1 75 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (M"
D09-1088,C08-1071,0,0.0174641,"). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F1 88. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer"
D09-1088,P06-1055,0,0.0775428,"ion separation for expressing grammatical relations. 13 The startegy of adding grammatical functions as statesplits is used in, e.g., German (Rafferty and Manning, 2008). 14 Due to the difference in the size of the grammars, one could argue that smoothing will bridge the gap between the HD and RR modeling strategies. However, the better size/accuracy trade-off shown here for RR models suggests that they provide a good bias/variance balancing point, especially for feature-rich models characterizing morphologically rich languages. A promising strategy then would be to smooth or split-and-merge (Petrov et al., 2006)) RR-based models rather than to add an elaborate smoothing component to configurationally-based HD models. 849 configurational —————– nonconfigurational Chinese&gt;English&gt;{German,Hebrew}&gt;Warlpiri 9 Acknowledgements We thank Jelle Zuidema, Inbal Tsarfati, David McCloskey and Yoav Golderg for excellent comments on earlier versions. We also thank Miles Osborne and Tikitu de Jager for comments on the camera-ready draft. All errors are our own. The work of the first author is funded by the Dutch Science Foundation (NWO) grant 017.001.271. Figure 5: The Configurationality Scale The HD assumptions tak"
D09-1088,W08-1006,0,0.0781646,"inative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F1 92.1 (McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts 1 Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F1 75, F1 79, F1 83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD models (about 2 points absolute improvement in F1 ) in parsing the Modern Hebrew treebank. In particular, RR models show better performance in identifying the constituents for which syntactic positions are relatively free. Secondly, we show a novel va"
D09-1088,C04-1024,0,0.021683,"ning at a single position. Figure 3: The Relational-Realizational Approach 845 GF Description PRD SBJ OBJ COM Predicative Elements Grammatical Subjects Direct Objects Indirect Objects FInite Complements Infinitival Complements A Conjunct within a Conjunction Structure IC CNJ sign the syntactic as well as morphological analysis to the surface segments.9 We use the standard development/training/test split as in (Tsarfaty and Sima’an, 2008). Since our goal is a detailed comparison and fine-grained analysis of the results we concentrate on the development set. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. Applicable to. . . VP, PREDP NP, SBAR NP NP, PP SBAR VP All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(Cln , . . . , Ch , . . . , Crn |P ) HD-PCFG Head Left Branch? Right Branch? Left Arg/Mod Right Arg/Mod Left Final? Right Final? P(Ch |P ) P(L:∆l1 , H:∆h |Ch , P ) P(Ch , R:∆r1 |∆h , Ch , P ) P(Cli , ∆li+1 |L , ∆li , Ch , P ) P(Cri , ∆ri+1 |R , ∆ri , Ch , P ) P(C1 |L , ∆ln−1 , Ch , P ) P(Cn |R , ∆rn−1 , Ch , P ) RR-PCFG Projection Configuration Realization Adjunction P"
D09-1088,P08-1043,1,\N,Missing
D09-1088,D07-1096,0,\N,Missing
D11-1036,H91-1060,0,0.0649648,"y other structure. For (6a)–(6b), for instance, we get that (6a)t (6b) is a flat tree over pre-terminals where “would” and “have” are labeled with ‘vg’ and “worked” is the head, labeled with ‘*’. The generalization of two functional trees provides us with one structure that reflects the common and consistent content of the two trees. These structures thus provide us with a formally well-defined gold standard for cross-treebank evaluation. Step 3: Measuring Distances. Our functional trees superficially look like constituency-based trees, so a simple proposal would be to use Parseval measures (Black et al., 1991) for comparing the parsed trees against the new generalized gold trees. Parseval scores, however, have two significant drawbacks. First, they are known to be too restrictive with respect to some errors and too permissive with respect to others (Carroll et al., 1998; K¨ubler and Telljohann, 2002; Roark, 2002; Rehbein and van Genabith, 2007). Secondly, F1 scores would still penalize structures that are correct with respect to the original gold, but are not there in the generalized structure. Here we propose to adopt measures that are based on tree edit distance (TED) instead. TEDbased measures a"
D11-1036,W06-2920,0,0.320185,"normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. 385 Evelina Andersson Uppsala University Sweden Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambow, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in pe"
D11-1036,D10-1096,0,0.0623753,"ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus may not cover all possi"
D11-1036,cer-etal-2010-parsing,0,0.0712638,"Missing"
D11-1036,de-marneffe-etal-2006-generating,0,0.0629635,"Missing"
D11-1036,emms-2008-tree,0,0.0227087,"y trees is simply the cost of all edit operations that are required to turn a parse tree into its gold standard, normalized with respect to the overall size of the dependency tree and subtracted from a unity.3 Here we apply the idea of defining scores by TED costs normalized relative to the size of the tree and substracted from a unity, and extend it from fixed-size dependency trees to ordered trees of arbitrary size. 3 The size of a dependency tree, either parse or gold, is always fixed by the number of terminals. 390 Our formalization follows closely the formulation of the T-Dice measure of Emms (2008), building on his thorough investigation of the formal and empirical differences between TED-based measures and Parseval. We first define for any ordered and labeled tree π the following operations. relabel-node change the label of node v in π delete-node delete a non-root node v in π with parent u, making the children of v the children of u, inserted in the place of v as a subsequence in the left-to-right order of the children of u. insert-node insert a node v as a child of u in π making it the parent of a consecutive subsequence of the children of u. An edit script ES(π1 , π2 ) = {e0 , e1 .."
D11-1036,W07-2416,0,0.532412,"a dependency treebank is well-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. 386 Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a functional one marks its relation to other parts of the sentence. The two kinds of heads may or may not coincide in a single word form (Zwicky, 1993). Common examples refer to prepositional phrases, such as the phrase “on Sunday”. This phrase has two possible analyses, one selects a lexical head (1a) and the oth"
D11-1036,J93-2004,0,0.0481286,"gh endeavor, and suggest ways to extend the protocol to additional evaluation scenarios. 2 The Challenge: Treebank Theories Dependency treebanks contain information about the grammatically meaningful elements in the utterance and the grammatical relations between them. Even if the formal representation in a dependency treebank is well-defined according to current standards (K¨ubler et al., 2009), there are different ways in which the trees can be used to express syntactic content (Rambow, 2010). Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. Different conversion algorithms implicitly make different assumptions about how to represent linguistic content in the data. When multiple conversion algorithms are applied to the same data, we end up with different dependency trees for the same sentences (Johansson and Nugues, 2007; Choi and Palmer, 2010; de Marneffe et al., 2006). Some common cases of discrepancies are as follows. 386 Lexical vs. Functional Head Choice. In linguistics, there is a distinction between lexical heads and functional heads. A lexical head carries the semantic gist of a phrase while a f"
D11-1036,E06-1011,0,0.0210463,"ed in the supplementary material. 392 The Default, OldLTH and CoNLL schemes mainly differ in their coordination structure, and the Functional and Lexical schemes differ in their selection of a functional and a lexical head, respectively. All schemes use the same inventory of labels.7 The LTH parameter settings for the different schemes are elaborated in the supplementary material. The Setup We use two different parsers: (i) MaltParser (Nivre et al., 2007b) with the arc eager algorithm as optimized for English in (Nivre et al., 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006). Both parsers were trained on the different instances of sections 2-21 of the PTB obeying the different annotation schemes in Table 3. Each trained model was used to parse section 23. All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005). A more principled treatment of non-projective dependency trees is an important topic for future research. We evaluated the parses using labeled and unlabeled attachment scores, and using our TEDEVAL software package. Evaluation Our TEDEVAL software packag"
D11-1036,P05-1012,0,0.0298235,"a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. 385 Evelina Andersson Uppsala University Sweden Different annotation schemes often make different assumptions with respect to how lingui"
D11-1036,P08-1006,0,0.0911077,"nt experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus"
D11-1036,P05-1013,1,0.901195,"orated in the supplementary material. The Setup We use two different parsers: (i) MaltParser (Nivre et al., 2007b) with the arc eager algorithm as optimized for English in (Nivre et al., 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006). Both parsers were trained on the different instances of sections 2-21 of the PTB obeying the different annotation schemes in Table 3. Each trained model was used to parse section 23. All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005). A more principled treatment of non-projective dependency trees is an important topic for future research. We evaluated the parses using labeled and unlabeled attachment scores, and using our TEDEVAL software package. Evaluation Our TEDEVAL software package implements the pipeline described in Section 3. We convert all parse and gold trees into functional trees using the algorithm defined in Section 3, and for each pair of parsing experiments we calculate a shared gold standard using generalization determined through a chart-based greedy algorithm.8 Our scoring procedure uses the TED algorith"
D11-1036,C04-1010,1,0.779898,"rent representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. 385 Evelina Andersson Uppsala University Sweden Different annotation schemes often make different assumptions wi"
D11-1036,C10-1094,1,0.913352,"Missing"
D11-1036,N10-1049,0,0.239506,"al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. 385 Evelina Andersson Uppsala University Sweden Different annotation schemes often make different assumptions with respect to how linguistic content is represented in a treebank (Rambow, 2010). The consequence of such annotation discrepancies is that when we compare parsing results across different experiments, even ones that use the same parser and the same set of sentences, the gap between results in different experiments may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll"
D11-1036,P11-1067,0,0.251711,"nts may not reflect a true gap in performance, but rather a difference in the annotation decisions made in the respective treebanks. Different methods have been proposed for making dependency parsing results comparable across experiments. These methods include picking a single gold standard for all experiments to which the parser output should be converted (Carroll et al., 1998; Cer et al., 2010), evaluating parsers by comparing their performance in an embedding task (Miyao et al., 2008; Buyko and Hahn, 2010), or neutralizing the arc direction in the native representation of dependency trees (Schwartz et al., 2011). Each of these methods has its own drawbacks. Picking a single gold standard skews the results in favor of parsers which were trained on it. Transforming dependency trees to a set of pre-defined labeled dependencies, or into task-based features, requires the use of heuristic rules that run the risk of distorting correct information and introducing noise of their own. Neutralizing the direction of arcs is limited to unlabeled evaluation and local context, and thus may not cover all possible discrepancies. This paper proposes a new three-step protocol for cross-experiment parser evaluation, and"
D11-1036,A97-1014,0,0.192686,". In the future we plan to use this procedure for comparing constituency and dependency parsers. A conversion from constituency-based trees into functional trees 394 is straightforward to define: simply replace the node labels with the grammatical function of their dominating arc – and the rest of the pipeline follows. A pre-condition for cross-framework evaluation is that all representations encode the same set of grammatical relations by, e.g., annotating arcs in dependency trees or decorating nodes in constituency trees. For some treebanks this is already the case (Nivre and Megyesi, 2007; Skut et al., 1997; Hinrichs et al., 2004) while for others this is still lacking. Recent studies (Briscoe et al., 2002; de Marneffe et al., 2006) suggest that evaluation through a single set of grammatical relations as the common denominator is a linguistically sound and practically useful way to go. To guarantee extensions for crossframework evaluation it would be fruitful to make sure that resources use the same set of grammatical relation labels across different formal representation types. Moreover, we further aim to inquire whether we can find a single set of grammatical relation labels that can be used a"
D11-1036,W03-3023,0,0.160471,"ations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground. 1 Introduction Data-driven dependency parsing has seen a considerable surge of interest in recent years. Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). As it turns out, however, such evaluation procedures are sensitive to the annotation choices in the data on which the parser was trained. 385 Evelina Andersson Uppsala University Sweden Different annotation schemes often make"
D11-1036,C10-2013,1,\N,Missing
D11-1036,D07-1096,1,\N,Missing
D14-1136,Q13-1005,0,0.0320222,"by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, c October 25-29, 2014, Doha, Qatar. 201"
D14-1136,P92-1024,0,0.397955,"ete SM — we partition this set into a training set and a test set that are disjoint. We train our statistical model on the examples in the training set and automatically analyze the requirements in the test set. We then compare the predicted semantic analyses of the test set with the human-annotated (henceforth, gold) semantic analyses of this test set, and empirically quantify our prediction accuracy. Metrics. Our semantic LSC objects have the form of a tree (reflecting the sequence of nested events in our scenarios). Therefore, we can use standard tree evaluation metrics, such as ParseEval (Black et al., 1992), to evaluate the accuracy of the prediction. Overall, we define three metrics to evaluate the accuracy of the LSC trees: POS: the POS metric is the percentage of part-of-speech tags predicted correctly. LSC-F1: F1 is the harmonic means of the precision and recall of the predicted tree. LSC-EM: EM is 1 if the predicted tree is an exact match to the gold tree, and 0 otherwise. In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield between the gold and parse trees for the same senN=1 Gen-Only Gen+Seed POS 85.52 91.59 LSC-F1 84.40 88.05 LSC-EM 9.52 14.29 SM-T"
D14-1136,J93-2003,0,0.0276651,"ies different mentions of the same string to a single element. 4.2 Discourse-Based Modeling We assume a given document D ∈ D and aim to find the most probable system model M ∈ M that satisfies the requirements. We assume that M reflects a single domain that the stakeholders have in mind, and we are provided with an ambiguous natural language evidence, an elicited discourse D, in which they convey it. We instantiate this view as a noisy channel model (Shannon, 1948), which provides the foundation for many NLP applications, such as speech recognition (Bahl et al., 1983) and machine translation (Brown et al., 1993). According to the noisy channel model, when a signal is received it does not uniquely identify the message being sent. A probabilistic model is then used to decode the original message. In our case, the signal is the discourse and the message is the overall system model. In formal terms, we want to find a model M that maximises the following: different snapshots reflected in different requireF ments, i.e., M = i mi . We then rephrase: P (M ) = P (m1 , ..., mn ) P (D|M ) = P (d1 , ...., dn |m1 , ..., mn ) These events may be seen as points in a high dimensional space. In actuality, they are to"
D14-1136,J14-1005,0,0.0363474,"on and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et"
D14-1136,P13-1127,0,0.0402221,"Missing"
D14-1136,P11-1060,0,0.0233176,"conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, c October"
D14-1136,E12-1006,1,0.804687,"is the harmonic means of the precision and recall of the predicted tree. LSC-EM: EM is 1 if the predicted tree is an exact match to the gold tree, and 0 otherwise. In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield between the gold and parse trees for the same senN=1 Gen-Only Gen+Seed POS 85.52 91.59 LSC-F1 84.40 88.05 LSC-EM 9.52 14.29 SM-TED 84.25 85.17 SM-EM 9.52 14.29 Table 3: Sentence-Based modeling: Accuracy results on the Phone development set. tence,6 so we cannot use ParseEval. Therefore, we implement a distance-based metrics in the spirit of Tsarfaty et al. (2012). Then, to evaluate the accuracy of the SM, we use two kinds of scores: SM-TED: TED is the normalized edit distance between the predicted and gold SM trees, subtracted from a unity. SM-EM: EM is 1 if the predicted SM is an exact match with the gold SM, 0 otherwise. Data. We have a small seed of correctly annotated requirements-specification case studies that describe simple reactive systems in the LSC language. Each document contains a sequence of requirements, each of which is annotated with the correct LSC diagram. The entire program is grounded in a java implementation. As training data, we"
D19-1681,D15-1138,0,0.0158621,"tion of the world may be provided via visual sensors (Misra et al., 2018; Blukis et al., 2018; Nguyen et al., 2018; Yan et al., 2018; Anderson et al., 2018) or as a symbolic world representation. This work focuses on navigation based on a symbolic world representation (referred to as a map). Previous datasets for NL navigation based on a symbolic world representation, HCRC (Anderson et al., 1991; Vogel and Jurafsky, 2010; Levit and Roy, 2007) and SAIL (MacMahon et al., 2006; Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Fried et al., 2017; Andreas and Klein, 2015) present relatively simple worlds, with a small fixed set of entities known to the navigator in advance. Such representations bypass the great complexity of real urban navigation, which consists of long paths and an abundance of previously unseen entities of different types. In this work we introduce Realistic Urban Navigation (RUN), where we aim to interpret navigation instructions relative to a rich symbolic representation of the world, given by a real dense urban map. To address RUN, we designed and collected a new dataset based on OpenStreetMap, in which we align NL instructions to their c"
D19-1681,D14-1134,0,0.0149609,"In NL navigation studies, the representation of the world may be provided via visual sensors (Misra et al., 2018; Blukis et al., 2018; Nguyen et al., 2018; Yan et al., 2018; Anderson et al., 2018) or as a symbolic world representation. This work focuses on navigation based on a symbolic world representation (referred to as a map). Previous datasets for NL navigation based on a symbolic world representation, HCRC (Anderson et al., 1991; Vogel and Jurafsky, 2010; Levit and Roy, 2007) and SAIL (MacMahon et al., 2006; Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Fried et al., 2017; Andreas and Klein, 2015) present relatively simple worlds, with a small fixed set of entities known to the navigator in advance. Such representations bypass the great complexity of real urban navigation, which consists of long paths and an abundance of previously unseen entities of different types. In this work we introduce Realistic Urban Navigation (RUN), where we aim to interpret navigation instructions relative to a rich symbolic representation of the world, given by a real dense urban map. To address RUN, we designed and collected a new dataset based on OpenStreetMap"
D19-1681,Q13-1005,0,0.023137,"to execute the instruction. In NL navigation studies, the representation of the world may be provided via visual sensors (Misra et al., 2018; Blukis et al., 2018; Nguyen et al., 2018; Yan et al., 2018; Anderson et al., 2018) or as a symbolic world representation. This work focuses on navigation based on a symbolic world representation (referred to as a map). Previous datasets for NL navigation based on a symbolic world representation, HCRC (Anderson et al., 1991; Vogel and Jurafsky, 2010; Levit and Roy, 2007) and SAIL (MacMahon et al., 2006; Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Fried et al., 2017; Andreas and Klein, 2015) present relatively simple worlds, with a small fixed set of entities known to the navigator in advance. Such representations bypass the great complexity of real urban navigation, which consists of long paths and an abundance of previously unseen entities of different types. In this work we introduce Realistic Urban Navigation (RUN), where we aim to interpret navigation instructions relative to a rich symbolic representation of the world, given by a real dense urban map. To address RUN, we designed and collected a new dataset ba"
D19-1681,N18-1177,0,0.0559372,"Missing"
D19-1681,D19-1107,0,0.036059,"Missing"
D19-1681,P17-1089,0,0.0628382,"Missing"
D19-1681,D12-1040,0,0.017612,"needs to be located in order to execute the instruction. In NL navigation studies, the representation of the world may be provided via visual sensors (Misra et al., 2018; Blukis et al., 2018; Nguyen et al., 2018; Yan et al., 2018; Anderson et al., 2018) or as a symbolic world representation. This work focuses on navigation based on a symbolic world representation (referred to as a map). Previous datasets for NL navigation based on a symbolic world representation, HCRC (Anderson et al., 1991; Vogel and Jurafsky, 2010; Levit and Roy, 2007) and SAIL (MacMahon et al., 2006; Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Fried et al., 2017; Andreas and Klein, 2015) present relatively simple worlds, with a small fixed set of entities known to the navigator in advance. Such representations bypass the great complexity of real urban navigation, which consists of long paths and an abundance of previously unseen entities of different types. In this work we introduce Realistic Urban Navigation (RUN), where we aim to interpret navigation instructions relative to a rich symbolic representation of the world, given by a real dense urban map. To address RUN, we desi"
D19-1681,P13-1022,0,0.0627761,"Missing"
D19-1681,D18-1287,0,0.318794,"Missing"
D19-1681,N18-1203,0,0.0219351,"of six components we describe in turn – Encoder, Decoder, Attention, Entity Abstraction, World-State Processor, Execution-System. The complete architecture is depicted in Fig. 2. The Encoder takes the sequence of words that assembles a single sentence and encodes it as a vector using a biLSTM (Graves and Schmidhuber, 2005). The Decoder is an LSTM generating a sequence of actions that the execution-system can perform, according to weights defined by an Attention layer. The Entity Abstraction component deals with out-of-vocabulary words (OOV). We adopt a similar approach to Iyer et al. (2017); Suhr et al. (2018), replacing phrases in the sentences which refer to previously unseen entities with variables, prior to delivering the sentence to the Encoder. E.g., “Walk from Macy’s to 7th street” turns into “Walk from X1 to Y1”. Variables are typed (streets, restaurants, etc.) and are numbered based on their order of occurrence in the sentence. The numbering resets after every utterance, so the model remains with a handful of typed entityvariables. The World-State Processor maps variables to the entities on the map which are mentioned in the sentence. The world-state representation consists of two vectors,"
D19-1681,P10-1083,0,0.194821,"ction”, the instruction needs to be interpreted, and a specific object in the world (the intersection) needs to be located in order to execute the instruction. In NL navigation studies, the representation of the world may be provided via visual sensors (Misra et al., 2018; Blukis et al., 2018; Nguyen et al., 2018; Yan et al., 2018; Anderson et al., 2018) or as a symbolic world representation. This work focuses on navigation based on a symbolic world representation (referred to as a map). Previous datasets for NL navigation based on a symbolic world representation, HCRC (Anderson et al., 1991; Vogel and Jurafsky, 2010; Levit and Roy, 2007) and SAIL (MacMahon et al., 2006; Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014; Fried et al., 2017; Andreas and Klein, 2015) present relatively simple worlds, with a small fixed set of entities known to the navigator in advance. Such representations bypass the great complexity of real urban navigation, which consists of long paths and an abundance of previously unseen entities of different types. In this work we introduce Realistic Urban Navigation (RUN), where we aim to interpret navigation instructions relative to a"
D19-1681,1983.tc-1.13,0,0.282193,"Missing"
D19-3044,C16-1033,1,0.948647,"dependency tree for a given path as Dependency Parsing (DEP). For an input sentence x, our goal is to jointly predict a single pair of MD(x) and DEP(x) that are consistent with one another, and form the most-likely analysis of the sentence. The MD component is the transition-based morphological parser of More and Tsarfaty The Architectural Design The core of O NLP is YAP (Yet Another Parser), a morpho-syntactic parser for morphological and syntactic analysis of Hebrew Texts. YAP reimplements and extends the structure-prediction framework of Zhang and Clark (2011). We describe YAP in detail in More and Tsarfaty (2016) and More et al. (2019). Here we only provide a bird’s eye view of the architecture. 1 We use the annotation conventions of Sima’an et al. (2001) that underlie the Hebrew SPMRL scheme http:// www.spmrl.org/spmrl2013-sharedtask.html. 260 (2016), which is formally based on the structureprediction framework of Zhang and Clark (2011). MD accepts a sentence lattice MA(x) as input and delivers a selected sequence of arcs (morphemes) MD(x) as output. The transition-based system for MD selects arcs for MD one at a time. It decodes the lattice using beam-search, and keeps the K-best paths at each step,"
D19-3044,P06-1084,0,0.572289,"NLP in general and Hebrew parsing in particular are known to be challenging, due to interesting linguistic properties, the scarcity of annotated data, and the small research community around. So, Hebrew has been seriously understudied in NLP. During the early 2000s, the MILA knowledge center was established, where the two of the main Hebrew resources for NLP were developed: the Hebrew treebank (Sima’an et al., 2001) and the Hebrew Lexicon (Itai and Wintner, 2008). Morphological Taggers for Hebrew using local linear-context have been trained on these data and were made available for free use (Adler and Elhadad, 2006; Bar-haim et al., 2008). However, their performance was not on a par with parallel tools for English and thus insufficient for commercial use. Hebrew dependency parsing was initially 11 7 Conclusion This paper presents O NLP, a complete languageprocessing framework for automatic annotation of Modern Hebrew Texts. The framework covers morphological segmentation, POS tags, lemmas and features, and dependency parsing, predicted jointly. The system is easy to install and to use, and we support multiple forms of usage fitting user-personas with different needs. We hope the availability of an open-"
D19-3044,P06-3009,1,0.736291,"etc. This is further complicated by the lack of diacritics in standardized texts, meaning that most vowels are not present, and thus out of context no reading is a-priory more likely than the others. Only in context the correct interpretation and segmentation become apparent. These facts create an apparent loop in the design of NLP pipelines for Hebrew: syntactic parsing requires morphological disambiguation – but morphological disambiguation requires syntactic context. This apparent loop has called for the development of joint systems rather than pipelines, for Semitic languages processing (Tsarfaty, 2006; Green and Manning, 2010). This joint hypothesis has proven useful for Hebrew and Arabic phrasestructure parsing (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Goldberg and Elhadad, 2011). The O NLP suite is a dependency-based parsing framework implementing this joint hypothesis, over the entire morpho-syntactic searchspace, as depicted in Figure 1 (More et al., 2019). 3 Figure 1: The Joint Morpho-Syntactic Search-Space. Lattice paths vary in length. Each lattice path can be assigned an exponential number of dependency trees. In YAP we embrace the extreme morphological ambiguity in He"
D19-3044,W06-2920,0,0.159054,"an use jq and sed (or any other json and line processing tools) to format the (tab separated value) responses and reassemble the output. Check our appendix for an illustration. 5 Technical Details and Forms of Use YAP is implemented in the Go language.7 It requires 6GB of RAM to run, and employs a simple 3-step installation, given in the supplementray material. The input to the system is a tokenized sentence, with tokens appearing one per line, and a line break after every sentence.8 The output is a dependency tree (where each node in the tree is a lattice arc) provided in the CoNLL-X format (Buchholz and Marsi, 2006). YAP is trained on the Hebrew section of the SPMRL shared task. It also makes use of the broad-coverage lexicon of Itai and Wintner (2008) for finding all potential lattice paths. In case of out-of-vocabulary (OOV) items, we employ a simple heuristic where we suggest the 10 most-likely analyses of rare tokens observed during training. Educational Use |The Online Demo In 2018 we decided to create an online demo of the system, for educational purposes: (i) To expose NLP/AI researchers to NLP capabilities available for Hebrew. (ii) To educate non-CS scientists and engineers who work with Hebrew"
D19-3044,P13-2103,1,0.905845,", Present, Future, Imperative, Infinitive].5 The Annotation Scheme Lemmas Each segment is also assigned a lemma, i.e., the cannonical representation of its core (uninflected) meaning.6 For Hebrew nouns and adjectives, the lemma is chosen to be the Masculine-Singular form. For verbs, the lemma is in the Masculine-Singular-3per form in Past tense. We deliver automatic morpho-syntactic annotation of Hebrew texts based on the scheme of the SPMRL Hebrew dependency treebank. The SPMRL Hebrew scheme employs the labels of Sima’an et al. (2001) for morphology and POS tags, and the Unified-SD scheme of Tsarfaty (2013) for the labeled dependencies.2 Specifically, we deliver the following annotation layers: 3 In UD they are called words. In Hebrew NLP they are called segments. We use morphemes or segments herein. 4 A is used in cases where all analyses are valid, such as in Beinoni form - ‘!( ’אוכלתI/you/she eat.singular.feminine) 5 Present-tense verbs and participles are tagged ‘Beinoni’. 6 Note that due to high morphological fusion in Hebrew, simple surface-based stemming will not suffice. 2 With an eye for future comparability, we further developed a conversion algorithm to convert the dependency tree f"
D19-3044,W09-3819,0,0.0212359,"Analysis using ./yap hebma to generate a sentence lattice containing all possible morphological breakdowns of each token. YAP will save the lattice to the file specified via the -out flag. 9 E.g., https://www.youtube.com/watch?v= TFwQeoKpznA&feature=youtu.be 10 Acronyms in Hebrew are written with a quotation mark before the last letter, e.g. ‘!( ’ארה!”בUSA) . 7 https://golang.org/ 8 We assume the tokenization convention of MILA (Itai and Wintner, 2008). 262 Tasks MILA NITE Hebrew-NLP Adler Goldberg Pipelines UDPipe CoreNLP ONLP Tok MA MD X X X X X X X POS Lem X Feats Deps Joint provided by Goldberg and Elhadad (2009), but the parser provided unlabeled dependency, and the pipeline relied on Adler’s morphological tagger. This left the predicted dependency trees inaccurate and unsatisfying. Joint morpho-syntactic models for constituency-based parsing based on Tsarfaty (2010) showed good performance on benchmark data, but was never released for open use. With the development of the UD treebanks collection, general frameworks such as UDPipe (Straka et al., 2016) and CoreNLP (Manning et al., 2014) have been trained on the Hebrew UD treebank, and made the model available. However, these models provide performanc"
D19-3044,W10-1401,1,0.930046,"Missing"
D19-3044,P11-2124,0,0.0186689,"than the others. Only in context the correct interpretation and segmentation become apparent. These facts create an apparent loop in the design of NLP pipelines for Hebrew: syntactic parsing requires morphological disambiguation – but morphological disambiguation requires syntactic context. This apparent loop has called for the development of joint systems rather than pipelines, for Semitic languages processing (Tsarfaty, 2006; Green and Manning, 2010). This joint hypothesis has proven useful for Hebrew and Arabic phrasestructure parsing (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Goldberg and Elhadad, 2011). The O NLP suite is a dependency-based parsing framework implementing this joint hypothesis, over the entire morpho-syntactic searchspace, as depicted in Figure 1 (More et al., 2019). 3 Figure 1: The Joint Morpho-Syntactic Search-Space. Lattice paths vary in length. Each lattice path can be assigned an exponential number of dependency trees. In YAP we embrace the extreme morphological ambiguity in Hebrew. That is, we do not aim to resolve morphological ambiguity via preprocessing. The input to YAP is the complete Morphological Analysis (MA) of an input sentence x, termed here MA(x). MA(x) is"
D19-3044,J11-1005,0,0.101638,"n (MD), and to the task of selecting the most likely dependency tree for a given path as Dependency Parsing (DEP). For an input sentence x, our goal is to jointly predict a single pair of MD(x) and DEP(x) that are consistent with one another, and form the most-likely analysis of the sentence. The MD component is the transition-based morphological parser of More and Tsarfaty The Architectural Design The core of O NLP is YAP (Yet Another Parser), a morpho-syntactic parser for morphological and syntactic analysis of Hebrew Texts. YAP reimplements and extends the structure-prediction framework of Zhang and Clark (2011). We describe YAP in detail in More and Tsarfaty (2016) and More et al. (2019). Here we only provide a bird’s eye view of the architecture. 1 We use the annotation conventions of Sima’an et al. (2001) that underlie the Hebrew SPMRL scheme http:// www.spmrl.org/spmrl2013-sharedtask.html. 260 (2016), which is formally based on the structureprediction framework of Zhang and Clark (2011). MD accepts a sentence lattice MA(x) as input and delivers a selected sequence of arcs (morphemes) MD(x) as output. The transition-based system for MD selects arcs for MD one at a time. It decodes the lattice usin"
D19-3044,C10-1045,0,0.350354,"rther complicated by the lack of diacritics in standardized texts, meaning that most vowels are not present, and thus out of context no reading is a-priory more likely than the others. Only in context the correct interpretation and segmentation become apparent. These facts create an apparent loop in the design of NLP pipelines for Hebrew: syntactic parsing requires morphological disambiguation – but morphological disambiguation requires syntactic context. This apparent loop has called for the development of joint systems rather than pipelines, for Semitic languages processing (Tsarfaty, 2006; Green and Manning, 2010). This joint hypothesis has proven useful for Hebrew and Arabic phrasestructure parsing (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Goldberg and Elhadad, 2011). The O NLP suite is a dependency-based parsing framework implementing this joint hypothesis, over the entire morpho-syntactic searchspace, as depicted in Figure 1 (More et al., 2019). 3 Figure 1: The Joint Morpho-Syntactic Search-Space. Lattice paths vary in length. Each lattice path can be assigned an exponential number of dependency trees. In YAP we embrace the extreme morphological ambiguity in Hebrew. That is, we do not a"
D19-3044,P11-2033,0,0.0954166,"rlie the Hebrew SPMRL scheme http:// www.spmrl.org/spmrl2013-sharedtask.html. 260 (2016), which is formally based on the structureprediction framework of Zhang and Clark (2011). MD accepts a sentence lattice MA(x) as input and delivers a selected sequence of arcs (morphemes) MD(x) as output. The transition-based system for MD selects arcs for MD one at a time. It decodes the lattice using beam-search, and keeps the K-best paths at each step, scored according to morpheme-level and token-level features, weighted via structured-perceptron learning. The DEP component is a re-implementation of the Zhang and Nivre (2011) dependency parser for English, adapted for Hebrew. We assume an ArcEager transition system and beam-search decoding. Feature weights are learned via the structured perceptron. We employ a carefully-designed feature set that reflects linguistic properties of Hebrew such as its rich morphological paradigms, flexible word-order, agreement, etc. This provides SOTA results on Hebrew dependency parsing, albeit in Oracle (i.e., gold morphology) scenario. Seen that both the MD and DEP realize the same formal framework and computational machinery, we can easily unify them and treat the morpho-synactic"
D19-3044,Q19-1003,1,0.926432,"requires morphological disambiguation – but morphological disambiguation requires syntactic context. This apparent loop has called for the development of joint systems rather than pipelines, for Semitic languages processing (Tsarfaty, 2006; Green and Manning, 2010). This joint hypothesis has proven useful for Hebrew and Arabic phrasestructure parsing (Goldberg and Tsarfaty, 2008; Green and Manning, 2010; Goldberg and Elhadad, 2011). The O NLP suite is a dependency-based parsing framework implementing this joint hypothesis, over the entire morpho-syntactic searchspace, as depicted in Figure 1 (More et al., 2019). 3 Figure 1: The Joint Morpho-Syntactic Search-Space. Lattice paths vary in length. Each lattice path can be assigned an exponential number of dependency trees. In YAP we embrace the extreme morphological ambiguity in Hebrew. That is, we do not aim to resolve morphological ambiguity via preprocessing. The input to YAP is the complete Morphological Analysis (MA) of an input sentence x, termed here MA(x). MA(x) is a lattice structure, consisting of all possible morphological analysis possibilities of the input sentence, as seen in the middle of Figure 1. Each lattice arc is a tuple specifying t"
D19-3044,C08-1112,1,\N,Missing
D19-3044,P14-5010,0,\N,Missing
D19-3044,L16-1680,0,\N,Missing
E09-1038,W07-2219,1,0.896176,"Missing"
E09-1038,C08-1112,1,0.875724,"Missing"
E09-1038,P08-1083,1,0.830098,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,P06-3009,1,0.912813,"). The remaining question is how to estimate p(hw, tKC i|tKC ). Here, we use either the LexFilter (estimated over all rare events) or LexProbs (estimated via the semisupervised emission probabilities)models, as defined in Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the"
E09-1038,adler-etal-2008-tagging,1,0.917245,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,D07-1022,0,0.0399562,"Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the lattice. The Viterbi parse over the lattice chooses a lattice path, which induces a segmentation over the input sentence. Thus, parsing and segmentation are performed jointly. Lexical rules in the model are defined over the latt"
E09-1038,P08-1043,1,0.708181,"he original Treebank, KC which is the Treebank converted to use the KC Analyzer tagset, and Layered, which is the layered representation described above. The details of the lexical models vary according to the representation we choose to work with. For the TB setting, our lexical rules are of the form 9 Details of the grammar: all functional information is removed from the non-terminals, finite and non-finite verbs, as well as possessive and other PPs are distinguished, definiteness structure of constituents is marked, and parent annotation is employed. It is the same grammar as described in (Goldberg and Tsarfaty, 2008). 331 ttb → w. Only the Baseline models are relevant here, as the tagset is not compatible with that of the external lexicon. For the KC setting, our lexical rules are of the form tkc → w, and their probabilities are estimated as described above. Note that this setting requires our trees to be tagged with the new (KC) tagset, and parsed sentences are also tagged with this tagset. For the Layered setting, we use lexical rules of the form ttb → w. Reliable events are estimated as usual, via relative frequency over the original treebank. For rare events, we estimate p(ttb → w|ttb ) = p(ttb → tkc"
E09-1038,P08-1085,1,0.870295,"Missing"
E09-1038,W07-0808,1,0.858605,"Missing"
E09-1038,P06-1084,1,\N,Missing
E12-1006,J93-2004,0,0.058587,"ute the argument structure of natural language sentences. The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing. The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data (Rambow, 2010). For example, grammatical relations may be encoded on top of dependency arcs in a dependency tree (Mel’ˇcuk, 1988), they may decorate nodes in a phrase-structure tree (Marcus et al., 1993; Maamouri et al., 2004; Sima’an et al., 2001), or they may be read off of positions in A popular way to address this has been to pick one of the frameworks and convert all parser outputs to its formal type. When comparing constituency-based and dependency-based parsers, for instance, the output of constituency parsers has often been converted to dependency structures prior to evaluation (Cer et al., 2010; Nivre et al., 2010). This solution has various drawbacks. First, it demands a conversion script that maps one representation type to another when some theoretical assumptions in one framewor"
E12-1006,H05-1066,0,0.266722,"Missing"
E12-1006,W09-4636,0,0.0191592,": 0.9239 L: 0.7946 MF Trees Dep Trees STB ut Dep Dep T ED E VAL T ED E VAL M ULTIPLE S INGLE U: 0.9266 U: 0.9264 L: 0.8225 L: 0.8372 U: 0.9275 U: 0.9272 L: 0.8121 L: 0.8275 U: 0.9281 N/A L: 0.7861 Table 4: Swedish cross-framework evaluation: T ED EVAL scores against the native gold and the generalized gold. Boldface scores are the highest in their column. Table 2: English cross-framework evaluation: T ED EVAL scores against gold and generalized gold. Boldface scores are highest in their column. Italic scores are highest for dependency parsers in their column. parsers, using the HunPoS tagger (Megyesi, 2009), but let the Berkeley parser predict its own tags. We use the same evaluation metrics and procedures as before. Prior to evaluating RR trees using ParsEval we strip off the added function nodes. Prior to evaluating them using TedEval we strip off the phrase-structure nodes. Tables 3 and 4 summarize the parsing results for the different Swedish parsers. In the leftmost column of table 3 we present the constituencybased evaluation measures. Interestingly, the Berkeley RR instantiation performs better than when training the Berkeley parser on PS trees. These constituency-based scores however hav"
E12-1006,H05-1078,0,0.0254613,"ground for the different representation types, (ii) computing the theoretical common ground for each test sentence, and (iii) counting only what counts, that is, measuring the distance between the common ground and the parse tree while discarding annotation-specific edits. A pre-condition for applying our protocol is the availability of a relational interpretation of trees in the different frameworks. For dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs. For constituency trees with an inherent mapping of nodes onto grammatical relations (Merlo and Musillo, 2005; Gabbard et al., 2006; Tsarfaty and Sima’an, 2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational 6 Conclusion We developed a protocol for comparing parsing results across different theories and representation types"
E12-1006,P81-1022,0,0.779636,"Missing"
E12-1006,C10-1094,1,0.906234,"Missing"
E12-1006,P06-1055,0,0.021181,"t set representing sets of constraints on which the different gold theories agree. 2 48 The ordering can be alphabetic, thematic, etc. We can now define δ for the ith framework, as the error of parsei relative to its native gold standard goldi and to the generalized gold gen. This is the edit cost minus the cost of the script turning parsei into gen intersected with the script turning goldi into gen. The underlying intuition is that if an operation that was used to turn parsei into gen is used to discard theory-specific information from goldi , its cost should not be counted as error. parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of Tsarfaty and Sima’an (2008). All experiments use the Talbanken Swedish Treebank (STB) data. δ(parsei , goldi , gen) = cost(ES ∗ (parsei , gen)) 4.1 We use sections 02–21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis. We use two different native gold standard"
E12-1006,N10-1049,0,0.143845,"o make sure that we are not comparing apples and oranges? Introduction The goal of statistical parsers is to recover a formal representation of the grammatical relations that constitute the argument structure of natural language sentences. The argument structure encompasses grammatical relationships between elements such as subject, predicate, object, etc., which are useful for further (e.g., semantic) processing. The parses yielded by different parsing frameworks typically obey different formal and theoretical assumptions concerning how to represent the grammatical relationships in the data (Rambow, 2010). For example, grammatical relations may be encoded on top of dependency arcs in a dependency tree (Mel’ˇcuk, 1988), they may decorate nodes in a phrase-structure tree (Marcus et al., 1993; Maamouri et al., 2004; Sima’an et al., 2001), or they may be read off of positions in A popular way to address this has been to pick one of the frameworks and convert all parser outputs to its formal type. When comparing constituency-based and dependency-based parsers, for instance, the output of constituency parsers has often been converted to dependency structures prior to evaluation (Cer et al., 2010; Ni"
E12-1006,P11-1067,0,0.106813,"rsion of their output into dependencies. The conversion to SD allows one to compare results across formal frameworks, but not without a cost. The conversion introduces a set of annotation specific decisions which may introduce a bias into the evaluation. In the middle column of Table 1 we report the T ED E VAL metrics measured against the generalized gold standard for all parsing frameworks. We can now confirm that the constituency-based parsers significantly outperform the dependency parsers, and that this is not due to specific theoretical decisions which are seen to affect LAS/UAS metrics (Schwartz et al., 2011). For the dependency parsers we now see that Malt outperforms MST on labeled dependencies slightly, but the difference is insignificant. The fact that the discrepancy in theoretical assumptions between different frameworks indeed affects the conversion-based evaluation procedure is reflected in the results we report in Table 2. Here the leftmost and rightmost columns report T ED E VAL scores against the own native gold (S INGLE) and the middle column against the generalized gold (M ULTIPLE). Had the theories for SD and PTBttl SD been identical, T ED E VAL S INGLE and T ED E VAL M ULTIPLE would"
E12-1006,H91-1060,0,0.607655,"e show that our extended protocol, which can handle linearlyordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in a representation-independent and language-independent fashion. Preliminaries: Relational Schemes for Cross-Framework Parse Evaluation Traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types. Dependency trees are evaluated using attachment scores (Buchholz and Marsi, 2006), phrase-structure trees are evaluated using ParsEval (Black et al., 1991), LFG-based parsers postulate an evaluation procedure based on fstructures (Cahill et al., 2008), and so on. From a downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized. The bottom line is simply which parsing framework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relations that constitute the predicate-argument structures of sentences, provid"
E12-1006,W06-2920,0,0.644417,"ncy-based parsing models, all trained on the Swedish treebank data. All in all we show that our extended protocol, which can handle linearlyordered labeled trees with arbitrary branching, can soundly compare parsing results across frameworks in a representation-independent and language-independent fashion. Preliminaries: Relational Schemes for Cross-Framework Parse Evaluation Traditionally, different statistical parsers have been evaluated using specially designated evaluation measures that are designed to fit their representation types. Dependency trees are evaluated using attachment scores (Buchholz and Marsi, 2006), phrase-structure trees are evaluated using ParsEval (Black et al., 1991), LFG-based parsers postulate an evaluation procedure based on fstructures (Cahill et al., 2008), and so on. From a downstream application point of view, there is no significance as to which formalism was used for generating the representation and which learning methods have been utilized. The bottom line is simply which parsing framework most accurately recovers a useful representation that helps to unravel the human-perceived interpretation. Relational schemes, that is, schemes that encode the set of grammatical relati"
E12-1006,J08-1003,0,0.0581572,"Missing"
E12-1006,cer-etal-2010-parsing,0,0.0124214,"Missing"
E12-1006,P05-1022,0,0.0115084,"hich the different gold theories agree. 2 48 The ordering can be alphabetic, thematic, etc. We can now define δ for the ith framework, as the error of parsei relative to its native gold standard goldi and to the generalized gold gen. This is the edit cost minus the cost of the script turning parsei into gen intersected with the script turning goldi into gen. The underlying intuition is that if an operation that was used to turn parsei into gen is used to discard theory-specific information from goldi , its cost should not be counted as error. parser (Petrov et al., 2006) and the Brown parser (Charniak and Johnson, 2005). All experiments use Penn Treebank (PTB) data. For Swedish, we compare MaltParser and MSTParser with two variants of the Berkeley parser, one trained on phrase structure trees, and one trained on a variant of the Relational-Realizational representation of Tsarfaty and Sima’an (2008). All experiments use the Talbanken Swedish Treebank (STB) data. δ(parsei , goldi , gen) = cost(ES ∗ (parsei , gen)) 4.1 We use sections 02–21 of the WSJ Penn Treebank for training and section 00 for evaluation and analysis. We use two different native gold standards subscribing to different theories of encoding gr"
E12-1006,de-marneffe-etal-2006-generating,0,0.00660104,"Missing"
E12-1006,N06-1024,0,0.0162642,"representation types, (ii) computing the theoretical common ground for each test sentence, and (iii) counting only what counts, that is, measuring the distance between the common ground and the parse tree while discarding annotation-specific edits. A pre-condition for applying our protocol is the availability of a relational interpretation of trees in the different frameworks. For dependency frameworks this is straightforward, as these relations are encoded on top of dependency arcs. For constituency trees with an inherent mapping of nodes onto grammatical relations (Merlo and Musillo, 2005; Gabbard et al., 2006; Tsarfaty and Sima’an, 2008), a procedure for reading relational schemes off of the trees is trivial to implement. For parsers that are trained on and parse into bare-bones phrase-structure trees this is not so. Reading off the relational structure may be more costly and require interjection of additional theoretical assumptions via manually written scripts. Scripts that read off grammatical relations based on tree positions work well for configurational 6 Conclusion We developed a protocol for comparing parsing results across different theories and representation types which is framework-ind"
E12-1006,gimenez-marquez-2004-svmtool,0,0.0182849,"Missing"
E12-1006,C08-1112,1,0.919669,"Missing"
E12-1006,D11-1036,1,0.0585547,"ne framework may be incompatible with the other one. In the constituency-to-dependency case, some constituency-based structures (e.g., coordination 44 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44–54, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics 2 and ellipsis) do not comply with the single head assumption of dependency treebanks. Secondly, these scripts may be labor intensive to create, and are available mostly for English. So the evaluation protocol becomes language-dependent. In Tsarfaty et al. (2011) we proposed a general protocol for handling annotation discrepancies when comparing parses across different dependency theories. The protocol consists of three phases: converting all structures into function trees, for each sentence, generalizing the different gold standard function trees to get their common denominator, and employing an evaluation measure based on tree edit distance (TED) which discards edit operations that recover theory-specific structures. Although the protocol is potentially applicable to a wide class of syntactic representation types, formal restrictions in the procedur"
E12-1006,nivre-etal-2006-maltparser,1,\N,Missing
J13-1003,P05-1038,0,0.0571577,"Missing"
J13-1003,P08-1067,0,0.0549641,"Missing"
J13-1003,P03-1054,0,0.0158058,"odern Standard Arabic (Semitic) and French (Romance), the last article of this special issue, by Green et al., may be seen as an applications paper, treating the task of MWE recognition as a side effect of a joint model for parsing and MWE identiﬁcation. The key problem here is knowing what to consider a minimal unit for parsing, and how to handle parsing in realistic scenarios where MWEs have not yet been identiﬁed. The authors present two parsing models for such a task: a factored model including a factored lexicon that integrates morphological knowledge into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and Sima’an 2003) in a Bayesian framework, extended to include speciﬁc features that ease the extraction of tree fragments matching MWEs. Interestingly, those very different models do provide the same range of performance when confronted with predicted morphology input. Additional important challenges that are exposed in the context of this study concern the design of experiments for cross-linguistic comparison i"
J13-1003,P95-1037,0,0.302286,"Missing"
J13-1003,J93-2004,0,0.0463151,"Missing"
J13-1003,P06-1055,0,0.078628,"een, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in marking the same grammatical functions differently across languages for increasing the conﬁdence of a disambiguation decision in one language by observing a parallel non-ambi"
J13-1003,C04-1024,0,0.0319019,"French German Hebrew Hungarian 18 Constituency-Based Dependency-Based Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013 Seeker and Kuhn 2013 Green, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in mar"
J13-1003,A97-1014,0,0.288535,"Missing"
J13-1003,W10-1401,1,0.919998,"Missing"
J13-1003,W07-2219,1,0.883448,"Missing"
J13-1003,A00-2018,0,\N,Missing
J13-1003,C10-1011,0,\N,Missing
J13-1003,P97-1003,0,\N,Missing
J13-1003,W06-2920,0,\N,Missing
J13-1003,W08-2102,0,\N,Missing
J13-1003,P05-1022,0,\N,Missing
J13-1003,P08-1109,0,\N,Missing
J13-1003,C92-3126,0,\N,Missing
J13-1003,P99-1065,0,\N,Missing
J13-1003,P03-1013,0,\N,Missing
J13-1003,D07-1096,1,\N,Missing
J13-1003,N10-1115,0,\N,Missing
J13-2006,E03-1005,0,0.0840901,"Missing"
J13-2006,W02-1503,0,0.0511658,"snan 2000), HeadDriven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal metaframework allows computational linguists to formalize their hypotheses and intuitions about a language’s grammatical behavior and then explore how these representational choices affect the processing of natural language utterances. Many of the aforementioned approaches have engendered large-scale platforms that can be used and reused to provide formal description of grammars for different languages, such as Par-Gram for LFG (Butt et al. 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger, and Oepen 2002). FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG treats constructions as the basic units of grammatical organization in language. The constructions are viewed as learned associations between form (e.g., sounds, morphemes, syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG does not impose a strict separation between lexicon and grammar—indeed, it is perhaps best known as treat"
K17-3027,C16-1033,1,0.899247,"To compute Score(y), y is mapped to a global feature vector Φ(y) = {φi (y)} where each feature is a count of occurrences defined by feature functions. Given this vector, Score(y) is calculated as the dot product of Φ(y) and a weights vector ω ~: XX Score(y) = Φ(y) · ω ~ = ωi φi (cj ) Our Framework We use the transition-based framework of Zhang and Clark (2011), originally designed for syntactic processing using the generalized perceptron and beam search, which we briefly cover in subsection 2.1. We first describe the standalone transition system and model for morphological disambiguation of (More and Tsarfaty, 2016) (2.2), and Arc Standard transition system together with a richlinguistic feature model (2.3). We then present our approach to joint morpho-syntactic processing which unifies both transition systems (2.4). We present our baseline approach to data-driven morphological analysis, followed by our Modern Hebrew lexical resource (2.5). 2.1 cj ∈y i Following Zhang and Clark (2011), we learn the weights vector ω ~ via the generalized perceptron, using the early-update averaged variant of Collins and Roark (2004). For decoding, the framework uses the beam search algorithm, which helps mitigate otherwis"
K17-3027,P16-1231,0,0.0311713,"wo-level principle in place, corpora can be provided with raw text, syntactic words as the nodes of syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an MRL to argue that one last piece of the puzzle is missing: a universal scheme for access to lexical resources."
K17-3027,D12-1133,0,0.0223061,"tream. With the latter two-level principle in place, corpora can be provided with raw text, syntactic words as the nodes of syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an MRL to argue that one last piece of the puzzle is missing: a universal scheme for access t"
K17-3027,Q13-1034,0,0.0228337,"n place, corpora can be provided with raw text, syntactic words as the nodes of syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an MRL to argue that one last piece of the puzzle is missing: a universal scheme for access to lexical resources. We discuss our resul"
K17-3027,W06-2920,0,0.349166,"Missing"
K17-3027,L16-1262,1,0.890887,"Missing"
K17-3027,P04-1015,0,0.0607104,"t describe the standalone transition system and model for morphological disambiguation of (More and Tsarfaty, 2016) (2.2), and Arc Standard transition system together with a richlinguistic feature model (2.3). We then present our approach to joint morpho-syntactic processing which unifies both transition systems (2.4). We present our baseline approach to data-driven morphological analysis, followed by our Modern Hebrew lexical resource (2.5). 2.1 cj ∈y i Following Zhang and Clark (2011), we learn the weights vector ω ~ via the generalized perceptron, using the early-update averaged variant of Collins and Roark (2004). For decoding, the framework uses the beam search algorithm, which helps mitigate otherwise irrecoverable errors in the transition sequence. 2.2 Morphological Disambiguation The morphological disambiguator (MD) component of our parser is based on More and Tsarfaty (2016), modified only to accommodate UD POS tags and morphological features. We provide a brief exposition of the transition system, and refer the reader to the original paper for an in-depth explanation (More and Tsarfaty, 2016). The input to the transition-based MD is a lattice L of an input stream of k surface tokens x = x1 , ..."
K17-3027,C12-1059,0,0.0144035,"purposes of the shared task we apply a post processing step in which the first root node encountered (in left-to-right order) is designated as the only root node, and all other root nodes are set as its modifier with the “punct” dependency label. Of course, this means that our transition system only applies to projective trees — the oracle will indeed fail given a non-projective tree, and our transition system cannot output one. In addition, since we are using the Arc Standard transition system, which has been shown to not be arcdecomposable, we cannot employ a dynamic oracle during training (Goldberg and Nivre, 2012). The rich-linguistic feature model for our dependency parser, inspired by Zhang and Nivre (2011), applies the rich non-local features to arc standard (where this is possible), such as to accommodate the free word order of MRLs. We provide an appendix with a detailed comparison of the two feature models. M Ds : (L, p, i, M ) → (L, q, i, M ∪ {m}) Where p = b, q = e, and s relates the transition to the disambiguated morpheme m using a parameterized delexicalization s = DLEXoc (m): ( ( , , , t, g) if t ∈ OC DLEXOC (m) = ( , , f, t, g) otherwise 2.4 Given standalone morphological and syntactic dis"
K17-3027,D11-1109,0,0.031463,"be provided with raw text, syntactic words as the nodes of syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an MRL to argue that one last piece of the puzzle is missing: a universal scheme for access to lexical resources. We discuss our results for a lexiconb"
K17-3027,W14-6111,1,0.914603,"Missing"
K17-3027,L16-1680,0,0.0672534,"Missing"
K17-3027,tsarfaty-goldberg-2008-word,1,0.753238,"tation guidelines and (ii) corpora text is provided via a two-level representation of the input stream. With the latter two-level principle in place, corpora can be provided with raw text, syntactic words as the nodes of syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an"
K17-3027,P14-1125,0,0.0160944,"syntactic trees, and the relationship between them, in a harmonized scheme. This representation is crucial to the participation of Morphologically Rich Languages (MRLs) in end-to-end parsing tasks. The availability of a wide range of language corpora in this manner provides a unique opportunity for the advancement of (universal) joint morphosyntactic processing, introduced by Tsarfaty and Goldberg (2008) in a generative setting and advocated for in a variety of settings (Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014). To this end, our submission is a joint morpho-syntactic processor in a transition-based framework. We present our submission (OpenU-NLP-Lab), with models trained only on the train sets (Nivre et al., 2017b), parsing all 81 test treebanks of UD v2 corpora (Nivre et al., 2017a) participating in the CoNLL 2017 UD Shared Task (Zeman et al., 2017). We use the results of our processor on an MRL to argue that one last piece of the puzzle is missing: a universal scheme for access to lexical resources. We discuss our results for a lexiconbacked approach, compared to a data-driven one. The goal of our"
K17-3027,J11-1005,0,0.033815,"andard for lexical resource access. 2 F (x) = argmaxy∈GEN (x) Score(y) How we define Score is therefore crucial to the performance of the model, since it must capture the relation of a generated sequence (and its derived output) to that of an oracle’s output. To compute Score(y), y is mapped to a global feature vector Φ(y) = {φi (y)} where each feature is a count of occurrences defined by feature functions. Given this vector, Score(y) is calculated as the dot product of Φ(y) and a weights vector ω ~: XX Score(y) = Φ(y) · ω ~ = ωi φi (cj ) Our Framework We use the transition-based framework of Zhang and Clark (2011), originally designed for syntactic processing using the generalized perceptron and beam search, which we briefly cover in subsection 2.1. We first describe the standalone transition system and model for morphological disambiguation of (More and Tsarfaty, 2016) (2.2), and Arc Standard transition system together with a richlinguistic feature model (2.3). We then present our approach to joint morpho-syntactic processing which unifies both transition systems (2.4). We present our baseline approach to data-driven morphological analysis, followed by our Modern Hebrew lexical resource (2.5). 2.1 cj"
K17-3027,P11-2033,0,0.503768,"(in left-to-right order) is designated as the only root node, and all other root nodes are set as its modifier with the “punct” dependency label. Of course, this means that our transition system only applies to projective trees — the oracle will indeed fail given a non-projective tree, and our transition system cannot output one. In addition, since we are using the Arc Standard transition system, which has been shown to not be arcdecomposable, we cannot employ a dynamic oracle during training (Goldberg and Nivre, 2012). The rich-linguistic feature model for our dependency parser, inspired by Zhang and Nivre (2011), applies the rich non-local features to arc standard (where this is possible), such as to accommodate the free word order of MRLs. We provide an appendix with a detailed comparison of the two feature models. M Ds : (L, p, i, M ) → (L, q, i, M ∪ {m}) Where p = b, q = e, and s relates the transition to the disambiguated morpheme m using a parameterized delexicalization s = DLEXoc (m): ( ( , , , t, g) if t ∈ OC DLEXOC (m) = ( , , f, t, g) otherwise 2.4 Given standalone morphological and syntactic disambiguation systems in the same framework, we integrate them into a joint morpho-syntactic proces"
K18-2021,P06-1084,0,0.0586057,"The Contribution of Lexical Resources: Analysis of the Case for Modern Hebrew 5 We then moved on to experiment with yap’s complete pipeline, including a data-driven morphological analyzer (MA) to produce input lattices, transition-based morphological disambiguation and transition-based parsing. The results now dropped relative to the UDPipe baseline and relative to our own yap DEP system, from 59.19 to 52.25 LAS. Now, interestingly, when we replace the baseline data-driven MA learned from the treebank alone with an MA backed with an external broad-coverage lexicon called HebLex (adapted from (Adler and Elhadad, 2006)), the LAS results arrive at 60.94 LAS, outperforming the results obtained by yap DEP (UDPipe morphology with yap dependencies) and close much of the gap with the UDPipe full model. This suggests that much of the parser error stems from missing lexical knowledge concerning the morphologically rich and ambiguous word forms, rather than parser errors. Discussion and Conclusion This paper presents our submission to the C O NLL 2018 UD S HARED TASK. Our submitted system assumed UDpipe up to and including morphological disambiguation, and employed a state-of-theart transition-based parsing model to"
K18-2021,P16-1231,0,0.0171197,"vement in LAS, obtaining 71.39 (much beyond the baseline) without changing any of the parsing algorithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr sequoia fr spoken fro srcmf ga idt gl ctg gl"
K18-2021,D12-1133,0,0.0256841,"see a significant improvement in LAS, obtaining 71.39 (much beyond the baseline) without changing any of the parsing algorithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr sequoia fr spoken fro sr"
K18-2021,Q13-1034,0,0.019858,"ning 71.39 (much beyond the baseline) without changing any of the parsing algorithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr sequoia fr spoken fro srcmf ga idt gl ctg gl treegal got proiel g"
K18-2021,C16-1033,1,0.942768,"and Ct ⊂ C a set of terminal configurations. A transition sequence y = tn (tn−1 (...t1 (cs (x)))) for an input x starts with an initial configuration cs (x) and results in a terminal configuration cn ∈ Ct . In order to determine which transition t ∈ T to apply given a configuration c ∈ C, we define a model that learns to predict the transition that would be chosen by an oracle function O : C → T , which has access to the gold output. We employ an objective function 2.3 Morphological Analysis Morphological Disambiguation The morphological disambiguation (MD) component of our parser is based on More and Tsarfaty (2016), modified to accommodate UD POS tags and morphological features. We provide here a brief exposition of the transition system, as shall be needed for our later analysis, and refer the reader to the original paper for an in-depth discussion (More and Tsarfaty, 2016). A configuration for morphological disambiguation CM D = (L, n, i, M ) consists of a lattice L, an index n representing a node in L, an index i s.t. 0 ≤ i &lt; k representing a specific token’s lattice, and a set of disambiguated morphemes M . F (x) = argmaxy∈GEN (x) Score(y) 2 i https://github.com/habeanf/yap 209 The initial configura"
K18-2021,L16-1262,1,0.887249,"Missing"
K18-2021,W06-2920,0,0.118744,"eanf@gmail.com Abstract The UD scheme (Nivre et al., 2016) adheres to two main principles: (i) there is a single set of POS tags, morphological properties, and dependency labels for all treebanks, and their annotation obeys a single set of annotation principles, and (ii) the text is represented in a two-level representation, clearly mapping the written spacedelimited source tokens to the (morpho)syntactic words which participate in the dependency tree. The C O NLL 2018 UD S HARED TASK is a multilingual parsing evaluation campaign wherein, contrary to previous shared tasks such as CoNLL-06/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) corpora are provided with raw text, and the end goal is to provide a complete morpho-syntactic representation, including automatically resolving all of the token-word discrepancies. Contrary to the previous SPMRL shared tasks (Seddah et al., 2013, 2014), the output of all systems obeys a single annotation scheme, allowing for reliable cross-system and cross-language evaluation. This paper presents the system submitted by the ONLP lab to the shared task, including the dependency models trained on the train sets, assuming morphologically disambiguated input tokens by UDpipe"
K18-2021,P04-1015,0,0.0583278,"s generated by a morphological analysis (MA) component, the lattice concatenate the lattices for the whole input sentence x. Each lattice-arc in L has a morphosyntactic representation (MSR) defined as m = (b, e, f, t, g), with b and e marking the start and end nodes of m in L, f a form, t a universal partof-speech tag, and g a set of attribute=value universal features. These lattice-arc correspond to potential nodes in the intended dependency tree. cj ∈y Following Zhang and Clark (2011), we learn the weights vector ω ~ via the generalized perceptron, using the early-update averaged variant of Collins and Roark (2004). For decoding, the framework uses the beam search algorithm, which helps mitigate otherwise irrecoverable errors in the transition sequence. 2.2 Our Framework The parsing system presented by the ONLP Lab for this task is based on yap — yet another parser, a transition-based parsing system that relies on the formal framework of Zhang and Clark (2011), an efficient computational framework designed for structure prediction and based on the generalized perceptron for learning and beam search for decoding. This section briefly describes the formal settings and specific models available via yap.2 2"
K18-2021,D11-1109,0,0.0342412,"nd the baseline) without changing any of the parsing algorithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr sequoia fr spoken fro srcmf ga idt gl ctg gl treegal got proiel grc perseus grc pr"
K18-2021,W14-6111,1,0.901087,"Missing"
K18-2021,L16-1680,0,0.0474053,"Missing"
K18-2021,tsarfaty-goldberg-2008-word,1,0.76452,"lattice. For this setting, we see a significant improvement in LAS, obtaining 71.39 (much beyond the baseline) without changing any of the parsing algorithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr"
K18-2021,K18-2001,0,0.0593274,"Missing"
K18-2021,P14-1125,0,0.0156898,"ithms involved. So, for morphologically rich and ambiguous languages it appears that lexical coverage is a major factor affecting task performance, especially in the resource scarce case. We additionally intend to replace the handcrafted feature model with neural-network based feature extraction mechanisms, and we aim to explore universal morphosyntactic parsing via joint morphosyntactic modeling, as previously advocated in different settings (Tsarfaty and Goldberg, 2008; Bohnet and Nivre, 2012; Andor et al., 2016; Bohnet et al., 2013; Li et al., 2011; Bohnet and Nivre, 2012; Li et al., 2014; Zhang et al., 2014).. Note that the upper-bound of our parser, when given a completely disambiguated morphological input stream, provides LAS of 79.33, which is a few points above the best system (Stanford) in the raw-to-dependencies scenario. 212 Treebank af afribooms ar padt bg btb br keb bxr bdt ca ancora cs cac cs fictree cs pdt cs pud cu proiel da ddt de gsd el gdt en ewt en gum en lines en pud es ancora et edt eu bdt fa seraji fi ftb fi pud fi tdt fo oft fr gsd fr sequoia fr spoken fro srcmf ga idt gl ctg gl treegal got proiel grc perseus grc proiel he htb hi hdtb hr set hsb ufal hu szeged UAS 79.83 71.62"
K18-2021,J11-1005,0,0.0236599,"f the morphological analysis alternatives of k surface tokens of the input stream x = x1 , ..., xk , such that each Li = M A(xi ) is generated by a morphological analysis (MA) component, the lattice concatenate the lattices for the whole input sentence x. Each lattice-arc in L has a morphosyntactic representation (MSR) defined as m = (b, e, f, t, g), with b and e marking the start and end nodes of m in L, f a form, t a universal partof-speech tag, and g a set of attribute=value universal features. These lattice-arc correspond to potential nodes in the intended dependency tree. cj ∈y Following Zhang and Clark (2011), we learn the weights vector ω ~ via the generalized perceptron, using the early-update averaged variant of Collins and Roark (2004). For decoding, the framework uses the beam search algorithm, which helps mitigate otherwise irrecoverable errors in the transition sequence. 2.2 Our Framework The parsing system presented by the ONLP Lab for this task is based on yap — yet another parser, a transition-based parsing system that relies on the formal framework of Zhang and Clark (2011), an efficient computational framework designed for structure prediction and based on the generalized perceptron fo"
K18-2021,P11-2033,0,0.273193,"arametric model of More and Tsarfaty (2016) to score the transitions at each step. Since lattices may have paths of different length and we use beam search for decoding, the problem of variable-length transition sequences arises. We follow More and Tsarfaty (2016), using the EN DT OKEN transition to mitigate the biases induced by variable-length sequences. 2.4 Syntactic Disambiguation A syntactic configuration is a triplet CDEP = (σ, β, A) where σ is a stack, β is a buffer, and A a set of labeled arcs. For dependency parsing, we use a specific variant of Arc Eager that was first presented by (Zhang and Nivre, 2011). The differences between plain arc-eager and the arc-zeager variant are detailed in Figure 1. The features defined for the parametric model also follows the definition of non-local features by Zhang and Nivre (2011), with one difference: we created one version of each feature with a morphological signature (all feature values of the relevant node) and one without. this allows to capture phenomena like agreement. 2.5 Shared Task Implementation Joint Morpho-Syntactic Processing Given the standalone morphological and syntactic disambiguation it is possible to embed the two into 3 210 https://gol"
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L18-1608,W06-2920,0,0.370592,"Missing"
L18-1608,W02-1503,0,0.157572,"tactic parsing, in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages. Keywords: Morphology, Universal Dependencies, Morphological Analysis, Morphological Ambiguity 1. Introduction The development of the universal dependencies (UD) framework and its treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows many shared tasks and multilingual evaluation campaigns in which the linguistic representation schemes across different languages vary (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Butt et al., 2002; Zeman et al., 2012). The UD treebanks collection, in contrast, obeys a single set of annotation guidelines, and respects the discrepancies between surface input tokens and the output nodes in the syntax trees (a.k.a., the two-level representation principle.)1 The UD initiative has paved the way to the development of cross-lingual models for word segmentation, part-of-speech tagging and dependency parsing (Straka and Strakov´a, 2017), as well as cross-linguistic typological investigations (Futrell et al., 2015). Recently, the CoNLL 2017 Shared Task on Multilingual UD Parsing (Zeman et al., 20"
L18-1608,coltekin-2010-freely,1,0.858208,"Missing"
L18-1608,C16-1033,1,0.861239,"4), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different se"
L18-1608,K17-3027,1,0.780254,"004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different segmentation approach compared to the METU-Sabancı Turkish Treebank (Oflazer et al., 2003). In addition, form and lemma representations, POS tags and morphological tag sets have changed. The existing morphological analyzers are not compatible with this new representation. Thus we intro"
L18-1608,L16-1262,1,0.908557,"Missing"
L18-1608,pasha-etal-2014-madamira,1,0.772852,"w, and Turkish. For these morphological analyzers, their pre-existing morphological analyses adhere to schemes that differ from those employed in the respective UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of Mo"
L18-1608,L18-1292,1,0.841663,"L format. 3.2. Converted Morphological Lexicons As a complement to the CoNLL-UL-compatible analyzers described above, we have created a set of 53 CoNLL-ULcompatible morphological lexicons covering 38 languages, based on existing freely available resources.9 The source lexicons, the conversion processes and the resulting inventory of freely available CoNLL-UL lexicons are described https://conllul.github.io 3850 6 https://camel.abudhabi.nyu.edu/calima-star/ https://github.com/habeanf/yap 8 https://github.com/coltekin/TRmorph/tree/trmorph2 9 http://pauillac.inria.fr/˜sagot/udlexicons.html 7 in (Sagot, 2018).10 Here we only provide in Table 3 two examples converted from the Lefff , the Alexina lexicon for French. The first one illustrates the 1-to-1 case, with an entry converted from the following original entry: encodent encoder v P3p, which includes the wordform (i.e. the [source and tree] token) encodent ‘encode3pl.pres.ind ’, its lemma, its Lefff POS and its Lefff morphosyntactic tag. The other example illustrates the 1-to-m case with the source token auxquels, which is analyzable as reflecting the sequence of two tree tokens a` lesquels ‘to which’. 4. Related Work and Perspective Our work ov"
L18-1608,W13-4917,1,0.923267,"Missing"
L18-1608,Q15-1026,1,0.906774,"Missing"
L18-1608,K17-3009,0,0.0605054,"Missing"
L18-1608,C16-1325,1,0.883776,"Missing"
L18-1608,W17-1320,1,0.835643,"ctive UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices"
L18-1608,P12-2002,1,0.826771,"y. As a result of the latter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to main"
L18-1608,P13-2103,1,0.786276,"atter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to maintain compatibilit"
L18-1608,zeman-etal-2012-hamledt,0,0.319438,"Missing"
L18-1608,K17-3001,1,0.904845,"Missing"
P06-3009,A00-2018,0,0.131907,"rate an intermediate level of part-of-speech (POS) tagging into our model. The key idea is that POS tags that are assigned to morphological segments at the word level coincide with the lowest level of non-terminals in the syntactic parse trees (cf. (Charniak et al., 1996)). Thus, POS tags can be used to pass information between the different tasks yet ensuring agreement between the two. 3 Related Work 4.1 As of yet there is no statistical parser for MH. Parsing models have been developed for different languages and state-of-the-art results have been reported for, e.g., English (Collins, 1997; Charniak, 2000). However, these models show impoverished morphological treatment, and they have not yet been successfully applied for MH parsing. (Sima’an et al., 2001) present an attempt to parse MH sentences based on a small, annotated corpus by applying a general-purpose Tree-gram model. However, their work presupposes correct morphological disambiguation prior to parsing. 5 In order to treat morphological phenomena a few stand-alone morphological analyzers have been developed for MH.6 Most analyzers consider words in isolation, and thus propose multiple analyses for each word. Analyzers which also attemp"
P06-3009,P97-1003,0,0.0293405,"sks, we incorporate an intermediate level of part-of-speech (POS) tagging into our model. The key idea is that POS tags that are assigned to morphological segments at the word level coincide with the lowest level of non-terminals in the syntactic parse trees (cf. (Charniak et al., 1996)). Thus, POS tags can be used to pass information between the different tasks yet ensuring agreement between the two. 3 Related Work 4.1 As of yet there is no statistical parser for MH. Parsing models have been developed for different languages and state-of-the-art results have been reported for, e.g., English (Collins, 1997; Charniak, 2000). However, these models show impoverished morphological treatment, and they have not yet been successfully applied for MH parsing. (Sima’an et al., 2001) present an attempt to parse MH sentences based on a small, annotated corpus by applying a general-purpose Tree-gram model. However, their work presupposes correct morphological disambiguation prior to parsing. 5 In order to treat morphological phenomena a few stand-alone morphological analyzers have been developed for MH.6 Most analyzers consider words in isolation, and thus propose multiple analyses for each word. Analyzers"
P06-3009,P05-1071,0,0.0311419,"r oil (n) ‘f + mnh’ she + mana that + counted that (rel) counted (v) Table 3: Morphological Analyses of the Wordform ‘fmnh’ a. NP b. NP N A N ildh.FS child.FS fmnh.FS fat.FS ild.MS child.MS CP Rel V f that mnh.MS counted.MS Figure 3: Ambiguity Resolution in Different Syntactic Contexts A related research agenda is the development of part-of-speech taggers for MH and other Semitic languages. Such taggers need to address the segmentation of words into morphemes to which distinct morphosyntactic categories can be assigned (cf. figure 2). It was illustrated for both MH (BarHaim, 2005) and Arabic (Habash and Rambow, 2005) that an integrated approach towards making morphological (segmentation) and syntactic (POS tagging) decisions within the same architecture yields excellent results. The present work follows up on insights gathered from such studies, suggesting that an integrated framework is an adequate solution for the apparent circularity in morphological and syntactic processing of MH. is easily disambiguated, as it is the only one maintaining agreement with the modified noun. In light of the above, we would want to conclude that syntactic processing must precede morphological analysis; however, this would"
P08-1043,P06-1084,0,0.386084,"and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint ta"
P08-1043,P08-1083,1,0.870844,"Missing"
P08-1043,W05-0706,0,0.205522,"Missing"
P08-1043,E06-1047,0,0.0142792,"n is modified by a proceeding space-delimited adjective. It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”). This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct"
P08-1043,D07-1022,0,0.226712,"e aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings of ACL-08: HLT, pages 371–379, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before ex"
P08-1043,P07-1029,1,0.791089,"ates the lexeme l spans from node ni to node nj ). GL is then used to parse the string tn1 . . . tnk−1 , where tni is a terminal corresponding to the lattice span between node ni and ni+1 . Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task. To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segm"
P08-1043,P05-1071,0,0.053423,"Missing"
P08-1043,itai-etal-2006-computational,0,0.102035,"irst 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.) Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incom5 The comparison to performance on version 2.0 is meaningless not only because of the change in size, but also conceptual changes in the annotation scheme 6 Unfortunatley running our setup on the v2.0 data set is currently not possible due to missing tokens-morphemes alignment in the v2.0 treebank. 7 We thank Shay Cohen for providing us with their data set and evaluation Software. 376 patible with the one of the Hebrew Treebank. 8 For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen"
P08-1043,J95-3004,0,0.127064,"Missing"
P08-1043,C04-1024,0,0.0393827,"004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser, 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain , a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 8 Mapping between the two schemes involves nondeterministic many-to-many mappings, and in some cases require a change in t"
P08-1043,D07-1046,0,0.0692168,"Missing"
P08-1043,H05-1060,0,0.639388,"t counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional. A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P (REL, VB|fmnh, context) = P (REL|f)P (VB|mnh, REL)P (REL, VB |context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models. We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities. Hence, we take the probability of the event fmnh analyzed as REL VB to be P (REL → f|REL) × P (VB → mnh|VB) This means that we generate f and mnh independently depending on their corresponding PoS tags, and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equally"
P08-1043,tsarfaty-goldberg-2008-word,1,0.785506,"espond to the original surface form as super-segmental morphology. An additional case of super-segmental morphology is the case of Pronominal Clitics. Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements. The additional morphological material in such cases appears after the stem and realizes the extended meaning. The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008). 2 Modern Hebrew Structure Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens. The form fmnh, for example, can be understood as the verb “lubricated”, the possessed noun “her oil”, the adjective “fat” or the verb “got fat”. Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibili"
P08-1043,W07-2219,1,0.840778,"Missing"
P08-1043,P06-3009,1,0.525742,"all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings of ACL-08: HLT, pages 371–379, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here w"
P08-1043,W05-0702,0,0.0394917,"Missing"
P12-2002,P06-1084,0,0.124895,"lattice structure, as illustrated in Figure 2. 1 We use the Hebrew transliteration in Sima’an et al. (2001). The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010). 2 7 Figure 2: The morphological segmentation possibilities of BCLM HNEIM. Double-circles are word boundaries. In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals. In such cases, existing evaluation metrics break down. To understand why, consider the trees in Figure 1. Metrics like PARS E VAL (Black et al., 1991) calculate the harmonic means of precision and recall on labeled spans hi, labe"
P12-2002,H91-1060,0,0.0520979,"(assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios. 1 Introduction A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence’s human-perceived interpretation. Current state-of-the-art parsers assume that the spacedelimited words in the input are the basic units of syntactic analysis. Standard evaluation procedures and metrics (Black et al., 1991; Buchholz and Marsi, 2006) accordingly assume that the yield of the parse tree is known in advance. This assumption breaks down when parsing morphologically rich languages (Tsarfaty et al., 2010), where every space-delimited word may be effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree. In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highl"
P12-2002,W06-2920,0,0.759533,"nted and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios. 1 Introduction A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence’s human-perceived interpretation. Current state-of-the-art parsers assume that the spacedelimited words in the input are the basic units of syntactic analysis. Standard evaluation procedures and metrics (Black et al., 1991; Buchholz and Marsi, 2006) accordingly assume that the yield of the parse tree is known in advance. This assumption breaks down when parsing morphologically rich languages (Tsarfaty et al., 2010), where every space-delimited word may be effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree. In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highly ambiguous and in order to"
P12-2002,D07-1022,0,0.66495,"at is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated. The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence. One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down. Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance. This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice"
P12-2002,W10-1412,0,0.163913,"s by MaltParser (MP) and EasyFirst (EF), trained on the treebank converted into unlabeled dependencies, and parsing the entire dev-set. For constituency-based parsing we use two models trained by the Berkeley parser (Petrov et al., 2006) one on phrase-structure (PS) trees and one on relational-realizational (RR) trees (Tsarfaty and Sima’an, 2008). In the raw scenario we let a latticebased parser choose its own segmentation and tags (Goldberg, 2011b). For dependency parsing we use MaltParser (Nivre et al., 2007b) optimized for Hebrew by Ballesteros and Nivre (2012), and the EasyFirst parser of Goldberg and Elhadad (2010) with the features therein. Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by Adler and Elhadad (2006). We use the standard split of the Hebrew treebank (Sima’an et al., 2001) and its conversion into unlabeled dependencies (Goldberg, 2011a). We use PARS E VAL for evaluating phrase-structure trees, ATTACH S CORES for evaluating dependency trees, and T ED E VAL for evaluating all trees in all scenarios. We implement S EG E VAL for evaluating segmentation based on our T ED E VAL implementation, replacing the tree distance and size with str"
P12-2002,P11-2124,0,0.1301,"to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance. This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice structure. We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing performance for the Semitic language Modern Hebrew, using the best performing systems, both constituencybased and dependency-based (Tsarfaty, 2010; Goldberg, 2011a). Our experiments demonstrate that, for all parsers, significant performance gaps between realistic and non-realistic scenarios crucially depend on the kind of information initially provided to the parser. The tool and metrics that we provide are completely general and can straightforwardly apply to other languages, treebanks and different tasks. 6 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 6–10, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics (tree1) (tree2) TOP TOP PP PP IN 0 B1 “in” IN NP NP 0 B1"
P12-2002,C10-1045,0,0.706763,"h word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated. The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence. One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down. Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance. This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice structure. We demonstrate the informativeness of our me"
P12-2002,P05-1071,0,0.0219498,"teration in Sima’an et al. (2001). The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010). 2 7 Figure 2: The morphological segmentation possibilities of BCLM HNEIM. Double-circles are word boundaries. In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals. In such cases, existing evaluation metrics break down. To understand why, consider the trees in Figure 1. Metrics like PARS E VAL (Black et al., 1991) calculate the harmonic means of precision and recall on labeled spans hi, label, ji where i, j are terminal boundaries. Now, the NP dominating “shadow of"
P12-2002,P06-1055,0,0.0346381,"onalrealizational trees (RR). We parse all sentences in the dev set. RR extra decoration is removed prior to evaluation. Gold Predicted Raw Gold Predicted Raw MP MP MP EF EF EF S EG E VAL 100.00 100.00 95.07 100.00 100.00 95.07 ATT S CORES U: 83.59 U: 82.00 N/A U: 84.68 U: 83.97 N/A T ED E VAL U: 91.76 U: 91.20 U: 87.03 U: 92.25 U: 92:02 U: 87.75 Table 2: Dependency parsing results by MaltParser (MP) and EasyFirst (EF), trained on the treebank converted into unlabeled dependencies, and parsing the entire dev-set. For constituency-based parsing we use two models trained by the Berkeley parser (Petrov et al., 2006) one on phrase-structure (PS) trees and one on relational-realizational (RR) trees (Tsarfaty and Sima’an, 2008). In the raw scenario we let a latticebased parser choose its own segmentation and tags (Goldberg, 2011b). For dependency parsing we use MaltParser (Nivre et al., 2007b) optimized for Hebrew by Ballesteros and Nivre (2012), and the EasyFirst parser of Goldberg and Elhadad (2010) with the features therein. Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by Adler and Elhadad (2006). We use the standard split of the Hebrew treebank"
P12-2002,roark-etal-2006-sparseval,0,0.0433542,"rphological analysis of an input sentence x is then a lattice L obtained through the concatenation of the lattices L1 , . . . , Ln where MA(w1 ) = L1 , . . . , MA(wn ) = Ln . Now, let x = w1 , . . . , wn be a sentence with a morphological analysis lattice MA(x) = L. We define the output space YMA(x)=L for h (abbreviated YL ), as the set of linearly-ordered labeled trees such that the yield of LEX entries hs1 , p1 i,. . . ,hsk , pk i in each tree (where si ∈ T and pi ∈ N , and possibly k 6= n) corresponds to a path through the lattice L. 3 A tool that could potentially apply here is SParseval (Roark et al., 2006). But since it does not respect word-boundaries, it fails to apply to such lattices. Cohen and Smith (2007) aimed to fix this, but in their implementation syntactic nodes internal to word boundaries may be lost without scoring. 8 TED (p, g) |p |+ |g |− 2 The term |p |+ |g |− 2 is a normalization factor defined in terms of the worst-case scenario, in which the parser has only made incorrect decisions. We would need to delete all lexemes and nodes in p and add all the lexemes and nodes of g, except for roots. An Example Both trees in Figure 1 are contained in YL for the lattice L in Figure 2. If"
P12-2002,D07-1046,0,0.0803698,"lustrated in Figure 2. 1 We use the Hebrew transliteration in Sima’an et al. (2001). The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). Examples for similar phenomena in Arabic may be found in Green and Manning (2010). 2 7 Figure 2: The morphological segmentation possibilities of BCLM HNEIM. Double-circles are word boundaries. In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals. In such cases, existing evaluation metrics break down. To understand why, consider the trees in Figure 1. Metrics like PARS E VAL (Black et al., 1991) calculate the harmonic means of precision and recall on labeled spans hi, label, ji where i, j are termin"
P12-2002,C08-1112,1,0.882329,"Missing"
P12-2002,W10-1401,1,0.874484,"formance of the best parsing systems to date in the different scenarios. 1 Introduction A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence’s human-perceived interpretation. Current state-of-the-art parsers assume that the spacedelimited words in the input are the basic units of syntactic analysis. Standard evaluation procedures and metrics (Black et al., 1991; Buchholz and Marsi, 2006) accordingly assume that the yield of the parse tree is known in advance. This assumption breaks down when parsing morphologically rich languages (Tsarfaty et al., 2010), where every space-delimited word may be effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree. In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated. The multiple morphological analyses of input words may be represented via a lattice that encodes the diff"
P12-2002,D11-1036,1,0.867769,"ad of terminal boundaries (Tsarfaty, 2006) will fail here too, since the missing overt definite article H will cause similar misalignments. Metrics for dependencybased evaluation such as ATTACHMENT S CORES (Buchholz and Marsi, 2006) suffer from similar problems, since they assume that both trees have the same nodes — an assumption that breaks down in the case of incorrect morphological segmentation. Although great advances have been made in parsing MRLs in recent years, this evaluation challenge remained unsolved.3 In this paper we present a solution to this challenge by extending T ED E VAL (Tsarfaty et al., 2011) for handling trees over lattices. 3 The Proposal: Distance-Based Metrics Input and Output Spaces We view the joint task as a structured prediction function h : X → Y from input space X onto output space Y. Each element x ∈ X is a sequence x = w1 , . . . , wn of spacedelimited words from a set W. We assume a lexicon LEX , distinct from W, containing pairs of segments drawn from a set T of terminals and PoS categories drawn from a set N of nonterminals. Edit Scripts and Edit Costs We assume a set A={ADD(c, i, j),DEL(c, i, j),ADD(hs, pi, i, j), DEL (hs, pi, i, j)} of edit operations which can ad"
P12-2002,E12-1006,1,0.855597,"hat we train the parsers for predicted on a treebank containing predicted tags. There is however a great drop when moving from predicted to raw, which confirms that evaluation benchmarks on gold input as in Nivre et al. (2007a) do not provide a realistic indication of parser performance. For all tables, T ED E VAL results are on a similar scale. However, results are not yet comparable across parsers. RR trees are flatter than bare-bone PS trees. PS and DEP trees have different label sets. Cross-framework evaluation may be conducted by combining this metric with the cross-framework protocol of Tsarfaty et al. (2012). 5 Conclusion We presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation. Our contribution is both technical, providing an evaluation tool that can be straightforwardly applied for parsing scenarios involving trees over lattices,4 and methodological, suggesting to evaluate parsers in all possible scenarios in order to get a realistic indication of parser performance. Acknowledgements We thank Shay Cohen, Yoav Goldberg and Spence Green for discussion of this challenge. This work was supported by"
P12-2002,P06-3009,1,0.918431,"segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated. The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence. One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down. Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance. This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined"
P12-2002,P08-1043,1,\N,Missing
P12-2002,D07-1096,1,\N,Missing
P12-2002,ballesteros-nivre-2012-maltoptimizer-system,1,\N,Missing
P13-2103,P06-1084,0,0.20163,"For cross-language evaluation, we can limit the depth of the hierarchy, and convert the more specific notions to their most-specific ancestor in the evaluation set. Systems. We present two systems that predict U-SD labels along with morphological and syntactic information, using [DEP], a dependency parser (Nivre et al., 2007), and [RR], a RelationalRealizational (RR) constituency parser (Tsarfaty and Sima’an, 2008). DEP is trained directly on the dependency version of the U-SD resource. Since it cannot predict its own segmentation, automatic segments and tags are predicted using the system of Adler and Elhadad (2006). The constituency4 Despite significant advances in parsing Hebrew, as of yet there has been no functional evaluation of Hebrew parsers. E.g., Goldberg and Elhadad (2010) evaluate on unlabeled dependencies, Tsarfaty (2010) evaluate on constituents. This is largely due to the lack of standard resources and guidelines for annotating functional structures in such a language. 5 The resources can be downloaded at http://www. tsarfaty.com/heb-sd/. 3 Technically, this is done by deleting a line adding a property to the morphology column in the CoNLL format. 581 (2) gf root - root hd - head (governor,"
P13-2103,cer-etal-2010-parsing,0,0.00594249,"Missing"
P13-2103,P12-2003,0,0.0309101,"Missing"
P13-2103,C10-1094,0,0.0580678,"Missing"
P13-2103,P06-1055,0,0.0089079,"f yet there has been no functional evaluation of Hebrew parsers. E.g., Goldberg and Elhadad (2010) evaluate on unlabeled dependencies, Tsarfaty (2010) evaluate on constituents. This is largely due to the lack of standard resources and guidelines for annotating functional structures in such a language. 5 The resources can be downloaded at http://www. tsarfaty.com/heb-sd/. 3 Technically, this is done by deleting a line adding a property to the morphology column in the CoNLL format. 581 (2) gf root - root hd - head (governor, argument-taking) based model is trained on U-SD-labeled RR trees using Petrov et al. (2006). We use the lattice-based extension of Goldberg and Elhadad (2011) to perform joint segmentation and parsing. We evaluate three input scenarios: [Gold] gold segmentation and gold tags, [Predicted] gold segments, and [Raw] raw words. We evaluate parsing results with respect to basic U-SD trees, for 42 labels. We use TedEval for joint segmentation-tree evaluation (Tsarfaty et al., 2012b) and follow the cross-parser evaluation protocol of Tsarfaty et al. (2012a). prd - verbal predicate exist - head of an existential phrase nhd - head of a nominal phrase ghd - genitive head of a nominal phrase de"
P13-2103,N10-1049,0,0.0274294,"hierarchy and design principles similarly emphasize English-like structures and underrepresent morphosyntactic argument-marking alternatives. We define an extension of SD, called Unified-SD (U-SD), which unifies the annotation of structurally and morphologically marked relations via an inheritance hierarchy. We extend SD with a functional branch, and provide a principled treatment of morpho-syntactic argument marking. Based on the U-SD scheme we create a new parallel resource for the MRL Modern Hebrew, whereby aligned constituency and dependency trees reflect equivalent U-SD annotations (cf. Rambow (2010)) for the same set of sentences. We present two systems that can automatically learn U-SD annotations, from the dependency and the constituency versions respectively, delivering high baseline accuracy on the prediction task. Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in syntactic parse-trees. The SD representation is useful for parser evaluation, for downstream applications, and, ultimately, for natural language understanding, however, the design of SD focuses on structurally-marked relations and under-represents morphosyntactic realization pa"
P13-2103,de-marneffe-etal-2006-generating,0,0.0348833,"Missing"
P13-2103,C08-1112,1,0.858335,"Missing"
P13-2103,W10-1412,0,0.108183,"stems. We present two systems that predict U-SD labels along with morphological and syntactic information, using [DEP], a dependency parser (Nivre et al., 2007), and [RR], a RelationalRealizational (RR) constituency parser (Tsarfaty and Sima’an, 2008). DEP is trained directly on the dependency version of the U-SD resource. Since it cannot predict its own segmentation, automatic segments and tags are predicted using the system of Adler and Elhadad (2006). The constituency4 Despite significant advances in parsing Hebrew, as of yet there has been no functional evaluation of Hebrew parsers. E.g., Goldberg and Elhadad (2010) evaluate on unlabeled dependencies, Tsarfaty (2010) evaluate on constituents. This is largely due to the lack of standard resources and guidelines for annotating functional structures in such a language. 5 The resources can be downloaded at http://www. tsarfaty.com/heb-sd/. 3 Technically, this is done by deleting a line adding a property to the morphology column in the CoNLL format. 581 (2) gf root - root hd - head (governor, argument-taking) based model is trained on U-SD-labeled RR trees using Petrov et al. (2006). We use the lattice-based extension of Goldberg and Elhadad (2011) to perform"
P13-2103,W10-1401,1,0.89593,"Missing"
P13-2103,P11-2124,0,0.0870464,"ers. E.g., Goldberg and Elhadad (2010) evaluate on unlabeled dependencies, Tsarfaty (2010) evaluate on constituents. This is largely due to the lack of standard resources and guidelines for annotating functional structures in such a language. 5 The resources can be downloaded at http://www. tsarfaty.com/heb-sd/. 3 Technically, this is done by deleting a line adding a property to the morphology column in the CoNLL format. 581 (2) gf root - root hd - head (governor, argument-taking) based model is trained on U-SD-labeled RR trees using Petrov et al. (2006). We use the lattice-based extension of Goldberg and Elhadad (2011) to perform joint segmentation and parsing. We evaluate three input scenarios: [Gold] gold segmentation and gold tags, [Predicted] gold segments, and [Raw] raw words. We evaluate parsing results with respect to basic U-SD trees, for 42 labels. We use TedEval for joint segmentation-tree evaluation (Tsarfaty et al., 2012b) and follow the cross-parser evaluation protocol of Tsarfaty et al. (2012a). prd - verbal predicate exist - head of an existential phrase nhd - head of a nominal phrase ghd - genitive head of a nominal phrase dep - dependent (governed, or an argument) arg - argument agent - age"
P13-2103,E12-1006,1,0.41418,"l of each node in the PS trees based on the morphological features, syntactic environment, and dash-feature (if exist), using deterministic grammar rules (Glinert, 1989). Specifically, we compare each edge with a set of templates, and, once finding a template that fits the morphological and syntactic profile of an edge, we assign functions to all daughters. This delivers PS trees where each node is annotated with a U-SD label (Figure 2a). At a second stage we project the inferred labels onto the arcs of the unlabeled dependency trees of Goldberg (2011), using the tree unification operation of Tsarfaty et al. (2012a). The result is a dependency tree aligned with the constituency tree where dependency arcs are labeled with the same function as the respective span in the PS tree.5 Universal Aspects of U-SD. The revised USD ontology provides a typological inventory of labels that describe different types of arguments (dep), argument-taking elements (hd), and argument-marking elements (func) in the grammar of different languages. Abstract (universal) concepts reside high in the hierarchy, and more specific distinctions, e.g., morphological markers of particular types, are daughters within more specific bran"
P13-2103,W03-2401,0,\N,Missing
P13-2103,W08-1301,0,\N,Missing
P17-1122,W02-2211,0,0.0980683,"1338 sentences (b = 0.10). The position effect contrasts markedly with the decrease of human-likeness ratings that (Cagan et al., 2014) ascribed to a learning effect: there, raters noticed the repetitive structure and took this to be a sign that the utterances were machine generated. The fact that we find no such effect means that our grammars successfully avoided such repetitiveness. 7 Related and Future Work NLG is often cast as a concept-to-text (C2T) challenge, where a structured record is transformed into an utterance expressing its content. C2T is usually addressed using template-based (Becker, 2002) or data-driven (Konstas and Lapata, 2013; Yuan et al., 2015) approaches. In particular, researchers explored data-driven grammar-based approaches (Cahill and van Genabith, 2006), often assuming a custom grammar (Konstas and Lapata, 2013) or a closed-domain approach (DeVault et al., 2008). ONLG in contrast is set in an open domain, and expresses multiple dimensions (grammaticality, sentiment, topic). In the context of social media, generating responses to tweets has been cast as a sequence-tosequence (seq2seq) transduction problem, and has been addressed using statistical machine translation ("
P17-1122,E06-1040,0,0.032229,"Missing"
P17-1122,W14-2708,1,0.701438,"articles. These responses address particular topics and reflect diverse sentiments towards them, in accordance to predefined user agendas. This is an open-ended and unstructured generation challenge, which is closely tied to the communicative goals of actual human responders. 1331 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1331–1341 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1122 In previous work we addressed the ONLG challenge using a template-based approach (Cagan et al., 2014). The proposed system generated subjective responses to articles, driven by user agendas. While the evaluation showed promising results in human-likeness and relevance ratings, the template-based system suffers from low output variety, which leads to a learning effect that reduced the perceived human-likeness of generated responses over time. In this work we tackle ONLG from a datadriven perspective, aiming to circumvent such learning effects and repetitive patterns in templatebased generation. Here, we approach generation via automatically inducing broad-coverage generative grammars from a la"
P17-1122,P06-1130,0,0.101657,"Missing"
P17-1122,P16-1094,0,0.0144087,", 2008). ONLG in contrast is set in an open domain, and expresses multiple dimensions (grammaticality, sentiment, topic). In the context of social media, generating responses to tweets has been cast as a sequence-tosequence (seq2seq) transduction problem, and has been addressed using statistical machine translation (SMT) methods (Ritter et al., 2011; Hasegawa et al., 2013). In this seq2seq setup, moods and sentiments expressed in the past are replicated or reused, but these responses do not target particular topics and are not driven by a concrete user agenda. An exception is a recent work by Li et al. (2016), exploring a persona-based conversational model, and Xu et al. (2016) who encode loose structured knowledge to condition the generation on. These studies present a stepping stone towards full-fledge neural ONLG architectures with some control over the user characteristics. The surge of interest in neural network generation architectures has spawned the development of seq2seq models based on encoder-decoder setup (Sordoni et al. (2015); Li et al. (2016, 2017) and references therein). These architectures require a very large dataset to train on. In the future we aim to extend our dataset and ex"
P17-1122,J03-4003,0,0.501391,"ses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users’ agendas, based on automatically acquired wide-coverage generative grammars. We compare three types of grammatical representations that we design for ONLG. The grammars interleave different layers of linguistic information, and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima’an, 2008) inspired grammar gets better language model scores than lexicalized grammars a` la Collins (2003), and that the latter gets better humanevaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content. 1 Reut Tsarfaty Introduction Interaction in social media has become increasingly prevalent nowadays. It fundamentally changes the way businesses and consumers behave (Qualman, 2012), it is instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009) and it also affects political regimes (Howard et al., 2011; Lamer, 2012). In particular, automatic interaction in natural language in social"
P17-1122,W08-1111,0,0.0303774,"ed. The fact that we find no such effect means that our grammars successfully avoided such repetitiveness. 7 Related and Future Work NLG is often cast as a concept-to-text (C2T) challenge, where a structured record is transformed into an utterance expressing its content. C2T is usually addressed using template-based (Becker, 2002) or data-driven (Konstas and Lapata, 2013; Yuan et al., 2015) approaches. In particular, researchers explored data-driven grammar-based approaches (Cahill and van Genabith, 2006), often assuming a custom grammar (Konstas and Lapata, 2013) or a closed-domain approach (DeVault et al., 2008). ONLG in contrast is set in an open domain, and expresses multiple dimensions (grammaticality, sentiment, topic). In the context of social media, generating responses to tweets has been cast as a sequence-tosequence (seq2seq) transduction problem, and has been addressed using statistical machine translation (SMT) methods (Ritter et al., 2011; Hasegawa et al., 2013). In this seq2seq setup, moods and sentiments expressed in the past are replicated or reused, but these responses do not target particular topics and are not driven by a concrete user agenda. An exception is a recent work by Li et a"
P17-1122,P13-1095,0,0.142376,"in natural language. So far, generation of human-like interaction in general has been addressed mostly commercially, where there is a movement towards online response automation (Owyang, 2012; Mah, 2012), and movement away from script-based interaction towards interactive chat bots (Mori et al., 2003; Feng et al., 2006). These efforts provide an automated one-size-fits-all type of interaction, with no particular expression of particular sentiments, topics, or opinions. In academia, work on generating human-like interaction focused so far on generating responses to tweets (Ritter et al., 2011; Hasegawa et al., 2013) or taking turns in short dialogs (Li et al., 2017). However, the architectures assumed in these studies implement sequence to sequence (seq2seq) mappings, which do not take into account topics, sentiments or agendas of the intended responders. Many real-world tasks and applications would benefit from automatic interaction that is generated intendedly based on a certain user profile or agenda. For instance, this can help promoting a political candidate or a social idea in social media, aiding people forming and expressing opinions on specific topics, or, in human-computer interfaces (HCI), mak"
P17-1122,D17-1230,0,0.0883042,"eraction in general has been addressed mostly commercially, where there is a movement towards online response automation (Owyang, 2012; Mah, 2012), and movement away from script-based interaction towards interactive chat bots (Mori et al., 2003; Feng et al., 2006). These efforts provide an automated one-size-fits-all type of interaction, with no particular expression of particular sentiments, topics, or opinions. In academia, work on generating human-like interaction focused so far on generating responses to tweets (Ritter et al., 2011; Hasegawa et al., 2013) or taking turns in short dialogs (Li et al., 2017). However, the architectures assumed in these studies implement sequence to sequence (seq2seq) mappings, which do not take into account topics, sentiments or agendas of the intended responders. Many real-world tasks and applications would benefit from automatic interaction that is generated intendedly based on a certain user profile or agenda. For instance, this can help promoting a political candidate or a social idea in social media, aiding people forming and expressing opinions on specific topics, or, in human-computer interfaces (HCI), making the computer-side generated utterances more mea"
P17-1122,P14-5010,0,0.00747412,"divided by the length of the sentence to obtain a per-word average and avoid length biases.1 Materials. We collected a new corpus of news articles and corresponding user comments from R the NY-Times web site, using their open Community API. We focus on sports news, which gave us 3,583 news articles and 13,100 user comments, or 55,700 sentences. The articles are then used for training a topic model using the Mallet library (McCallum, 2002). Next, we use the comments in the corpus to induce the grammars. To obtain our Base representation we parse the sentences using the Stanford CoreNLP suite (Manning et al., 2014) which can provide both phrase-structure and sentiment annotation. To obtain our Lexicalized representation we follow the same procedure, this time also using a head-finder which locates the head word for each non-terminal. To obtain the Relational-Realizational representation we followed the algorithm described in Tsarfaty et al. (2011), which, given both a constituency parse and a dependency parse of a sentence, unifies them into a lexicalized and functional phrasestructure. The merging is based on matching spans over words within the sentence.2 Setup. We simulated several scenarios. In each"
P17-1122,D11-1054,0,0.295884,"an-like interactions in natural language. So far, generation of human-like interaction in general has been addressed mostly commercially, where there is a movement towards online response automation (Owyang, 2012; Mah, 2012), and movement away from script-based interaction towards interactive chat bots (Mori et al., 2003; Feng et al., 2006). These efforts provide an automated one-size-fits-all type of interaction, with no particular expression of particular sentiments, topics, or opinions. In academia, work on generating human-like interaction focused so far on generating responses to tweets (Ritter et al., 2011; Hasegawa et al., 2013) or taking turns in short dialogs (Li et al., 2017). However, the architectures assumed in these studies implement sequence to sequence (seq2seq) mappings, which do not take into account topics, sentiments or agendas of the intended responders. Many real-world tasks and applications would benefit from automatic interaction that is generated intendedly based on a certain user profile or agenda. For instance, this can help promoting a political candidate or a social idea in social media, aiding people forming and expressing opinions on specific topics, or, in human-comput"
P17-1122,D13-1170,0,0.12417,"fine and implement an efficient 3 The Architecture grammar-based generator, termed here the decoder, which carries out the generation and calA bird’s-eye view of the architecture we propose is culates the objective function in Eq. (4). The algodepicted in Figure 1. The process consists of an ofrithm is described in Section 5. fline component containing (I) corpus collection, 1333 4 The Grammars Base Grammar. A central theme in this research is generating sentences that express a certain sentiment. Our base grammatical representation is inspired by the Stanford sentiment classification parser (Socher et al., 2013) which annotates every non-ternminal node with one of five sentiment classes s ∈ {−2, −1, 0, 1, 2}. Formally, each non-terminal in our base grammar includes a constituency category C and a sentiment class label s. The derivation of depth-1 trees with a parent node p and two daughters d1 , d2 will thus appear as follows: Cp [sp ] → Cd1 [sd1 ] Cd2 [sd2 ] The generative story imposed by this grammar is quite simple: each non-terminal node annotated with a sentiment can generate either a sequence of non-terminal daughters, or a single terminal node. An example of a subtree and its generation seque"
P17-1122,N15-1020,0,0.040082,"Missing"
P17-1122,D11-1036,1,0.712365,"entences. The articles are then used for training a topic model using the Mallet library (McCallum, 2002). Next, we use the comments in the corpus to induce the grammars. To obtain our Base representation we parse the sentences using the Stanford CoreNLP suite (Manning et al., 2014) which can provide both phrase-structure and sentiment annotation. To obtain our Lexicalized representation we follow the same procedure, this time also using a head-finder which locates the head word for each non-terminal. To obtain the Relational-Realizational representation we followed the algorithm described in Tsarfaty et al. (2011), which, given both a constituency parse and a dependency parse of a sentence, unifies them into a lexicalized and functional phrasestructure. The merging is based on matching spans over words within the sentence.2 Setup. We simulated several scenarios. In each, the system generates sentences with one grammar (G ∈ {Base, Lex, RR}) and one scoring scheme (with/without topic model scores). The results of each simulation are 5,000 responses for each variant of the system, consisting of 1,000 sentences for each sentiment class, s ∈ {−2, −1, 0, 1, 2}. The same 5000 generated sentences were used in"
P17-1122,C08-1112,1,0.821081,"Missing"
Q19-1003,P06-1084,0,0.8435,"yk (cmd , (σ, β, A)) = Td otherwise. (8) 38 In contrast with the pipeline architecture, both MDFirst and ArcGreedyk perform joint morphosyntactic parsing, in the sense that the framework aims to maximize a joint global score over both morphological and dependency transitions. This is formally depicted as follows in (9), where cmd and cet are the resulting configurations of MD and ET transitions respectively, and cdep are the resulting configurations of syntactic transitions: with the annotation scheme of the lexical resources of Itai and Wintner (2008), and in particular the HEBLEX lexicon of Adler and Elhadad (2006). We use the standard train/dev/test sets split, train on the train set (5,000 sentences) with a detailed investigation on dev (500), and confirm our results on test (716). Implementation: We implemented from scratch a fully integrated, transition-based, multilingual natural language processor, written in Go.13 Our implementation uses a general purpose morphological analyzer, which for Hebrew is backed by the BGU HEBLEX lexicon (Adler and Elhadad, 2006). We implemented the morphological disambiguator, dependency parser, and joint integration strategies defined herein. We implemented and experi"
Q19-1003,P16-1231,0,0.14833,"s of UDPipe, but within a joint architecture, allowing the use of information from any layer when disambiguating another. Joint morphological and syntactic processing has been addressed in the context of phrasestructure parsing for Semitic languages, showing empirical advantages over pipeline architectures (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). In the context of dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) integrated tagging and dependency parsing, improving state-of-theart accuracy for a set of typologically different languages. Andor et al. (2016) use the joint transition system proposed by Bohnet and Nivre (2012), and improve it using a globally Related and Future Work Monolingual MA&D for Modern Hebrew has been previously addressed in standalone settings using Hidden Markov Models (Bar-haim et al., 2008; Adler, 2007). While these results are adequate for some downstream applications, using Adler’s MA&D for dependency parsing, for instance, significantly harms parsing performance (Goldberg and Elhadad, 2010). More recently, More and Tsarfaty (2016) presented a standalone transition-based MA&D which jointly solves morphological segment"
Q19-1003,D12-1133,0,0.258552,"gata. Submission batch: 5/2018; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  ambiguation, Tsarfaty (2006) hypothesised that joint morphosyntactic parsing, where morphological information may assist syntactic disambiguation and vice versa, may be better suited. This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase–structure parsing for Semitic languages (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). For dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue of retokenizing ambiguous input tokens. More recently, Seeker and Centinoglu (2015) presented a graph-based framework for lattice parsing of Turkish also covering morphological segmentation. Their system takes a ‘‘product of experts’’ approach wherein the morphological paths and dependency trees are handled via two distinct models (a linear model over bigrams for MD and an arc-factor model for dependencies), reaching agreement"
Q19-1003,Q13-1034,0,0.244527,"18; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  ambiguation, Tsarfaty (2006) hypothesised that joint morphosyntactic parsing, where morphological information may assist syntactic disambiguation and vice versa, may be better suited. This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase–structure parsing for Semitic languages (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). For dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue of retokenizing ambiguous input tokens. More recently, Seeker and Centinoglu (2015) presented a graph-based framework for lattice parsing of Turkish also covering morphological segmentation. Their system takes a ‘‘product of experts’’ approach wherein the morphological paths and dependency trees are handled via two distinct models (a linear model over bigrams for MD and an arc-factor model for dependencies), reaching agreement via a dual decomposition"
Q19-1003,W06-2920,0,0.204946,"levant in context. This has clear ramifications for dependency parsing. Figure 1 shows a lattice that captures all possible analyses of the Hebrew phrase ‘‘bclm hneim,’’ literally: ‘‘in-the-shadow-of-them the-pleasant,’’ translated ‘‘in their pleasant shadow.’’ Each lattice arc corresponds to a potential node in a dependency tree. Dark circles mark morpheme boundaries, double circles mark token boundaries. The top tree depicts a correct syntactic analysis. In the bottom tree, incorrectly disambiguated tokens lead to a wrong syntactic analysis. Previous dependency parsing evaluation campaigns (Buchholz and Marsi, 2006; Nivre et al., 2007) assumed that the correct morphological analysis and disambiguation (MA&D) of the input stream is known in advance. In realistic end-toend parsing scenarios, however, this is of course not so. To overcome this, pipeline architectures where MA&D precedes parsing have been set up. These pipelines are suboptimal since they suffer from error propagation, and since local linear context available for automatic MA&D may be insufficient for accurate morphological disambiguation. For this, actual syntactic context may be required (Tsarfaty, 2006). To resolve this apparent loop, whe"
Q19-1003,D14-1082,0,0.00882086,"he standard Hebrew benchmark. For various Chinese parsing tasks, joint systems for word segmentation and syntactic parsing have been shown to outperform pipeline settings (Li et al., 2011; Zhang et al., 2014), but these systems assume transitions over equallength character-based sequences, and thus they are not applicable to the setup of variable-length lattice paths, as demonstrated in Figure 1. With the surge of interest in deep learning for NLP (Goldberg, 2016), research in dependency parsing seeks to replace engineered feature models with neural networks that induce a model automatically (Chen and Manning, 2014; Zhou 44 et al., 2015; Andor et al., 2016). Furthermore, the concept of word embedding introduced by Mikolov et al. (2013) allows for words to have vector representations, such that syntactic and semantic similarities are embodied in the vector space. However, these kinds of architectures are not immediately applicable to parsing Hebrew and other MRLs. Pretraining word embeddings is non-trivial for ambiguous input tokens, unless resorting to pipeline ‘‘segmentation-first’’ scenarios. Similarly, parsing architectures based in RNNs require morphologically disambiguated forms as input, which pre"
Q19-1003,D11-1109,0,0.0605039,"Missing"
Q19-1003,D07-1022,0,0.248062,"utational Linguistics, vol. 7, pp. 33–48, 2019. Action Editor: Masaaki Nagata. Submission batch: 5/2018; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  ambiguation, Tsarfaty (2006) hypothesised that joint morphosyntactic parsing, where morphological information may assist syntactic disambiguation and vice versa, may be better suited. This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase–structure parsing for Semitic languages (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). For dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue of retokenizing ambiguous input tokens. More recently, Seeker and Centinoglu (2015) presented a graph-based framework for lattice parsing of Turkish also covering morphological segmentation. Their system takes a ‘‘product of experts’’ approach wherein the morphological paths and dependency trees are handled via two distinct models (a linear model over bi"
Q19-1003,P04-1015,0,0.0962714,"cal analysis is required for syntactic parsing and syntactic analysis is required for morphological dis34 To compute Score(y ), y is mapped to a global feature vector Φ(y ) of size d multiplied by a  of the same size. The global weights vector ω feature vector Φ(y ) consists of local feature vectors, each of which is defined via a set of functions {φi : C → N }di=1 which count the occurrences of a prespecified pattern in a given configuration in y . Following Zhang and Clark  ∈ Rd via (2011), we learn the weights vector ω the generalized perceptron using the early-update averaged variant of Collins and Roark (2004). Decoding is based on the beam search algorithm, where a number of high-scoring candidate sequences are maintained in the beam in order to mitigate irrecoverable prediction errors that characterize greedy search procedures. At each step, the transition system applies all transitions to all candidates, and keeps the B highest-scoring candidates. During learning, the perceptron algorithm iterates through a gold-annotated corpus. Each sentence is parsed (decoded) with the last known weights, and if the parsed result differs from the gold, the weights are updated. The learning is stopped when ove"
Q19-1003,C16-1033,1,0.93964,"of configurations that are obtained by applying transitions t1 ...tn ∈ T sequentially. That is, starting with an initial configuration c0 = cs (x), we find y = c0 , ..., cn such that ci+1 = ti+1 (ci ) and cn ∈ Ct . Thus, each y depicts a sequence of decisions that constructs a valid analysis for x at the relevant linguistic level. For each task we employ an objective function F (x) as follows, where GEN (x) holds all the transition sequences that generate relevant candidates: 2.2 The Morphological Framework Our departure point for morphological disambiguation (MD) is the transition system of More and Tsarfaty (2016), currently established as the state of the art for Hebrew MA&D.4 The input to the system is a lattice L that captures the range of valid morphological analyses for the input tokens x = x1 , . . . , xk , as illustrated in the middle of Figure 1. The goal of the MD system is to select a sequence of contiguous arcs in L which represents the morphological disambiguation of x in context. Formally, we define for each token xi its tokenlattice Li = M A(xi ) where each lattice-arc in Li corresponds to a potential node in the dependency tree. Each lattice-arc has a morphosyntactic representation (MSR)"
Q19-1003,W04-0308,0,0.0384807,"flexible word order in MRLs, and that is applicable to Arc-Standard. We call this feature set rich linguistic features (RLF). The essence of the two feature sets is the same, but we replace features relying on positions of nodes with features relying on the labeled grammatical functions of these nodes.11 To construct our features, we define properties that capture the linguistic information of selectional preferences and subcategorization frames (Tesni`ere, 1959; Chomsky, 1965). To capture the Arc Standard: A straightforward method of bottom-up left-to-right incremental parsing as proposed in Nivre (2004). We assume the definition by K¨ubler et al. (2009). Arc Eager: Following Abney and Johnson (1991), Arc Eager defines a variant of Arc Standard that allows to eagerly attach a rightdependent to its head while allowing more dependents to attach to it. We assume the definition by K¨ubler et al. (2009). 8 To avoid confusion between lattice arcs and dependency arcs, we refer to lattice arcs mi ∈ M as ‘‘morphemes.’’ 9 A transition system can introduce an artificial root node that can head any partial tree in the sentence. The root node allows for multiple partial trees (a forest) to be related only"
Q19-1003,N10-1115,0,0.569979,"onent into a joint parser encompassing a single transition system, a single objective function, joint learning, and joint decoding. We apply this system to parsing Modern Hebrew and empirically confirm that predicting MA&D in the joint settings improves upon standalone MA&D, and upon recently reported Hebrew MA&D results. Our system further improves endto-end dependency parsing results in comparison to existing state-of-the-art parsers in pipeline scenarios, it significantly outperforms the joint parser of Seeker and Centinoglu (2015), and it substantially outperforms the dependency parser of Goldberg and Elhadad (2010), so far considered the de facto standard for Hebrew dependency parsing. The contribution of this paper is thus threefold. First, we define a language-agnostic joint morphosyntactic parser in a transition-based framework. Secondly, we empirically confirm that MA&D benefits from syntactic parsing, and in realistic end-to-end parsing scenarios, also vice versa. Finally, we present a new set of strong Hebrew end-to-end parsing results and deliver an Figure 1: The morphological and syntactic interactions in the analysis of the Hebrew phrase ‘‘bclm hneim’’ according to the Hebrew SPMRL annotation."
Q19-1003,P11-2124,0,0.409534,"). Further assume that both the Gold and Predicted dependency trees contain the correct dependency arc between b (‘‘in’’) and bit (‘‘house’’) labeled pobj. In simple LAS terms, the arcs that would be compared for the purpose of evaluation are: Gold Dep: pobj(1,3), det(3,2) Predicted Dep: pobj(1,2). So the pobj predicted arc will be considered an error, even though the relation between forms is correct, and accordingly both UAS and LAS will be 0. To address this issue, we define an F1 accuracy measure with respect to the forms of arc edges, 15 This is effectively equivalent to the F1 metric in Goldberg and Elhadad (2011) and Seeker and Centinoglu (2015). 40 Strategy System Standalone Pipeline Pipeline MDFirst MDFirst ArcGreedy3 ArcGreedy3 M&T 2016 Standard ZEager Standard ZEager Standard ZEager MD F1 Full/POS 93.32/94.09 93.32/94.09 93.32/94.09 94.39/95.19 94.71/95.49 94.56/95.36 94.62/95.45 Dep F1 Un/labeled n/a / n/a 80.44/73.86 80.82/74.28 80.32/73.22 80.50/73.53 80.60/73.43 80.73/73.89 Table 1: Joint morpho-syntactic parsing of the Modern Hebrew dev set with infused morphological lattices. Strategy System Standalone Pipeline Pipeline MDFirst MDFirst ArcGreedy3 ArcGreedy3 M&T 2016 Standard ZEager Standard"
Q19-1003,J08-4003,0,0.0317014,"stem for a morphological sequence M = m1 ...mn is a triplet where σ is a stack of morphemes mi ∈ VS , β is a buffer of morphemes mi ∈ VS , and A is a set of labeled dependency arcs (mi , r, mj ) ∈ V S × R × VS . Arc (Z)Eager: In our reproduction of the state-of-the-art results presented by Zhang and Nivre (2011) for English, we discovered in the code a variant of Arc Eager that we call Arc (Z)Eager, which has interesting subtle variations from Arc Eager, including a second stack holding head nodes, and certain hard constraints on the application of several transitions.10 An empirical study by Nivre (2008) compares the performance of Arc Standard and Arc Eager for 13 languages, amongst them Arabic and Turkish, both considered MRLs with some degree of wordorder freedom. For these languages, Arc Standard slightly outperformed Arc Eager. On a different but related note, our preliminary experiments on English and Hebrew show that the Arc ZEager variant always outperforms Arc Eager. However, the question which of the two, ArcStandard or Arc-ZEager, will be more suited for parsing Hebrew, remains open for our empirical investigation in Section 3. Cdep = (σ, β, A). A configuration represents a partial"
Q19-1003,Q15-1026,0,0.432566,"e morphological information may assist syntactic disambiguation and vice versa, may be better suited. This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase–structure parsing for Semitic languages (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). For dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue of retokenizing ambiguous input tokens. More recently, Seeker and Centinoglu (2015) presented a graph-based framework for lattice parsing of Turkish also covering morphological segmentation. Their system takes a ‘‘product of experts’’ approach wherein the morphological paths and dependency trees are handled via two distinct models (a linear model over bigrams for MD and an arc-factor model for dependencies), reaching agreement via a dual decomposition setup. In this work, we present a novel, languageagnostic, transition-based framework for end-toend morphosyntactic dependency parsing. The framework unifies a morphological and a syntactic component into a joint parser encompa"
Q19-1003,L16-1680,0,0.0939423,"Missing"
Q19-1003,P06-3009,1,0.848657,"present a new state of the art for Hebrew dependency parsing. 1 Introduction NLP research in recent years has shown increasing interest in parsing typologically different languages, as evident, for instance, by the 1 2 http://universaldependencies.org/. Using the transliteration of Sima’an et al. (2001). 33 Transactions of the Association for Computational Linguistics, vol. 7, pp. 33–48, 2019. Action Editor: Masaaki Nagata. Submission batch: 5/2018; Revision batch: 8/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  ambiguation, Tsarfaty (2006) hypothesised that joint morphosyntactic parsing, where morphological information may assist syntactic disambiguation and vice versa, may be better suited. This joint morphosyntactic hypothesis has been taken up and successfully confirmed in the context of phrase–structure parsing for Semitic languages (Goldberg and Tsarfaty, 2008; Cohen and Smith, 2007; Green and Manning, 2010). For dependency parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) present language-agnostic transition-based frameworks for jointly parsing and tagging input words, though without addressing the complex issue o"
Q19-1003,P13-2103,1,0.891644,"that precedes two sequential scores-drop on dev. For pipeline models, we test distinct stopping conditions for the morphological and the syntactic models, each based on its own standalone scores. Experiments Goal: We aim to test the hypothesis that joint syntactic and morphological disambiguation is better than a pipeline by empirically comparing the Pipeline, MDFirst and ArcGreedy3 parsing strategies in our unified transition-based morphosyntactic framework.12 Data: We use the Modern Hebrew section of the SPMRL shared task (Seddah et al., 2014), derived from the Hebrew Unified-SD version of Tsarfaty (2013). For the purpose of this work, we harmonized the treebank annotation scheme 13 https://golang.org by Google. Dispensing with Arc Eager, which underperformed Arc ZEager in all settings in all our preliminary investigations. 12 14 We set k = 3 because some features of Zhang and Nivre (2011) require three morphemes in the buffer. 39 rather than their node indices. Formally, let Mp be the predicted morphological disambiguation of x, and let Ap be the predicted dependency tree over Mp . Likewise, let Mg , Ag be the gold-standard morphological disambiguation and dependency tree of x. We now replace"
Q19-1003,P12-2002,1,0.7769,"h MA&D run, one for full MD including segmentation, tagging and morphological features (MD Full), and one for segmentation and tags only (MD POS). In our example, the revised arcs will now be: Evaluating Dependencies: Evaluating joint morpho-syntactic dependency parsing performance is non-trivial, because the gold and parse trees may have a different number of nodes, which precludes the application of standard attachment scores; it suffices that an incorrect segmentation occurs early in the sequence, then off-by-one indices in the remainder of the sentence deem the rest of the arcs incorrect (Tsarfaty et al., 2012). Let us illustrate this effect. Consider the Hebrew phrase ‘‘bbit’’ (translated ‘‘in the house’’) that appears as a single space-delimited token. Now consider the two following MD alternatives, with and without the Hebrew covert definite article. We also include here the indices of the disambiguated morphemes in their linear order: Now, the parser will be credited for identifying the pobj arc correctly, as desired, and the dependency scores will be: P r = 1, Re = 0.5, and F1 = 0.67. Gold Dep: pobj(b,bit),det(bit,h) Predicted Dep: pobj(b,bit). Results: Tables 1–4 present our morphosyntactic pa"
Q19-1003,W10-1401,1,0.937297,"Missing"
Q19-1003,J11-1005,0,0.0706853,"4 present our experiments and analysis, respectively. Section 5 discusses related and future work, and Section 6 concludes. 2 The Proposal: Transition-Based Joint Morpho-Syntactic Parsing 2.1 Formal Settings We cast end-to-end morphosyntactic parsing as a structure prediction function F : X → Y, where x ∈ X is a sequence of raw input tokens and y ∈ Y is a dependency representation where the nodes in the tree correspond to disambiguated morphosyntactic units we refer to as morphemes.3 We assume that F is realized in a transitionbased framework augmented with the structure prediction method of Zhang and Clark (2011). We start off with a completely general definition of a transition system as a quadruple S = (C, T, cs , Ct ), with C a set of configurations, T a set of transitions, cs an initialization function, and Ct ⊂ C a set of terminal configurations. We then define different instantiations of S for the different (morphological, syntactic, morphosyntactic) parsing tasks. In each instantiation, a transition sequence y for x is a sequence of configurations that are obtained by applying transitions t1 ...tn ∈ T sequentially. That is, starting with an initial configuration c0 = cs (x), we find y = c0 , .."
Q19-1003,P15-1117,0,0.0306778,"Missing"
Q19-1003,P11-2033,0,0.71635,"= m1 ...ml be the sequence of l arcs selected by the MD component.8 We denote a dependency graph for the sequence M = m1 ...ml as GM = (VM , AM ), where VM is a set of nodes corresponding to the arcs of M and AM ⊆ VM × R × VM is a set of labeled arcs between the elements of VM . A configuration of an arc system for a morphological sequence M = m1 ...mn is a triplet where σ is a stack of morphemes mi ∈ VS , β is a buffer of morphemes mi ∈ VS , and A is a set of labeled dependency arcs (mi , r, mj ) ∈ V S × R × VS . Arc (Z)Eager: In our reproduction of the state-of-the-art results presented by Zhang and Nivre (2011) for English, we discovered in the code a variant of Arc Eager that we call Arc (Z)Eager, which has interesting subtle variations from Arc Eager, including a second stack holding head nodes, and certain hard constraints on the application of several transitions.10 An empirical study by Nivre (2008) compares the performance of Arc Standard and Arc Eager for 13 languages, amongst them Arabic and Turkish, both considered MRLs with some degree of wordorder freedom. For these languages, Arc Standard slightly outperformed Arc Eager. On a different but related note, our preliminary experiments on Eng"
Q19-1003,C10-1045,0,\N,Missing
Q19-1003,P08-1043,1,\N,Missing
Q19-1003,P14-1125,0,\N,Missing
Q19-1003,D07-1096,0,\N,Missing
Q19-1003,W14-6111,1,\N,Missing
tsarfaty-goldberg-2008-word,J98-4004,0,\N,Missing
tsarfaty-goldberg-2008-word,D07-1022,0,\N,Missing
tsarfaty-goldberg-2008-word,C04-1024,0,\N,Missing
tsarfaty-goldberg-2008-word,W00-1201,0,\N,Missing
tsarfaty-goldberg-2008-word,J03-4003,0,\N,Missing
tsarfaty-goldberg-2008-word,H94-1020,0,\N,Missing
tsarfaty-goldberg-2008-word,P03-1054,0,\N,Missing
tsarfaty-goldberg-2008-word,P04-1042,0,\N,Missing
tsarfaty-goldberg-2008-word,P06-3009,1,\N,Missing
tsarfaty-goldberg-2008-word,P05-1022,0,\N,Missing
tsarfaty-goldberg-2008-word,W07-2219,1,\N,Missing
tsarfaty-goldberg-2008-word,P06-1023,0,\N,Missing
tsarfaty-goldberg-2008-word,W05-0706,0,\N,Missing
tsarfaty-goldberg-2008-word,P08-1043,1,\N,Missing
tsarfaty-goldberg-2008-word,P06-1084,0,\N,Missing
tsarfaty-goldberg-2008-word,P03-1013,0,\N,Missing
W07-2219,J97-4005,0,0.0494528,"new one and is familiar from formal theories of syntax such as HPSG (Sag et al., 2003) and LFG (Kaplan and Bresnan, 1982). Here we propose to reframe systematic morphological decoration of syntactic categories at all levels of the hierarchy as 161 (a) (b) Figure 6: The Expansion Possibilities of a Non-Terminal Node: Expanding the NP from figure 4 in a three-dimensional parameterization Space an additional dimension of statistical estimation for learning unlexicalized treebank PCFGs. Our proposal deviates from various stochastic extensions of such constraints-based grammatical formalisms (cf. (Abney, 1997)) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. To the best of our knowledge, this proposal has not been empirically explored before. 4 Experimental Setup Our goal is to determine the optimal strategy for learning treebank grammars for MH and to contrast it with bi-dimensional strategies explored for English. The methodology we use is adopted from (Klein and Manning, 2003) and our procedure is identical to the one described in (Johnson, 1998). We define transformations over the treebank that accept as input"
W07-2219,W04-1602,0,0.029522,"Missing"
W07-2219,W00-1201,0,0.0471462,"etween these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least as important as the vertical and hori"
W07-2219,P99-1065,0,0.335891,"2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a role in disambiguation at least"
W07-2219,J03-4003,0,0.629361,"age Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rath"
W07-2219,P03-1013,0,0.619341,"Linguistics Klein and Manning (2003) systematize the distinction between these two forms of parametrization by drawing them on a horizontal-vertical grid: parent encoding is vertical (external to the rule) whereas head-outward generation is horizontal (internal to the rule). By varying the value of the parameters along the grid, Klein and Manning (2003) tune their treebank grammar to achieve improved performance. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000). However, accuracy results for parsing languages other than English still lag behind.1 We propose that for various languages including the Semitic family, e.g. Modern Hebrew (MH) and Modern Standard Arabic (MSA), a third dimension of parametrization is necessary for encoding linguistic information relevant for breaking false independence assumptions. In Semitic languages, arguments may move around rather freely and the phrase-structure of clause-level categories is often shallow. For such languages agreement features play a rol"
W07-2219,P06-1087,0,0.0178255,"figure 1, for example, it is agreement on gender and number that reveals the subject-predicate dependency between surface forms. Figure 1 also shows that agreement features help to reveal such relations between higher levels of constituents as well. Determining the child constituents that contribute each of the features is not a trivial matter either. To illustrate the extent and the complexity of that matter let us consider definiteness in MH, which is morphologically marked (as an h prefix to the stem, glossed here explicitly as “the-”) and behaves as a syntactic 3 See (Wintner, 2000) and (Goldberg et al., 2006) for formal and statistical accounts (respectively) of noun phrases in MH. 158 (b) S N.MS.D hmnhl the-manager.MS.D VP.FS V.FS htpjrh resigned.FS ShVi NPhNNTi.FS.D NNT.FS sganit deputy.FS N.MS.D hmnhl the-manager.MS.D VPhVi).FS V.FS htpjrh resigned.FS Figure 3: Phrase-Level Agreement Features and HeadDependencies in MH: The direction of percolating definiteness in MH is distinct of that of the head (marking hhead-tagi) property (Danon, 2001). Definite noun-phrases exhibit agreement with other modifying phrases, and such agreement helps to determine the internal structure, labels, and the correc"
W07-2219,J98-4004,0,0.360276,"ation for Parsing Morphologically Rich Languages Reut Tsarfaty and Khalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which"
W07-2219,P03-1054,0,0.275613,"ng Morphologically Rich Languages Reut Tsarfaty and Khalil Sima’an Institute for Logic, Language and Computation University of Amsterdam Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract 1 Dimensions of Unlexicalized Parsing Probabilistic Context Free Grammars (PCFGs) are the formal backbone of most high-accuracy statistical parsers for English, and a variety of techniques was developed to enhance their performance relative to the na¨ıve treebank implementation — from unlexicalized extensions exploiting simple category splits (Johnson, 1998; Klein and Manning, 2003) to fully lexicalized parsers that condition events below a constituent upon the head and additional lexical content (Collins, 2003; Charniak, 1997). While it is clear that conditioning on lexical content improves the grammar’s disambiguation capabilities, Klein and Manning (2003) demonstrate that a wellcrafted unlexicalized PCFG can close the gap, to a large extent, with current state-of-the-art lexicalized parsers for English. Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditi"
W07-2219,P05-1010,0,0.0254241,"ts on a the top-down head-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the catego"
W07-2219,P06-1055,0,0.0540878,"tion process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivat"
W07-2219,W05-1512,0,0.0127231,"d-outward generation process. Figure 6(a) focuses on a selected NP node highlighted in figure 4 and shows its expansion possibilities in three dimensions. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as"
W07-2219,C04-1024,0,0.0826761,"ed headdriven baseline a` la (Collins, 2003) located on the (0, 0, 0) coordinate. We transform the treebank trees in correspondence with different points in the threedimensional space defined by (h, v, d). The models we implement are marked in the coordinate-system depicted in figure 7. The implementation details of the transformations we use are spelled out in tables 3–4. Procedure We implement different models that correspond to different instantiations of h, v and d. For each instantiation we transform the training set and learn a PCFG using Maximum Likelihood estimates, and we use BitPar (Schmidt, 2004), an efficient general-purpose parser, to parse unseen sentences. The input to the parser is a sequence of word segments where each segment corresponds to a single POS tag, possibly decorated with morphological features. This setup assumes partial morphological disambiguation (namely, segmentation) but crucially we do not disambiguate their respective POS categories. This setup is more appropriate for using general-purpose parsing tools and it makes our results comparable to studies in other languages.8 8 Our working assumption is that better performance of a parsing model in our setup will im"
W07-2219,P06-3009,1,0.688506,"ons. Figure 6(b) illustrates how the depth expansion interacts with both parent anno(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003) tation and neighbor dependencies thereby affecting both distributions. 3.1 A Note on State-Splits Recent studies (Klein and Manning, 2003; Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006) suggest that category-splits help in enhancing the performance of treebank grammars, and a previous study on MH (Tsarfaty, 2006) outlines specific POS-tags splits that improve MH parsing accuracy. Yet, there is a major difference between category-splits, whether manually or automatically acquired, and the kind of state-splits that arise from agreement features that refine phrasal categories. While category-splits aim at each category in isolation, agreement features apply to a whole set of categories all at once, thereby capturing refinement of the categories as well as linguistically motivated co-occurrences between them. Individual category-splits are viewed as taking place in a twodimensional space and it is hard to"
W07-2219,J04-4004,0,\N,Missing
W07-2219,H94-1020,0,\N,Missing
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W10-1405,J97-4005,0,0.0751119,"al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, name"
W10-1405,W09-3819,0,0.0509563,"d HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for Hebrew they obtained tiny-to-no improvement. A question thus emerges whether there are any benefits in explicitly incorporating morphosyntactic agreement patterns into our models. This question is a manifestation of a greater issue, namely, whether it is beneficial to represent complex patterns of morphology in the statistical parsing model, or whether configurational information subsume the relevant patterns, as it is commonly assumed in constituencybased parsing. Here we claim that agreement features are useful for statistical parsing provided that they are represented and parametrized"
W10-1405,P06-1087,0,0.0288127,"configurational languages (Hale, 1983) where the order of words is known to be (relatively) free. Agreement features encompass information concerning the functional relations between constituents in the syntactic structure, but whether incorporating agreement features in a statistical parsing model leads to improved performance has so far remained an open question and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 199"
W10-1405,P03-1046,0,0.0265043,"formance has so far remained an open question and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew i"
W10-1405,P03-1054,0,0.0448913,"structure of node labels in phrase-structure trees.5 Category-label state-splits can reflect the different morphosyntactic behavior of different non-terminals of the same type. Using such supervised, linguistically motivated, state-splits, based on the phraselevel marking of morphological information is one may build an efficient implementation of a PCFGbased parsing model that takes into account morphological features. State-split models were shown to obtain state-of-the-art performance with little computational effort. Supervised state-splits for constituency-based unlexicalized parsing in (Klein and Manning, 2003) in an accurate English parser. For the pair of Hebrew sentences (2b), the morphological state-split context-free representation of the domain S is as described at the top of figure 1.6 The Relational-Realizational (RR) Model A different way to implement a syntactic model that conform to the relaxed LH is by separating the inflectional features of surface words from their grammatical functions in the syntactic representation and let5 While agreement patterns in feature-rich grammars give rise to re-entrancies that break context-freeness, GPSG shows that using feature-percolation we can get qui"
W10-1405,J08-1002,0,0.0218147,"ion and saw contradictory results. ∗ The first author is currently a researcher at the department of Linguistics and Philology at Uppsala University. Taking Semitic languages as an example, it was shown that an SVM-based shallow parser (Goldberg et al., 2006) does not benefit from including agreement features for NP chunking in Hebrew. Phrase-structure based parsers for Arabic systematically discard morphological features from their label-set and never parametrize agreement explicitly (Maamouri et al., 2008). Models based on deep grammars such as CCG (Hockenmaier and Steedman, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. For formalisms that do incorporate morphology, generative models are may leak probability due to unification failures (Abney, 1997). Even results from dependency parsing remain inconclusive. It was shown for dependency parsing that case, definiteness and animacy features are useful to enhance parsing (e.g., (Øvrelid and Nivre, 2007)), agreement patterns are often excluded. When agreement features were included as features in dependency parser for Hebrew in (Goldberg and Elhadad, 2009) for"
W10-1405,P06-1055,0,0.207766,"Missing"
W10-1405,C08-1112,1,0.813509,"Missing"
W10-1405,D09-1088,1,0.904577,"Missing"
W10-1405,P08-1043,1,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-2708,W09-0613,0,0.0139461,"part-ofspeech (POS) tags that realizes the relevant referring expression. The POS tags in the resulting sequences are ultimately place holders for words from a lexicon Σ. In order to generate a variety of expression forms — nouns, adjectives and verbs — these items are selected randomly from a finegrained lexicon we defined. The sentiment (positive or negative) is expressed in a similar fashion via templates and randomly selected lexical entries for the POS slots, after calculating the overall sentiment for the intersection as stated above. Our generation implementation is based on SimpleNLG (Gatt and Reiter, 2009) which is a surface realizer API that allows us to create the desired templates and functions, and aggregates content into coherent sentences. The templates and functions that we defined are depicted in Figure 2. Relation CompetesWith CompetesWith Creates Target Samsung Google iOS Table 1: A knowledge graph snippet. statement that has the target node of this relation as its subject. The related sentence generation uses the same template-based mechanism as before. In principle, this process may be repeated any number of times and express larger parts of the KB. Here we only add one single knowl"
W14-2708,baccianella-etal-2010-sentiwordnet,0,0.00533847,"ined model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the document. We use the semantic/lexical method as implemented in Kathuria (2012). We rely on a WSD sentiment classifier that uses the SentiWordNet (Baccianella et al., 2010) database and calculates the positivity and negativity scores of a document based on the positivity and negativity of individual words. The result of the sentiment analysis is a pair of values, indicating the positive and negative sentiments of the document-based scores for individual words. We use the larger of these two values as the sentiment value for the whole document.4 A, C ⊆ P(Σ) × [−n..n] Our generation component accepts the result of the intersection as input and relies on a templatebased grammar and a set of functions for generating referring expressions in order to construct the ou"
W14-2708,W02-2211,0,0.424258,"ion. In particular, it would be interesting to test whether a novel mechanism for joint inference of topic/sentiment distributions could lead to improvement in the human-likeness of the generated responses. The syntactic and semantic means of expression that we use are based on bare bone templates and fine-grained POS tags (Theune et al., 2001). These may potentially be expanded with different ways to express subject/object relations, relations between phrases, polarity of sentences, and so on. Additional approaches to generation can factor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. 5 Conclusion We presented a system for generating responses that are directly tied to responders’ agendas and document content. To the best of our knowledge, this is the first system to generate subjective responses directly reflecting users’ agendas. Our response generation architecture provides an easyto-use and easy-to-extend"
W14-2708,P13-1095,0,0.203981,"sociation for Computational Linguistics Document sentiments are attributed to the author, whereas agenda sentiments are attributed to the user (henceforth: the responder). For each non-empty intersection of the topics in the document and in the agenda, our responsegeneration system aims to generate utterances that are fluent, human-like, and effectively engage readers. The generation is based on three assumptions, roughly reflecting the Gricean maxims of cooperative interaction (Grice, 1967). Online user responses should then be: pressed in the past are replicated or reused. A recent study by Hasegawa et al. (2013) modifies Ritter’s approach to produce responses that elicit an emotion from the addressee. Yet, these responses do not target particular topics and are not driven by a user agenda. The present paper addresses the problem of generating novel, subjective, responses to online opinionated articles. We formally define the document-to-response mapping problem and suggest an end-to-end system to solve it. Our system integrates a range of NLP and NLG technologies (including topic models, sentiment analysis, and the integration of a knowledge graph) to design a flexible generation mechanism that allow"
W14-2708,C10-2028,0,0.0321251,"in Social Media: An Agenda-Driven Architecture and a Turing-Like Test Tomer Cagan School of Computer Science The Interdisciplinary Center Herzeliya, Israel cagan.tomer@idc.ac.il Stefan L. Frank Centre for Language Studies Mathematics and Computer Science Radboud University Weizmann Institute of Science Nijmegen, The Netherlands Rehovot, Israel s.frank@let.ru.nl tsarfaty@weizmann.ac.il Abstract mercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such wo"
W14-2708,W08-1111,0,0.378557,"nism for joint inference of topic/sentiment distributions could lead to improvement in the human-likeness of the generated responses. The syntactic and semantic means of expression that we use are based on bare bone templates and fine-grained POS tags (Theune et al., 2001). These may potentially be expanded with different ways to express subject/object relations, relations between phrases, polarity of sentences, and so on. Additional approaches to generation can factor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. 5 Conclusion We presented a system for generating responses that are directly tied to responders’ agendas and document content. To the best of our knowledge, this is the first system to generate subjective responses directly reflecting users’ agendas. Our response generation architecture provides an easyto-use and easy-to-extend solution encompassing a range of NLP and NLG techniques. We evaluated both the h"
W14-2708,D11-1054,0,0.411258,"enerating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exNatural language traffic in social media (blogs, microblogs, talkbacks) enjoys vast monitoring and analysis efforts. However, the question whether computer systems can generate such content in order to effectively interact with humans has been only sparsely attended to. This paper presents an architecture for generating subjective responses to opinionated articles based on users’ agenda, documents’ topic"
W14-2708,J05-1002,0,0.0623606,"Missing"
W14-2708,N09-1054,0,0.0181741,"tract mercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 20"
W14-2708,P13-1108,0,0.0216656,"Missing"
W14-6111,W13-4916,0,0.216734,"Missing"
W14-6111,H91-1060,0,0.0351043,"hological features for each input segment were not. In the raw setting, there was no gold information, i.e., morphological segmentation, POS tags and morphological features for each input token had to be predicted as part of the parsing task. To lower the entry cost, participants were provided with reasonable baseline (if not state-of-the-art) morphological predictions (either disambiguated – in most cases– or ambiguous prediction in lattice forms). As a consequence of the raw scenario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation"
W14-6111,J92-4003,0,0.188544,"description of the unlabeled-data preparation that is required in the context of parsing MRLs, and the core labeled data that is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks,"
W14-6111,W06-2920,0,0.830006,"aselines for parsing morphologically rich languages (MRLs). The goals were both to provide a focal point for researchers interested in parsing MRLs and consequently to advance the state of the art in this area of research. The shared task focused on parsing nine morphologically rich languages, from different typological language families, in both a constituent-based and a dependency-based format. The set of nine typologically diverse languages comprised data sets for Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish. Compared to previous multilingual shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the SPMRL shared task targeted parsing in realistic evaluation scenarios, in which the analysis of morphologically ambiguous input tokens is not known in advance. An additional novelty of the SPMRL shared task is that it allowed for both a dependency-based and a constituentbased parse representation. This setting relied on an intricate and careful data preparation process which ensured consistency between the constituent and the dependency version by aligning the two representation types at the token level and at the level of part-of-speech tags. For all languages, we pr"
W14-6111,W09-3821,0,0.0148692,"Missing"
W14-6111,W10-1409,1,0.562386,"t is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexic"
W14-6111,W13-4909,0,0.0198006,"Missing"
W14-6111,W13-4905,1,0.815015,"e Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the"
W14-6111,E09-1038,1,0.836445,"gically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations wi"
W14-6111,C10-1045,0,0.0646224,"X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the T"
W14-6111,P09-2056,0,0.0145597,"re newswire wiki (edited) balanced size (tree tokens) 120M 150M 120M 205M 160M 100M 40M 100M 24M morph X* X X+mwe X X X X X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both insta"
W14-6111,P08-2015,0,0.0097065,"when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabele"
W14-6111,mengel-lezius-2000-xml,0,0.179115,"reebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increased awareness of and continued interest in the topic of the shared task. Results, cross-parser and cross-data analysis, and shared task description papers will be made available at http://www.spmrl.org/spmrl2014-sharedtask.html. Acknowledgments We"
W14-6111,nivre-etal-2006-talbanken05,0,0.0181359,"match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increase"
W14-6111,W11-3808,0,0.0159627,"l of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations will be provided in (Seddah et al., 2014). Note that we could not ensur"
W14-6111,seeker-kuhn-2012-making,0,0.0136617,"ue, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Po"
W14-6111,W10-1401,1,0.903368,"Missing"
W14-6111,E12-1006,1,0.924838,"enario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation for constituent parsing. We hope that further shared tasks will try to generalize such an evaluation. Indeed, having predicted function labels would ease labeled T ED E VAL evaluation and favor a full parsing chain evaluation. Nevertheless, the choice of T ED E VAL allowed us to go beyond the standard cross-parser evaluation within one setting and approach cross-framework (constituent vs. dependency (Tsarfaty et al., 2012a)) and cross-language evaluation, thus pushing the envelope"
W14-6111,P13-2103,1,0.454845,"the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedi"
W14-6111,J13-1003,1,\N,Missing
W14-6111,C08-1112,1,\N,Missing
W14-6111,P12-2002,1,\N,Missing
W14-6111,W13-4917,1,\N,Missing
W14-6111,vincze-etal-2010-hungarian,0,\N,Missing
W14-6111,D07-1096,1,\N,Missing
W18-6016,W06-2920,0,0.110262,"ng a single head and deriving labeled and unlabeled dependencies. To overcome this, Tsarfaty (2010) devised a set of rules based on the daughter-dependencies, function tags and empty elements, to automatically derive the relationalrealizational (RR) version of the HTB. In the RR HTB, each node is marked with its relational network (an unordered set of grammatical functions) mapped to the ordered syntactic constituents. The RR HTB retained the morphological conventions and core non-core distinction of the original HTB. In a parallel effort, and with the surge of interest in dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007),6 Goldberg (2011) automatically converted the HTB into its first, unlabeled, dependency version. The automatic conversion procedure assumed that heads are functional rather than lexical. As a result, the coordination marker would head coordination structures, the accusative marker would head direct object phrases, and so on. On top of that, in order to remain compatible with the wide-coverage lexicon of Itai and Wintner (2008), this version of the HTB adopted the POS tags scheme of Adler (2007), rather than the POS tags of Sima’an et al. (2001) Based on this version, Gold"
W18-6016,C16-1033,1,0.847649,"linguistic annotation principles. The reasons for these differences were sometimes practical, e.g., a new version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles. The HTB thus enabled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting these scores they use HTB versions that reflect distinct schemes, sometime repo"
W18-6016,W09-3819,0,0.142728,"esentation types, that in turn reflect different, and sometimes contradictory, linguistic annotation principles. The reasons for these differences were sometimes practical, e.g., a new version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles. The HTB thus enabled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting thes"
W18-6016,W07-0808,0,0.0245355,"1 Predicate types: the case of auxiliaries As part of the shift towards a lexically-driven analysis, structural changes were made to sentences containing auxiliary elements and copulas. There are three main sets of these: (i) Auxiliary elements marking modality, (ii) Auxiliary verbs which mostly mark habituality, but occasionally participate in negation or tense inflection when the predicate has no past/future form, and (iii) Positive or negative copulars. Modals do not constitute any uniform syntactic class in Hebrew, and there is an ongoing debate as to the POS of each modal expression (cf. Netzer et al. (2007)). In line with Netzer et al’s conclusion, these are tagged as AUX in the UD HTB. In UDv2, the modal served as the head of the clause, while the following predicate was labeled xcomp, as it is consistently realized in Hebrew in infinitive form. As of UDv2New, those modals which are tagged as AUX are also labeled aux, and the subsequent predicate receives the label which was attributed to the modal. See Table 1. In the opposite direction, auxiliary verbs, such as the ones in sets ii and iii were tagged as VERB. As the UDv2 scheme dedicates an AUX tag to function words in auxiliary functions eve"
W18-6016,W10-1412,0,0.0516882,"rent, and sometimes contradictory, linguistic annotation principles. The reasons for these differences were sometimes practical, e.g., a new version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles. The HTB thus enabled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting these scores they use HTB versions that reflect"
W18-6016,P11-2124,0,0.0268622,"bled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting these scores they use HTB versions that reflect distinct schemes, sometime reporting different metrics, which makes the numerical comparison between the respective results meaningless (Tsarfaty et al., 2011). This is why the UD initiative comes as a blessing, not only for the cross-linguistic parsing community but also for the Hebrew NLP community — by presenting a unique opportunity to standardize the res"
W18-6016,W14-6111,1,0.909138,"Missing"
W18-6016,W04-1602,0,0.0477991,"Association for Computational Linguistics 2 Ideally, the current UDv2 version would make for such a standard Hebrew resource. Unfortunately though, many of the conversion processes since Sima’an et al. (2001) to the present UDv2 have been automatic or semi-automatic, with no point of systematic qualitative validation. This resulted in odd, and sometime plain wrong, dependency structures, with respect to the UD scheme. Previous Work and the Trajectory of the Modern Hebrew Treebank Following the first treebanking efforts, in English (Marcus et al., 1993), Chinese (Xue et al., 2005), and Arabic (Maamouri and Bies, 2004), and with the surge of interest in developing statistical, broad-coverage, parsing models, Sima’an et al. (2001) introduced a pilot treebanking study and a Hebrew treebank (HTB), which included 500 sentences from the Hebrew newspaper ha’aretz, morphologically segmented and morpho-syntactically annotated with part-of-speech tags, morphological features, and labeled phrase-structure trees. Following the annotation practices at the time, much of the tagging and labeling scheme was adopted almost as is from the UPenn Treebank (Marcus et al., 1993). However, due to its rich morphology and Semitic"
W18-6016,J93-2004,0,0.0611324,"8), pages 133–143 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics 2 Ideally, the current UDv2 version would make for such a standard Hebrew resource. Unfortunately though, many of the conversion processes since Sima’an et al. (2001) to the present UDv2 have been automatic or semi-automatic, with no point of systematic qualitative validation. This resulted in odd, and sometime plain wrong, dependency structures, with respect to the UD scheme. Previous Work and the Trajectory of the Modern Hebrew Treebank Following the first treebanking efforts, in English (Marcus et al., 1993), Chinese (Xue et al., 2005), and Arabic (Maamouri and Bies, 2004), and with the surge of interest in developing statistical, broad-coverage, parsing models, Sima’an et al. (2001) introduced a pilot treebanking study and a Hebrew treebank (HTB), which included 500 sentences from the Hebrew newspaper ha’aretz, morphologically segmented and morpho-syntactically annotated with part-of-speech tags, morphological features, and labeled phrase-structure trees. Following the annotation practices at the time, much of the tagging and labeling scheme was adopted almost as is from the UPenn Treebank (Marc"
W18-6016,D07-1046,0,0.0139084,"rent versions of the treebank reflect different theories and formal representation types, that in turn reflect different, and sometimes contradictory, linguistic annotation principles. The reasons for these differences were sometimes practical, e.g., a new version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles. The HTB thus enabled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all prese"
W18-6016,P06-3009,1,0.695657,"nk reflect different theories and formal representation types, that in turn reflect different, and sometimes contradictory, linguistic annotation principles. The reasons for these differences were sometimes practical, e.g., a new version was derived to answer an emerging technological need, and sometimes socio-academic, e.g., because different teams adopted different linguistic theories as their underlying annotation principles. The HTB thus enabled the development of many statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment sc"
W18-6016,P13-2103,1,0.856603,"Wintner (2008), this version of the HTB adopted the POS tags scheme of Adler (2007), rather than the POS tags of Sima’an et al. (2001) Based on this version, Goldberg and Elhadad (2009) presented the first Hebrew dependency parsing results, only unlabeled attachment scores (UAS) at this point. Here too, as with the phrasestructure trees, it was impossible to devise an external procedure that would infer dependency labels for the unlabeled arcs — and there were no labeled dependencies to train such a labeler on. At that point, where the need for Hebrew labeled dependencies had become pressing, Tsarfaty (2013) presented the Unified-Stanford Dependencies (Unified-SD) version of the HTB, extending the Stanford dependencies (SD) scheme to cover both morphological and syntactic phenomena. Similar to SD, U-SD assumed a labeling hierarchy, with several changes: the hierarchy now included branches for head-types (hd), dependency types (dep), and functional types (func). In particular, dependencies in the func branch mark syntactic functions that are in fact interchangeable with morphology, when considering these functions from a typological perspective. Tsarfaty used the U-SD labels to edit three versions"
W18-6016,P12-2002,1,0.794689,"statistical morphological and syntactic processing models (Adler, 2007; Bar-haim et al., 2008; Shacham and Wintner, 2007; Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2009; Tsarfaty, 2010; Goldberg and Elhadad, 2010, 2011; More and Tsarfaty, 2016; More et al., In Press), but these models were trained on vastly different versions of the treebank, obeying different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting these scores they use HTB versions that reflect distinct schemes, sometime reporting different metrics, which makes the numerical comparison between the respective results meaningless (Tsarfaty et al., 2011). This is why the UD initiative comes as a blessing, not only for the cross-linguistic parsing community but also for the Hebrew NLP community — by presenting a unique opportunity to standardize the resources and metrics used"
W18-6016,D11-1036,1,0.826775,"ing different theories and annotation schemes, which then rendered the reported results mostly non-comparable. Hebrew dependency parsing presents an acute version of this syndrome. Studies such as Goldberg and Elhadad (2011), Tsarfaty et al. (2012), More et al. (In Press), as well as the SPMRL shared tasks (Seddah et al., 2013, 2014), all present attachment scores on Hebrew dependency parsing. But for reporting these scores they use HTB versions that reflect distinct schemes, sometime reporting different metrics, which makes the numerical comparison between the respective results meaningless (Tsarfaty et al., 2011). This is why the UD initiative comes as a blessing, not only for the cross-linguistic parsing community but also for the Hebrew NLP community — by presenting a unique opportunity to standardize the resources and metrics used for Hebrew parsing. The Hebrew treebank (HTB), consisting of 6221 morpho-syntactically annotated newspaper sentences, has been the only resource for training and validating statistical parsers and taggers for Hebrew, for almost two decades now. During these decades, the HTB has gone through a trajectory of automatic and semi-automatic conversions, until arriving at its UD"
