2013.iwslt-evaluation.21,P08-1114,0,0.0603296,"Missing"
2013.iwslt-evaluation.21,D11-1125,0,0.0575627,"Missing"
2013.iwslt-evaluation.21,C12-1121,0,0.0270855,"Missing"
2013.iwslt-evaluation.21,W02-1001,0,0.205679,"Missing"
2013.iwslt-evaluation.21,N13-1025,0,0.0269954,"Missing"
2013.iwslt-evaluation.21,E03-1076,0,0.128305,"Missing"
2013.iwslt-evaluation.21,W13-2236,1,0.835887,"Missing"
2013.iwslt-evaluation.21,2012.eamt-1.60,0,0.0650087,"Missing"
2013.iwslt-evaluation.21,D07-1104,0,\N,Missing
2013.iwslt-evaluation.21,P11-2031,0,\N,Missing
2013.iwslt-evaluation.21,P10-4002,0,\N,Missing
2013.iwslt-evaluation.21,2012.iwslt-evaluation.5,0,\N,Missing
2013.iwslt-evaluation.21,J07-2003,0,\N,Missing
2013.iwslt-evaluation.21,P12-1002,1,\N,Missing
2013.iwslt-evaluation.21,P13-2121,0,\N,Missing
2013.iwslt-evaluation.21,N13-1073,0,\N,Missing
2013.mtsummit-papers.2,W12-3155,1,0.930194,"cal consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is important in a CAT scenario, where several users might use the same global model but individual local models. 3 Online Adaptation in SMT Cesa-Bianchi and Lugosi (2006) presented a protocol for online learning with expert advice. This protoc"
2013.mtsummit-papers.2,2011.iwslt-evaluation.18,1,0.82902,"D. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machi"
2013.mtsummit-papers.2,W12-3156,0,0.0256807,"f SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial transl"
2013.mtsummit-papers.2,2010.iwslt-papers.3,1,0.879186,"itive. The log-linear interpolation weights are optimized using the standard MERT procedure provided with the Moses toolkit. The baseline system also provides a list of k-best translations. In the online discriminative re-ranking approach, this k-best list is rescored according to lexicalized sparse features including phrase pairs and target-side n-grams. 3.2 Constrained Search for Feedback Exploitation In order to extract information for system refinement from user feedback, source and user translation need to be aligned at the phrase-level. We use a constrained search technique described in Cettolo et al. (2010) to achieve this, which optimizes the coverage of both source and target sentences given a set of translation options. The search produces exactly one phrase segmentation and alignment, and allows gaps such that some source and target words may be uncovered. Unambiguous gaps (i.e. one on the source and one on the target side) can then be aligned. It differs in this respect from forced decoding which produces an alignment only when the target is fully reachable with the given models. From the phrase alignment, three types of phrase pairs can be collected: (i) new phrase pairs by aligning unambi"
2013.mtsummit-papers.2,D08-1024,0,0.0689582,"this is the first comparison of generative and discriminative online adaptation methods in a CAT scenario. The discriminative approach allows to perform feature development and training independently of the underlying SMT system. In the generative approach, the model is simple, however, updates have to be communicated to the decoder. In sum, the gains of both approaches add up to average BLEU improvements of 4 points over a baseline non-adapted model. 2 Previous Work Online learning methods in SMT are found in the context of stochastic methods for discriminative training (Liang et al., 2006; Chiang et al., 2008), Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 11–18. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating"
2013.mtsummit-papers.2,W02-1001,0,0.263479,"und in the cache receives a score of 0, i.e. no reward. n-grams crossing over contiguous translation options are not taken into account. Note that the proposed feature is simply a stateless function which rewards approved translation options, which are expected to be of high quality. To control the influence of the local language model, the additional weight is optimized with the Simplex algorithm; weights of the baseline system tuned with MERT are taken as fixed. 3.5 Online Discriminative Re-Ranking The learner used in our online discriminative re-ranking approach is a structured perceptron (Collins, 2002). We use lexicalized sparse features defined by two feature templates: First, all phrase pairs found by the decoder (for system translations) or by the constrained search (for the user translation) are used as features. Second, we use features defined by target-side n-grams from n = 1, . . . , 4 in the user translation. Our features are not indicator functions, but use the number of source words (for the first type of features) and the number of words in target-side n-grams (for the second type of features) as values. Given a feature representation f (x, y) for a source-target pair (x, y), and"
2013.mtsummit-papers.2,2010.amta-papers.21,0,0.646236,"line discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is important in a CAT scenario, where several users might use the same global model but individual local models. 3 Online Adaptation in SMT Cesa-Bianchi and Lugosi (2006) presented a protocol for online learning with expert advice. This protocol can be adapted to our scenario of online adaptation in SMT as follows: Train global model M"
2013.mtsummit-papers.2,W07-0733,0,0.0548648,"licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been prese"
2013.mtsummit-papers.2,P07-2045,1,0.0112703,"erative components of translation model (TM) (Section 3.3) and language model (LM) (Section 3.4) and adaptation via discriminative reranking (Section 3.5). Different refinements result in different modes of combination of global and local models (step 0). Both generative and discriminative adaptation modes deploy a constrained search technique (Section 3.2) to extract information relevant for system refinement from the received user feedback (step 3). Translation (step 2) employs a standard phrase-based SMT engine. 3.1 Baseline System The MT engine is built upon the open source toolkit Moses (Koehn et al., 2007). The global translation and the lexicalized reordering models are estimated on parallel training data with default 12 Annex Allegato to the all’ 3.3 Technical Offer Offerta Tecnica Figure 1: Phrase segmentation and alignment. setting. The global 5-gram LM smoothed through the improved Kneser-Ney technique is estimated on the target monolingual side of the parallel training data using the IRSTLM toolkit (Federico et al., 2008). Models are case-sensitive. The log-linear interpolation weights are optimized using the standard MERT procedure provided with the Moses toolkit. The baseline system als"
2013.mtsummit-papers.2,N10-1062,0,0.179294,"BLEU improvements of 4 points over a baseline non-adapted model. 2 Previous Work Online learning methods in SMT are found in the context of stochastic methods for discriminative training (Liang et al., 2006; Chiang et al., 2008), Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 11–18. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. or streaming scenarios for incremental adaptation of the core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show"
2013.mtsummit-papers.2,P06-1096,0,0.193911,"Missing"
2013.mtsummit-papers.2,D12-1037,0,0.045275,"e core components of SMT (Levenberg et al., 2010). However, the online learning protocol is applied in these approaches to training data only, i.e., parameters are updated on a per-example basis on the training set, while testing is done by retranslating the full test set using the final model. Further related work can be found in the application of incremental learning to domain adaptation in SMT. Here a local and a global model have to be combined, either in a log-linear combination (Koehn and Schroeder, 2007), with a fill-up method (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Carpuat and Simard (2012) show that increased translation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypothe"
2013.mtsummit-papers.2,W04-3225,0,0.303316,"ranslation consistency does not correlate with better translation quality, however, translation errors are indicated by inconsistencies. Our approach can be seen as a successful approach to improve translation quality by enforcing local consistency through online learning. Cesa-Bianchi et al. (2008) are the first to apply online discriminative re-ranking to a CAT scenario. Incremental adaptations of the generative components of SMT have been presented for a related scenario, interactive machine translation, where an MT component produces hypotheses based on partial translations of a sentence (Nepveu et al., 2004; Ortiz-Mart´ınez et al., 2010). Our online learning protocol is similar, but operating on the sentence instead of word or phrase level. Incremental adaptations have also been presented for larger batches of data (Bertoldi et al., 2012). In terms of granularity, our scenario is most similar to the work by Hardt and Elming (2010), where the Moses training procedure is employed to update the phrase table immediately after a reference becomes available. Our work, however, focuses on adapting both language and translation model with techniques where the global model remains unchanged. This is impo"
2013.mtsummit-papers.2,N10-1079,0,0.347766,"Missing"
2013.mtsummit-papers.2,P02-1040,0,0.0869298,"ange of millions of sentence pairs. Then for each document d, consisting of a few hundred up to a thousand sentences, a local model Md is created. For each example, first the static global model Mg and the current local model Md are combined into a model Mg+d . Then the input xt is translated into yˆt using the model Mg+d . Finally the local model Md is refined on feedback yt that is received immediately after producing yˆt . Evaluations reported in this paper take the local predictions yˆt and compare them to the user translations yt for each document, e.g., using |d| BLEU {(ˆ yt , yt )}t=1 (Papineni et al., 2002). Note that this setup differs from the more standard scenario where the whole test set is re-translated using the learned model. However, the evaluation in our online learning scenario is still fair since only feedback from previous test set examples is used to update the current model. We present three techniques for refinements of local SMT models (step 4), namely adaptations of the generative components of translation model (TM) (Section 3.3) and language model (LM) (Section 3.4) and adaptation via discriminative reranking (Section 3.5). Different refinements result in different modes of c"
2014.iwslt-papers.12,N03-1017,0,0.0526435,"ct features and tune weights for the new phrases. We find gains of 0.3 − 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. 1. Introduction Decoding in SMT amounts to searching for the most probable (Viterbi) derivation of a target string given the source string. Standard SMT decoders perform at the same time a search for the optimal segmentation of the source sentence into disjoint spans of words, which are translated by rules encoding bi-phrases. This means that irrespective of whether phrases are contiguous [1], non-contiguous [2, 3], or hierarchical [4], the application of phrase rules at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches th"
2014.iwslt-papers.12,H05-1095,0,0.0342639,"weights for the new phrases. We find gains of 0.3 − 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. 1. Introduction Decoding in SMT amounts to searching for the most probable (Viterbi) derivation of a target string given the source string. Standard SMT decoders perform at the same time a search for the optimal segmentation of the source sentence into disjoint spans of words, which are translated by rules encoding bi-phrases. This means that irrespective of whether phrases are contiguous [1], non-contiguous [2, 3], or hierarchical [4], the application of phrase rules at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches that did attempt to integ"
2014.iwslt-papers.12,N10-1140,0,0.0173193,"weights for the new phrases. We find gains of 0.3 − 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. 1. Introduction Decoding in SMT amounts to searching for the most probable (Viterbi) derivation of a target string given the source string. Standard SMT decoders perform at the same time a search for the optimal segmentation of the source sentence into disjoint spans of words, which are translated by rules encoding bi-phrases. This means that irrespective of whether phrases are contiguous [1], non-contiguous [2, 3], or hierarchical [4], the application of phrase rules at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches that did attempt to integ"
2014.iwslt-papers.12,J07-2003,0,0.755869,"ases. We find gains of 0.3 − 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. 1. Introduction Decoding in SMT amounts to searching for the most probable (Viterbi) derivation of a target string given the source string. Standard SMT decoders perform at the same time a search for the optimal segmentation of the source sentence into disjoint spans of words, which are translated by rules encoding bi-phrases. This means that irrespective of whether phrases are contiguous [1], non-contiguous [2, 3], or hierarchical [4], the application of phrase rules at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches that did attempt to integrate overlapping phra"
2014.iwslt-papers.12,D09-1107,0,0.682873,"at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches that did attempt to integrate overlapping phrases into SMT decoding in the past [5, 6, 7] were handicapped mostly by the additional decoding complexity. The need to counterbalance exponential growth of the search space with very restrictive reordering constraints prevented these approaches to be competitive with state-of-theart phrase-based SMT. The exception is Tribble et al. [8] who reported significant gains for using overlapping phrases over their own baseline. The key idea in this approach is to circumvent decoder integration and instead to generate overlapping phrases offline, by merging existing contiguous phrases into longer bi-phrases that have overlapping words in both s"
2014.iwslt-papers.12,2010.amta-papers.31,0,0.353223,"at decoding time disallows overlapping words. However, the use of overlapping phrases might have several advantages: First, they may enhance fluency in positions that would otherwise be phrase boundaries. Second, overlapping phrases may provide additional statistical support for long and rare phrases extracted from the training data. Finally, and most importantly, overlapping phrases may constitute new phrases that have never been seen in the training data but may be applicable to the test data. The few approaches that did attempt to integrate overlapping phrases into SMT decoding in the past [5, 6, 7] were handicapped mostly by the additional decoding complexity. The need to counterbalance exponential growth of the search space with very restrictive reordering constraints prevented these approaches to be competitive with state-of-theart phrase-based SMT. The exception is Tribble et al. [8] who reported significant gains for using overlapping phrases over their own baseline. The key idea in this approach is to circumvent decoder integration and instead to generate overlapping phrases offline, by merging existing contiguous phrases into longer bi-phrases that have overlapping words in both s"
2014.iwslt-papers.12,P10-4002,0,0.338231,"rases over their own baseline. The key idea in this approach is to circumvent decoder integration and instead to generate overlapping phrases offline, by merging existing contiguous phrases into longer bi-phrases that have overlapping words in both source and target. In this work, we will revive this approach, and extend it to hierarchical phrases. We show how to merge and filter overlapping phrases created from hierarchical and nonhierarchical phrases, and how to extract and tune features for the new phrases. An experimental comparison with a stateof-the-art hierarchical phrase-based decoder [9] shows gains of 0.3 − 0.6 BLEU points on two datasets for German-toEnglish translation. 2. Related Work The potential of overlapping phrases to improve fluency and to smooth prediction of long and rare phrases has been discovered independently in a few instances in prior work. The crux of most of these approaches is an efficient integration of overlapping phrases into decoding. For example, the exponential number of translation hypotheses arising from overlapping phrases has been managed in beam search decoding frameworks by reordering constraints that allow only adjacent non-overlapping phras"
2014.iwslt-papers.12,W11-2137,0,0.0119714,"ntly in a few instances in prior work. The crux of most of these approaches is an efficient integration of overlapping phrases into decoding. For example, the exponential number of translation hypotheses arising from overlapping phrases has been managed in beam search decoding frameworks by reordering constraints that allow only adjacent non-overlapping phrases to be swapped [5, 7]. This reordering constraint seems to be too restrictive since it impacts translation quality in comparison to state-of-the-art phrasebased SMT. Alternatively, sampling-based approaches [6] or graphsearch techniques [10, 11, 12] have been used for decoding with overlapping phrases. These approaches suffer from search errors due to necessary abstractions in sampling or due to necessary approximations in adaptation of graph search algorithms to SMT decoding. The work related closest to our approach is that of Tribble et al. [8, 13]. Their key idea is to circumvent decoder integration and instead to generate overlapping phrases in an offline manner. In contrast to our work, their approach is restricted to merging contiguous phrases. Furthermore, they extract a single feature (based on phrase-internal word alignments) fo"
2014.iwslt-papers.12,2003.mtsummit-papers.53,0,0.0543376,"ts that allow only adjacent non-overlapping phrases to be swapped [5, 7]. This reordering constraint seems to be too restrictive since it impacts translation quality in comparison to state-of-the-art phrasebased SMT. Alternatively, sampling-based approaches [6] or graphsearch techniques [10, 11, 12] have been used for decoding with overlapping phrases. These approaches suffer from search errors due to necessary abstractions in sampling or due to necessary approximations in adaptation of graph search algorithms to SMT decoding. The work related closest to our approach is that of Tribble et al. [8, 13]. Their key idea is to circumvent decoder integration and instead to generate overlapping phrases in an offline manner. In contrast to our work, their approach is restricted to merging contiguous phrases. Furthermore, they extract a single feature (based on phrase-internal word alignments) for new phrases and do not learn discriminative weights. A similar idea has also been presented for Example-Based MT 236 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 [14, 15] where the focus is on combining given overlapping phrases by a"
2014.iwslt-papers.12,2003.mtsummit-papers.4,0,0.0405775,"work related closest to our approach is that of Tribble et al. [8, 13]. Their key idea is to circumvent decoder integration and instead to generate overlapping phrases in an offline manner. In contrast to our work, their approach is restricted to merging contiguous phrases. Furthermore, they extract a single feature (based on phrase-internal word alignments) for new phrases and do not learn discriminative weights. A similar idea has also been presented for Example-Based MT 236 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 [14, 15] where the focus is on combining given overlapping phrases by a new search algorithm. An alternative to enriching the repository of phrases with overlapping phrase rules is the design of context-sensitive features for discriminative training. Target context is clearly exploited by large language models. Word-sense disambiguation inspired features [16] allow to exploit source context, and recent approaches successfully merged source and target context into a powerful decoding feature [17]. However, these approaches are orthogonal to our work. 3. Generating Overlapping Phrases with and without V"
2014.iwslt-papers.12,P07-1005,0,0.0326144,"nts) for new phrases and do not learn discriminative weights. A similar idea has also been presented for Example-Based MT 236 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 [14, 15] where the focus is on combining given overlapping phrases by a new search algorithm. An alternative to enriching the repository of phrases with overlapping phrase rules is the design of context-sensitive features for discriminative training. Target context is clearly exploited by large language models. Word-sense disambiguation inspired features [16] allow to exploit source context, and recent approaches successfully merged source and target context into a powerful decoding feature [17]. However, these approaches are orthogonal to our work. 3. Generating Overlapping Phrases with and without Variables Hierarchical phrases can be formalized as rules of a synchronous CFG [4]. We denote terminals consisting of contiguous phrases by T, and the single non-terminal variable by NT. The key idea is to merge base rules into new rules by pivoting on overlapping words. We apply this idea to base rules consisting of terminals only (T rules) and to bas"
2014.iwslt-papers.12,P14-1129,0,0.0264242,"f the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 [14, 15] where the focus is on combining given overlapping phrases by a new search algorithm. An alternative to enriching the repository of phrases with overlapping phrase rules is the design of context-sensitive features for discriminative training. Target context is clearly exploited by large language models. Word-sense disambiguation inspired features [16] allow to exploit source context, and recent approaches successfully merged source and target context into a powerful decoding feature [17]. However, these approaches are orthogonal to our work. 3. Generating Overlapping Phrases with and without Variables Hierarchical phrases can be formalized as rules of a synchronous CFG [4]. We denote terminals consisting of contiguous phrases by T, and the single non-terminal variable by NT. The key idea is to merge base rules into new rules by pivoting on overlapping words. We apply this idea to base rules consisting of terminals only (T rules) and to base rules including non-terminals (NT rules). As a first step, we apply the technique of [18] to extract rules for German-to-English translat"
2014.iwslt-papers.12,D07-1104,0,0.152211,"target context into a powerful decoding feature [17]. However, these approaches are orthogonal to our work. 3. Generating Overlapping Phrases with and without Variables Hierarchical phrases can be formalized as rules of a synchronous CFG [4]. We denote terminals consisting of contiguous phrases by T, and the single non-terminal variable by NT. The key idea is to merge base rules into new rules by pivoting on overlapping words. We apply this idea to base rules consisting of terminals only (T rules) and to base rules including non-terminals (NT rules). As a first step, we apply the technique of [18] to extract rules for German-to-English translation from the News Commentary and TED data (see Section 5.1). Tables 1 and 2 show the token counts of rule shapes for the extracted grammars. We see that base rules consisting of terminals only (rule shape T-T) are quite frequent in the extracted grammars for both datasets. To these rules, the ideas of [8], namely merging all base rules that have overlapping words on both source and target can be applied directly. For base rules including non-terminals (rule shape including NT), merging of rules can be done at word overlaps in terminals at the hea"
2014.iwslt-papers.12,E14-1042,0,0.0115979,"t words e: M axLexF givenE = − X i log10 pmax (fi |e) (1) log10 pmax (ei |f ). (2) and M axLexEgivenF = − X i Second, we add a new feature that indicates whether a rule is created by merging as follows: ( 1 if the rule is new, (3) N ewRule = 0 otherwise. Third, we calculate the following standard statistics among new rules that were merged from base rules extracted for the test set: SampleCountF = log10 (1 + count F ) (5) CountEF = log10 (1 + count EF ) (6) IsSingletonF = ( 1 if count F = 1, 0 otherwise. IsSingletonF E = ( 1 0 if count EF = 1, otherwise. (7) (8) Last, we take inspiration from [19]’s adaptive features that combine counts from a lookup in post-editing data with counts from the suffix array sample extracted for the test set. In our case, this corresponds to combining count statistics for new rules only (denoted by subscript L) with count statistics for base rules extracted for the test set (denoted by subscript S): EgivenF Coherent = − log10 ((count EFS +count EFL )/ (count FS + count FL )) (9) SampleCountF = log10 (1+count FS +count FL ) (10) 238 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Count 373"
2014.iwslt-papers.12,P12-1002,1,0.913405,"ise. train-lm tune test Sentences Words de Words en 136,227 3,005,252 2,909,346 180,657 3,797,500 1,057 26,205 25,660 1,064 23,593 22,518 TED train train-lm tune test Sentences Words de Words en 139,563 2,195,030 2,332,370 158,641 1,172 21,270 21,679 746 11,831 12,734 2,715,777 Table 6: News Commentary and TED de-en parallel data. 5.1. Systems and Data otherwise. 1 if the rule is new, train 5. Translation Experiments if count FS + count FL = 1, ( NC (16) Discriminative tuning is performed on the respective tuning sets of the News Commentary and TED data. We use the pairwise ranking learner of [20] for this purpose. In addition to the standard handful of dense feature, sparse features for rule shapes, rule identifiers, and bigrams in rule source and target are extracted from grammar rules. The data used in our experiments are the German-English parallel data provided in the News Commentary and TED releases of WMT 20071 and IWSLT 20132 , respectively. Table 6 gives the basic data statistics for News Commentary (NC) and TED data. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder cdec [9]3 . We built grammars using its implementation of the suffix array"
2014.iwslt-papers.12,N13-1073,0,0.0201258,"s, rule identifiers, and bigrams in rule source and target are extracted from grammar rules. The data used in our experiments are the German-English parallel data provided in the News Commentary and TED releases of WMT 20071 and IWSLT 20132 , respectively. Table 6 gives the basic data statistics for News Commentary (NC) and TED data. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder cdec [9]3 . We built grammars using its implementation of the suffix array extraction method described in [18]. Word alignments are built from all parallel data using fast align [21]. SCFG models use the same settings as described in [4]. For language modeling, we built a modified Kneser-Ney smoothed 5-gram language 1 http://statmt.org/wmt07/shared-task.html 2 http://www.iwslt2013.org/ 3 http://www.cdec-decoder.org 239 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 News Commentary Base rules Merged rules Unique Applicable in test Unique all > 1 token T5 T8 T 11 129,400 72,322 6,823 4,434 3,286 364,642 171,715 100,513 352,171 167,125 98,268 6,311 3,414 2,203 5,739 3,165 2,054 TED Base rules Merged rules"
2014.iwslt-papers.12,W11-2123,0,0.0265244,"of base rules and merged rules with terminals only before and after filtering. News Commentary Base rules Merged rules Unique Applicable in test Unique all NT 17 NT 20 NT 23 694,105 14,107 11,592 9,774 563,980 324,790 198,447 556,476 319,919 194,880 18,588 13,794 10,915 14,919 11,039 8,690 TED Base rules Merged rules Unique Applicable in test Unique all NT 17 NT 20 NT 23 643,132 14,684 12,256 10,334 1,980,618 1,345,298 908,066 1,940,402 1,316,680 887,474 34,696 26,856 21,118 28,293 21,750 16,938 Table 4: Counts of base rules and merged rules with nonterminals before and after filtering. model [22, 23]. All data were normalized, tokenized and lowercased; German compounds were split. For tokenization, lowercasing and other preprocessing steps we used the scripts distributed with the Moses SMT toolkit [24]. For compound splitting in German texts a standard empirical approach of [25] was employed. MERT PairRank News Commentary TED 24.95 25.69† 25.94 25.90 Table 7: Baseline results for News Commentary and TED talks German-to-English translation. Statistically significant differences to MERT are denoted with † . 5.2. Experimental Results Table 7 shows BLEU [26] results for MERT [27] optimization"
2014.iwslt-papers.12,P13-2121,0,0.0177841,"of base rules and merged rules with terminals only before and after filtering. News Commentary Base rules Merged rules Unique Applicable in test Unique all NT 17 NT 20 NT 23 694,105 14,107 11,592 9,774 563,980 324,790 198,447 556,476 319,919 194,880 18,588 13,794 10,915 14,919 11,039 8,690 TED Base rules Merged rules Unique Applicable in test Unique all NT 17 NT 20 NT 23 643,132 14,684 12,256 10,334 1,980,618 1,345,298 908,066 1,940,402 1,316,680 887,474 34,696 26,856 21,118 28,293 21,750 16,938 Table 4: Counts of base rules and merged rules with nonterminals before and after filtering. model [22, 23]. All data were normalized, tokenized and lowercased; German compounds were split. For tokenization, lowercasing and other preprocessing steps we used the scripts distributed with the Moses SMT toolkit [24]. For compound splitting in German texts a standard empirical approach of [25] was employed. MERT PairRank News Commentary TED 24.95 25.69† 25.94 25.90 Table 7: Baseline results for News Commentary and TED talks German-to-English translation. Statistically significant differences to MERT are denoted with † . 5.2. Experimental Results Table 7 shows BLEU [26] results for MERT [27] optimization"
2014.iwslt-papers.12,P07-2045,0,0.00468589,"324,790 198,447 556,476 319,919 194,880 18,588 13,794 10,915 14,919 11,039 8,690 TED Base rules Merged rules Unique Applicable in test Unique all NT 17 NT 20 NT 23 643,132 14,684 12,256 10,334 1,980,618 1,345,298 908,066 1,940,402 1,316,680 887,474 34,696 26,856 21,118 28,293 21,750 16,938 Table 4: Counts of base rules and merged rules with nonterminals before and after filtering. model [22, 23]. All data were normalized, tokenized and lowercased; German compounds were split. For tokenization, lowercasing and other preprocessing steps we used the scripts distributed with the Moses SMT toolkit [24]. For compound splitting in German texts a standard empirical approach of [25] was employed. MERT PairRank News Commentary TED 24.95 25.69† 25.94 25.90 Table 7: Baseline results for News Commentary and TED talks German-to-English translation. Statistically significant differences to MERT are denoted with † . 5.2. Experimental Results Table 7 shows BLEU [26] results for MERT [27] optimization of dense feature weights, and for pairwise ranking [20] optimization of sparse feature weights. MERT runs were repeated three times to account for optimizer instability [28]. The pairwise ranking technique"
2014.iwslt-papers.12,E03-1076,0,0.144044,"Missing"
2014.iwslt-papers.12,2001.mtsummit-papers.68,0,0.104806,"Missing"
2014.iwslt-papers.12,P03-1021,0,0.0596238,"Missing"
2014.iwslt-papers.12,P11-2031,0,0.0147683,"uted with the Moses SMT toolkit [24]. For compound splitting in German texts a standard empirical approach of [25] was employed. MERT PairRank News Commentary TED 24.95 25.69† 25.94 25.90 Table 7: Baseline results for News Commentary and TED talks German-to-English translation. Statistically significant differences to MERT are denoted with † . 5.2. Experimental Results Table 7 shows BLEU [26] results for MERT [27] optimization of dense feature weights, and for pairwise ranking [20] optimization of sparse feature weights. MERT runs were repeated three times to account for optimizer instability [28]. The pairwise ranking technique was stable in this respect. Statistical significance is measured using Approximate Randomization [29, 30] where result differences with a p-value smaller than 0.05 are considered significant. In order to investigate a possible correspondence of the patterns of composition and usage shown in Table 5, we evaluate overlapping phrases merged from base T rules and base NT rules separately. Table 8 shows BLEU results for different frequency cutoffs for base rules (see Section 3) and different feature sets (see Section 4) on the News Commentary data for German-to-Engl"
2014.iwslt-papers.12,W05-0908,1,0.783532,"PairRank News Commentary TED 24.95 25.69† 25.94 25.90 Table 7: Baseline results for News Commentary and TED talks German-to-English translation. Statistically significant differences to MERT are denoted with † . 5.2. Experimental Results Table 7 shows BLEU [26] results for MERT [27] optimization of dense feature weights, and for pairwise ranking [20] optimization of sparse feature weights. MERT runs were repeated three times to account for optimizer instability [28]. The pairwise ranking technique was stable in this respect. Statistical significance is measured using Approximate Randomization [29, 30] where result differences with a p-value smaller than 0.05 are considered significant. In order to investigate a possible correspondence of the patterns of composition and usage shown in Table 5, we evaluate overlapping phrases merged from base T rules and base NT rules separately. Table 8 shows BLEU results for different frequency cutoffs for base rules (see Section 3) and different feature sets (see Section 4) on the News Commentary data for German-to-English translation. All results are nominal improvements over the PairRank baseline in Table 7, with several statistically significant result"
2015.eamt-1.23,E14-1022,0,0.0316374,"Missing"
2015.eamt-1.23,J07-2003,0,0.153367,"ry2 . 2 ti,best = We also tested query constructions involving a larger set of CLIR(qi , ti,j ) = max p∈Hg(qi ) X wSMT ·φSMT (e(qi )) e∈p + wn-gr · φn-gr (e(qi ), ti,j )) where p is a path through the hypergraph, e the set of edges on the path, φ are feature values of an edge, w the corresponding weights, and · denotes the vector dot product. We explore two different ways to incorporate n-gram features φn-gr in addition to the SMT feature set φSMT . Unigram oracle. Since n-gram features are nonlocal and the size of the hypergraph grows when adding n-gram features for orders higher than n = 1 (Chiang, 2007), we restrict our first model to unigram precision and a brevity penalty feature; the latter is only active at goal state. In this way, two additional features are inserted into the log-linear model, using the TM match candidate as an oracle. possible translations, but found that using the 1-best translation prediction of the baseline system yielded superior results. 171 Additional language model. To be able to include higher-order n-gram matches, we add the match candidates as an additional language model to the decoder.This approach makes use of the fact that cdec handles the extension of th"
2015.eamt-1.23,P11-2031,0,0.0622202,"olingual TM deviate. We assume that we have parallel data from patent claims and the task is to translate text from a different genre, patent descriptions, for which only data in the target language available as well as a small amount of bitext to tune parameters on – a typical domain adaptation scenario. The available monolingual data is used to extend both the language model as well as the target-language TM (Table 4). We report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) evaluation scores. Statistical significance of all results was assessed following the method described in Clark et al. (2011) using the source code provided by the authors10 . 4 http://datahub.io/de/dataset/opus We tested for Chinese characters by checking if they were in the Unicode range [0x4E00, 0x9FFF]. 6 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 7 http://www.cl.uni-heidelberg.de/ statnlpgroup/pattr/ 8 http://research.nii.ac.jp/ntcir/ ntcir-10/ 9 We merged, shuffled and again split up the data into three sets, to generate a devtest set. 5 4.1 Results Results in Table 5 show that adding the TM information always improves over the baseline, up to 1.23 BLEU and -3.77 TER. Improvements in TER (th"
2015.eamt-1.23,C14-1192,0,0.134478,"of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integrated model uses a coarse-to-fine approach for integrating TM information into an SMT system: First, efficient retrieval is done using locality-sensitive hashing on large corpora. Second, a more fine-grained search for the best match is performed for a given sentence. Lastly, a reranking step uses this i"
2015.eamt-1.23,N13-1073,0,0.0760802,"Missing"
2015.eamt-1.23,P10-4002,0,0.0938166,"irect translation baseline in cross-language information retrieval. In addition to this simple model, we explore two methods that operate on the full translation hypergraph of the query. Both techniques are similar to the translation retrieval technique presented by Dong et al. (2014). They perform Viterbi search on a translation lattice of the input sentence that is enriched, besides the default SMT features, with n-gram features that indicate the overlap status between the current state in the lattice and a given TM match. We adopt this approach for the hypergraph built by the cdec decoder (Dyer et al., 2010). As a cross-lingual similarity measure we then compute the Viterbi score on the query hypergraph Hg(qi ) for each match candidate ti,j , i.e. Fine-grained matching In the standard bilingual case, choosing the best TM match amounts to selecting the sentence pair (s, t) from the coarse candidate set LSH(qi ) that achieves the highest fuzzy match score FMS of the (source) query qi against the source side si,j of the TM pair, and returning its target side ti,j . (s, t)i,best = argmax FMS(qi , si,j ). (s,t)i,j ∈LSH(qi ) For the target-language scenario, however, this step is not straightforward. W"
2015.eamt-1.23,P10-1064,0,0.0503747,"Missing"
2015.eamt-1.23,C10-2043,0,0.0262102,"Missing"
2015.eamt-1.23,D11-1125,0,0.14862,"rior results. 171 Additional language model. To be able to include higher-order n-gram matches, we add the match candidates as an additional language model to the decoder.This approach makes use of the fact that cdec handles the extension of the hypergraph to accommodate for the non-local higher order ngrams. Cube pruning (Chiang, 2007) is used to make the search feasible. In both cases, we keep the weights of the SMT features fixed, which have been optimized for translation performance on a development set, and only adjust the additional weights in relation. This is done by pairwise ranking (Hopkins and May, 2011). The gold standard ranking of the TM candidates is given by FMS(ti,j , ri ) with respect to the reference ri for qi . The learning goal is to adjust the weights of the n-gram features so as to rank the TM match highest that has the smallest distance to the reference. Note, that we do not optimize the translation performance of the derivation, which corresponds to the Viterbi path. This could potentially replace the re-ranking step and we plan to explore this option in the future. 3.3 on the n-best list of SMT outputs by TER match against the reference. domain acquis (en-fr) oo3 (en-zh) ntcir"
2015.eamt-1.23,2010.jec-1.4,0,0.0189629,"only be available in the target language. We show consistent and significant improvements on different domains (IT, legal, patents) for different language pairs (including Chinese, Japanese, English, French, and German), achieving results compara169 ble to or better than using a target-language reference of source-side matches. 2 Related Work Work on integrating MT and SMT can be divided into approaches at the sentence level that decide whether to pass SMT or TM output to the user (He et al., 2010a,b), and approaches that merge both techniques at a sub-sentential level (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Wang et al., 2013). While the goal of the former is to improve human translation effort in a CAT environment, the second line of research aims to improve SMT performance. Bic¸ici and Dymetman (2008) were among the first to propose a combined system. They start by identifying matching subsequences between the current sentence and a fuzzy match retrieved from a translation memory. Source and target of the match together with the corresponding alignment are used to construct a non-contiguous bi-phrase, which is added to the SMT grammar with a strong weight. The d"
2015.eamt-1.23,2014.amta-researchers.19,0,0.0173054,"the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integrated model uses a coa"
2015.eamt-1.23,P11-1124,0,0.0352287,"Missing"
2015.eamt-1.23,P02-1040,0,0.0966343,"target retrieval approach in more a realistic setting, we furthermore set up an experiment for the English-German patent task, where SMT training data and monolingual TM deviate. We assume that we have parallel data from patent claims and the task is to translate text from a different genre, patent descriptions, for which only data in the target language available as well as a small amount of bitext to tune parameters on – a typical domain adaptation scenario. The available monolingual data is used to extend both the language model as well as the target-language TM (Table 4). We report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) evaluation scores. Statistical significance of all results was assessed following the method described in Clark et al. (2011) using the source code provided by the authors10 . 4 http://datahub.io/de/dataset/opus We tested for Chinese characters by checking if they were in the Unicode range [0x4E00, 0x9FFF]. 6 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 7 http://www.cl.uni-heidelberg.de/ statnlpgroup/pattr/ 8 http://research.nii.ac.jp/ntcir/ ntcir-10/ 9 We merged, shuffled and again split up the data into three sets, to generate a devtest set. 5"
2015.eamt-1.23,tiedemann-2012-parallel,0,0.062411,"Missing"
2015.eamt-1.23,I05-3027,0,0.0501945,"Missing"
2015.eamt-1.23,2007.mtsummit-papers.63,0,0.106416,"Missing"
2015.eamt-1.23,P13-1002,0,0.0603814,"t and significant improvements on different domains (IT, legal, patents) for different language pairs (including Chinese, Japanese, English, French, and German), achieving results compara169 ble to or better than using a target-language reference of source-side matches. 2 Related Work Work on integrating MT and SMT can be divided into approaches at the sentence level that decide whether to pass SMT or TM output to the user (He et al., 2010a,b), and approaches that merge both techniques at a sub-sentential level (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Wang et al., 2013). While the goal of the former is to improve human translation effort in a CAT environment, the second line of research aims to improve SMT performance. Bic¸ici and Dymetman (2008) were among the first to propose a combined system. They start by identifying matching subsequences between the current sentence and a fuzzy match retrieved from a translation memory. Source and target of the match together with the corresponding alignment are used to construct a non-contiguous bi-phrase, which is added to the SMT grammar with a strong weight. The decoder is then run as usual using the augmented gram"
2015.eamt-1.23,2009.mtsummit-papers.14,0,0.0403594,"rt (2010), Zhechev and van Genabith (2010), and Ma et al. (2011), force the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integr"
2015.eamt-1.23,2006.amta-papers.25,0,0.0239129,"re a realistic setting, we furthermore set up an experiment for the English-German patent task, where SMT training data and monolingual TM deviate. We assume that we have parallel data from patent claims and the task is to translate text from a different genre, patent descriptions, for which only data in the target language available as well as a small amount of bitext to tune parameters on – a typical domain adaptation scenario. The available monolingual data is used to extend both the language model as well as the target-language TM (Table 4). We report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) evaluation scores. Statistical significance of all results was assessed following the method described in Clark et al. (2011) using the source code provided by the authors10 . 4 http://datahub.io/de/dataset/opus We tested for Chinese characters by checking if they were in the Unicode range [0x4E00, 0x9FFF]. 6 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 7 http://www.cl.uni-heidelberg.de/ statnlpgroup/pattr/ 8 http://research.nii.ac.jp/ntcir/ ntcir-10/ 9 We merged, shuffled and again split up the data into three sets, to generate a devtest set. 5 4.1 Results Results in Table 5"
2015.eamt-1.23,steinberger-etal-2006-jrc,0,0.106816,"Missing"
2015.eamt-1.23,C14-1039,0,0.0239557,"Ma et al. (2011), force the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integr"
2015.eamt-1.23,D07-1080,0,0.0343973,"le 3: Number of test sentences with source side fuzzy match score in a certain range. We trained a baseline SMT system using the cdec decoder (Dyer et al., 2010) and the accompanying tools, i.e. fast align (Dyer et al., 2013) on each data set. A 6-gram language model was genre parallel train (en-de) dev/devtest/test (en-de) LM-train (de) TM (de) cl. desc. cl.+descr. cl.+desc. size (sent.) 6M 1K (each) 16.2M 16.2M Table 4: Data for domain adaptation scenario. trained with SRILM (Stolcke, 2002) on the target side of the training data. The weights of the loglinear model were optimized with MIRA (Watanabe et al., 2007) on a held-out development set reserved for this purpose (dev). We employed the baseline model to produce query translations and hypergraphs for the cross-lingual retrieval of target matches as well as to produce 500-best lists, which we re-ranked according to our model given the best match found after fine-grained retrieval. Retrieval and re-ranking parameters were optimized on an additional held-out (devtest) set. All presented results were obtained on a third (test) data set. To compare source and different target retrieval methods in a fair setting, we used the bilingual data from training"
2015.eamt-1.23,W10-3806,0,0.0367772,"Missing"
2015.eamt-1.23,2014.amta-researchers.13,0,\N,Missing
2015.iwslt-evaluation.6,N04-1023,0,\N,Missing
2015.iwslt-evaluation.6,D11-1033,0,\N,Missing
2015.iwslt-evaluation.6,W10-1711,0,\N,Missing
2015.iwslt-evaluation.6,2010.iwslt-evaluation.22,0,\N,Missing
2015.iwslt-evaluation.6,C10-1043,0,\N,Missing
2015.iwslt-evaluation.6,P10-4002,0,\N,Missing
2015.iwslt-evaluation.6,J03-1002,0,\N,Missing
2015.iwslt-evaluation.6,2006.iwslt-evaluation.14,0,\N,Missing
2015.iwslt-evaluation.6,2010.eamt-1.29,0,\N,Missing
2015.iwslt-evaluation.6,D15-1119,0,\N,Missing
2015.iwslt-evaluation.6,N15-1123,1,\N,Missing
2015.iwslt-evaluation.6,J07-2003,0,\N,Missing
2015.iwslt-evaluation.6,D13-1140,0,\N,Missing
2015.iwslt-evaluation.6,W08-0509,0,\N,Missing
2015.iwslt-evaluation.6,W13-2236,1,\N,Missing
2015.iwslt-evaluation.6,P12-1002,1,\N,Missing
2015.iwslt-evaluation.6,P13-2121,0,\N,Missing
2015.iwslt-evaluation.6,P03-1021,0,\N,Missing
2015.iwslt-evaluation.6,D11-1125,0,\N,Missing
2015.mtsummit-papers.13,D14-1132,0,0.067137,"ed by the empirical distribution p˜(x, y) = T1 t=0 1[x = xt ]1[y = yt ] for i.i.d. training data {(xt , yt )}Tt=0 . This yields the objective 0 Ep(x,y)p [∆y (y 0 )] = ˜ w (y |x) T 1X T t=0 X y 0 ∈Y(x ∆yt (y 0 )pw (y 0 |xt ). (4) t) While being continuous and differentiable, the expected loss criterion is typically nonconvex. For example, in SMT, expected loss training for the standard task loss BLEU leads to highly non-convex optimization problems. Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4): 0 ∇Ep(x,y)p [∆y (y 0 )] ˜ w (y |x) h i = Ep(x,y) Epw (y0 |x) [∆y (y 0 )φ(x, y 0 )] − Epw (y0 |x) [∆y (y 0 )] Epw (y0 |x) [φ(x, y 0 )] ˜ h i 0 0 0 0 0 = Ep(x,y)p ∆ (y )(φ(x, y ) − E [φ(x, y )]) . y ˜ pw (y |x) w (y |x) 4 Bandit Structured Prediction Bandit feedback in structured prediction means that the gold standard output structure y, with respect to which the objective function is evaluated, is not revealed to the learner. Thus we can neither calculate the gradient of the objective function (4)"
2015.mtsummit-papers.13,P09-1010,0,0.046252,"to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning. Saluja and Zhang (2014) present an incorporation of binary feedback into an latent structured SVM for discriminative SMT training. NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al. (2015). Their model differs from ours in that it is structured as a sequence of states at which actions and rewards are computed, however, the theoretical foundation of both types of models can be traced back to Polyak and Tsypkin (1973)’s pseudogradient framework . 3 Expected Loss Minimization under Full Information The expected loss learning criterion for structured prediction is defined as a minimization of the expectation of a task loss function with respect to the conditional distribution over structured outputs (Gimpel and Smith, 2010; Yuille and He, 2012). More formally"
2015.mtsummit-papers.13,P11-2031,0,0.0232481,"udes an in-domain language model. We show the standard evaluation of the corpusBLEU metric evaluated under MAP inference. The range of possible improvements is given by the difference of the BLEU score of the in-domain model and the BLEU score of the out-ofdomain model – nearly 3 BLEU points. Bandit learning can improve the out-of-domain baseline by about 1.26 BLEU points (Bandit Structured Prediction) and by about 1.52 BLEU points (Dueling Bandits). All result differences are statistically significant at a p-value of 0.0001, using an Approximate Randomization test (Riezler and Maxwell, 2005; Clark et al., 2011). Figure 1 shows that per-sentence BLEU is a difficult metric to provide single-point feedback, yielding a non-smooth progression of loss values against iterations for Bandit Structured Prediction. The progression of loss values is smoother and empirical convergence speed is faster for Dueling Bandits since it can exploit preference judgements instead of having to trust real-valued feedback. 7 Discussion We presented an approach to Bandit Structured Prediction that is able to learn from feedback in form of an evaluation of a task loss function for single predicted structures. Our experimental"
2015.mtsummit-papers.13,P11-2071,0,0.0323814,"Missing"
2015.mtsummit-papers.13,E14-1042,0,0.0926432,"loration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term). Crucially, in this scenario it is unrealistic to expect more detailed feedback than a user click on the displayed ad. Similar to the online advertising scenario, there are many potential applications of bandit learning to NLP situations where feedback is limited for various reasons. For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Post-editing feedback clearly is limited by its high cost and by the required expertise of users, however, current approaches force the full information supervised scenario onto the problem of learning from user post-edits. 1 The name is inherited from a model where in each round a gambler pulls an arm of a different slot machine (“one-armed bandit”), with the goal of maximizing his reward relative to the maximal possible reward, without apriori knowledge of the optimal slot machine. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p"
2015.mtsummit-papers.13,N13-1073,0,0.0391633,"test set for early stopping at different iterations for the SMT task. domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum´e and Jagarlamudi, 2011). We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013). The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al., 2013). The out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain and 1 in-domain language model, 1 word penalty, 5 translation model features) that are tuned with MERT (Och, 2003) on a dev set of Europarl data (dev2006, 2,00"
2015.mtsummit-papers.13,D14-1130,0,0.109721,"to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term). Crucially, in this scenario it is unrealistic to expect more detailed feedback than a user click on the displayed ad. Similar to the online advertising scenario, there are many potential applications of bandit learning to NLP situations where feedback is limited for various reasons. For example, online learning has been applied successfully in interactive statistical machine translation (SMT) (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Post-editing feedback clearly is limited by its high cost and by the required expertise of users, however, current approaches force the full information supervised scenario onto the problem of learning from user post-edits. 1 The name is inherited from a model where in each round a gambler pulls an arm of a different slot machine (“one-armed bandit”), with the goal of maximizing his reward relative to the maximal possible reward, without apriori knowledge of the optimal slot machine. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 160 Bandit learning"
2015.mtsummit-papers.13,P12-1031,0,0.0438648,"y) is PT approximated by the empirical distribution p˜(x, y) = T1 t=0 1[x = xt ]1[y = yt ] for i.i.d. training data {(xt , yt )}Tt=0 . This yields the objective 0 Ep(x,y)p [∆y (y 0 )] = ˜ w (y |x) T 1X T t=0 X y 0 ∈Y(x ∆yt (y 0 )pw (y 0 |xt ). (4) t) While being continuous and differentiable, the expected loss criterion is typically nonconvex. For example, in SMT, expected loss training for the standard task loss BLEU leads to highly non-convex optimization problems. Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4): 0 ∇Ep(x,y)p [∆y (y 0 )] ˜ w (y |x) h i = Ep(x,y) Epw (y0 |x) [∆y (y 0 )φ(x, y 0 )] − Epw (y0 |x) [∆y (y 0 )] Epw (y0 |x) [φ(x, y 0 )] ˜ h i 0 0 0 0 0 = Ep(x,y)p ∆ (y )(φ(x, y ) − E [φ(x, y )]) . y ˜ pw (y |x) w (y |x) 4 Bandit Structured Prediction Bandit feedback in structured prediction means that the gold standard output structure y, with respect to which the objective function is evaluated, is not revealed to the learner. Thus we can neither calculate the gradient of the obj"
2015.mtsummit-papers.13,P13-2121,0,0.0139274,"where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum´e and Jagarlamudi, 2011). We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013). The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al., 2013). The out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain and 1 in-domain language model, 1 word penalty, 5 translation model features) that are tuned with MERT (Och, 2003) on a dev set of Europarl data (dev2006, 2,000 sentences). The full-information in-domain SMT model gives an upper bound by MERT tuning the out-ofdomain model on in-domain development data (nc-dev2007, 1,05"
2015.mtsummit-papers.13,P07-2045,0,0.00435947,"a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum´e and Jagarlamudi, 2011). We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013). The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al., 2013). The out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain and 1 in-domain language model, 1 word penalty, 5 translation model features) that are tuned with MERT (Och, 2003) on a dev set of Europarl data (dev2006, 2,000 sentences). The full-information in-domain SMT model gives an upper bound by MERT tuning the out-ofdomain m"
2015.mtsummit-papers.13,W07-0733,0,0.143768,"anking. 0.275 corpus-BLEU 0.270 Dueling BanditStruct out-domain SMT 0.265 0.260 0.255 50 100 150 200 250 300 iteration×5000 350 400 450 500 Figure 1: Corpus-BLEU on test set for early stopping at different iterations for the SMT task. domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. We use the data from the WMT 2007 shared task for domain adaptation experiments in a popular benchmark setup from Europarl to NewsCommentary for French-to-English (Koehn and Schroeder, 2007; Daum´e and Jagarlamudi, 2011). We tokenized and lowercased our data using the moses toolkit, and prepared word alignments by fast align (Dyer et al., 2013). The SMT setup is phrase-based translation using non-unique 5,000-best lists from moses (Koehn et al., 2007) and a 4-gram language model (Heafield et al., 2013). The out-of-domain baseline SMT model is trained on 1.6 million parallel Europarl data and includes the English side of Europarl and in-domain NewsCommentary in the language model. The model uses 15 dense features (6 lexicalized reordering features, 1 distortion, 1 outof-domain an"
2015.mtsummit-papers.13,P03-1021,0,0.657157,"ty estimate of the predicted translation in this scenario. Another example is SMT domain adaptation where the translations of a large out-of-domain model are re-ranked based on bandit feedback on in-domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. The goal of this paper is to develop algorithms for structured prediction from bandit feedback, tailored to NLP problems. We investigate possibilities to “banditize” objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP. Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005). Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Sha"
2015.mtsummit-papers.13,P14-1083,1,0.821069,"kin (1973)’s pseudogradient framework. We apply their simple and elegant framework directly to give asymptotic guarantees for our algorithm. NLP Applications. In the area of NLP, recently algorithms for response-based learning have been proposed to alleviate the supervision problem by extracting supervision signals from taskbased feedback to system predictions. For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning. Saluja and Zhang (2014) present an incorporation of binary feedback into an latent structured SVM for discriminative SMT training. NLP applications based on reinforcement learning ha"
2015.mtsummit-papers.13,P06-2101,0,0.666279,"of the predicted translation in this scenario. Another example is SMT domain adaptation where the translations of a large out-of-domain model are re-ranked based on bandit feedback on in-domain data. This can also be seen as a simulation of personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structures. The goal of this paper is to develop algorithms for structured prediction from bandit feedback, tailored to NLP problems. We investigate possibilities to “banditize” objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP. Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005). Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Shalev-Shwartz, 2012) as we"
2015.mtsummit-papers.13,2006.amta-papers.25,0,0.103717,"Missing"
2015.mtsummit-papers.13,K15-1001,1,0.737202,"have been proposed to alleviate the supervision problem by extracting supervision signals from taskbased feedback to system predictions. For example, Goldwasser and Roth (2013) presented an online structured learning algorithm that uses positive executability of a semantic parse against a database to convert a predicted parse into a gold standard structure for learning. Riezler et al. (2014) apply a similar idea to SMT by using the executability of a semantic parse of a translated database query as signal to convert a predicted translation into gold standard reference in structured learning. Sokolov et al. (2015) present a coactive learning approach to structured learning in SMT where instead of a gold standard reference a slight improvement over the prediction is shown to be sufficient for learning. Saluja and Zhang (2014) present an incorporation of binary feedback into an latent structured SVM for discriminative SMT training. NLP applications based on reinforcement learning have been presented by Branavan et al. (2009) or Chang et al. (2015). Their model differs from ours in that it is structured as a sequence of states at which actions and rewards are computed, however, the theoretical foundation"
2015.mtsummit-papers.13,W04-3201,0,0.0403686,"le-point user feedback to predicted structures. The goal of this paper is to develop algorithms for structured prediction from bandit feedback, tailored to NLP problems. We investigate possibilities to “banditize” objectives such as expected loss (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010) that have been proposed for structured prediction in NLP. Since most current approaches to bandit optimization rely on a multiclass classification scenario, the first challenge of our work is to adapt bandit learning to structured prediction over exponentially large structured output spaces (Taskar et al., 2004; Tsochantaridis et al., 2005). Furthermore, most theoretical work on online learning with bandit feedback relies on convexity assumptions about objective functions, both in the nonstochastic adversarial setting (Flaxman et al., 2005; Shalev-Shwartz, 2012) as well as in the stochastic optimization framework (Spall, 2003; Nemirovski et al., 2009; Bach and Moulines, 2011). Our case is a non-convex optimization problem, which we analyze in the simple and elegant framework of pseudogradient adaptation that allows us to show convergence of the presented algorithm (Polyak and Tsypkin, 1973; Polyak,"
2015.mtsummit-papers.13,N15-1175,0,0.124434,"distribution p˜(x, y) = T1 t=0 1[x = xt ]1[y = yt ] for i.i.d. training data {(xt , yt )}Tt=0 . This yields the objective 0 Ep(x,y)p [∆y (y 0 )] = ˜ w (y |x) T 1X T t=0 X y 0 ∈Y(x ∆yt (y 0 )pw (y 0 |xt ). (4) t) While being continuous and differentiable, the expected loss criterion is typically nonconvex. For example, in SMT, expected loss training for the standard task loss BLEU leads to highly non-convex optimization problems. Despite of this, most approaches rely on gradientdescent techniques for optimization (see Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by following the opposite direction of the gradient of (4): 0 ∇Ep(x,y)p [∆y (y 0 )] ˜ w (y |x) h i = Ep(x,y) Epw (y0 |x) [∆y (y 0 )φ(x, y 0 )] − Epw (y0 |x) [∆y (y 0 )] Epw (y0 |x) [φ(x, y 0 )] ˜ h i 0 0 0 0 0 = Ep(x,y)p ∆ (y )(φ(x, y ) − E [φ(x, y )]) . y ˜ pw (y |x) w (y |x) 4 Bandit Structured Prediction Bandit feedback in structured prediction means that the gold standard output structure y, with respect to which the objective function is evaluated, is not revealed to the learner. Thus we can neither calculate the gradient of the objective function (4) nor evaluate the task l"
2015.mtsummit-wpslt.4,D14-1132,0,0.0243344,"ack to predicted structure. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 − BLEU loss criterion (Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)’s dueling bandits algorithm to a Structured Dueling Bandits algorithm. It compares a current weight vector wt with a neighboring point wt0 along a direction ut , performing exploration (controlled by δ, line 5) by probing random directions, and exploitation (controlled"
2015.mtsummit-wpslt.4,2010.iwslt-papers.3,0,0.0160284,"man-assisted SMT to patent data and data from legal and IT domains. The crucial steps of the response-based online learning protocol are instantiated in online learning from post-edits as follows: Prediction: Most probable sentence translation. Response: User post-edit. Learning: Dynamically extend phrase table and language model; update feature weights by reranking. The key difference between online learning from post-edits instead of from reference translations is the dynamic extension of the phrase table. W¨aschle et al. (2013) and Bertoldi et al. (2014) use a constrained search technique (Cettolo et al., 2010) that optimizes the coverage of both source and target sentences. It produces exactly one phrase segmentation and alignment, and allows gaps such that some source and target words may be uncovered. It differs in this respect from forced decoding which produces an alignment only when the target is fully reachable with the given models. Assume the following phrase segmentation and alignment: 1 111k 2 245k train + 1,088k test, available under www.cl.uni-heidelberg.de/boostclir train + 1,455k test, available under www.cl.uni-heidelberg.de/wikiclir Proceedings of 6th Workshp on Patent and Scientifi"
2015.mtsummit-wpslt.4,E14-1042,0,0.0200348,"professional translators leads to improved productivity of translators, and to improved quality of final translations (Koehn, 2009; Garcia, 2011; Green et al., 2013). This scenario can be turned on its head by focusing on the human post-editor supporting the SMT system, leading to a mutually beneficial cycle of human-assisted machine translation where the SMT system performs online learning from human post-edits. The goal is to improve translation consistency for a given document, and to offer the user the experience of a system that immediately learns from corrections (W¨aschle et al., 2013; Denkowski et al., 2014; Green et al., 2014). Patent data are especially well-suited for an application of this scenario because of the high repetitivity of patent documents. W¨aschle et al. (2013) and Bertoldi et al. (2014) recently presented an application of human-assisted SMT to patent data and data from legal and IT domains. The crucial steps of the response-based online learning protocol are instantiated in online learning from post-edits as follows: Prediction: Most probable sentence translation. Response: User post-edit. Learning: Dynamically extend phrase table and language model; update feature weights by"
2015.mtsummit-wpslt.4,D14-1130,0,0.0119074,"leads to improved productivity of translators, and to improved quality of final translations (Koehn, 2009; Garcia, 2011; Green et al., 2013). This scenario can be turned on its head by focusing on the human post-editor supporting the SMT system, leading to a mutually beneficial cycle of human-assisted machine translation where the SMT system performs online learning from human post-edits. The goal is to improve translation consistency for a given document, and to offer the user the experience of a system that immediately learns from corrections (W¨aschle et al., 2013; Denkowski et al., 2014; Green et al., 2014). Patent data are especially well-suited for an application of this scenario because of the high repetitivity of patent documents. W¨aschle et al. (2013) and Bertoldi et al. (2014) recently presented an application of human-assisted SMT to patent data and data from legal and IT domains. The crucial steps of the response-based online learning protocol are instantiated in online learning from post-edits as follows: Prediction: Most probable sentence translation. Response: User post-edit. Learning: Dynamically extend phrase table and language model; update feature weights by reranking. The key di"
2015.mtsummit-wpslt.4,N15-1123,1,0.902723,"LIR, and is instead optimized to match fluent, human reference translations. In contrast, retrieval systems often use bag-of-word representations, stopword filtering, and stemming techniques during document scoring, and queries are rarely fluent, grammatical natural language queries (Downey et al., 2008). Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking (Nikoulina et al., 2012) or ranking (Sokolov et al., 2014). The most direct integration of SMT and CLIR has been presented by Hieber and Riezler (2015) in an approach to Bag-of-Words Forced Decoding (BOW-FD) where IR features for words in the bag-of-words representation of documents force the SMT decoder to prefer relevant documents with high probability. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Full translation hypergraph. Response: BM25 scores of partial translation hypotheses. Learning: Jointly optimize SMT and IR feature weights by direct ranking optimization on relevant documents. The key advantage of this approach clearly is the exploitation of the full t"
2015.mtsummit-wpslt.4,E12-1012,0,0.0153012,"ralized term matching (Ture et al., 2012a,b). In both DT and PSQ, the integration of SMT remains agnostic about its use for CLIR, and is instead optimized to match fluent, human reference translations. In contrast, retrieval systems often use bag-of-word representations, stopword filtering, and stemming techniques during document scoring, and queries are rarely fluent, grammatical natural language queries (Downey et al., 2008). Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking (Nikoulina et al., 2012) or ranking (Sokolov et al., 2014). The most direct integration of SMT and CLIR has been presented by Hieber and Riezler (2015) in an approach to Bag-of-Words Forced Decoding (BOW-FD) where IR features for words in the bag-of-words representation of documents force the SMT decoder to prefer relevant documents with high probability. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Full translation hypergraph. Response: BM25 scores of partial translation hypotheses. Learning: Jointly optimize SMT and IR feature weights by"
2015.mtsummit-wpslt.4,P03-1021,0,0.0138225,"em is adapted to a user solely by single-point user feedback to predicted structure. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 − BLEU loss criterion (Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)’s dueling bandits algorithm to a Structured Dueling Bandits algorithm. It compares a current weight vector wt with a neighboring point wt0 along a direction ut , performing exploration (controlled by δ, li"
2015.mtsummit-wpslt.4,P06-2101,0,0.0242413,"d to a user solely by single-point user feedback to predicted structure. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 − BLEU loss criterion (Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)’s dueling bandits algorithm to a Structured Dueling Bandits algorithm. It compares a current weight vector wt with a neighboring point wt0 along a direction ut , performing exploration (controlled by δ, line 5) by probing random d"
2015.mtsummit-wpslt.4,2015.mtsummit-papers.13,1,0.866667,"post-edits of predicted translations, or than reference translations generated from scratch. This framework can be seen as a first step towards personalized machine translation where a given large SMT system is adapted to a user solely by single-point user feedback to predicted structure. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 − BLEU loss criterion (Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)’s dueling"
2015.mtsummit-wpslt.4,C12-1164,0,0.0118741,"ia NDCG PRES .5691 .5671 ∗ .5963 .7219 .7165 ∗ .7528 MAP Patents NDCG PRES .2554 .2659 ∗ .2883 .5397 .5508 ∗ .5819 .5680 .5851 ∗ .6251 Table 1: Retrieval results of baseline systems and BOW-FD on large-scale CLIR task. ∗ denotes significant difference compared to both baselines. of CLIR to a pipeline of direct translation (DT) and monolingual retrieval (Chin et al., 2008). Only recently, research approaches to probabilistic structured queries (PSQ) have been presented, that is, they include (weighted) translation alternatives into the query structure to allow a more generalized term matching (Ture et al., 2012a,b). In both DT and PSQ, the integration of SMT remains agnostic about its use for CLIR, and is instead optimized to match fluent, human reference translations. In contrast, retrieval systems often use bag-of-word representations, stopword filtering, and stemming techniques during document scoring, and queries are rarely fluent, grammatical natural language queries (Downey et al., 2008). Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking (Nikoulina et al., 2012) or ranking (Soko"
2015.mtsummit-wpslt.4,2013.mtsummit-papers.2,1,0.800025,"Missing"
2015.mtsummit-wpslt.4,N15-1175,0,0.0120364,"ucture. The crucial steps of the response-based online learning protocol are instantiated in this approach as follows: Prediction: Exploration/exploitation sampling of sentence translation. Response: User feedback on loss value of sampled translation. Learning: Stochastic update using unbiased estimate of gradient. Sokolov et al. (2015) presented two algorithms for online structured prediction in SMT from bandit feedback that implement these ideas as follows. Algorithm 1 optimizes an expected 1 − BLEU loss criterion (Och (2003), Smith and Eisner (2006), He and Deng (2012), Auli et al. (2014), Wuebker et al. (2015), inter alia) by performing simultaneous exploration/exploitation by sampling translations from a Gibbs model (line 6) , and using the obtained user feedback (line 7) to perform an update in the negative direction of the instantaneous gradient (line 8). The second algorithm extends Yue and Joachims (2009)’s dueling bandits algorithm to a Structured Dueling Bandits algorithm. It compares a current weight vector wt with a neighboring point wt0 along a direction ut , performing exploration (controlled by δ, line 5) by probing random directions, and exploitation (controlled by γ, line 8) by taking"
2020.coling-main.487,2016.amta-researchers.10,0,0.0255674,"tion are combined in an ensemble. ∗ This work was done while the author visited the Department of Computational Linguistics, Heidelberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into lea"
2020.coling-main.487,P17-2061,0,0.0152124,"or visited the Department of Computational Linguistics, Heidelberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into learning-to-rank approaches for IR has previously been presented by Sch"
2020.coling-main.487,N19-1423,0,0.041773,"neural learning-to-rank model that learns a relevance score S(~cq , ~cd ) for a vector representation of an English query ~cq and a foreign-language document ~cd . These vector representations are computed by a convolutional feature map over a “sentence matrix”, with rows consisting of vector representations of words in a query or document (pre-trained using word2vec (Mikolov et al., 2013) on the corpora described below), and columns in the size of the length of the query or document. This choice of embeddings might seem simplistic compared to recent approaches to contextual word embeddings (Devlin et al., 2019; Peters et al., 2018), however, it is motivated by the goal of understanding the relative benefit of meta-text embeddings based on a manageable architecture for learning-to-rank with text-only embeddings. Let ~x1:n = [~x1 ; ~x2 ; . . . ; ~xn ] be the concatenation of word vectors for a query or a document of n words, where each word vector is of dimensionality k, and let ~xi:i+h−1 = [~xi ; ~xi+1 ; . . . ; ~xi+h−1 ] denote the concatenation of word vectors in a window of width h starting from position i. The parameters of a ~ ∈ Rhk×m which is applied to a window of h words, and extracts vector"
2020.coling-main.487,W18-1802,1,0.852435,"elberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into learning-to-rank approaches for IR has previously been presented by Schamoni and Riezler (2015), albeit not in a neural IR framework. The"
2020.coling-main.487,Q17-1024,0,0.0405582,"Missing"
2020.coling-main.487,kobus-etal-2017-domain,0,0.0127429,"artment of Computational Linguistics, Heidelberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into learning-to-rank approaches for IR has previously been presented by Schamoni and Riezler (2"
2020.coling-main.487,C16-1252,0,0.0286802,"nformation for our patent retrieval task consists of the International Patent Classification (IPC) codes.5 More precisely, we use the sub-group level of the IPC based domestic classification ECLA for English and FI for Japanese. Our dataset contains 50,329 unique IPC subdivisions for English and 23,020 for Japanese. We employ the same DeepWalk training strategy to learn classification embeddings as for Wikipedia categories. For strictly hierarchical graphs like the IPC-tree, there exist algorithms that better capture hierarchical structures (Nickel and Kiela (2017), Alsuhaibani et al. (2019), Li et al. (2016), inter alia). However, we still chose our previous method for its simplicity and for comparability to previous results. Evaluating truly hierarchical embeddings for patent retrieval is planned as future work. 5 Experiments To evaluate the efficacy of our model, we conduct experiments on Wikipedia data for three language pairs, and on patent data for a single language pair. We conduct additional experiments in the patent domain to evaluate alternative models that combine information differently. Finally, we integrate standard tf-idf and evaluate if our models scale up to realistic retrieval sc"
2020.coling-main.487,N18-1202,0,0.0455657,"ank model that learns a relevance score S(~cq , ~cd ) for a vector representation of an English query ~cq and a foreign-language document ~cd . These vector representations are computed by a convolutional feature map over a “sentence matrix”, with rows consisting of vector representations of words in a query or document (pre-trained using word2vec (Mikolov et al., 2013) on the corpora described below), and columns in the size of the length of the query or document. This choice of embeddings might seem simplistic compared to recent approaches to contextual word embeddings (Devlin et al., 2019; Peters et al., 2018), however, it is motivated by the goal of understanding the relative benefit of meta-text embeddings based on a manageable architecture for learning-to-rank with text-only embeddings. Let ~x1:n = [~x1 ; ~x2 ; . . . ; ~xn ] be the concatenation of word vectors for a query or a document of n words, where each word vector is of dimensionality k, and let ~xi:i+h−1 = [~xi ; ~xi+1 ; . . . ; ~xi+h−1 ] denote the concatenation of word vectors in a window of width h starting from position i. The parameters of a ~ ∈ Rhk×m which is applied to a window of h words, and extracts vectors convolution involve"
2020.coling-main.487,N18-2073,1,0.404112,", more sophisticated techniques are needed to aid similarity computation and ranking by meta-textual category information. In this paper, we show how to apply neural embedding methods to meta-textual categories. We show that meta-textual information is an incomplete representation of queries and documents and does not suffice for a standalone computation of similarity and ranking. However, an incorporation of pre-trained embeddings of meta-textual categories into a neural learning-to-rank approach yields significant improvements over a learning-to-rank approach that uses text-only embeddings (Sasaki et al., 2018). In our approach, enhanced embeddings are created by concatenating text embeddings with meta-textual category embeddings, and a fully connected weight layer is learned on top of the concatenation of the enhanced embeddings of a query-document pair by a deep multilayer perceptron that is optimized for pairwise ranking. We present experiments on three different language pairs for cross-lingual retrieval in the Wikipedia domain, and show improvements of up to 2 NDCG points by incorporating meta-textual embeddings in a learning-to-rank framework. Additional experiments on a single language pair i"
2020.coling-main.487,W16-2209,0,0.0301984,"ork was done while the author visited the Department of Computational Linguistics, Heidelberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into learning-to-rank approaches for IR has previously bee"
2020.coling-main.487,N16-1005,0,0.0291018,"n an ensemble. ∗ This work was done while the author visited the Department of Computational Linguistics, Heidelberg, Germany. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 5558 Proceedings of the 28th International Conference on Computational Linguistics, pages 5558–5568 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work Embeddings of meta-textual information via so-called “side constraints” has been done successfully in neural machine translation applications (Chen et al., 2016; Sennrich et al., 2016; Sennrich and Haddow, 2016; Chu et al., 2017; Kobus et al., 2017; Johnson et al., 2017; Jehl and Riezler, 2018). Metainformation such as domain labels, product categories, or politeness level, has been incorporated in various ways, including a choice of feeding a meta-information tag at the source side or the target side, the choice of attaching it to each sentence or to each word, and the choice of learning embeddings vi learning-to-rank training or via devising a specific representation for the meta-information embeddings. An incorporation of meta-information into learning-to-rank approache"
2020.coling-main.487,D13-1175,1,0.92409,"pes are the unique categories, for patents they are the unique IPC-codes. dataset language pair # target documents # meta types source/target # train queries # dev queries # dev documents # test queries # test documents Wikipedia En-Ja En-De En-Fr 1, 071, 292 2, 091, 278 1, 894, 397 1, 023M/172k 1, 023M/282k 1, 023M/332k 298, 468 656, 735 762, 336 42, 638 93, 820 108, 905 same as target 42, 638 93, 819 108, 905 same as target Patent Ja-En 1, 039, 656 23k/50k 107, 061 2, 000 100, 000 2, 000 100, 000 formation from Wikipedia. For the patent retrieval task, we extend data previously published by Sokolov et al. (2013) with information from the International Patent Classification (IPC) system. Table 1 lists the statistics of the extended datasets. Both data extensions are publicly available.2 4.1 Cross-Lingual Retrieval in Wikipedia The task of cross-lingual retrieval on Wikipedia data is as follows: given a query in a source language, identify the corresponding article in the target language and all other articles that are relevant to this target article, i.e. having incoming and outgoing links to the article in the target language. Pages that connect articles which are irrelevant to each other, e.g. disam"
2020.eamt-1.15,D16-1025,0,0.0273643,"icly available.2 2 Related Work Prior work closest to ours is that of Marie and Max (2015); Domingo et al. (2017); Petrushkov et al. (2018), however, these works were conducted by simulating error markings by heuristic matching of machine translations against independently created human reference translations. Thus the question of the practical feasibility of machine learning from noisy human error markings is left open. User studies on machine learnability from human post-edits, together with thorough performance analyses with mixed effects models, have been presented by Green et al. (2014); Bentivogli et al. (2016); Karimova et al. (2018). Albeit showcasing the potential of improving NMT through human corrections of machine-generated outputs, these works do not consider “weaker” annotation modes like error markings. User studies on the process and effort of machine translation postediting are too numerous to list—a comprehensive overview is given in Koponen (2016). In contrast to works on interactive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interaction with the human and allo"
2020.eamt-1.15,P18-1008,0,0.0139015,"ctor: Domain train dev test WMT17 IWSLT17 Selection 5,919,142 206,112 1035 corr / 1042 mark 2,169 2,385 3,004 1,138 1,043 Table 4: Data sizes (en-de), official splits from WMT17 and IWSLT17. Our target-domain data is a subset of selected talks from IWSLT2017 training data totalling 3,120 sentences. pθ (yt |x; y&lt;t ) = softmax(NNθ (x; y&lt;t )). (2) 4.1 There are various options for building the architecture of the neural model NNθ , such as recurrent (Bahdanau et al., 2015), convolutional (Gehring et al., 2017) or attentional (Vaswani et al., 2017) encoder-decoder architectures (or a mix thereof (Chen et al., 2018)). Learning from Error Corrections. The standard supervised learning mode in human-in-theloop machine translation assumes a fully corrected output y ∗ for an input x that is treated similar to a gold standard reference translation (Turchi et al., 2017). Model adaptation can be performed by maximizing the likelihood of the user-provided corrections where L(θ) = T XX x,y ∗ ∗ log pθ (yt∗ |x; y&lt;t ), (3) t=1 using stochastic gradient descent techniques (Bottou et al., 2018). Learning from Error Markings. A weaker feedback mode is to let a human teacher mark the correct parts of the machine-generate"
2020.eamt-1.15,P11-2031,0,0.0488223,"it of the selected talks that was annotated in the user-mode, since the purpose of this split was the evaluation of user preference. There is no overlap in the three data splits, but they have the same distribution over topics, so that we can both measure local adaptation and draw comparisons between modes. Data sizes are given in Table 4. Evaluation. The models are evaluated with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009)11 against references translations. Significance is tested with approximate randomization for three runs for each system (Clark et al., 2011). 4.2 Results Corrections, Markings and Quality Judgments. Table 5 compares the models after fine-tuning with 10 Pre-trained model: https://github.com/ joeynmt/joeynmt/blob/master/README. md#wmt17; modified fork of Joey NMT: https: //github.com/StatNLP/joeynmt/tree/mark 11 Computed with MultEval v0.5.1 (Clark et al., 2011) on tokenized outputs. System 1 WMT baseline TER ↓ BLEU ↑ METEOR ↑ 58.6 23.9 42.7 57.4? 57.9? 24.6? 24.1 44.7? 44.2? 57.5? 57.4? 58.1? 24.4? 24.6? 24.1 44.0? 44.2? 43.5? 57.4? 57.6? 24.6? 24.5? 44.7? 43.8? Error Corrections 2 3 Full Small Error Markings 4 5 6 0/1 -0.5/0.5 ran"
2020.eamt-1.15,P18-1165,1,0.852498,"teractive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interaction with the human and allows to investigate, filter, pre-process, or augment the human feedback signal before making a machine learning update. Machine learning from human feedback beyond the scope of translations, has considered learning from human pairwise preferences (Christiano et al., 2017), from human corrective feedback (Celemin et al., 2018), or from sentence-level reward signals on a Likert scale (Kreutzer et al., 2018). However, none of these studies has considered error markings on tokens of output sequences, despite its general applicability to a wide range of learning tasks. 2 https://www.cl.uni-heidelberg.de/ statnlpgroup/humanmt/ 3 User Study on Human Error Markings and Corrections The goal of the annotation study is to compare the novel error marking mode to the widely adopted machine translation post-editing mode. We are interested in finding an interaction scenario that costs little time and effort, but still allows to teach the machine how to improve its translations. In this section we present the"
2020.eamt-1.15,1997.mtsummit-papers.1,0,0.400497,"achine learnability from human post-edits, together with thorough performance analyses with mixed effects models, have been presented by Green et al. (2014); Bentivogli et al. (2016); Karimova et al. (2018). Albeit showcasing the potential of improving NMT through human corrections of machine-generated outputs, these works do not consider “weaker” annotation modes like error markings. User studies on the process and effort of machine translation postediting are too numerous to list—a comprehensive overview is given in Koponen (2016). In contrast to works on interactive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interaction with the human and allows to investigate, filter, pre-process, or augment the human feedback signal before making a machine learning update. Machine learning from human feedback beyond the scope of translations, has considered learning from human pairwise preferences (Christiano et al., 2017), from human corrective feedback (Celemin et al., 2018), or from sentence-level reward signals on a Likert scale (Kreutzer et al., 2018). However, none of these studie"
2020.eamt-1.15,D18-1397,0,0.056331,"ith mixed effects models, have been presented by Green et al. (2014); Bentivogli et al. (2016); Karimova et al. (2018). Albeit showcasing the potential of improving NMT through human corrections of machine-generated outputs, these works do not consider “weaker” annotation modes like error markings. User studies on the process and effort of machine translation postediting are too numerous to list—a comprehensive overview is given in Koponen (2016). In contrast to works on interactive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interaction with the human and allows to investigate, filter, pre-process, or augment the human feedback signal before making a machine learning update. Machine learning from human feedback beyond the scope of translations, has considered learning from human pairwise preferences (Christiano et al., 2017), from human corrective feedback (Celemin et al., 2018), or from sentence-level reward signals on a Likert scale (Kreutzer et al., 2018). However, none of these studies has considered error markings on tokens of output sequences, despite its general app"
2020.eamt-1.15,D14-1130,0,0.094238,"and markings is publicly available.2 2 Related Work Prior work closest to ours is that of Marie and Max (2015); Domingo et al. (2017); Petrushkov et al. (2018), however, these works were conducted by simulating error markings by heuristic matching of machine translations against independently created human reference translations. Thus the question of the practical feasibility of machine learning from noisy human error markings is left open. User studies on machine learnability from human post-edits, together with thorough performance analyses with mixed effects models, have been presented by Green et al. (2014); Bentivogli et al. (2016); Karimova et al. (2018). Albeit showcasing the potential of improving NMT through human corrections of machine-generated outputs, these works do not consider “weaker” annotation modes like error markings. User studies on the process and effort of machine translation postediting are too numerous to list—a comprehensive overview is given in Koponen (2016). In contrast to works on interactive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interacti"
2020.eamt-1.15,2016.amta-researchers.9,0,0.0221587,"rom human post-edits, together with thorough performance analyses with mixed effects models, have been presented by Green et al. (2014); Bentivogli et al. (2016); Karimova et al. (2018). Albeit showcasing the potential of improving NMT through human corrections of machine-generated outputs, these works do not consider “weaker” annotation modes like error markings. User studies on the process and effort of machine translation postediting are too numerous to list—a comprehensive overview is given in Koponen (2016). In contrast to works on interactive-predictive translation (Foster et al., 1997; Knowles and Koehn, 2016; Peris et al., 2017; Domingo et al., 2017; Lam et al., 2018), our approach does not require an online interaction with the human and allows to investigate, filter, pre-process, or augment the human feedback signal before making a machine learning update. Machine learning from human feedback beyond the scope of translations, has considered learning from human pairwise preferences (Christiano et al., 2017), from human corrective feedback (Celemin et al., 2018), or from sentence-level reward signals on a Likert scale (Kreutzer et al., 2018). However, none of these studies has considered error ma"
2020.eamt-1.15,D19-3019,1,0.850103,"Missing"
2020.eamt-1.15,P17-1138,1,0.848168,"ures are complex and expert knowledge is required, as for example in neural machine translation (NMT) (Bahdanau et al., 2015). Approaches that propose to train sequenceto-sequence prediction models by reinforcement learning from task-specific scores, for example BLEU in machine translation (MT), shift the problem by simulating such scores by evaluating machine translation output against expert-generated c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. reference structures (Ranzato et al., 2016; Bahdanau et al., 2017; Kreutzer et al., 2017; Sokolov et al., 2017). An alternative approach that proposes to considerably reduce human annotation effort by allowing to mark errors in machine outputs, for example erroneous words or phrases in a machine translation, has recently been proposed and been investigated in simulation studies by Marie and Max (2015); Domingo et al. (2017); Petrushkov et al. (2018). This approach takes the middle ground between supervised learning from error corrections as in machine translation post-editing1 (or from translations created from scratch) and reinforcement learning from sequence-level bandit feedba"
2020.eamt-1.15,W19-6610,1,0.83125,"acher mark the correct parts of the machine-generated output yˆ (Marie and Max, 2015; Petrushkov et al., 2018; Domingo et al., 2017). As a consequence every token in the output receives a reward δtm , either δt+ if marked as correct, or δt− otherwise. Petrushkov et al. (2018) proposed a model with δt+ = 1 and δt− = 0, but this weighting schemes leads to the ignorance of incorrect outputs in the gradient and the rewarding of correct tokens. Instead, we find it beneficial to penalize incorrect tokens, with e.g. δt− = −0.5, and reward correct tokens δt+ = 0.5, which aligns with the findings from Lam et al. (2019). The objective of the learning system is to maximize the likelihood of the correct parts of the output where L(θ) = T XX x,ˆ y t=1 δtm log pθ (ˆ yt |x; yˆ&lt;t ). (4) NMT Fine-Tuning NMT Model and Data. The goal is to adapt a general-domain NMT model to a new domain with either post-edits or markings. For the generaldomain NMT system, we use the pre-trained 4layer LSTM encoder-decoder Joey NMT WMT17 model (Kreutzer et al., 2019) for translations from English to German10 . The model is trained on a joint vocabulary with 30k subwords (Sennrich et al., 2016). Model outputs are de-tokenized and un-B"
2020.eamt-1.15,D15-1120,0,0.317209,"simulating such scores by evaluating machine translation output against expert-generated c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. reference structures (Ranzato et al., 2016; Bahdanau et al., 2017; Kreutzer et al., 2017; Sokolov et al., 2017). An alternative approach that proposes to considerably reduce human annotation effort by allowing to mark errors in machine outputs, for example erroneous words or phrases in a machine translation, has recently been proposed and been investigated in simulation studies by Marie and Max (2015); Domingo et al. (2017); Petrushkov et al. (2018). This approach takes the middle ground between supervised learning from error corrections as in machine translation post-editing1 (or from translations created from scratch) and reinforcement learning from sequence-level bandit feedback (this includes self-supervised learning where all outputs are rewarded uniformly). Error markings are highly promising since they suggest an interaction mode with low annotation cost, yet they can enable precise token-level credit/blame assignment, and thus can lead to an effective finegrained discriminative sig"
2020.eamt-1.15,P02-1040,0,0.112809,"e, dropout and batch size for this fine-tuning step are tuned on the IWSLT17 dev set. For the marking mode, the weights δ + and δ − are tuned in addition. As test data, we use the split of the selected talks that was annotated in the user-mode, since the purpose of this split was the evaluation of user preference. There is no overlap in the three data splits, but they have the same distribution over topics, so that we can both measure local adaptation and draw comparisons between modes. Data sizes are given in Table 4. Evaluation. The models are evaluated with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009)11 against references translations. Significance is tested with approximate randomization for three runs for each system (Clark et al., 2011). 4.2 Results Corrections, Markings and Quality Judgments. Table 5 compares the models after fine-tuning with 10 Pre-trained model: https://github.com/ joeynmt/joeynmt/blob/master/README. md#wmt17; modified fork of Joey NMT: https: //github.com/StatNLP/joeynmt/tree/mark 11 Computed with MultEval v0.5.1 (Clark et al., 2011) on tokenized outputs. System 1 WMT baseline TER ↓ BLEU ↑ METEOR ↑ 58.6 23.9 42.7 57.4? 57.9? 24"
2020.eamt-1.15,P18-2052,0,0.265094,"translation output against expert-generated c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. reference structures (Ranzato et al., 2016; Bahdanau et al., 2017; Kreutzer et al., 2017; Sokolov et al., 2017). An alternative approach that proposes to considerably reduce human annotation effort by allowing to mark errors in machine outputs, for example erroneous words or phrases in a machine translation, has recently been proposed and been investigated in simulation studies by Marie and Max (2015); Domingo et al. (2017); Petrushkov et al. (2018). This approach takes the middle ground between supervised learning from error corrections as in machine translation post-editing1 (or from translations created from scratch) and reinforcement learning from sequence-level bandit feedback (this includes self-supervised learning where all outputs are rewarded uniformly). Error markings are highly promising since they suggest an interaction mode with low annotation cost, yet they can enable precise token-level credit/blame assignment, and thus can lead to an effective finegrained discriminative signal for machine learning and data filtering. Our"
2020.eamt-1.15,P16-1162,0,0.0182289,"t+ = 0.5, which aligns with the findings from Lam et al. (2019). The objective of the learning system is to maximize the likelihood of the correct parts of the output where L(θ) = T XX x,ˆ y t=1 δtm log pθ (ˆ yt |x; yˆ&lt;t ). (4) NMT Fine-Tuning NMT Model and Data. The goal is to adapt a general-domain NMT model to a new domain with either post-edits or markings. For the generaldomain NMT system, we use the pre-trained 4layer LSTM encoder-decoder Joey NMT WMT17 model (Kreutzer et al., 2019) for translations from English to German10 . The model is trained on a joint vocabulary with 30k subwords (Sennrich et al., 2016). Model outputs are de-tokenized and un-BPEd before being presented to the annotators. With the help of human annotations we then adapt this model to the domain of TED talk transcripts by continuing learning on the annotated data. Hyperparameters including learning rate schedule, dropout and batch size for this fine-tuning step are tuned on the IWSLT17 dev set. For the marking mode, the weights δ + and δ − are tuned in addition. As test data, we use the split of the selected talks that was annotated in the user-mode, since the purpose of this split was the evaluation of user preference. There"
2020.eamt-1.15,2006.amta-papers.25,0,0.154381,"luding learning rate schedule, dropout and batch size for this fine-tuning step are tuned on the IWSLT17 dev set. For the marking mode, the weights δ + and δ − are tuned in addition. As test data, we use the split of the selected talks that was annotated in the user-mode, since the purpose of this split was the evaluation of user preference. There is no overlap in the three data splits, but they have the same distribution over topics, so that we can both measure local adaptation and draw comparisons between modes. Data sizes are given in Table 4. Evaluation. The models are evaluated with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009)11 against references translations. Significance is tested with approximate randomization for three runs for each system (Clark et al., 2011). 4.2 Results Corrections, Markings and Quality Judgments. Table 5 compares the models after fine-tuning with 10 Pre-trained model: https://github.com/ joeynmt/joeynmt/blob/master/README. md#wmt17; modified fork of Joey NMT: https: //github.com/StatNLP/joeynmt/tree/mark 11 Computed with MultEval v0.5.1 (Clark et al., 2011) on tokenized outputs. System 1 WMT baseline TER ↓ BLEU ↑ METEOR ↑"
2020.eamt-1.15,W17-4756,1,0.9058,"Missing"
2020.lrec-1.441,N18-1008,0,0.184776,"h translation has recently been shown to be feasible using a single sequence-to-sequence neural model, trained on parallel data consisting of source audio, source text and target text. The crucial advantage of such end-toend approaches is the avoidance of error propagation as in a pipeline approaches of speech recognition and text translation. While cascaded approaches have an advantage in that they can straightforwardly use large independent datasets for speech recognition and text translation, clever sharing of sub-networks via multi-task learning and two-stage modeling (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) has closed the performance gap between end-to-end and pipeline approaches. However, endto-end neural speech translation is very data hungry while available datasets must be considered large if they exceed 100 hours of audio. For example, the widely used Fisher and Callhome Spanish-English corpus (Post et al., 2013) comprises 162 hours of audio and 138, 819 parallel sentences. Larger corpora for end-to-end speech translation have only recently become available for speech translation from English sources. For example, 236 hours of audio and 131, 395 parallel sentences are"
2020.lrec-1.441,N19-1202,0,0.283351,"Missing"
2020.lrec-1.441,L18-1001,0,0.549066,"nd and pipeline approaches. However, endto-end neural speech translation is very data hungry while available datasets must be considered large if they exceed 100 hours of audio. For example, the widely used Fisher and Callhome Spanish-English corpus (Post et al., 2013) comprises 162 hours of audio and 138, 819 parallel sentences. Larger corpora for end-to-end speech translation have only recently become available for speech translation from English sources. For example, 236 hours of audio and 131, 395 parallel sentences are available for English-French speech translation based on audio books (Kocabiyikoglu et al., 2018; Bérard et al., 2018). For speech translation of English TED talks, 400-500 hours of audio aligned to around 250, 000 parallel sentences depending on the language pair have been provided for eight target languages by Di Gangi et al. (2019). Pure speech recognition data are available in amounts of 1, 000 hours of read English speech and their transcriptions in the LibriSpeech corpus provided by Panayotov et al. (2015). When it comes to German sources, the situation regarding corpora for end-to-end speech translation as well as for speech recognition is dire. To our knowledge, the largest freel"
2020.lrec-1.441,2013.iwslt-papers.14,0,0.0958524,"ascaded approaches have an advantage in that they can straightforwardly use large independent datasets for speech recognition and text translation, clever sharing of sub-networks via multi-task learning and two-stage modeling (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) has closed the performance gap between end-to-end and pipeline approaches. However, endto-end neural speech translation is very data hungry while available datasets must be considered large if they exceed 100 hours of audio. For example, the widely used Fisher and Callhome Spanish-English corpus (Post et al., 2013) comprises 162 hours of audio and 138, 819 parallel sentences. Larger corpora for end-to-end speech translation have only recently become available for speech translation from English sources. For example, 236 hours of audio and 131, 395 parallel sentences are available for English-French speech translation based on audio books (Kocabiyikoglu et al., 2018; Bérard et al., 2018). For speech translation of English TED talks, 400-500 hours of audio aligned to around 250, 000 parallel sentences depending on the language pair have been provided for eight target languages by Di Gangi et al. (2019). P"
2020.lrec-1.441,Q19-1020,0,0.0580232,"hown to be feasible using a single sequence-to-sequence neural model, trained on parallel data consisting of source audio, source text and target text. The crucial advantage of such end-toend approaches is the avoidance of error propagation as in a pipeline approaches of speech recognition and text translation. While cascaded approaches have an advantage in that they can straightforwardly use large independent datasets for speech recognition and text translation, clever sharing of sub-networks via multi-task learning and two-stage modeling (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) has closed the performance gap between end-to-end and pipeline approaches. However, endto-end neural speech translation is very data hungry while available datasets must be considered large if they exceed 100 hours of audio. For example, the widely used Fisher and Callhome Spanish-English corpus (Post et al., 2013) comprises 162 hours of audio and 138, 819 parallel sentences. Larger corpora for end-to-end speech translation have only recently become available for speech translation from English sources. For example, 236 hours of audio and 131, 395 parallel sentences are available for English-"
2020.lrec-1.441,stuker-etal-2012-kit,0,0.0311357,"he language pair have been provided for eight target languages by Di Gangi et al. (2019). Pure speech recognition data are available in amounts of 1, 000 hours of read English speech and their transcriptions in the LibriSpeech corpus provided by Panayotov et al. (2015). When it comes to German sources, the situation regarding corpora for end-to-end speech translation as well as for speech recognition is dire. To our knowledge, the largest freely available corpora for German-English speech translation comprise triples for 37 hours of German audio, German transcription, and English translation (Stüker et al., 2012). Pure speech recognition data are available from 36 hours (Radeck-Arneth et al., 2015) to around 200 hours (Baumann et al., 2018). We present a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of 110 hours of German audio material aligned to over 50k parallel sentences. An even larger dataset comprising 547 hours of German speech aligned to German text is available for speech recognition. Our approach mirrors that of Kocabiyikoglu et al. (2018) in that we start from freely available audio books. The fact"
2021.acl-long.41,W15-4715,0,0.0195853,"nnotation modality: Touchdown annotators use panorama images along the route, while our instruction writers only see the rendered route on a map. See Section 4.3 for a more detailed discussion. 2 Our work puts the task of natural language navigation upside down by learning to generate humanlike navigation instructions from real-world map data instead of training an agent to follow human generated instructions. Prior work in this area has used rule-based systems to identify landmarks (Rousell and Zipf, 2017) or to generate landmarkbased navigation instructions (Dr¨ager and Koller, 2012; Cercas Curry et al., 2015). Despite having all points of interest on the map available, our approach learns to verbalize only those points of interest that have been deemed salient by inclusion in a human navigation instruction. Previous approaches that learn navigation instructions from data have Related Work and Datasets Mirowski et al. (2018) published a subset of Street View covering parts of New York City and Pittsburgh. Street View is a navigable environment that is build from connected real-world 360◦ panoramas. This data is used by Hermann et al. (2020) to train a visual agent to follow turn-by-turn instruction"
2021.acl-long.41,W18-6319,0,0.0245351,"une on the human generated instances. Both models and the seq2seq baseline are trained on 5,667 instances of our dataset. The best weights for each model are selected by token accuracy based early stopping on the 605 development instances. We consider two baselines. A rule based system that uses a single heuristic to construct instructions by stringing together all POIs and intersections along the route, and following each intersection by the turning direction. Similar, POIs are followed by ’left’ or ’right’ depending on which side 495 6.3 Evaluation Metrics BLEU is calculated with SacreBLEU (Post, 2018) on lower-cased and tokenized text. Length is the average length in number of tokens. Landmarks is the number of landmark occurreference: At the light with Fridays on the corner, turn right. Continue down the long street to the next light with Nine West on the right corner, then turn left. Go to the next light with Brooks Brothers on the right corner, then turn right and stop. rule based: Starbucks Coffee left subway entrance right Best Buy Mobile left Yankees right bus stop left bus stop left light right The Michelangelo left TGI Fridays left Pizza Hut left Bobby Van ’s left park right Men ’s"
2021.emnlp-main.647,D18-2029,0,0.0186085,"sociation for Computational Linguistics first determines an importance ranking over words and then changes each token in order (Word Importance Ranking, WIR in Table 1). Tokens can be changed to one of the 50 nearest neighbours in embedding space but are subject to constraints. The replacement token must have a cosine similarity of at least 0.7 in the attack’s embedding space, again in the counter-fitted GloVe embedding space by Mrkši´c et al. (2016). Additionally, replacement tokens must have the same part of speech and a sentence level similarity score given by a Universal Sentence Encoder (Cer et al., 2018) above a given threshold. The word importance ranking is determined by the difference in output when a given token is removed from the input. The above black-box attacks on NLP systems along with greedy and beam search are reimplemented by Morris et al. (2020). Their library, TextAttack, seeks to be a test bed for comparing adversarial attacks on NLP systems. To this end, a companion paper comparing different attacks has been released by Yoo et al. (2020). 3 Zeroth Order Optimization-Based Attack (ZOO) scent: n xt+1 1 X f (x + µui ) − f (x) ui . = xt − λ n µ i=1 Inspired by the zeroth order op"
2021.emnlp-main.647,2020.blackboxnlp-1.30,0,0.376051,"substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks. the search for adversarial examples by approximate gradient information can be transferred to adversarial attacks in NLP. To this end, we implemented a ZOO-inspired algorithm in the TextAttack framework (Morris et al., 2020) and compared it with their benchmark results (Yoo et al., 2020). Surprisingly, we found that search guided by a ZOO approach only yields minimal improvements, whereas heuristic search exploiting nearest neighbors yields competitive success rates at minimal query count. We conclude from these results that the victim models in current benchmark tasks are too easily broken and imperceptibility constraints are too strict, thus defeating meaningful research on black-box search methods for adversarial attacks. 2 Related Work Black-box attacks on NLP systems generally attempt to search over discrete tokens and, instead of bounding the perturbation in Euclidean s"
2021.emnlp-main.647,N19-1423,0,0.0104904,"e also implement a random baseline. Firstly, instead of determining the importance by deleting each token or replacing it with unk, the indices are randomly shuffled and taken as the attack order. Secondly, replacement tokens are chosen randomly as well. In the constr. setup, the random choice is over the Top-N tokens ranked by proximity in 5.3 Target Models and Goal Functions the counter-fitted GloVe embedding space (Mrkši´c et al., 2016), filtered by the constraints. In the un- All of the attacks in this paper target multiclass constr. setup, the same method is used to generate BERT models (Devlin et al., 2019) fine-tuned on the list of tokens, but no filtering is applied. Instead SNLI (Bowman et al., 2015) or Movie Reviews of iterating over them and selecting the token with (Pang and Lee, 2005). Both are provided by the the highest goal function value, the random base- TextAttack library (Morris et al., 2020) and are line samples a single token from the list, inserts used in the comparison done by Yoo et al. (2020). into the sentence, and returns if the token flips the Morris et al. (2020) use a goal function that seeks label along with the goal function value. If the goal to minimize the probabili"
2021.emnlp-main.647,2020.emnlp-demos.16,0,0.0749294,"Missing"
2021.emnlp-main.647,N16-1018,0,0.0423719,"Missing"
2021.emnlp-main.647,P05-1015,0,0.0748456,"order. Secondly, replacement tokens are chosen randomly as well. In the constr. setup, the random choice is over the Top-N tokens ranked by proximity in 5.3 Target Models and Goal Functions the counter-fitted GloVe embedding space (Mrkši´c et al., 2016), filtered by the constraints. In the un- All of the attacks in this paper target multiclass constr. setup, the same method is used to generate BERT models (Devlin et al., 2019) fine-tuned on the list of tokens, but no filtering is applied. Instead SNLI (Bowman et al., 2015) or Movie Reviews of iterating over them and selecting the token with (Pang and Lee, 2005). Both are provided by the the highest goal function value, the random base- TextAttack library (Morris et al., 2020) and are line samples a single token from the list, inserts used in the comparison done by Yoo et al. (2020). into the sentence, and returns if the token flips the Morris et al. (2020) use a goal function that seeks label along with the goal function value. If the goal to minimize the probability of the true label: function values is improved then the token is kept GT extAttack = 1 − f (x)l , and if not it is discarded–the goal function value is returned with the label and does"
2021.emnlp-main.647,D14-1162,0,0.0873201,"Missing"
2021.splurobonlp-1.6,D19-1204,0,0.0469427,"Missing"
2021.splurobonlp-1.6,2020.acl-main.187,0,0.0185237,".5 and δt− = −0.5). It is then possible to maximize the likelihood of the correct parts of the parse by optimizing a weighted supervised learning objective T PP δt log p(y˜t |x, y<t ). (Petrushkov et al., 2018) Related Work Yao et al. (2019) interpret interactive semantic parsing as a slot filling task, and present a hierarchical reinforcement learning model to learn which slots to fill in which order. They claim the automatic production of clarification questions by the agent as a main feature of their approach, however, what is actually used in their work is a set of 4 predefined templates. Elgohary et al. (2020) show an interpretation of the parse that is understandable for laypeople with a template-based approach, and present different approaches to utilize the user response to improve the parser. In their work, the explantion on the parser side is purely templatebased, whereas our work explicitly informs the clarification question by possible sources of parse ambiguities or errors. Considerable effort has been invested in the creation of large datasets for parsing into SQL representations. Yu et al. (2018) created a dataset called Spider which is a complex, cross-domain semantic parsing and text-to"
2021.splurobonlp-1.6,D19-3019,1,0.831182,"long)) inal NLmaps v2 dataset. Conceptually, bars and pubs may not be that different to each other, but OSM advises a strict distinction between bars and pubs 6 . While a pub sells alcohol on premise, a pub also sells food, the athmosphere is more relaxed and the music is quieter compared to a bar. Lastly, ambiguity was introduced because natural language words now map to multiple different OSM tags. This leads to the following data occurrences: location (e.g., Paris) and POI (e.g., cinema) are masked. This results in the dataset described in table 3. 4.2 Semantic Parsing We use the Joey NMT (Kreutzer et al., 2019) as framework to build a baseline parser. The basic Joey NMT architecture is modified to allow for a multi-source setup (see Figure 3 in the appendix) and for learning from markings.7 • shop Off Licenses in Birmingham As evaluation metrics we use exact match ac→ query(area(keyval(’name’,’Birmingham’)), 1 PN curacy, defined as nwr(keyval(’shop’,’alcohol’)), n=1 δ(predicted, gold) of a N qtype(findkey(’shop’))) predicted parse and the gold parse. Furthermore, we report F1 score as harmonic mean of recall, • How many closest Off License from Wall Street in Glasgow defined as the percentage of ful"
2021.splurobonlp-1.6,D18-1195,0,0.0612546,"Missing"
2021.splurobonlp-1.6,N16-1004,0,0.0267855,"Missing"
2021.splurobonlp-1.6,P18-1169,1,0.830781,"predicted parse and the gold parse. Furthermore, we report F1 score as harmonic mean of recall, • How many closest Off License from Wall Street in Glasgow defined as the percentage of fully correct answers → query(around(center(area( divided by the set size, and precision, defined as keyval(’name’,’Glasgow’)), the percentage of correct answers out of the set of nwr(keyval(’name’,’Wall Street’))), search(nwr(keyval(’shop’,’wine’))), answers with non-empty strings. maxdist(DIST A character-based Joey NMT semantic parser is INTOWN), topx(1)),qtype(count)) able to improve the results reported in Lawrence and Riezler (2018) on the dataset without deThe previous examples show that for the same duplication, as shown in Table 1. All results prekeyword ”Off License” both shop=alcohol and sented in the following are relative improvements shop=wine are valid interpretations. over our own baseline parser, reported on the deFinally, since the data was augmented first, and duplicated dataset for which no external baseline is only afterwards split into train, development and available. test sets, there is a lot of overlap between the train and test data. This is problematic because a proper 5 Generation of Clarification Q"
2021.splurobonlp-1.6,P18-2052,0,0.062043,"Missing"
2021.spnlp-1.4,N18-3012,1,0.638164,"th of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions. 1 Introduction When Natural Language Processing (NLP) systems are deployed in production, and interact with users (“the real world”), there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings (Kreutzer et al., 2018a), or collect user clicks (De Bona et al., 2010), or elicit user revisions (Trivedi et al., 2019) to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production. From a machine learning perspective, using interaction logs only for evaluation purposes is a lost op"
2021.spnlp-1.4,N10-1071,1,0.80341,"Missing"
2021.spnlp-1.4,P18-1165,1,0.670387,"th of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions. 1 Introduction When Natural Language Processing (NLP) systems are deployed in production, and interact with users (“the real world”), there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings (Kreutzer et al., 2018a), or collect user clicks (De Bona et al., 2010), or elicit user revisions (Trivedi et al., 2019) to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production. From a machine learning perspective, using interaction logs only for evaluation purposes is a lost op"
2021.spnlp-1.4,D18-1397,0,0.0495273,"data from which machine learning can succeed. 3.1 Deterministic Logging and Off-line Learning In order to not show inferior outputs to users, production NLP systems show the most likely output, which disables the typically crucial exploration component of RL. This effectively results in deterministic logging policies that lack explicit exploration, which makes an application of standard off1 The majority of RL research in NLP has focused on learning from online feedback (Sokolov et al., 2016; He et al., 2016; Li et al., 2016; Bahdanau et al., 2017; Nguyen et al., 2017; Nogueira and Cho, 2017; Lam et al., 2018). 2 The chatbot Tay might be one of the most illustrative examples for what can go wrong (Davis, 2016). When collecting quality judgments from human users in production systems, it would be risky to directly update the model online according to their 38 policy methods for counterfactual learning questionable. For example, techniques such as inverse propensity scoring (Rosenbaum and Rubin, 1983) or weighted importance sampling (Precup et al., 2000; Jiang and Li, 2016; Thomas and Brunskill, 2016), rely on sufficient exploration of the output space by the logging system as a prerequisite for coun"
2021.spnlp-1.4,W16-6634,0,0.0122898,"can be discouraged by employing a reward baseline, P where for example the average reward ∆ = 1t tt0 =1 δt0 is subtracted from each δt . This will cause output sequences worse than the running average to be discouraged rather than encouraged. The second option is to use the logged data Dlog to learn a reward estimator δˆ that can return a reward estimate for any pair (x, y). This estimator together with the IPS objective leads to the Doubly Robust (DR) objective (Dudik et al., 2011), 39 LDR = − raters (Callison-Burch, 2009). Similar observations have been made for other text generation tasks (Godwin and Piwek, 2016; Verberne et al., 2018). Nguyen et al. (2017) illustrated how badly machine translation systems can handle humanlevel noise in direct feedback for online RL with simulations. The level of noise in real-world human feedback may be so high that it prevents learning completely, as for example experienced in ecommerce machine translation logs (Kreutzer et al., 2018a). The issue is even higher in dialogue generation where there are a plenitude of acceptable responses (Pang et al., 2020). To this aim, inverse RL has been proposed to infer reward functions from responses indirectly (Takanobu et al.,"
2021.spnlp-1.4,P19-1358,0,0.0521248,"Missing"
2021.spnlp-1.4,P18-1169,1,0.849198,"n which contexts specific words are more suitable than in others. This has been explored in the context of machine translation (Lawrence et al., 2017b), utilizing the Deterministic Propensity Matching (DPM) objective LDPM = − T 1X δt πθ (˜ yt |xt ), T The first solution is to tune the learning rate and perform early stopping before the degenerate state can be reached. The second solution is to utilize a multiplicative control variate (Kong, 1992) for selfnormalization (Swaminathan and Joachims, 2015). For efficient gradient calculation, batches of size B can be reweighted one-step-late (OSL) (Lawrence and Riezler, 2018) using θ0 from some previous iteration: LOSL = 1 PB δb πθ (˜ yb |xb ) B − 1 Pb=1 . T 0 (˜ π y | x ) t t θ t=1 T (4) Self-normalization discourages increasing the probability of low reward data because this would take away probability mass from higher reward outputs and as a result. This introduces a bias in the estimator (that decreases as T increases), however, it makes learning under deterministic logging feasible, as has been shown for learning with real human feedback in a semantic parsing scenario (Lawrence and Riezler, 2018). This gives the RL agent an edge in learning in an environment"
2021.spnlp-1.4,P16-1153,0,0.0278473,"enarios. We address this and possible solutions in §3.1, while §3.2 focuses on how to obtain reliable data from which machine learning can succeed. 3.1 Deterministic Logging and Off-line Learning In order to not show inferior outputs to users, production NLP systems show the most likely output, which disables the typically crucial exploration component of RL. This effectively results in deterministic logging policies that lack explicit exploration, which makes an application of standard off1 The majority of RL research in NLP has focused on learning from online feedback (Sokolov et al., 2016; He et al., 2016; Li et al., 2016; Bahdanau et al., 2017; Nguyen et al., 2017; Nogueira and Cho, 2017; Lam et al., 2018). 2 The chatbot Tay might be one of the most illustrative examples for what can go wrong (Davis, 2016). When collecting quality judgments from human users in production systems, it would be risky to directly update the model online according to their 38 policy methods for counterfactual learning questionable. For example, techniques such as inverse propensity scoring (Rosenbaum and Rubin, 1983) or weighted importance sampling (Precup et al., 2000; Jiang and Li, 2016; Thomas and Brunskill, 20"
2021.spnlp-1.4,D17-1272,1,0.939342,"ctual learning. In fact, Langford et al. (2008) and Strehl et al. (2010) even give impossibility results for exploration-free counterfactual learning. One option is to hope for implicit exploration due to input or context variability. This has been observed for the case of online advertising (Chapelle and Li, 2011) and investigated theoretically (Bastani et al., 2017). In NLP, output sequences may overlap in some of the words, so the learner could infer from rewards in which contexts specific words are more suitable than in others. This has been explored in the context of machine translation (Lawrence et al., 2017b), utilizing the Deterministic Propensity Matching (DPM) objective LDPM = − T 1X δt πθ (˜ yt |xt ), T The first solution is to tune the learning rate and perform early stopping before the degenerate state can be reached. The second solution is to utilize a multiplicative control variate (Kong, 1992) for selfnormalization (Swaminathan and Joachims, 2015). For efficient gradient calculation, batches of size B can be reweighted one-step-late (OSL) (Lawrence and Riezler, 2018) using θ0 from some previous iteration: LOSL = 1 PB δb πθ (˜ yb |xb ) B − 1 Pb=1 . T 0 (˜ π y | x ) t t θ t=1 T (4) Self-n"
2021.spnlp-1.4,D16-1127,0,0.0126204,"ss this and possible solutions in §3.1, while §3.2 focuses on how to obtain reliable data from which machine learning can succeed. 3.1 Deterministic Logging and Off-line Learning In order to not show inferior outputs to users, production NLP systems show the most likely output, which disables the typically crucial exploration component of RL. This effectively results in deterministic logging policies that lack explicit exploration, which makes an application of standard off1 The majority of RL research in NLP has focused on learning from online feedback (Sokolov et al., 2016; He et al., 2016; Li et al., 2016; Bahdanau et al., 2017; Nguyen et al., 2017; Nogueira and Cho, 2017; Lam et al., 2018). 2 The chatbot Tay might be one of the most illustrative examples for what can go wrong (Davis, 2016). When collecting quality judgments from human users in production systems, it would be risky to directly update the model online according to their 38 policy methods for counterfactual learning questionable. For example, techniques such as inverse propensity scoring (Rosenbaum and Rubin, 1983) or weighted importance sampling (Precup et al., 2000; Jiang and Li, 2016; Thomas and Brunskill, 2016), rely on suff"
2021.spnlp-1.4,D17-1153,0,0.043931,"Missing"
2021.spnlp-1.4,D17-1061,0,0.0288048,"how to obtain reliable data from which machine learning can succeed. 3.1 Deterministic Logging and Off-line Learning In order to not show inferior outputs to users, production NLP systems show the most likely output, which disables the typically crucial exploration component of RL. This effectively results in deterministic logging policies that lack explicit exploration, which makes an application of standard off1 The majority of RL research in NLP has focused on learning from online feedback (Sokolov et al., 2016; He et al., 2016; Li et al., 2016; Bahdanau et al., 2017; Nguyen et al., 2017; Nogueira and Cho, 2017; Lam et al., 2018). 2 The chatbot Tay might be one of the most illustrative examples for what can go wrong (Davis, 2016). When collecting quality judgments from human users in production systems, it would be risky to directly update the model online according to their 38 policy methods for counterfactual learning questionable. For example, techniques such as inverse propensity scoring (Rosenbaum and Rubin, 1983) or weighted importance sampling (Precup et al., 2000; Jiang and Li, 2016; Thomas and Brunskill, 2016), rely on sufficient exploration of the output space by the logging system as a pr"
2021.spnlp-1.4,2020.acl-main.333,0,0.0125292,"LDR = − raters (Callison-Burch, 2009). Similar observations have been made for other text generation tasks (Godwin and Piwek, 2016; Verberne et al., 2018). Nguyen et al. (2017) illustrated how badly machine translation systems can handle humanlevel noise in direct feedback for online RL with simulations. The level of noise in real-world human feedback may be so high that it prevents learning completely, as for example experienced in ecommerce machine translation logs (Kreutzer et al., 2018a). The issue is even higher in dialogue generation where there are a plenitude of acceptable responses (Pang et al., 2020). To this aim, inverse RL has been proposed to infer reward functions from responses indirectly (Takanobu et al., 2019). T 1 Xh ˆ t, y ˜ t )) πθ (˜ (δt − δ(x yt |xt )+ T t=1 i X ˆ t, y ˜ 0 ) πθ (˜ δ(x y 0 |xt ) . ˜ 0 ∼πθ (˜ y y|xt ) This objective enables the exploration of other ˜ 0 that are not part of the original log and outputs y encourages them based on the reward value returned by the estimator. For the task of machine translation, Lawrence et al. (2017b) show this objective to be the most successful in their setup, and Kreutzer et al. (2018a) report simulation results that show that th"
2021.spnlp-1.4,2003.mtsummit-papers.51,0,0.141402,"dits” in the context of machine translation) as a negative signal, or recording whether the output is copied and/or shared without changes, which may be interpreted as a positive signal. However, the signal might be noisy, since the notion of output quality for natural language generation tasks is not a well-defined function to start with: Each input might have many possible valid outputs, each of which humans may judge differently, depending on many contextual and personal factors. In machine translation evaluation for instance, inter-rater agreements have traditionally been reported as low (Turian et al., 2003; Carl et al., 2011; Lommel et al., 2014), especially when quality estimates are collected from non-professional When it comes to the question which type of human feedback is most beneficial for training an RL agent, one finds a lot of blanket statements, e.g., referring to the advantages of pairwise comparisons (Thurstone, 1927). For instance, learning from human pairwise preferences from humans has been advertised for summarization (Christiano et al., 2017; Stiennon et al., 2020) and language modeling (Ziegler et al., 2019), but the reliability of the signal has not been evaluated. An except"
2021.spnlp-1.4,D19-1010,0,0.0274992,"and Piwek, 2016; Verberne et al., 2018). Nguyen et al. (2017) illustrated how badly machine translation systems can handle humanlevel noise in direct feedback for online RL with simulations. The level of noise in real-world human feedback may be so high that it prevents learning completely, as for example experienced in ecommerce machine translation logs (Kreutzer et al., 2018a). The issue is even higher in dialogue generation where there are a plenitude of acceptable responses (Pang et al., 2020). To this aim, inverse RL has been proposed to infer reward functions from responses indirectly (Takanobu et al., 2019). T 1 Xh ˆ t, y ˜ t )) πθ (˜ (δt − δ(x yt |xt )+ T t=1 i X ˆ t, y ˜ 0 ) πθ (˜ δ(x y 0 |xt ) . ˜ 0 ∼πθ (˜ y y|xt ) This objective enables the exploration of other ˜ 0 that are not part of the original log and outputs y encourages them based on the reward value returned by the estimator. For the task of machine translation, Lawrence et al. (2017b) show this objective to be the most successful in their setup, and Kreutzer et al. (2018a) report simulation results that show that this objective can significantly reduce the gap between offline and online policy learning, even if the reward estimator"
A00-2021,P99-1069,1,0.895577,"stochastic versions of HPSGs, categorial grammars and transformational grammars. 1 Introduction ""Unification-based"" Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999). Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus. Unfortunately, large parsed UBG corpora are not yet available. This restricts the kinds of models one can realistically expect to be able to estimate. For example, a model incorporating lexical selectional preferences of the kind * This research was supported by NSF awards 9720368, 9870676 and 9812169. 154 described below might have tens or hundreds of thousands of parameters, which one could not reasonably a t t e m p t to estimate from a corpus with on the"
A00-2021,P99-1014,1,0.822777,"ish National Corpus (Carroll and Rooth, 1998). We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always O B J for prepositional governors, and r is either SUBJ or OBJ for verbal governors). In order to avoid sparse data problems we smoothed this distribution over tuples as described in (Rooth et al., 1999). We assume that governor-relation pairs (g, r) and arguments a are independently generated from 25 hidden classes C, i.e.: I'Ik=lQJ(w)A~+J eZ_,~=lAjlj(~)(6 ) v - - ~ P((g,r,a)) = ~'~ Pe((g,r)lc)~)e(alc)ee(c) cEC Note that the auxiliary distributions Qj are treated as fixed distributions for the purposes of this estimation, even though each Qj may itself be a complex model obtained via a previous estimation process. Comparing (6) with (1) on page 2, we see that the two equations become identical if the reference distribution Q in (1) is replaced by a geometric mixture of the auxiliary distribu"
A00-2021,W98-1505,0,\N,Missing
A00-2021,J97-4005,0,\N,Missing
C00-2094,W98-1505,1,0.758778,"step was performed. Given a latent class model pLC ( ) for verb-noun pairs, and a sample n1 ; : : : ; nM of objects for a xed transitive verb, we calculate the probabilityPof an arbitrary object n N by p(n) = c C p(c; n) = Pc C pnoun (c)pLC (n c): This ne-tuning of the class parameters p(c) to the sample of objects for a xed verb is formalized again as a simple instance of the EM algorithm. In an experiment with English data, we used a clustering model with 35 classes. From the maximum probabil 2 2 j 2 ity parses derived for the British National Corpus with the head-lexicalized parser of Carroll and Rooth (1998), we extracted frequency tables for transitive verb-noun pairs. These tables were used to induce a small class-labeled lexicon (336 verbs). cross.aso:o 19 0.692 mind 74.2 road 30.3 line 28.1 bridge 27.5 room 20.5 border 17.8 boundary 16.2 river 14.6 street 11.5 atlantic 9.9 mobilize.aso:o 6 0.386 force 2.00 people 1.95 army 1.46 sector 0.90 society 0.90 worker 0.90 member 0.88 company 0.86 majority 0.85 party 0.80 Figure 2: Estimated frequencies of the objects of the transitive verbs cross and mobilize Fig. 2 shows the topmost parts of the lexical entries for the transitive verbs cross and mob"
C00-2094,J94-4003,0,0.0254214,"bilities, and at left are the 30 most probable verbs in the pLC (v 19) distribution. 19 is the class index. Those verb-noun pairs which were seen in the training data appear with a dot in the class matrix. Verbs with sux :as : s indicate the subject slot of an active intransitive. Similarily :aso : s denotes the subject slot of an active transitive, and :aso : o denotes the object slot of an active transitive. j j semantics required by the target-verb. We evaluated this simple method on a large number of real-world translations and got results comparable to related approaches such as that of Dagan and Itai (1994) where much more selectional information is used. 2 Lexicon Induction via EM-Based Clustering 2.1 EM-Based Clustering For clustering, we used the method described in Rooth et al. (1999). There classes are derived from distributional dataa sample of pairs of verbs and nouns, gathered by parsing an unannotated corpus and extracting the llers of grammatical relations. The semantically smoothed probability of a pair (v; n) is calculated class (LC) model as pLC (v; n) = Pc CinpLCa latent (c; v; n). The joint distribution is dened by pLC (c; v; n) = pLC (c)pLC (v c)pLC (n c). By construction, con"
C00-2094,W97-0209,0,0.116676,"Missing"
C00-2094,P99-1014,1,0.839824,"the class matrix. Verbs with sux :as : s indicate the subject slot of an active intransitive. Similarily :aso : s denotes the subject slot of an active transitive, and :aso : o denotes the object slot of an active transitive. j j semantics required by the target-verb. We evaluated this simple method on a large number of real-world translations and got results comparable to related approaches such as that of Dagan and Itai (1994) where much more selectional information is used. 2 Lexicon Induction via EM-Based Clustering 2.1 EM-Based Clustering For clustering, we used the method described in Rooth et al. (1999). There classes are derived from distributional dataa sample of pairs of verbs and nouns, gathered by parsing an unannotated corpus and extracting the llers of grammatical relations. The semantically smoothed probability of a pair (v; n) is calculated class (LC) model as pLC (v; n) = Pc CinpLCa latent (c; v; n). The joint distribution is dened by pLC (c; v; n) = pLC (c)pLC (v c)pLC (n c). By construction, conditioning of v and n on each other is solely made through the classes c. The parameters pLC (c), pLC (v c), pLC (n c) are estimated by a particularily simple version of the EM algorithm"
C00-2094,P95-1026,0,0.0295012,"an build upon a resolution of the related lexical ambiguities. Statistical approaches have been applied successfully to these problems. The great advantage of statistical methods over symbolic-linguistic methods has been deemed to be their eective exploitation of minimal linguistic knowledge. However, the best performing statistical approaches to lexical ambiguity resolution themselves rely on complex information sources such as lemmas, inected forms, parts of speech and arbitrary word classes [ : : : ] local and distant collocations, trigram sequences, and predicate argument association (Yarowsky (1995), p. 190) or large context-windows up to 1000 neighboring words (Schütze, 1992). Unfortunately, in many applications such information is not readily available. For instance, in incremental machine translation, it may be desirable to decide for the most probable translation of the arguments of a verb with only the translation of the verb as information source but no large window of surrounding translations available. In parsing, the attachment of a nominal head may have to be resolved with only information about the semantic roles of the verb but no other predicate argument associations at hand"
C00-2094,kilgarriff-rosenzweig-2000-english,0,\N,Missing
C08-1093,D07-1090,0,0.00692721,"is linked by a hidden alignment variable a = aJ1 , the optimal θˆ is found using unlabeleddata log-likelihood estimation techniques such as θˆ = arg max θ S X Y pθ (fs , a|es ) (6) s=1 a The final translation model is calculated from relative frequencies of phrases, i.e. consecutive sequences of words occurring in text. Phrases are extracted via various heuristics as larger blocks of aligned words from best word alignments, as described in Och and Ney (2004). Language modeling in our approach deploys an n-gram language model that assigns the following probability to a string w1L of words (see Brants et al. (2007)): P (w1L ) = ≈ L Y i=1 L Y i=1 P (wi |w1i−1 ) (7) i−1 P (wi |wi−n+1 ) (8) Estimation of n-gram probabilities is done by counting relative frequencies of n-grams in a corpus of user queries. Remedies against sparse data problems are achieved by various smoothing techniques, as described in Brants et al. (2007). For applications of the system to translate unseen queries, a standard dynamic-programming beam-search decoder (Och and Ney, 2004) that tightly integrates translation model and language model is used. Expansion terms are taken from those terms in the 5-best translations of the query tha"
C08-1093,J93-2003,0,0.0079883,"sparsity will be overcome by counting from huge datasets. The only attempt at smoothing that is made in this approach is a recurrence to words in query context, using equation 2, when equation 1 assigns zero probability to unseen pairs. 738 3 Query-Snippet Translation the EM algorithm (Dempster et al., 1977): The SMT system deployed in our approach is an implementation of the alignment template approach of Och and Ney (Och and Ney, 2004). The basic features of the model consist of a translation model and a language model which go back to the noisy channel formulation of machine translation in Brown et al. (1993). Their “fundamental equation of machine translation” defines the job of a ˆ translation system as finding the English string e that is a translation of a foreign string f such that ˆ = arg max P (e|f ) e e = arg max P (f |e)P (e) e (3) Equation 3 allows for a separation of a language model P (e), and a translation model P (f |e). Och and Ney (2004) reformulate equation 3 as a linear combination of feature functions hm (e, f ) and weights λm , including feature functions for translation models hi (e, f ) = P (f |e) and language models hj (e) = P (e): ˆ = arg max e e M X λm hm (e, f ) (4) m=1 T"
C08-1093,P03-1003,0,0.118956,"terms to query terms is required to ignore context in order to generalize, finding appropriate expansions for ambiguous query terms is difficult. Our approach is to look at the “word mismatch” problem as a problem of translating from a source language of queries into a target language of documents, represented as snippets. Since both queries and snippets are arguably natural language, statistical machine translation technology (SMT) is readily applicable to this task. In previous work, this has been done successfully for question answering tasks (Riezler et al., 2007; Soricut and Brill, 2006; Echihabi and Marcu, 2003; Berger et al., 2000), but not for web search in general. Cui et al.’s (2002) model is to our knowledge the first to deploy query-document relations for direct extraction of expansion terms for general web retrieval. Our SMT approach has two main advantages over Cui et al.’s model: Firstly, Cui et al.’s model relates document terms to query terms by using simple term frequency counts in session data, without considering smoothing techniques. Our approach deploys a sophisticated machine learning approach to word alignment, including smoothing techniques, to map query phrases to snippet phrases"
C08-1093,J03-1002,0,0.00268597,"inding the English string e that is a translation of a foreign string f such that ˆ = arg max P (e|f ) e e = arg max P (f |e)P (e) e (3) Equation 3 allows for a separation of a language model P (e), and a translation model P (f |e). Och and Ney (2004) reformulate equation 3 as a linear combination of feature functions hm (e, f ) and weights λm , including feature functions for translation models hi (e, f ) = P (f |e) and language models hj (e) = P (e): ˆ = arg max e e M X λm hm (e, f ) (4) m=1 The translation model used in our approach is based on the sequence of alignment models described in Och and Ney (2003). The relationship of translation model and alignment model for source language string f = f1J and target string e = eI1 is via a hidden variable describing an alignment mapping from source position j to target position aj : P (f1J |eI1 ) = X P (f1J , aJ1 |eI1 ) (5) aJ 1 The alignment aJ1 contains so-called null-word alignments aj = 0 that align source words to the empty word. The different alignment models described in Och and Ney (2003) each parameterize equation 5 differently so as to capture different properties of source and target mappings. All models are based on estimating parameters θ"
C08-1093,P07-1059,1,\N,Missing
C08-1093,J04-4002,0,\N,Missing
C08-1093,N06-1032,1,\N,Missing
C16-1297,P11-2031,0,0.0430832,"Missing"
C16-1297,P10-4002,0,0.0204651,"tem Our baseline English-German translation system is trained on 2.1 million sentence pairs (61/59 million English/German tokens) from the Europarl v73 corpus (1.78 million sentence pairs), the News Commentary v104 corpus (200K sentence pairs) and the MultiUN v15 corpus (150K sentence pairs). Word alignments are computed using MGIZA++6 , alignments are symmetrized using the grow-diag-final-end heuristic. A 4-gram count-based language model is estimated from the target side of the training data using lmplz (Heafield et al., 2013). All experiments use the hierarchical phrase-based decoder cdec (Dyer et al., 2010). Hierarchical phrase rules are extracted using cdec’s implementation of the suffix array extractor by Lopez (2007) with default settings. Our baselines use 21 decoder features (7 translation model features, 2 language model features, 7 pass through features, 3 arity penalty features, word penalty and glue rule count features), which are implemented in cdec. Feature weights are optimized on the WMT Newstest 2014 data set (3003 sentence pairs) using the pairwise ranking optimizer dtrain7 . We run dtrain for 15 epochs with the hyperparameters k-best size=100, loss-margin=1, and a learning rate o"
C16-1297,eisele-chen-2010-multiun,0,0.333865,"Missing"
C16-1297,W07-0717,0,0.0310615,"are learned on a small parallel data set from the domain or genre of interest. However, while many multilingual data sets, especially in the realm of user-generated data, contain document-level links, sentence-parallel training data are not always available. A small number of sentences can be manually translated for in-domain parameter tuning, but this ignores most of the available multilingual resource. Monolingual language model adaptation via concatenation or interpolation is one viable solution which makes use of the target side part of a collection (see e.g. Koehn and Schroeder (2007) or Foster and Kuhn (2007)). Additionally, there are several approaches to automatic parallel data extraction from cross-lingual document-level links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cross-lingual document-level"
C16-1297,N12-1023,0,0.514215,"vel links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cross-lingual document-level relevance annotation. We do so by re-formulating the structured ramp loss objective proposed by Chiang (2012) and Gimpel and Smith (2012) to incorporate graded and negative cross-lingual relevance signals. Using translation of Wikipedia entries as a running example, we evaluate the efficacy of our method along with the traditional approaches on a manually created in-domain test set. We show that our method is able to produce small, but significant, gains, even if only a weak relevance signal is used. Section 2 explains our learning objective and cost function. In Section 3 we describe the construction of our training and evaluation data, including pseudo-parallel data extraction. Section 4 contains details of our experimental s"
C16-1297,P13-2121,0,0.0140247,"erable percentage of parallel sentences. 4 Experiments 4.1 Out-of-domain translation system Our baseline English-German translation system is trained on 2.1 million sentence pairs (61/59 million English/German tokens) from the Europarl v73 corpus (1.78 million sentence pairs), the News Commentary v104 corpus (200K sentence pairs) and the MultiUN v15 corpus (150K sentence pairs). Word alignments are computed using MGIZA++6 , alignments are symmetrized using the grow-diag-final-end heuristic. A 4-gram count-based language model is estimated from the target side of the training data using lmplz (Heafield et al., 2013). All experiments use the hierarchical phrase-based decoder cdec (Dyer et al., 2010). Hierarchical phrase rules are extracted using cdec’s implementation of the suffix array extractor by Lopez (2007) with default settings. Our baselines use 21 decoder features (7 translation model features, 2 language model features, 7 pass through features, 3 arity penalty features, word penalty and glue rule count features), which are implemented in cdec. Feature weights are optimized on the WMT Newstest 2014 data set (3003 sentence pairs) using the pairwise ranking optimizer dtrain7 . We run dtrain for 15 e"
C16-1297,W07-0733,0,0.00942637,", parameters of an SMT system are learned on a small parallel data set from the domain or genre of interest. However, while many multilingual data sets, especially in the realm of user-generated data, contain document-level links, sentence-parallel training data are not always available. A small number of sentences can be manually translated for in-domain parameter tuning, but this ignores most of the available multilingual resource. Monolingual language model adaptation via concatenation or interpolation is one viable solution which makes use of the target side part of a collection (see e.g. Koehn and Schroeder (2007) or Foster and Kuhn (2007)). Additionally, there are several approaches to automatic parallel data extraction from cross-lingual document-level links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cro"
C16-1297,2005.mtsummit-papers.11,0,0.129836,"Missing"
C16-1297,D07-1104,0,0.013518,"tokens) from the Europarl v73 corpus (1.78 million sentence pairs), the News Commentary v104 corpus (200K sentence pairs) and the MultiUN v15 corpus (150K sentence pairs). Word alignments are computed using MGIZA++6 , alignments are symmetrized using the grow-diag-final-end heuristic. A 4-gram count-based language model is estimated from the target side of the training data using lmplz (Heafield et al., 2013). All experiments use the hierarchical phrase-based decoder cdec (Dyer et al., 2010). Hierarchical phrase rules are extracted using cdec’s implementation of the suffix array extractor by Lopez (2007) with default settings. Our baselines use 21 decoder features (7 translation model features, 2 language model features, 7 pass through features, 3 arity penalty features, word penalty and glue rule count features), which are implemented in cdec. Feature weights are optimized on the WMT Newstest 2014 data set (3003 sentence pairs) using the pairwise ranking optimizer dtrain7 . We run dtrain for 15 epochs with the hyperparameters k-best size=100, loss-margin=1, and a learning rate of 1e−5 . The final weights are averaged over all epochs. Performance of our baseline system (baseline 1) is given i"
C16-1297,W15-1521,0,0.0280975,"from a set of relevant documents D+ and d− from a set of “contrast documents”, D− , according to some cross-lingual relevance signal. In our experiments, we first use random sampling. We also try out a weighted sampling strategy, if the relevance signal is weaker. In this case, we want to sample a document more frequently from D+ , if it is more similar to the input document. We calculate cross-lingual document similarity by using document representations from bilingual word embeddings. The embeddings are learned from the aligned parallel training corpus using the Bilingual Skip-gram model of Luong et al. (2015).1 Document representations are computed by averaging over all word representations in the document, weighted by the inverse document frequencies of the words. Cosine similarity is used to measure similarity between the current source document and the documents in D+ . We use weighted reservoir sampling (Efraimidis and Spirakis, 2006) to draw a document weighted by its similarity to the current source document. The contrast document d− is drawn randomly from D− , but is re-drawn if d− is more similar to the input then d+ . Search. In lines 7 and 8 we identify the “good” and “bad” hypotheses h+"
C16-1297,J05-4003,0,0.0511297,"generated data, contain document-level links, sentence-parallel training data are not always available. A small number of sentences can be manually translated for in-domain parameter tuning, but this ignores most of the available multilingual resource. Monolingual language model adaptation via concatenation or interpolation is one viable solution which makes use of the target side part of a collection (see e.g. Koehn and Schroeder (2007) or Foster and Kuhn (2007)). Additionally, there are several approaches to automatic parallel data extraction from cross-lingual document-level links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cross-lingual document-level relevance annotation. We do so by re-formulating the structured ramp loss objective proposed by Chiang (2012) and Gimpel and Smith (2012) to incorporate grade"
C16-1297,P14-2080,1,0.754506,"Missing"
C16-1297,P12-1002,1,0.93573,"s identical to the original structured ramp loss (Equation 1), but still allows to include positive and negative relevance signals via the cost function. We apply a linear scaling operation to squash our new cost function to return values between 0 and 1. 2.2 Implementation and learning Parallelized stochastic subgradient descent. Algorithm 1 shows our learning procedure. Optimization is done using stochastic subgradient descent (SSD) as proposed for ramp loss by Keshet and McAllester (2011). In order to be able to train on thousands of documents, we use the method described in Algorithm 4 of Simianer et al. (2012), which splits training data into shards (line 1 in Algorithm 1), trains one epoch on each shard (line 3 to 12), and then applies feature selection by `1 /`2 regularization (line 13) before starting the next epoch. Sampling. For each training example, we first sample a document pair (d+ , d− ) (line 6). The sample() procedure draws documents d+ from a set of relevant documents D+ and d− from a set of “contrast documents”, D− , according to some cross-lingual relevance signal. In our experiments, we first use random sampling. We also try out a weighted sampling strategy, if the relevance signal"
C16-1297,P13-1135,0,0.0129419,"nces can be manually translated for in-domain parameter tuning, but this ignores most of the available multilingual resource. Monolingual language model adaptation via concatenation or interpolation is one viable solution which makes use of the target side part of a collection (see e.g. Koehn and Schroeder (2007) or Foster and Kuhn (2007)). Additionally, there are several approaches to automatic parallel data extraction from cross-lingual document-level links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cross-lingual document-level relevance annotation. We do so by re-formulating the structured ramp loss objective proposed by Chiang (2012) and Gimpel and Smith (2012) to incorporate graded and negative cross-lingual relevance signals. Using translation of Wikipedia entries as a running example, we evaluate"
C16-1297,2015.iwslt-papers.1,0,0.10932,"ot always available. A small number of sentences can be manually translated for in-domain parameter tuning, but this ignores most of the available multilingual resource. Monolingual language model adaptation via concatenation or interpolation is one viable solution which makes use of the target side part of a collection (see e.g. Koehn and Schroeder (2007) or Foster and Kuhn (2007)). Additionally, there are several approaches to automatic parallel data extraction from cross-lingual document-level links, such as Munteanu and Marcu (2005)’s work on news data, or more recent work on Wikipedia by Wołk and Marasek (2015), and on websites by Smith et al. (2013). We argue that these approaches work well if the cross-lingual links are a strong signal for parallelism, but fail if the signal linking documents across languages is weaker. We propose a method for tuning sparse lexicalized features on large amounts of multilingual data which contain some cross-lingual document-level relevance annotation. We do so by re-formulating the structured ramp loss objective proposed by Chiang (2012) and Gimpel and Smith (2012) to incorporate graded and negative cross-lingual relevance signals. Using translation of Wikipedia en"
C16-2002,P13-2009,0,0.0185604,"tion”. This will ask the Geolocation API [http://www.w3schools.com/html/html5_geolocation.asp] to locate the user’s device. Of course, the user is first asked for permission. Once the user’s GPS location has been determined, Nominatim’s reverse geocoding feature can provide the name of the city in which the GPS coordinates lie. Semantic Parsing. After these preprocessing steps have been accomplished, the question is sent to a semantic parser. The parser employed here is a SMT system that translates from natural language to a machine readable language (MRL), following an approach introduced by Andreas et al. (2013). A MRL for the OSM domain as well as a corpus, NL MAPS, containing 2,380 questionMRL pairs was introduced by Haas and Riezler (2016) [http://www.cl.uni-heidelberg.de/ statnlpgroup/nlmaps/]. Using this corpus, split into 1,500 training examples and 880 test examples, a SMT system can be trained, using GIZA++ (Och and Ney, 2003) and the SMT framework CDEC (Dyer et al., 2010), including a MERT run. Finally, the MRL returned by the SMT system is executed against the OSM database using an extension of the Overpass API, OVERPASS NL MAPS [https://github.com/carhaas/overpass-nlmaps]. All relevant inf"
C16-2002,P10-4002,0,0.0132602,"have been accomplished, the question is sent to a semantic parser. The parser employed here is a SMT system that translates from natural language to a machine readable language (MRL), following an approach introduced by Andreas et al. (2013). A MRL for the OSM domain as well as a corpus, NL MAPS, containing 2,380 questionMRL pairs was introduced by Haas and Riezler (2016) [http://www.cl.uni-heidelberg.de/ statnlpgroup/nlmaps/]. Using this corpus, split into 1,500 training examples and 880 test examples, a SMT system can be trained, using GIZA++ (Och and Ney, 2003) and the SMT framework CDEC (Dyer et al., 2010), including a MERT run. Finally, the MRL returned by the SMT system is executed against the OSM database using an extension of the Overpass API, OVERPASS NL MAPS [https://github.com/carhaas/overpass-nlmaps]. All relevant information is collected from the returned database objects and is then compiled into 2 different output formats. Answer Presentation. Answers are returned in the interface’s answer box in text format. Additionally, the output, formatted in GeoJSON, is used to place markers in the appropriate GPS locations on an interactive map. If a marker is clicked, this output supplies fur"
C16-2002,N16-1088,1,0.918871,"se, the user is first asked for permission. Once the user’s GPS location has been determined, Nominatim’s reverse geocoding feature can provide the name of the city in which the GPS coordinates lie. Semantic Parsing. After these preprocessing steps have been accomplished, the question is sent to a semantic parser. The parser employed here is a SMT system that translates from natural language to a machine readable language (MRL), following an approach introduced by Andreas et al. (2013). A MRL for the OSM domain as well as a corpus, NL MAPS, containing 2,380 questionMRL pairs was introduced by Haas and Riezler (2016) [http://www.cl.uni-heidelberg.de/ statnlpgroup/nlmaps/]. Using this corpus, split into 1,500 training examples and 880 test examples, a SMT system can be trained, using GIZA++ (Och and Ney, 2003) and the SMT framework CDEC (Dyer et al., 2010), including a MERT run. Finally, the MRL returned by the SMT system is executed against the OSM database using an extension of the Overpass API, OVERPASS NL MAPS [https://github.com/carhaas/overpass-nlmaps]. All relevant information is collected from the returned database objects and is then compiled into 2 different output formats. Answer Presentation. A"
C16-2002,D13-1161,0,0.0348053,"n be used to further improve the parser. However, this type of feedback is by far the hardest to give. A more likely scenario is one, where a user who regularly used the Overpass query language, visits the site and corrects the Overpass query in the feedback form. This would still provide a high quality supervision signal, though only partial. The same holds true for the other feedback questions where the correct output or the 0/1 feedback can be seen as a partial supervision signal of varying degrees of detail. We will use this feedback to test various algorithms for response-based learning (Kwiatowski et al. (2013), Berant et al. (2013), Goldwasser and Roth (2013), Szepesv´ari (2009), Bubeck and Cesa-Bianchi 9 (2012), inter alia) to improve the parser. A new challenge will be to incorporate the different levels of feedback into one algorithm. 5 Conclusion We presented an online interface with which the OSM database can be queried using natural language. While the parser is not yet close to answering every question posed, it already shows promising results. Considering that previously a simple question like “Where are 3 star hotels in Paris” needed detailed knowledge of OSM and the Overpass query languag"
C16-2002,J03-1002,0,0.00594004,"mantic Parsing. After these preprocessing steps have been accomplished, the question is sent to a semantic parser. The parser employed here is a SMT system that translates from natural language to a machine readable language (MRL), following an approach introduced by Andreas et al. (2013). A MRL for the OSM domain as well as a corpus, NL MAPS, containing 2,380 questionMRL pairs was introduced by Haas and Riezler (2016) [http://www.cl.uni-heidelberg.de/ statnlpgroup/nlmaps/]. Using this corpus, split into 1,500 training examples and 880 test examples, a SMT system can be trained, using GIZA++ (Och and Ney, 2003) and the SMT framework CDEC (Dyer et al., 2010), including a MERT run. Finally, the MRL returned by the SMT system is executed against the OSM database using an extension of the Overpass API, OVERPASS NL MAPS [https://github.com/carhaas/overpass-nlmaps]. All relevant information is collected from the returned database objects and is then compiled into 2 different output formats. Answer Presentation. Answers are returned in the interface’s answer box in text format. Additionally, the output, formatted in GeoJSON, is used to place markers in the appropriate GPS locations on an interactive map. I"
C16-2002,P14-1083,1,0.905367,"Missing"
C16-2004,aziz-etal-2012-pet,0,0.0278403,"line experiment, which compared comprehensibility of machine translations, post-edits and human made translations (Orr, 1967). With the rise of statistical MT user studies of CAT have gained considerable traction (Casacuberta et al., 2009; Alabau et al., 2013; Federico et al., 2014). Many different toolkits and user interfaces have been used in these studies, for example graphical interfaces for interactive MT specialized for patent translation (Pouliquen et al., 2011), interfaces for predictive translation memories (Green et al., 2014; Koehn, 2009), tools for monitoring post-editing efforts (Aziz et al., 2012), full workbenches supporting post-editing or interactive MT for translators (Alabau et al., 2013; Federico et al., 2014; Casacuberta et al., 2009), or also test-beds for post-editing (Denkowski, 2015). The latter being most similar to our work, even providing a small user study on potential effects of adaptive MT in post-editing. In most aforementioned interfaces users operate on the string-level and the MT engine is treated as a static black box. Denkowski (2015) is a notable exception, incorporating effective adaptation methods. These, however, also operate only on surface strings, and use"
C16-2004,J07-2003,0,0.447881,"interface that enables efficient and precise resolution of errors in the adaptive MT engine by leveraging user corrected alignments of translation units (e.g. phrases), in conjunction with standard adaptation methods. 3 System overview Our system can be disassembled into two distinct steps: generation of the output of the MT engine3 , and secondly the adaptation step. The first step is described in Figure 1. The second step, which is comprised of updating the models, is described in Section 5. 3 Throughout this paper the engine is assumed to be a hierarchical phrase-based SMT engine following Chiang (2007), with a SCFG as the core of its translation model. 17 Figure 2: Detail of the graphical user interface, translating from English (top row) to German. Phrases are shown as boxes, alignments are displayed with connecting lines. All target phrases may be interactively moved, deleted, or edited, as well as the alignment links. New phrases can also be added to the target side, only the source side and its segmentation are fixed. The active phrase has a bold border, finished phrases have a dark background. 4 Interfaces There are two user interfaces implemented: a standard text interface and a graph"
C16-2004,C14-2028,0,0.0283546,"to the user (6, 7). 2 Related work and motivation Post-editing of MT output is an old idea, going back until the first steps in MT, see e.g. the overview in Koponen (2016). But actual user studies were relatively seldom, as they are expensive to conduct, even more so in the 1960s: The earliest user study to our knowledge describes an offline experiment, which compared comprehensibility of machine translations, post-edits and human made translations (Orr, 1967). With the rise of statistical MT user studies of CAT have gained considerable traction (Casacuberta et al., 2009; Alabau et al., 2013; Federico et al., 2014). Many different toolkits and user interfaces have been used in these studies, for example graphical interfaces for interactive MT specialized for patent translation (Pouliquen et al., 2011), interfaces for predictive translation memories (Green et al., 2014; Koehn, 2009), tools for monitoring post-editing efforts (Aziz et al., 2012), full workbenches supporting post-editing or interactive MT for translators (Alabau et al., 2013; Federico et al., 2014; Casacuberta et al., 2009), or also test-beds for post-editing (Denkowski, 2015). The latter being most similar to our work, even providing a sm"
C16-2004,2009.mtsummit-btm.7,0,0.030332,"of the graphical interface. In a user study we show that using the proposed interface and adaptation methods, reductions in technical effort and time can be achieved. 1 Introduction Since the earliest beginnings of MT research, it has been obvious to many researchers and practitioners that automatic translation is an outstandingly hard problem and may need human participation for sufficient quality. Accordingly, about 70 years later, systems are not (yet) able to produce perfect, or, depending on the domain, comprehensible translations without human intervention. But, as for example shown by Guerberof (2009), the current quality is sufficient to be used in CAT scenarios, i.e. interactive MT or post-editing. CAT has gained more and more interest from the research community in recent years (Tatsumi, 2010; Koponen, 2016), and now (2016), commercial translation system providers implement and successfully use adaptive MT systems in production12 . Most previous studies in CAT were either evaluated by simulating user behavior or did not consider adaptive translation systems. We seek to conduct studies that examine real user behavior in an adaptive environment. Adapting MT systems to specific users can b"
C16-2004,2011.eamt-1.2,0,0.0461149,"studies were relatively seldom, as they are expensive to conduct, even more so in the 1960s: The earliest user study to our knowledge describes an offline experiment, which compared comprehensibility of machine translations, post-edits and human made translations (Orr, 1967). With the rise of statistical MT user studies of CAT have gained considerable traction (Casacuberta et al., 2009; Alabau et al., 2013; Federico et al., 2014). Many different toolkits and user interfaces have been used in these studies, for example graphical interfaces for interactive MT specialized for patent translation (Pouliquen et al., 2011), interfaces for predictive translation memories (Green et al., 2014; Koehn, 2009), tools for monitoring post-editing efforts (Aziz et al., 2012), full workbenches supporting post-editing or interactive MT for translators (Alabau et al., 2013; Federico et al., 2014; Casacuberta et al., 2009), or also test-beds for post-editing (Denkowski, 2015). The latter being most similar to our work, even providing a small user study on potential effects of adaptive MT in post-editing. In most aforementioned interfaces users operate on the string-level and the MT engine is treated as a static black box. De"
C16-2004,2015.mtsummit-papers.15,0,0.0430204,"not only produce a correct translation, but also to make a sensible alignment of source and target. What is shown can differ between MT approaches: In word-based or current neural MT systems we could simply show the (soft) alignments of words. In phrase-based systems, the source and target are segmented into phrases, and for the hierarchical phrase-based paradigm phrases may be discontinuous, resulting in many-to-many alignments. Our approach not only enables usage of richer structures for adaptation, it has already been shown that visualized word-alignment alone can positively affect users (Schwartz et al., 2015). From the user-corrected alignment, the MT engine can explicitly learn native corrections to its translation units, which are very valuable compared to updates that only use the surface data (strings). To evaluate and compare different adaptation approaches, the system collects timing information, as well as the number of clicks and keystrokes, all stages of input and output, and weight and model differentials. 5 Adaptation Our proposed MT engine adaptation partially follows (Denkowski, 2015), using the same adaptive language model and a similar weight adaptation technique, but differs in the"
C16-2004,P12-1002,1,0.840971,"in our implementation is as follows: (1) As shown in Figure 1, OOV is avoided by asking the user for translations of unknown words prior to decoding. (2) After post-editing a sentence, the phrase-segmented and aligned post-edit is compared to the initial machine translation, and lexical corrections and new rules are immediately added to the current grammar. (3) The source is then re-decoded with the augmented grammar, generating a k-best list which is re-ranked by BLEU+1 using the post-edit as reference translation. Weights are updated based on this list with pairwise ranking as described in Simianer et al. (2012). (4) Additionally, we implemented a rule extraction that closely follows the grammar extraction described in Chiang (2007), but instead of words it is using full phrases from the phrase alignment. This results in a large number of additional rules, as all rules (also with gaps) that are compatible with the phrase alignment are extracted for each post-edit provided by the user. To prevent overfitting4 , extracted rules are added to the system in a leave-one-out fashion (i.e. only to subsequent grammars). (5) Before the translation of the next sentence the adaptive language model described by D"
C16-2004,E12-1083,1,0.900495,"Missing"
C16-2004,2012.eamt-1.31,0,\N,Missing
C16-2004,2012.tc-1.5,0,\N,Missing
D13-1175,J07-2003,0,0.046989,"bstantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7 . Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4 https://github.com/redpony/cdec http://www.kyloo.net/software/doku.php/ mgiza:overview 6 http://www.statmt.org/moses/?n=Moses. SupportTools 7 http://kheafield."
D13-1175,J05-1003,0,0.522684,"us, unlike the direct translation approach, we compute weighted term and document frequencies for each sentence s in query F separately. The scoring (3) of a target document for a multiple sentence query then becomes: XX score(E|F ) = BM 25(tf (f, E), df (f )) (4) pnbest (e|f ) is estimated by calculating expectations of term translations from k-best translations: Pn a (e, f )D(k, F ) P k pnbest (e|f ) = Pn k=1 0 k=1 e0 ak (e , f )D(k, F ) 1691 Direct Phrase Table Learning from Relevance Rankings Pairwise Ranking using Boosting The general form of the RankBoost algorithm (Freund et al., 2003; Collins and Koo, 2005) defines a scoring function f (q, d) on query q and document d as a weighted linear combination of T weak learners ht PT such that f (q, d) = t=1 wt ht (q, d). Weak learners can belong to an arbitrary family of functions, but in our case they are restricted to the simplest case of unparameterized indicator functions selecting components of the feature vector φ(q, d) in (1) such that f is of the standard linear form (2). In our experiments, these features indicate the presence of pairs of uni- and bi-grams from the source-side vocabulary of query terms and the target-side vocabulary of document"
D13-1175,P10-4002,0,0.0398097,"2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7 . Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form o"
D13-1175,W11-2123,0,0.0239609,"icit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7 . Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4 https://github.com/redpony/cdec http://www.kyloo.net/software/doku.php/ mgiza:overview 6 http://www.statmt.org/moses/?n=Moses. SupportTools 7 http://kheafield.com/code/kenlm/ estimation/ 5 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard,"
D13-1175,E03-1076,0,0.0545136,"ii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight (2003) to katakana terms, which are often transliterations of English compound words. As these are usually not split by MeCab, they can cause a large number of out-ofvocabulary terms. 9 10 http://research.nii.ac.jp/ntcir/ntcir/ https://code.google.com/p/mecab/ train dev test #queries #relevant #unique docs 107,061 2,000 2,000 1,422,253 26,478 25,173 888,127 25,669 24,668 Table 1: Statistics of ranking data. For the English side of the training data, we applied a modified version of the tokenizer included in the Moses scripts. This tokenizer relies on a list of non-breaking prefixes which mark expres"
D13-1175,D07-1104,0,0.015639,"different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7 . Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4 https:/"
D13-1175,E12-1012,0,0.0547728,"sed approaches improves over 7 MAP points and over 15 PRES points over the respective translation-based system in a consensusbased voting approach following the Borda Count technique (Aslam and Montague, 2001). 2 Related Work Recent research in CLIR follows the two main paradigms of direct translation and probabilistic structured query approaches. An example for the first approach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection e"
D13-1175,P00-1056,0,0.14964,"subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4 https://github.com/redpony/cdec http://www.kyloo.net/software/doku.php/ mgiza:overview 6 http://www.statmt.org/moses/?n=Moses. SupportTools 7 http://kheafield.com/code/kenlm/ estimation/ 5 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translations with respect to the context of the full query. A projection of source language query terms f ∈ F into the target language is achieved by representing each source token f by its probabi"
D13-1175,P03-1021,0,0.0126995,"ches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7 . Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4 https://github.com/redpony/cdec http://www.kyloo.net/software/doku.php/ mgiza:overview 6 http://www.statmt.org/moses/?n=Moses. SupportTools 7 http://kheafield.com/code/kenlm/ estimation/ 5 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation t"
D13-1175,C12-1164,0,0.598244,"Missing"
D13-1175,2007.mtsummit-papers.63,0,0.110971,"o-English patent translation we used data provided by the organizers of the NTCIR9 workshop for the JP-EN PatentMT subtask. In particular, we used the data provided for NTCIR-7 (Fujii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight (2003) to katakana terms, which are often transliterations of English compound words. As these are usually not split by MeCab, they can cause a large number of out-ofvocabulary terms. 9 10 http://research.nii.ac.jp/ntcir/ntcir/ https://code.google.com/p/mecab/ train dev test #queries #relevant #unique docs 107,061 2,000 2,000 1,422,253 26,478 25,173 888,127 25,669 24,668 Table 1: Statistics of ranking data. For the English s"
D13-1175,1998.amta-tutorials.5,0,\N,Missing
D17-1272,P10-4002,0,0.0422429,"he learning experiments. We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (Koehn, 2005), the Common Crawl corpus3 and the News Commentary corpus (Koehn and Schroeder, 2007). The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus (Tiedemann, 2012). A second task uses the French-to-English Europarl data 3 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2570 As baseline, an out-of-domain system is built using the SCFG framework CDEC (Dyer et al., 2010) with dense features (10 standard features and 2 for the language model). After tokenizing and lowercasing the training data, the data were word aligned using CDEC’s fast align. A 4-gram language model is build on the target languages for the out-of-domain data using K EN LM (Heafield et al., 2013). For News, we additionally assume access to in-domain target language text and train another in-domain language model on that data, increasing the number of features to 14 for News. The framework uses a standard linear Gibbs model whose distribution can be peaked using a parameter α (see Equation (6"
D17-1272,P13-2121,0,0.041552,"Missing"
D17-1272,2005.mtsummit-papers.11,0,0.120444,". The algorithms used in policy evaluation and for stochastic-based policy learning are variants of these objectives that replace π ¯ by ρ¯ to yield estimators IPS+R, DR, and cˆ DR of the expected loss. All objectives will be employed in a domain adaptation scenario for machine translation. A system trained on out-of-domain data will be used to collect feedback on in-domain data. This data will serve as the logged data D in the learning experiments. We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (Koehn, 2005), the Common Crawl corpus3 and the News Commentary corpus (Koehn and Schroeder, 2007). The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus (Tiedemann, 2012). A second task uses the French-to-English Europarl data 3 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2570 As baseline, an out-of-domain system is built using the SCFG framework CDEC (Dyer et al., 2010) with dense features (10 standard features and 2 for the language model). After tokenizing and lowercasing the training data, the data were word aligned using CDEC’s"
D17-1272,W07-0733,0,0.247661,"icy learning are variants of these objectives that replace π ¯ by ρ¯ to yield estimators IPS+R, DR, and cˆ DR of the expected loss. All objectives will be employed in a domain adaptation scenario for machine translation. A system trained on out-of-domain data will be used to collect feedback on in-domain data. This data will serve as the logged data D in the learning experiments. We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (Koehn, 2005), the Common Crawl corpus3 and the News Commentary corpus (Koehn and Schroeder, 2007). The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus (Tiedemann, 2012). A second task uses the French-to-English Europarl data 3 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2570 As baseline, an out-of-domain system is built using the SCFG framework CDEC (Dyer et al., 2010) with dense features (10 standard features and 2 for the language model). After tokenizing and lowercasing the training data, the data were word aligned using CDEC’s fast align. A 4-gram language model is build on the target languages for the out-of-"
D17-1272,P17-1138,1,0.65715,"n response prediction for display advertising (Bottou et al., 2013). Similar to the computational advertising scenario, one could imagine a scenario where SMT systems are optimized from partial information in form of user feedback to predicted translations, instead of from manually created reference translations. This learning scenario has been investigated in the areas of bandit learning (Bubeck and CesaBianchi, 2012) or reinforcement learning (RL) (Sutton and Barto, 1998). Figure 1 illustrates the learning protocol using the terminology of bandit structured prediction (Sokolov et al., 2016; Kreutzer et al., 2017), where at each round, a system (corresponding to a policy in RL terms) makes a prediction (also called action in RL, or pulling an arm of a bandit), and receives a reward, which is used to update the system. The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system. A challenge arises by the fact that riskaverse commercial SMT systems deterministically log the most probable translation. The lack of sufficient exploration of t"
D17-1272,C12-1121,0,0.0338582,"Missing"
D17-1272,P03-1021,0,0.0883756,"age text and train another in-domain language model on that data, increasing the number of features to 14 for News. The framework uses a standard linear Gibbs model whose distribution can be peaked using a parameter α (see Equation (6)): Higher value of α will shift the probability of the one-best translation closer to 1 and all others closer to 0. Using α &gt; 1 during training will promote to learn models that are optimal when outputting the one-best translation. In our experiments, we found α = 5 to work well on validation data. Additionally, we tune a system using CDEC’s MERT implementation (Och, 2003) on the indomain data with their references. This fullinformation in-domain system conveys the best possible improvement using the given training data. It can thus be seen as the oracle system for the systems which are learnt using the same input-side training data, but have only bandit feedback available to them as a learning signal. All systems are evaluated using the corpus-level BLEU metric (Papineni et al., 2002). The logged data D is created by translating the in-domain training data of the corpora using macro avg. micro avg. TED News 0.67 15.03 0.23 10.87 Table 3: Evaluation of regressi"
D17-1272,P02-1040,0,0.0977351,"hat are optimal when outputting the one-best translation. In our experiments, we found α = 5 to work well on validation data. Additionally, we tune a system using CDEC’s MERT implementation (Och, 2003) on the indomain data with their references. This fullinformation in-domain system conveys the best possible improvement using the given training data. It can thus be seen as the oracle system for the systems which are learnt using the same input-side training data, but have only bandit feedback available to them as a learning signal. All systems are evaluated using the corpus-level BLEU metric (Papineni et al., 2002). The logged data D is created by translating the in-domain training data of the corpora using macro avg. micro avg. TED News 0.67 15.03 0.23 10.87 Table 3: Evaluation of regression-based reward estimation by average BLEU differences between estimated and true rewards. News TED with the goal of domain adaptation to news articles with the News Commentary corpus (Koehn and Schroeder, 2007). We split off two parts from the TED corpus to be used as validation and test data for the learning experiments. As validation data for the News Commentary corpus we use the splits provided at the WMT shared t"
D17-1272,tiedemann-2012-parallel,0,0.0182013,"jectives will be employed in a domain adaptation scenario for machine translation. A system trained on out-of-domain data will be used to collect feedback on in-domain data. This data will serve as the logged data D in the learning experiments. We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (Koehn, 2005), the Common Crawl corpus3 and the News Commentary corpus (Koehn and Schroeder, 2007). The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus (Tiedemann, 2012). A second task uses the French-to-English Europarl data 3 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2570 As baseline, an out-of-domain system is built using the SCFG framework CDEC (Dyer et al., 2010) with dense features (10 standard features and 2 for the language model). After tokenizing and lowercasing the training data, the data were word aligned using CDEC’s fast align. A 4-gram language model is build on the target languages for the out-of-domain data using K EN LM (Heafield et al., 2013). For News, we additionally assume access to in-domain target language text and"
D19-3019,W18-1820,0,0.146717,"all features that those toolkits have in common. 3 https://joeynmt.readthedocs.io 4 Demo video: https://youtu.be/PzWRWSIwSYc 5 111 Using https://github.com/AlDanial/cloc System Groundhog RNN Best RNN Transformer en-de lv-en layers en-de lv-en en-de lv-en NeuralMonkey OpenNMT-Py Nematus Sockeye Marian Tensor2Tensor 13.7 18.7 23.9 23.2 23.5 – 10.5 10.0 14.3 14.4 14.4 – 1/1 4/4 8/8 4/4 4/4 – 13.7 22.0 23.8 25.6 25.9 – 10.5 13.6 14.7 15.9 16.2 – – – – 27.5 27.4 26.3 – – – 18.1 17.6 17.7 Joey NMT 23.5 14.6 4/4 26.0 15.8 27.4 18.0 Table 2: Results on WMT17 newstest2017. Comparative scores are from Hieber et al. (2018). 3 WMT17. We use the settings of Hieber et al. (2018), using the exact same data, pre-processing, and evaluation using WMT17-compatible SacreBLEU scores (Post, 2018).6 We consider the setting where toolkits are used out-of-the-box to train a Groundhog-like model (1-layer LSTMs, MLP attention), the ‘best found’ setting where Hieber et al. train each model using the best settings that they could find, and the Transformer base setting.7 Table 2 shows that Joey NMT performs very well compared against other shallow, deep and Transformer models, despite its simple code base.8 The target group for J"
D19-3019,W18-1817,0,0.0270216,"e the accessibility of our toolkit in a user study where novices with general knowledge about Pytorch and NMT and experts work through a self-contained Joey NMT tutorial, showing that novices perform almost as well as experts in a subsequent code quiz. Joey NMT is available at https://github. com/joeynmt/joeynmt. 1 riezler@cl.uni-heidelberg.de Introduction Since the first successes of neural machine translation (NMT), various research groups and industry labs have developed open source toolkits specialized for NMT, based on new open source deep learning platforms. While toolkits like OpenNMT (Klein et al., 2018), XNMT (Neubig et al., 2018) and Neural Monkey (Helcl and Libovick´y, 2017) aim at readability and extensibility of their codebase, their target group are researchers with a solid background in machine translation and deep learning, and with experience in navigating, understanding and handling large code bases. However, none of the existing NMT tools has been designed primarily for readability or accessibility for novices, nor has anyone studied quality and accessibility of such code empirically. On the other hand, it is an important challenge for novices to understand how NMT is implemented,"
D19-3019,D15-1166,0,0.462605,"e target vocabulary. Through a softmax transformation, these scores can be interpreted as a probability distribution over the target vocabulary V that defines an index over target tokens vj . attentional models. In the following, a source sentence of length lx is represented by a sequence of one-hot encoded vectors x1 , x2 , . . . , xlx for each word. Analogously, a target sequence of length ly is represented by a sequence of one-hot encoded vectors y1 , y2 , . . . , yly . exp(ot [j]) p(yt = vj |x, y&lt;t ) = P|V| k=1 exp(ot [k]) 2.1.1 RNN Joey NMT implements the RNN encoder-decoder variant from Luong et al. (2015). 2.1.2 Joey NMT implements the Transformer from Vaswani et al. (2017), with code based on The Annotated Transformer blog (Rush, 2018). Encoder. The encoder RNN transforms the input sequence x1 , . . . , xlx into a sequence of vectors h1 , . . . , hlx with the help of the embeddings matrix Esrc and a recurrent computation of states hi = RNN(Esrc xi , hi−1 ); Transformer Encoder. Given an input sequence x1 , . . . , xlx , we look up the word embedding for each input word using Esrc xi , add a position encoding to it, and stack the resulting sequence of word embeddings to form matrix X ∈ Rlx ×d"
D19-3019,W18-1818,0,0.0203815,"toolkit in a user study where novices with general knowledge about Pytorch and NMT and experts work through a self-contained Joey NMT tutorial, showing that novices perform almost as well as experts in a subsequent code quiz. Joey NMT is available at https://github. com/joeynmt/joeynmt. 1 riezler@cl.uni-heidelberg.de Introduction Since the first successes of neural machine translation (NMT), various research groups and industry labs have developed open source toolkits specialized for NMT, based on new open source deep learning platforms. While toolkits like OpenNMT (Klein et al., 2018), XNMT (Neubig et al., 2018) and Neural Monkey (Helcl and Libovick´y, 2017) aim at readability and extensibility of their codebase, their target group are researchers with a solid background in machine translation and deep learning, and with experience in navigating, understanding and handling large code bases. However, none of the existing NMT tools has been designed primarily for readability or accessibility for novices, nor has anyone studied quality and accessibility of such code empirically. On the other hand, it is an important challenge for novices to understand how NMT is implemented, what features each toolkit i"
D19-3019,W18-6319,0,0.0353665,"em Groundhog RNN Best RNN Transformer en-de lv-en layers en-de lv-en en-de lv-en NeuralMonkey OpenNMT-Py Nematus Sockeye Marian Tensor2Tensor 13.7 18.7 23.9 23.2 23.5 – 10.5 10.0 14.3 14.4 14.4 – 1/1 4/4 8/8 4/4 4/4 – 13.7 22.0 23.8 25.6 25.9 – 10.5 13.6 14.7 15.9 16.2 – – – – 27.5 27.4 26.3 – – – 18.1 17.6 17.7 Joey NMT 23.5 14.6 4/4 26.0 15.8 27.4 18.0 Table 2: Results on WMT17 newstest2017. Comparative scores are from Hieber et al. (2018). 3 WMT17. We use the settings of Hieber et al. (2018), using the exact same data, pre-processing, and evaluation using WMT17-compatible SacreBLEU scores (Post, 2018).6 We consider the setting where toolkits are used out-of-the-box to train a Groundhog-like model (1-layer LSTMs, MLP attention), the ‘best found’ setting where Hieber et al. train each model using the best settings that they could find, and the Transformer base setting.7 Table 2 shows that Joey NMT performs very well compared against other shallow, deep and Transformer models, despite its simple code base.8 The target group for Joey NMT are novices who will use NMT in a seminar project, a thesis, or an internship. Common tasks are to re-implement a paper, extend standard models by a small nov"
D19-3019,D16-1137,0,0.146038,"g the best settings that they could find, and the Transformer base setting.7 Table 2 shows that Joey NMT performs very well compared against other shallow, deep and Transformer models, despite its simple code base.8 The target group for Joey NMT are novices who will use NMT in a seminar project, a thesis, or an internship. Common tasks are to re-implement a paper, extend standard models by a small novel element, or to apply them to a new task. In order to evaluate how well novices understand Joey NMT, we conducted a user study comparing the code comprehension of novices and experts. 3.1 de-en Wiseman and Rush (2016) Bahdanau et al. (2017) Joey NMT (RNN, word) Joey NMT (RNN, BPE32k) Joey NMT (Transformer, BPE32k) 22.5 27.6 27.1 27.3 31.0 Table 3: IWSLT14 test results. Conditions. The participation in the study was voluntary and not graded. Participants were not allowed to work in groups and had a maximum 6 BLEU+case.mixed+lang.[en-lv|en-de]+numrefs.1+smooth.exp+ test.wmt17+tok.13a+version.1.3.6 7 Note that the scores reported for other models reflect their state when evaluated in Hieber et al. (2018). 8 Blog posts like Rush (2018) and Bastings (2018) also offer simple code, but they do not perform as well"
D19-3019,W18-2509,0,\N,Missing
E12-1083,W09-0432,0,0.0136514,"tence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from diffe"
E12-1083,C10-2010,0,0.0156871,"parallel titles, 291,716 parallel abstracts, and 735,667 parallel claims sections. The lack of directly translated descriptions poses a serious limitation for patent translation, since this section constitutes the largest part of the document. It is possible to obtain comparable descriptions from related patents that have been filed in different countries and are connected through the patent family id. We extracted 172,472 patents that were both filed with the USPTO and the EPO and contain an English and a German description, respectively. For sentence alignment, we used the Gargantua5 tool (Braune and Fraser, 2010) that filters a sentence-length based alignment with IBM Model-1 lexical word translation probabilities, estimated on parallel data obtained from the first3 http://www.ir-facility.org/ prototypes/marec 4 A patent kind code indicates the document stage in the filing process, e.g., A for applications and B for granted patents, with publication levels from 1-9. See http:// www.wipo.int/standards/en/part_03.html. 5 http://gargantua.sourceforge.net pass alignment. This yields the parallel corpus listed in table 2 with high input-output ratios for claims, and much lower ratios for abstracts and des"
E12-1083,2011.eamt-1.5,0,0.236326,"Missing"
E12-1083,P11-2031,0,0.0240392,"en the text section sections. Similarly for IPC sections, small but statistically significant improvements over the individual and pooled baselines are achieved by multi-task tuning and averaging over IPC sections, excepting C and D. However, an advantage of multi-task tuning over averaging is hard to establish. Note that the averaging techniques implicitly benefit from a larger tuning set. In order to ascertain that the improvements by averaging are not 12 The aspect of averaging found in all of our multi-task learning techniques effectively controls for optimizer instability as mentioned in Clark et al. (2011). 825 test pooled-6k significance abstract claim title 0.3628 0.4696 0.3174 &lt; &lt; &lt; Table 14: Multi-task tuning on 6,000 sentences pooled from text sections. “&lt;” denotes a statistically significant difference to the best result. simply due to increasing the size of the tuning set, we ran a control experiment where we tuned the model on a pooled development set of 3 × 2, 000 sentences for text sections and on a development set of 8 × 2, 000 sentences for IPC sections. The results given in table 14 show that tuning on a pooled set of 6,000 text sections yields only minimal differences to tuning on"
E12-1083,P07-1033,0,0.0171436,"Missing"
E12-1083,2010.iwslt-papers.5,0,0.0129132,"se than the pooled model on three sections. This might be the result of inadequate tuning, since most of the time the MERT algorithm did not converge after the maximum number of iterations, due to the larger number of features when using several models. 9 One straightforward technique to exploit commonalities between tasks is pooling data from separate tasks into a single training set. Instead of a trivial enlargement of training data by pooling, we train the pooled models on the same amount of sentences as the individual models. For instance, the pooled model for the pairing of IPC Following Duh et al. (2010), we use the alignment model trained on the pooled data set in the phrase extraction phase of the separate models. Similarly, we use a globally trained lexical reordering model. 10 For assessing significance, we apply the approximate randomization method described in Riezler and Maxwell (2005). We consider pairwise differing results scoring a pvalue smaller than 0.05 as significant; the assessment is repeated three times and the average value is taken. 822 test train A B C D E F G H A B C D E F G H 0.5349 0.4846 0.5047 0.47 0.4486 0.4595 0.4935 0.4628 0.4475 0.4736 0.4257 0.4387 0.4458 0.4588"
E12-1083,N09-1068,0,0.0507069,"machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models. For example, starting from a separate SVM for each task, Evgeniou and Pontil (2004) present a regularization method that trades off optimization of the task-specific parameter vectors and the distance of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., `1,2 regularization 819 (Obozinski et al., 2010) or `1,∞ regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our expe"
E12-1083,W07-0717,0,0.21522,"vised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly"
E12-1083,2010.amta-papers.24,0,0.0210765,"s or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different"
E12-1083,W11-2123,0,0.0485676,"Missing"
E12-1083,W07-0733,0,0.180545,"s such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools"
E12-1083,P07-2045,0,0.00661785,"Missing"
E12-1083,2005.mtsummit-papers.11,0,0.0261868,"Missing"
E12-1083,P06-1096,0,0.14585,"Missing"
E12-1083,P03-1021,0,0.0476428,"ive results for individual models (tables 7 and 8) shows that replacing data from the same task by data from related tasks decreases translation performance in almost all cases. The exception is the title model that benefits from pooling and mixing with both abstracts and claims due to their richer data structure. SMT pipeline is not adaptable. Such situations arise if there are not enough data to train translation models or language models on the new tasks. However, we assume that there are enough parallel data available to perform meta-parameter tuning by minimum error rate training (MERT) (Och, 2003; Bertoldi et al., 2009) for each task. A generic algorithm for multi-task learning can be motivated as follows: Multi-task learning aims to take advantage of commonalities shared among tasks by learning several independent but related tasks together. Information is shared between tasks through a joint representation and in4.3 Multi-task minimum error rate training In contrast to task pooling and task mixtures, the specific setting addressed by multi-task minimum error rate training is one in which the generative 823 tuning test abstract claim title individual 0.3721 0.4711 0.3228 pooled avera"
E12-1083,2001.mtsummit-papers.68,0,0.0247572,"Missing"
E12-1083,W05-0908,1,0.793219,"to exploit commonalities between tasks is pooling data from separate tasks into a single training set. Instead of a trivial enlargement of training data by pooling, we train the pooled models on the same amount of sentences as the individual models. For instance, the pooled model for the pairing of IPC Following Duh et al. (2010), we use the alignment model trained on the pooled data set in the phrase extraction phase of the separate models. Similarly, we use a globally trained lexical reordering model. 10 For assessing significance, we apply the approximate randomization method described in Riezler and Maxwell (2005). We consider pairwise differing results scoring a pvalue smaller than 0.05 as significant; the assessment is repeated three times and the average value is taken. 822 test train A B C D E F G H A B C D E F G H 0.5349 0.4846 0.5047 0.47 0.4486 0.4595 0.4935 0.4628 0.4475 0.4736 0.4257 0.4387 0.4458 0.4588 0.4489 0.4484 0.5472 0.5161 0.5719 0.5106 0.4681 0.4761 0.5239 0.4914 0.4746 0.4847 0.462 0.5167 0.4531 0.4655 0.4629 0.4621 0.4438 0.4578 0.4134 0.4344 0.4771 0.4517 0.4414 0.4421 0.4523 0.4734 0.4249 0.4435 0.4591 0.4909 0.4565 0.4616 0.4318 0.4396 0.409 0.407 0.4073 0.422 0.4748 0.4588 0.41"
E12-1083,2008.iwslt-papers.6,0,0.0161786,"23 million sentence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation"
E12-1083,D08-1090,0,0.0452213,"statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and la"
E12-1083,2010.amta-commercial.14,0,0.490527,"Missing"
E12-1083,P09-1054,0,0.0597059,"arameter vectors from the previous iteration is computed. For each task d ∈ D, one iteration of standard MERT is called, continuing from weight vec(t−1) tor wd and minimizing translation loss function ld on the data from task d. The individually tuned weight vectors returned by MERT are then moved towards the previously calculated average by adding or subtracting a penalty term λ (t) for each weight component wd [k]. If a weight Figure 1: Multi-task MERT. 824 The weight updates and the clipping strategy can be motivated in a framework of gradient descent optimization under `1 -regularization (Tsuruoka et al., 2009). Assuming MERT as algorithmic minimizer11 of the loss function ld in equation 1, the weight update towards the average follows from the subgradient of the `1 regular(t) izer. Since wavg is taken as average over weights (t−1) (t) wd from the step before, the term wavg is con(t) stant with respect to wd , leading to the following subgradient (where sgn(x) = 1 if x &gt; 0, sgn(x) = −1 if x &lt; 0, and sgn(x) = 0 if x = 0): D D X ∂ 1 X (t−1) (t) λ ws wd − (t) D ∂wr [k] d=1 s=1 1 ! D 1 X (t−1) = λ sgn wr(t) [k] − ws [k] . D s=1 Gradient descent minimization tells us to move in the opposite direction of"
E12-1083,P07-1004,0,0.0248953,"patent corpus of over 23 million sentence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations"
E12-1083,2007.mtsummit-papers.63,0,0.735582,"el data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach. The machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model paramet"
E12-1083,C04-1059,0,0.033983,"ion in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investig"
E12-1083,P02-1040,0,\N,Missing
J08-1003,J97-4005,0,0.0398419,"Missing"
J08-1003,baldwin-etal-2004-road,0,0.0160361,"btain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive effo"
J08-1003,H91-1060,0,0.0647814,"Missing"
J08-1003,E03-1005,0,0.0139868,"tensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage"
J08-1003,J93-1002,0,0.305911,"0], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 howev"
J08-1003,P06-2006,0,0.0594671,"Missing"
J08-1003,W02-1503,0,0.0261788,"uages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf"
J08-1003,P04-1041,1,0.839632,"Missing"
J08-1003,C02-1013,0,0.117113,"al locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been success"
J08-1003,A00-2018,0,0.01612,"Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically"
J08-1003,P04-1014,0,0.0567308,"Missing"
J08-1003,P07-1032,0,0.0390822,"Missing"
J08-1003,P97-1003,0,0.151407,"arsers). The article also reports on a task-based evaluation experiment to rank the parsers using the grammatical relations as input to an anaphora resolution system. Preiss concluded that parser ranking using grammatical relations reﬂected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep uniﬁcation parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at th"
J08-1003,W03-1005,0,0.0353103,"Missing"
J08-1003,C86-1129,0,0.453477,"ion, as well as traces and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II CFG trees with LFG f-structure information. Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse the tree and deterministically add f-structure equations to the phrasal and leaf nodes of the tree, resulting in an f-structure annotated version of the tree. The annotations are then collected and passed on to a constraint solver which generates an f-structure (if the constraints are satisﬁable). We use a simple graph-uniﬁcation-based constraint solver ¨ (Eisele and Dorre 1986), extended to handle path, set-valued, disjunctive, and existential constraints. Given parser output without Penn-II style annotations and traces, the same algorithm is used to assign annotations to each node in the tree, whereas a separate module is applied at the level of f-structure to resolve any long-distance dependencies (see Section 2.3). 5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms (QLFs), and Underspeciﬁed Discourse Representation Structures (UDRSs). 86 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Tabl"
J08-1003,W03-1008,0,0.0100689,"resent research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser compa"
J08-1003,P03-1046,0,0.0243758,"results show that machine-learning-based resources can outperform deep, state-of-the-art hand-crafted resources with respect to the quality of dependencies generated. Treebank-based, deep and wide-coverage constraint-based grammar acquisition has become an important research topic: Starting with the seminal TAG-based work of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao, Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automati"
J08-1003,P02-1043,0,0.347852,"and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can be classiﬁed as “conversion”- or “annotation”-based. TAG-based approaches convert 1 Our use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations"
J08-1003,P02-1018,0,0.00917585,"s Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) ele"
J08-1003,N04-1013,1,0.397922,"is (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate strings to informatio"
J08-1003,W03-2401,1,0.923661,"ular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep gram"
J08-1003,P03-1054,0,0.0412189,"ime-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treeba"
J08-1003,P04-1042,0,0.0169367,"pendencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al"
J08-1003,H94-1020,0,0.0187269,"of (related) drawbacks: 1. Bracketed trees do not always provide NLP applications with enough information to carry out the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate–argument structures, or simple logical forms. 2. A number of alternative, but equally valid tree representations can potentially be given for the same input. To give just a few examples: In English, VPs containing modals and auxiliaries can be analyzed using (predominantly) binary branching rules (Penn-II [Marcus et al. 1994]), or employ ﬂatter analyses where modals and auxiliaries are sisters of the main verb (AP treebank [Leech and Garside 1991]), or indeed do without a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank bracketing guidelines can use “traditional” CFG categories such as S, NP, and so on (Penn-II) or a maximal projection-inspired analysis with IPs and DPs (Chinese Penn Treebank [Xue et al. 2004]). 3. Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), e"
J08-1003,E06-1011,0,0.0298537,"use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume"
J08-1003,C04-1204,0,0.0126897,"I treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser comparison is a non-trivial and t"
J08-1003,P04-1047,1,0.924795,"Missing"
J08-1003,E03-1025,0,0.233427,"uistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate s"
J08-1003,P02-1035,1,0.159909,"RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are sha"
J08-1003,C96-1045,1,0.742182,"Missing"
J08-1003,P97-1052,1,0.798137,"Missing"
J08-1003,C00-2137,0,0.0191341,"able tuples between the two systems. Approximate randomization produces shufﬂes by random assignments instead of evaluating all 2S possible assignments. Signiﬁcance levels are computed as the percentage of trials where the pseudo statistic, that is the test statistic computed on the shufﬂed data, is greater than or equal to the actual statistic, that is the test statistic computed on the test data. A sketch of an algorithm for approximate randomization testing is given in Figure 12. 21 Applications of this test to natural language processing problems can be found in Chinchor et al. (1993) and Yeh (2000). 104 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Table 10 Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for approximate randomization test for 10,000,000 randomizations. PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 .0003 <.0001 <.0001 - Table 10 gives the p-values (the smallest ﬁxed level at which the null hypothesis can be rejected) for comparing each parser against all of the other"
J08-1003,J98-4004,0,\N,Missing
J08-1003,W96-0213,0,\N,Missing
J08-1003,J03-4003,0,\N,Missing
J08-1003,J93-3001,0,\N,Missing
J10-3010,P05-1074,0,0.142721,"Missing"
J10-3010,J09-1008,0,0.0657227,"Missing"
J10-3010,D07-1090,0,0.0091505,"extracted as larger blocks of aligned words from the alignments in the intersection, as described in Och and Ney (2004). 573 Computational Linguistics Volume 36, Number 3 4.4 Language Modeling Language modeling in our approach deploys an n-gram language model that assigns the following probability to a string wL1 of words: P(wL1 ) = L  P(wi |wi1−1 ) i=1 ≈ L  1 P(wi |wii− −n+1 ) i=1 Estimation of n-gram probabilities is done by counting relative frequencies of n-grams in a corpus of user queries. Remedies for sparse data problems are achieved by various smoothing techniques, as described in Brants et al. (2007). The most important departure of our approach from standard SMT is the use of a language model trained on queries. Although this approach may seem counterintuitive from the standpoint of the noisy-channel model for SMT (Brown et al. 1993), it ﬁts perfectly into the linear model. Whereas in the ﬁrst view a query language model would be interpreted as a language model on the source language, in the linear model directionality of translation is not essential. Furthermore, the ultimate task of a query language model in our approach is to select appropriate phrase translations in the context of th"
J10-3010,J93-2003,0,0.0291462,"Missing"
J10-3010,P03-1003,0,0.102469,"precomputed. Similarly to this recent work, our approach uses data from user query logs, but as input to a monolingual SMT model for learning query rewrites. The SMT viewpoint has been introduced to the ﬁeld of IR by Berger and Lafferty (1999) and Berger et al. (2000), who proposed to bridge the “lexical chasm” by a retrieval model based on IBM Model 1 (Brown et al. 1993). Since then, ranking models based on monolingual SMT have seen various applications, especially in areas like Question Answering where a large lexical gap between questions and answers has to be bridged (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2008; Xue, Jeon, and Croft 2008). Whereas most applications of SMT ideas to IR problems used translation system scores for (re)ranking purposes, only a few approaches use SMT to generate actual query rewrites (Riezler, Liu, and Vasserman 2008). Similarly to Riezler, Liu, and Vasserman (2008), we use SMT to produce actual rewrites rather than for (re)ranking, and evaluate the rewrites in a query expansion task that leaves the ranking model of the search engine untouched. Lastly, monolingual SMT has been established"
J10-3010,D08-1043,0,0.0144558,"een queries and Web documents as a problem of translating from a source language of user queries to a target language of Web documents. We showed that a state-of-the-art SMT model can be applied to parallel data of user queries and snippets for clicked Web documents, and showed improvements over state-of-the-art probabilistic query expansion. Our experimental evaluation showed ﬁrstly that state-of-the-art SMT is robust and ﬂexible enough to capture the peculiarities of query–snippet translation, thus questioning the need for special-purpose models to control noisy translations as suggested by Lee et al. (2008). Furthermore, we showed that the combination of translation model and language model signiﬁcantly outperforms the combination of correlation-based model and language model. We chose to take advantage of the access the google.com search engine to evaluate the query rewrite systems by query expansion embedded in a real-word search task. Although this conforms with recent appeals for more extrinsic evaluations (Belz 2009), it decreases the reproducability of the evaluation experiment. In future work, we hope to apply SMT-based rewriting to other rewriting tasks such as query suggestions. Also, w"
J10-3010,J04-4002,0,0.230605,"iterative estimation, but on the other hand it makes the implicit assumption that data sparsity will be overcome by counting from huge data sets. The only attempt at smoothing that is made in this approach is shifting the burden to words in the query context, using Equation (2), when Equation (1) assigns zero probability to unseen pairs. Nonetheless, Cui et al. (2002) show signiﬁcant improvements over the local feedback technique of Xu and Croft (1996) in an evaluation on TREC data. 4. Query Expansion Using Monolingual SMT 4.1 Linear Models for SMT The job of a translation system is deﬁned in Och and Ney (2004) as ﬁnding the English string eˆ that is a translation of a foreign string f using a linear combination of feature functions hm (e, f) and weights λm as follows: eˆ = arg max e M  λm hm (e, f) m= 1 As is now standard in SMT, several complex features such as lexical translation models and phrase translation models, trained in source-target and target-source directions, are combined with language models and simple features such as phrase and word counts. In the linear model formulation, SMT can be thought of as a general tool for computing string similarities or for string rewriting. 572 Riezle"
J10-3010,W04-3219,0,0.133038,"Missing"
J10-3010,C08-1093,1,0.653825,"Missing"
J10-3010,P07-1059,1,0.882963,"approach uses data from user query logs, but as input to a monolingual SMT model for learning query rewrites. The SMT viewpoint has been introduced to the ﬁeld of IR by Berger and Lafferty (1999) and Berger et al. (2000), who proposed to bridge the “lexical chasm” by a retrieval model based on IBM Model 1 (Brown et al. 1993). Since then, ranking models based on monolingual SMT have seen various applications, especially in areas like Question Answering where a large lexical gap between questions and answers has to be bridged (Berger et al. 2000; Echihabi and Marcu 2003; Soricut and Brill 2006; Riezler et al. 2007; Surdeanu, Ciaramita, and Zaragoza 2008; Xue, Jeon, and Croft 2008). Whereas most applications of SMT ideas to IR problems used translation system scores for (re)ranking purposes, only a few approaches use SMT to generate actual query rewrites (Riezler, Liu, and Vasserman 2008). Similarly to Riezler, Liu, and Vasserman (2008), we use SMT to produce actual rewrites rather than for (re)ranking, and evaluate the rewrites in a query expansion task that leaves the ranking model of the search engine untouched. Lastly, monolingual SMT has been established in the NLP community as a useful expedient f"
J10-3010,W02-1023,0,0.0243283,"Missing"
J10-3010,P08-1082,0,0.0674998,"Missing"
J14-1009,J08-4004,0,0.259821,"the problem of reliability in data annotation, a solution to which is sought by methods for measuring and enhancing inter-annotator agreement. A seminal paper by Carletta (1996) and a follow-up survey 5 In this article, we concentrate on supervised machine learning. Semisupervised, transductive, active, or unsupervised learning deal with machine learning from incomplete or missing labelings where the general assumption of i.i.d. data is not questioned. See Dundar et al. (2007) for an approach of machine learning from non-i.i.d. data. 239 Computational Linguistics Volume 40, Number 1 paper by Artstein and Poesio (2008) have discussed this issue at length. Both papers refer to Krippendorff (2004, 1980a, page 428) who recommends that reliability data “have to be generated by coders that are widely available, follow explicit and communicable instructions (a data language), and work independently of each other. . . . [T]he more coders participate in the process and the more common they are, the more likely they can ensure the reliability of data.” Ironically, it seems as if the best inter-annotator agreement is achieved by techniques that are in conflict with these recommendations, namely, by using experts (Kil"
J14-1009,J09-4005,0,0.0925869,"Missing"
J14-1009,J09-1008,0,0.0121423,"agreement from crowdsourcing can be seen as a possible litmus test for a successful T-non-theoretical grounding of complex annotation tasks. Circularity issues will vanish because T-theoretical terms cannot be communicated directly to naive coders. 4.2 Grounding by Extrinsic Evaluation and Task-Related Annotation Another way to achieve T-non-theoretical grounding is extrinsic evaluation of NLP systems. This type of evaluation assesses “the effect of a system on something that is external to it, for example, the effect on human performance at a given task or the value added to an application” (Belz 2009) and has been demanded for at least 20 years (Sp¨arck Jones 1994). Extrinsic evaluation is advertised as a remedy against “closed problem” approaches (Sp¨arck Jones 1994) or against “closed circles” in intrinsic evaluation where system rankings produced by automatic measures are compared with human rankings which are themselves unfalsifiable (Belz 2009). 7 http://www.mturk.com. 8 See Fort, Adda, and Cohen (2011) for a discussion of the ethical dimensions of crowdsourcing services and their alternatives. 241 Computational Linguistics Volume 40, Number 1 An example of an extrinsic evaluation in"
J14-1009,P07-1056,0,0.0809599,"Missing"
J14-1009,W10-0701,0,0.0457761,"Missing"
J14-1009,P99-1042,0,0.0112743,"Missing"
J14-1009,N06-2015,0,0.0258818,"pers refer to Krippendorff (2004, 1980a, page 428) who recommends that reliability data “have to be generated by coders that are widely available, follow explicit and communicable instructions (a data language), and work independently of each other. . . . [T]he more coders participate in the process and the more common they are, the more likely they can ensure the reliability of data.” Ironically, it seems as if the best inter-annotator agreement is achieved by techniques that are in conflict with these recommendations, namely, by using experts (Kilgarriff 1999) or intensively trained coders (Hovy et al. 2006) for data annotation. Artstein and Poesio (2008) state explicitly that experts as coders, particularly long-term collaborators, [. . . ] may agree not because they are carefully following written instructions, but because they know the purpose of the research very well–which makes it virtually impossible for others to reproduce the results on the basis of the same coding scheme . . . . Practices which violate the third requirement (independence) include asking the coders to discuss their judgments with each other and reach their decisions by majority vote, or to consult with each other when pr"
J14-1009,E99-1046,0,0.0972165,"08) have discussed this issue at length. Both papers refer to Krippendorff (2004, 1980a, page 428) who recommends that reliability data “have to be generated by coders that are widely available, follow explicit and communicable instructions (a data language), and work independently of each other. . . . [T]he more coders participate in the process and the more common they are, the more likely they can ensure the reliability of data.” Ironically, it seems as if the best inter-annotator agreement is achieved by techniques that are in conflict with these recommendations, namely, by using experts (Kilgarriff 1999) or intensively trained coders (Hovy et al. 2006) for data annotation. Artstein and Poesio (2008) state explicitly that experts as coders, particularly long-term collaborators, [. . . ] may agree not because they are carefully following written instructions, but because they know the purpose of the research very well–which makes it virtually impossible for others to reproduce the results on the basis of the same coding scheme . . . . Practices which violate the third requirement (independence) include asking the coders to discuss their judgments with each other and reach their decisions by maj"
J14-1009,P13-1022,0,0.0131065,"thods have led von Luxburg, Williamson, and Guyon (2012) to pose the question “Clustering: Science or Art?”. They recommend to measure the usefulness of a clustering method for a particular task under consideration, that is, to always study clustering in the context of its end use. Extrinsic scenarios are not only useful for the purpose of evaluation. Rather, every extrinsic evaluation creates data that can be used as training data for another learning task (e.g., rankings of system outputs with respect to an extrinsic task can be used to train discriminative (re)ranking models). For example, Kim and Mooney (2013) use the successful completion of navigation tasks to create training data for reranking in grounded language learning. Nikoulina et al. (2012) use retrieval performance of translated queries to create data for reranking in statistical machine translation. Clarke et al. (2010) use the correct response for a query to a database of geographical facts to select data for structured learning of a semantic parser. Thus the extrinsic set-up can be seen as a general technique for T-non-theoretical grounding in training as well as in testing scenarios. Circularity issues will not arise in extrinsic set"
J14-1009,P08-1006,0,0.0115954,"s a remedy against “closed problem” approaches (Sp¨arck Jones 1994) or against “closed circles” in intrinsic evaluation where system rankings produced by automatic measures are compared with human rankings which are themselves unfalsifiable (Belz 2009). 7 http://www.mturk.com. 8 See Fort, Adda, and Cohen (2011) for a discussion of the ethical dimensions of crowdsourcing services and their alternatives. 241 Computational Linguistics Volume 40, Number 1 An example of an extrinsic evaluation in NLP is the evaluation of the effect of syntactic parsers on retrieval quality in a biomedical IR task (Miyao et al. 2008). Interestingly, the extrinsic set-up revealed a different system ranking than the standard intrinsic evaluation, according to F-scores on the Penn WSJ corpus. Another example is the area of clustering. Deficiencies in current intrinsic clustering evaluation methods have led von Luxburg, Williamson, and Guyon (2012) to pose the question “Clustering: Science or Art?”. They recommend to measure the usefulness of a clustering method for a particular task under consideration, that is, to always study clustering in the context of its end use. Extrinsic scenarios are not only useful for the purpose"
J14-1009,E12-1012,0,0.0132929,"efulness of a clustering method for a particular task under consideration, that is, to always study clustering in the context of its end use. Extrinsic scenarios are not only useful for the purpose of evaluation. Rather, every extrinsic evaluation creates data that can be used as training data for another learning task (e.g., rankings of system outputs with respect to an extrinsic task can be used to train discriminative (re)ranking models). For example, Kim and Mooney (2013) use the successful completion of navigation tasks to create training data for reranking in grounded language learning. Nikoulina et al. (2012) use retrieval performance of translated queries to create data for reranking in statistical machine translation. Clarke et al. (2010) use the correct response for a query to a database of geographical facts to select data for structured learning of a semantic parser. Thus the extrinsic set-up can be seen as a general technique for T-non-theoretical grounding in training as well as in testing scenarios. Circularity issues will not arise in extrinsic set-ups because the extrinsic task is by definition external to the system outputs to be tested or ranked. 4.3 Grounded Data in the Wild Halevy, N"
J14-1009,W02-1011,0,0.013728,"Missing"
J14-1009,D08-1027,0,0.053021,"Missing"
J14-1009,H94-1018,0,0.151097,"Missing"
J14-1009,P08-1082,0,0.0379573,"Missing"
J14-1009,W04-3201,0,0.0439356,"s and auxiliary assumptions are subjected to empirical testing. That is, if our predictions are not in accordance with our theory, we can only conclude that one of our many theoretical assumptions must be wrong, but we cannot know which one, and we can always ¨ modify our system of assumptions, leading to various ways of immunity of theories (Stegmuller 1986). This problem arises in Solution (1) as well as in Solution (2) 238 Riezler On the Problem of Theoretical Terms in Empirical CL and F measures the compatibility of pairs (x, y), for example, in the form of a linear discriminant function (Taskar et al. 2004; Tsochantaridis et al. 2005).5 The problem of theoretical terms arises in empirical CL in cases where a single theoretical tier is used both in manual data annotation (i.e., in the assignment of labels y to patterns x via the encoding of data pairs (x, y)), and in feature construction (i.e., in the association of labels y to patterns x via features φ(x, y)). This problem can be illustrated by looking at automatic methods for data annotation. For example, information retrieval (IR) in the patent domain uses citations of patents in other patents to automatically create relevance judgments for r"
J14-1009,P13-1006,0,0.0135516,"oretical grounding and exemplified how this criterion can be met by manual data annotation by using naive coders, or by embedding data annotation into a task extrinsic to the theory to be tested, or by using independently created language data that are available in the wild. Our suggestions for T-non-theoretical grounding are related to work on grounded language learning that is based on weak supervision in the form of the use of sentences in naturally occurring contexts. For example, the meaning of natural language expresssions can be grounded in visual scenes (Roy 2002; Yu and Ballard 2004; Yu and Siskind 2013) or actions in games or navigation tasks (Chen and Mooney 2008, 2011). Because of the ambiguous supervision, most such approaches work with latent representations and use unsupervised techniques in learning. Our suggestions for T-non-theoretical grounding can be used to avoid circularities in standard supervised learning. We think that this criterion should be considered a necessary condition for an empirical science, in addition to ensuring reliability of measurements. Our negligence of related issues such as validity of measurements (see Krippendorff 1980b) shows that there is a vast methodo"
J14-1009,J96-1001,0,\N,Missing
J14-1009,W10-2903,0,\N,Missing
J14-1009,J08-3001,0,\N,Missing
J14-1009,N12-1017,0,\N,Missing
J14-1009,J11-2010,0,\N,Missing
J14-1009,J96-2004,0,\N,Missing
J14-1009,W10-0711,0,\N,Missing
K15-1001,D13-1160,0,0.0277749,"t al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 3.1 1: I"
K15-1001,P10-4002,0,0.0458185,"Missing"
K15-1001,N13-1073,0,0.0443787,"terations 12000 16000 0.90 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 0.80 0.70 0.32 0.31 0.60 0.50 TER regret 0.29 20000 iterations 0.40 0.30 0.30 0.20 0.10 0.00 0 4000 8000 12000 16000 0 20000 4000 iterations 8000 12000 16000 0.29 20000 iterations Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part). Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013). In all experiments, training is started with the Moses default weights. The size of the n-best list, where used, was set to 1,000. Irrespective of the use of re-scaling in perceptron training, a constant learning rate of 10−5 was used for learning from simulated feedback, and 10−4 for learning from surrogate translations. Our experiments on online learning require a random sequence of examples for learning. Following the techniques described in Bertsekas (2011) to generate random sequences for incremental optimization, we compared cyclic order (K epochs of T examples in fixed order), randomi"
K15-1001,W12-3160,0,0.0138411,"eory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedbac"
K15-1001,N12-1023,0,0.0909,"t al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joach"
K15-1001,D14-1130,0,0.0779747,"et and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in resp"
K15-1001,D08-1024,0,0.0319351,"an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of co"
K15-1001,N09-1025,0,0.0172291,"nversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate refe"
K15-1001,W11-2123,0,0.015573,"G corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.html 2 This condition is too strong for large datasets. However, we believe that a weaker condition based on ideas from the 5 0.90 α = 0.1 α = 0.5 α = 1.0 α = 0.1 α = 0.5 α = 1.0 0.80 0.70 0.31 0.60 0.50 TER regret 0.32 0.40 0.30 0.30 0.20 0.10 0"
K15-1001,P07-2045,0,0.00485873,", respectively. Finally, we denote by DT,K = Tt=1 ∆2t,K , and by wT,K the final weight vector returned after K epochs. We state a condition of convergence2 : 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.html 2 This condition is too strong for large datasets. However,"
K15-1001,W02-1001,0,0.173415,"rithm. p DT,K T The theorem can be interpreted as a bound on the generalization error (lefthand-side) by the empirical error (the first two righthand-side terms) and the variance caused by the finite sample (the third term in the theorem). The result follows directly from McDiarmid’s concentration inequality. Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002). The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected loss on unseen data we would like to bound. We assume that the algorithm is fed with a sequence of examples x1 , . . . , xT , and at each epoch k = 1, . . . , K it makes a prediction yt,k . The correct label is yt∗ . For k = 1, . . . , K and t = 1, . . . , T , let `t,k = U (xt , yt∗ ) − U (xt , yt,k ), and denote by ∆t,k and ξt,k the distance at epoch k for example t, and the slack at epoch k for example Pt, respective"
K15-1001,D13-1161,0,0.0218824,"ase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the"
K15-1001,E14-1042,0,0.0580624,"ang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter al"
K15-1001,P06-1096,0,0.0940262,"Missing"
K15-1001,P06-1091,0,0.030103,"additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contri"
K15-1001,P03-1021,0,0.10804,"Missing"
K15-1001,2013.mtsummit-papers.2,1,0.871215,"Missing"
K15-1001,W10-1723,0,0.0209251,"epoch k for example t, and the slack at epoch k for example Pt, respectively. Finally, we denote by DT,K = Tt=1 ∆2t,K , and by wT,K the final weight vector returned after K epochs. We state a condition of convergence2 : 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.ht"
K15-1001,2006.iwslt-evaluation.14,0,0.0304547,"extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confin"
K15-1001,potet-etal-2012-collection,0,0.0397471,"Missing"
K15-1001,D07-1080,0,0.0262121,"alization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that c"
K15-1001,N12-1026,0,0.0190537,"ts for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedback in form of an i"
K15-1001,2012.amta-papers.14,0,0.0403179,"slation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and"
K15-1001,D13-1112,0,0.0167655,"ea of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where in"
K15-1001,N04-1023,0,0.0432168,"Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edit"
K15-1001,P12-1002,1,0.832752,"ov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user"
K15-1001,2006.amta-papers.25,0,0.0856923,"essarily optimal, object y¯t with respect to a utility function U . The key asset of coactive learning is the ability of the learner to converge to predictions that are close to optimal structures yt∗ , although the utility function is unknown to the learner, and only weak feedback in form of slightly improved structures y¯t is seen in training. We present a proof-of-concept experiment in which translation feedback of varying grades is chosen from the n-best list of an “optimal” model that has access to full information. We show that weak feedback structures correspond to improvements in TER (Snover et al., 2006) over predicted structures, and that learning from weak feedback minimizes regret and TER. 2 Related Work Our work builds on the framework of coactive learning, introduced by Shivaswamy and Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch"
N03-1026,P98-1006,0,0.0296492,"Missing"
N03-1026,1999.mtsummit-1.20,0,0.00935554,"tures and associated functional (f -)structures for each input sentence, represented in packed form (see Maxwell and Kaplan (1989)). For sentence condensation we are only interested in the predicate-argument structures encoded in f -structures. For example, Fig. 1 shows an f -structure manually selected out of the 40 f -structures for the sentence: A prototype is ready for testing, and Leary hopes to set requirements for a full system by the end of the year. The transfer component for the sentence condensation system is based on a component previously used in a machine translation system (see Frank (1999)). It consists of an ordered set of rules that rewrite one f -structure into another. Structures are broken down into flat lists of facts, and rules may add, delete, or change individual facts. Rules may be optional or obligatory. In the case of optional rules, transfer of a single input structure may lead to multiple alternate output structures. The transfer component is designed to operate on packed input from the parser and can also produce packed representations of the condensation alternatives, using methods adapted from parse packing.1 An example rule that (optionally) removes an adjunct"
N03-1026,P02-1036,0,0.0263896,"Missing"
N03-1026,A00-1043,0,0.664223,"e-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator. 1 Introduction Recent work in statistical text summarization has put forward systems that do not merely extract and concatenate sentences, but learn how to generate new sentences from hSummary, T exti tuples. Depending on the chosen task, such systems either generate single-sentence “headlines” for multi-sentence text (Witbrock and Mittal, 1999), or they provide a sentence condensation module designed for combination with sentence extraction systems (Knight and Marcu, 2000; Jing, 2000). The challenge for such systems is to guarantee the grammaticality and summarization quality of the system output, i.e. the generated sentences need to be syntactically wellformed and need to retain the most salient information of the original document. For example a sentence extraction system might choose a sentence like: The UNIX operating system, with implementations from Apples to Crays, appears to have the advantage. from a document, which could be condensed as: UNIX appears to have the advantage. In the approach of Witbrock and Mittal (1999), selection and ordering of summary terms is b"
N03-1026,P99-1069,1,0.800125,"tee that a non-empty set of reduced f -structures yielding grammatical strings in generation is passed on to the next system component. In case of fragmentary input to the transfer component, grammaticaliy of the output is guaranteed for the separate fragments. In other words, strings generated from a reduced fragmentary f -structure will be as grammatical as the string that was fed into the parsing component. After filtering by the generator, the remaining f structures were weighted by the stochastic disambiguation component. Similar to stochastic disambiguation for constraint-based parsing (Johnson et al., 1999; Riezler et al., 2002), an exponential (a.k.a. log-linear or maximumentropy) probability model on transferred structures is estimated from a set of training data. The data for estimation consists of pairs of original sentences y and goldstandard summarized f -structures s which were manually selected from the transfer output for each sentence. For training data {(sj , yj )}m j=1 and a set of possible summarized structures S(y) for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L(λ) of a summarized f -structure given the sentence. O"
N03-1026,W89-0203,0,0.0649687,"velopment purposes and system comparison. As shown in an experimental evaluation, a close correspondence can be established for rankings produced by the f -structure based automatic evaluation and a manual evaluation of generated strings. 2 Statistical Sentence Condensation in the LFG Framework 2.1 Parsing and Transfer In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al. (2002)). The parser produces a set of context-free constituent (c-)structures and associated functional (f -)structures for each input sentence, represented in packed form (see Maxwell and Kaplan (1989)). For sentence condensation we are only interested in the predicate-argument structures encoded in f -structures. For example, Fig. 1 shows an f -structure manually selected out of the 40 f -structures for the sentence: A prototype is ready for testing, and Leary hopes to set requirements for a full system by the end of the year. The transfer component for the sentence condensation system is based on a component previously used in a machine translation system (see Frank (1999)). It consists of an ordered set of rules that rewrite one f -structure into another. Structures are broken down into"
N03-1026,2001.mtsummit-papers.68,0,0.0609127,"Missing"
N03-1026,P02-1035,1,0.846547,"or for the construction and manual structural disambiguation of a reusable gold standard test set. Matching against the test set can be done automatically and rapidly, and is repeatable for development purposes and system comparison. As shown in an experimental evaluation, a close correspondence can be established for rankings produced by the f -structure based automatic evaluation and a manual evaluation of generated strings. 2 Statistical Sentence Condensation in the LFG Framework 2.1 Parsing and Transfer In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al. (2002)). The parser produces a set of context-free constituent (c-)structures and associated functional (f -)structures for each input sentence, represented in packed form (see Maxwell and Kaplan (1989)). For sentence condensation we are only interested in the predicate-argument structures encoded in f -structures. For example, Fig. 1 shows an f -structure manually selected out of the 40 f -structures for the sentence: A prototype is ready for testing, and Leary hopes to set requirements for a full system by the end of the year. The transfer component for the sentence condensation system is based on"
N03-1026,P02-1040,0,\N,Missing
N03-1026,C98-1006,0,\N,Missing
N04-1013,W02-1503,1,0.403196,"Missing"
N04-1013,P02-1031,0,0.0550515,"Missing"
N04-1013,P01-1037,0,0.0605449,"Missing"
N04-1013,W03-2401,1,0.865907,"o have poor run-time performance. The Collins parser is thought to be accurate and fast and thus to represent a reasonable trade-off between “good-enough” output, speed, and robustness. This paper reports on some experiments that put this conventional wisdom to an empirical test. We investigated the accuracy of recovering semantically-relevant grammatical dependencies from the tree-structures produced by the Collins parser, comparing these dependencies to gold-standard dependencies which are available for a subset of 700 sentences randomly drawn from section 23 of the Wall Street Journal (see King et al. (2003)). We compared the output of the XLE system, a deep-grammar-based parsing system using the English Lexical-Functional Grammar previously constructed as part of the Pargram project (Butt et al., 2002), to the same gold standard. This system incorporates sophisticated ambiguity-management technology so that all possible syntactic analyses of a sentence are computed in an efficient, packed representation (Maxwell and Kaplan, 1993). In accordance with LFG theory, the output includes not only standard context-free phrase-structure trees but also attribute-value matrices (LFG’s f(unctional) structur"
N04-1013,kingsbury-palmer-2002-treebank,0,0.0735438,"Missing"
N04-1013,J93-4001,0,0.36456,"eλ·f (x) P where Zλ (y) = x∈X(y) eλ·f (x) is a normalizing constant over the set X(y) of parses for sentence y, λ is a vector of log-parameters, f is a vector of featurevalues, and λ · f (x) is a vector dot product denoting the (log-)weight of parse x. Dynamic-programming algorithms that allow the efficient estimation and searching of log-linear models from a packed parse representation without enumerating an exponential number of parses have been recently presented by Miyao and Tsujii (2002) and Geman and Johnson (2002). These algorithms can be readily applied to the packed and/or-forests of Maxwell and Kaplan (1993), provided that each conjunctive node is annotated with feature-values of the loglinear model. In the notation of Miyao and Tsujii (2002), such a feature forest Φ is defined as a tuple hC, D, r, γ, δi where C is a set of conjunctive nodes, D is a set of disjunctive nodes, r ∈ C is the root node, γ : D → 2C is a conjunctive daughter function, and δ : C → 2D is a disjunctive daughter function. A dynamic-programming solution to the problem of finding most probable parses is to compute the weight φd of each disjunctive node as the maximum weight of its conjunctive daugher nodes, i.e., φd = max φc"
N04-1013,A00-2030,0,0.0339111,"Missing"
N04-1013,W96-0213,0,0.0424379,"Missing"
N04-1013,P02-1057,0,0.0127301,"Missing"
N04-1013,P01-1067,0,0.00839852,"Missing"
N04-1013,C92-1025,0,\N,Missing
N04-1013,J03-4003,0,\N,Missing
N04-1013,P02-1036,0,\N,Missing
N04-1013,P02-1035,1,\N,Missing
N06-1032,W02-1503,0,0.120623,"s of a sentence-aligned bilingual corpus. Similar to phrase-based SMT, our approach starts with an improved word-alignment that is created by intersecting alignment matrices for both translation directions, and refining the intersection alignment by adding directly adjacent alignment points, and alignment points that align previously unaligned words (see Och et al. (1999)). Next, source and target sentences are parsed using source and target LFG grammars to produce a set of possible f(unctional) dependency structures for each side (see Riezler et al. (2002) for the English grammar and parser; Butt et al. (2002) for German). The two f-structures that most preserve dependencies are selected for further consideration. Selecting the most similar instead of the most probable f-structures is advantageous for rule induction since it provides for higher coverage with simpler rules. In the third step, the manyto-many word alignment created in the first step is used to define many-to-many correspondences between the substructures of the f-structures selected in the second step. The parsing process maintains an association between words in the string and particular predicate features in the f-structure, and th"
N06-1032,2003.mtsummit-papers.6,0,0.0657277,"not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT system that deploys a grammar-based generator (Menezes and Richardson, 2001) on ngram based automatic evaluation scores (Papineni et al., 2001; Doddington, 2002). Another data point, however, is given by Charniak et al. (2003) who show that parsing-based language modeling can improve grammaticality of translations, even if these improvements are not recorded under n-gram based evaluation measures. 1 A notable exception to this kind of approach is Chiang (2005) who introduces syntactic information into phrase-based SMT via hierarchical phrases rather than by external parsing. 248 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 248–255, c New York, June 2006. 2006 Association for Computational Linguistics In this paper we would like to step away from n-gram base"
N06-1032,P05-1033,0,0.0208347,"over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT system that deploys a grammar-based generator (Menezes and Richardson, 2001) on ngram based automatic evaluation scores (Papineni et al., 2001; Doddington, 2002). Another data point, however, is given by Charniak et al. (2003) who show that parsing-based language modeling can improve grammaticality of translations, even if these improvements are not recorded under n-gram based evaluation measures. 1 A notable exception to this kind of approach is Chiang (2005) who introduces syntactic information into phrase-based SMT via hierarchical phrases rather than by external parsing. 248 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 248–255, c New York, June 2006. 2006 Association for Computational Linguistics In this paper we would like to step away from n-gram based automatic evaluation scores for a moment, and investigate the possible contributions of incorporating a grammar-based generator into a dependency-based SMT system. We present a dependency-based SMT model that integrates the idea of mult"
N06-1032,P05-1066,0,0.00817839,"translation with multi-word units excels at modeling local ordering and short idiomatic expressions, however, it lacks a mechanism to learn long-distance dependencies and is unable to generalize to unseen phrases that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (Xia and McCord, 2004; Collins et al., 2005), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) 1 . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are pr"
N06-1032,P05-1067,0,0.042124,"eralize to unseen phrases that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (Xia and McCord, 2004; Collins et al., 2005), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) 1 . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT s"
N06-1032,koen-2004-pharaoh,0,0.0458489,"ansferred. When the chart is complete, the outputs of the transfer rules are unified to make sure they are consistent (for instance, that the transfer rules did not produce two determiners for the same noun). Selection of the most probable transfer output is done by beamdecoding on the transfer chart. LFG grammars can be used bidirectionally for parsing and generation, thus the existing English grammar used for parsing the training data can The statistical components of our system are modeled on the statistical components of the phrasebased system Pharaoh, described in Koehn et al. (2003) and Koehn (2004). Pharaoh integrates the following 8 statistical models: relative frequency of phrase translations in source-to-target and targetto-source direction, lexical weighting in source-totarget and target-to-source direction, phrase count, language model probability, word count, and distortion probability. Correspondingly, our system computes the following statistics for each translation: 251 1. log-probability of source-to-target transfer rules, where the probability r(e|f) of a rule that transfers source snippet f into target snippet e is estimated by the relative frequency r(e|f) = P count(f ==&gt; e"
N06-1032,C04-1090,0,0.0134477,"able to generalize to unseen phrases that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (Xia and McCord, 2004; Collins et al., 2005), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) 1 . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT a"
N06-1032,W01-1406,0,0.242339,"es deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT system that deploys a grammar-based generator (Menezes and Richardson, 2001) on ngram based automatic evaluation scores (Papineni et al., 2001; Doddington, 2002). Another data point, however, is given by Charniak et al. (2003) who show that parsing-based language modeling can improve grammaticality of translations, even if these improvements are not recorded under n-gram based evaluation measures. 1 A notable exception to this kind of approach is Chiang (2005) who introduces syntactic information into phrase-based SMT via hierarchical phrases rather than by external parsing. 248 Proceedings of the Human Language Technology Conference of the North American Chapter of t"
N06-1032,W99-0604,0,0.336715,"ructure snippets, and models and trains statistical components according to stateof-the-art SMT systems. Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator. An experimental evaluation shows that the incorporation of a grammar-based generator into an SMT framework provides improved grammaticality while achieving state-of-the-art quality on in-coverage examples, suggesting a possible hybrid framework. 1 Introduction Recent approaches to statistical machine translation (SMT) piggyback on the central concepts of phrasebased SMT (Och et al., 1999; Koehn et al., 2003) and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process. Phrase-based translation with multi-word units excels at modeling local ordering and short idiomatic expressions, however, it lacks a mechanism to learn long-distance dependencies and is unable to generalize to unseen phrases that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. T"
N06-1032,P03-1021,0,0.0441554,"for Computational Linguistics In this paper we would like to step away from n-gram based automatic evaluation scores for a moment, and investigate the possible contributions of incorporating a grammar-based generator into a dependency-based SMT system. We present a dependency-based SMT model that integrates the idea of multi-word translation units from phrasebased SMT into a transfer system for dependency structure snippets. The statistical components of our system are modeled on the phrase-based system of Koehn et al. (2003), and component weights are adjusted by minimum error rate training (Och, 2003). In contrast to phrase-based SMT and to the above cited dependency-based SMT approaches, our system feeds dependency-structure snippets into a grammar-based generator, and determines target language ordering by applying n-gram and distortion models after grammar-based generation. The goal of this ordering model is thus not foremost to reflect the ordering of the reference translations, but to improve the grammaticality of translations. Since our system uses standard SMT techniques to learn about correct lexical choice and idiomatic expressions, it allows us to investigate the contribution of"
N06-1032,2001.mtsummit-papers.68,0,0.0148498,"a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT system that deploys a grammar-based generator (Menezes and Richardson, 2001) on ngram based automatic evaluation scores (Papineni et al., 2001; Doddington, 2002). Another data point, however, is given by Charniak et al. (2003) who show that parsing-based language modeling can improve grammaticality of translations, even if these improvements are not recorded under n-gram based evaluation measures. 1 A notable exception to this kind of approach is Chiang (2005) who introduces syntactic information into phrase-based SMT via hierarchical phrases rather than by external parsing. 248 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 248–255, c New York, June 2006. 2006 Association for"
N06-1032,P05-1034,0,0.0885234,"es that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (Xia and McCord, 2004; Collins et al., 2005), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) 1 . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the training corpus are preferred. The advantage of such models over grammar-based generation seems to be supported, for example, by Quirk et al.’s (2005) improvements over phrase-based SMT as well as over an SMT system that deploys a"
N06-1032,W05-0908,1,0.876722,"Missing"
N06-1032,P02-1035,1,0.546809,"endency structure snippets operates on the paired sentences of a sentence-aligned bilingual corpus. Similar to phrase-based SMT, our approach starts with an improved word-alignment that is created by intersecting alignment matrices for both translation directions, and refining the intersection alignment by adding directly adjacent alignment points, and alignment points that align previously unaligned words (see Och et al. (1999)). Next, source and target sentences are parsed using source and target LFG grammars to produce a set of possible f(unctional) dependency structures for each side (see Riezler et al. (2002) for the English grammar and parser; Butt et al. (2002) for German). The two f-structures that most preserve dependencies are selected for further consideration. Selecting the most similar instead of the most probable f-structures is advantageous for rule induction since it provides for higher coverage with simpler rules. In the third step, the manyto-many word alignment created in the first step is used to define many-to-many correspondences between the substructures of the f-structures selected in the second step. The parsing process maintains an association between words in the string and p"
N06-1032,N03-1026,1,0.891162,"Missing"
N06-1032,C04-1073,0,0.00495032,"process. Phrase-based translation with multi-word units excels at modeling local ordering and short idiomatic expressions, however, it lacks a mechanism to learn long-distance dependencies and is unable to generalize to unseen phrases that share non-overt linguistic information. Publicly available statistical parsers can provide the syntactic information that is necessary for linguistic generalizations and for the resolution of non-local dependencies. This information source is deployed in recent work either for pre-ordering source sentences before they are input to to a phrase-based system (Xia and McCord, 2004; Collins et al., 2005), or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005) 1 . While these approaches deploy dependency-style grammars for parsing source and/or target text, a utilization of grammar-based generation on the output of translation models has not yet been attempted in dependency-based SMT. Instead, simple target language realization models that can easily be trained to reflect the ordering of the reference translations in the"
N06-1032,2001.mtsummit-ebmt.4,0,\N,Missing
N06-1032,P02-1040,0,\N,Missing
N06-1032,N03-1017,0,\N,Missing
N10-1071,J90-1003,0,0.114694,"e describe below, we employ pointwise mutual information (PMI) as a measure of the association between two terms or queries. Let wi and wj be two strings that we want to measure the amount of association between. Let p(wi ) and p(wj ) be the probability of observing wi and wj in a given model; e.g., relative frequencies estimated from occurrence counts in a corpus. We also define p(wi , wj ) as the joint probability of wi and wj ; i.e., the probability of the two strings occurring together. We define PMI as follows: PMI(wi , wj ) = log p(wi , wj ) . p(wi )p(wj ) (1) PMI has been introduced by Church and Hanks (1990) as word assosiatio ratio, and since then been used extensively to model semantic similarity. Among several desirable properties, it correlates well with human judgments (Recchia and Jones, 2009). 2.2 alization and specialization (Lau and Horvitz, 1999; Rieh and Xie, 2006). Boldi et al. (2009) show how knowledge of transition types can positively impact query reformulation. We would like to exploit this information as well. However, rather than building a dedicated supervised classifier for this task we try to capture it directly at the source. First, we notice how string features; e.g., lengt"
N10-1071,P02-1035,1,0.748069,"reduces the number of comparisons to |Pq |= 480 j=i+1 |li ||lj |. For bipartite ranking of p i=1 positive and n negative instances, |Pq |= p · n comparisons are necessary. 3.2.4 Log-linear Models for Bipartite Ranking A probabilistic model for bipartite ranking can be defined as the conditional probability of the set of relevant rewrites, i.e., rewrites at rank level 1, given all rewrites at rank levels 1 and 2. A formalization in the family of log-linear models yields the following logistic loss function `llm that was used for discriminative estimation from sets of partially labeled data in Riezler et al. (2002): P hw,φ(xqi )i xqi ∈xq |yqi =1 e `llm (w) = − log P . hw,φ(xqi )i xqi ∈xq e The gradient of `llm is calculated as a difference between two expectations: ∂ `llm = −pw [φk |xq ; yqi = 1] + pw [φk |xq ] . ∂wk The SGD computation for the log-linear model is dominated by the computation of expectations for each query. The logistic loss for bipartite ranking is henceforth referred to as the log-linear model. 4 Experimental Results In the experiments reported in this paper, we trained linear ranking models on 1 billion query-rewrite pairs using 60 dense features, combined of the building blocks of s"
N15-1123,D08-1024,0,0.250173,"t al. (2014) from parallel training data (over 104M words) consisting of the Europarl corpus (Koehn, 2005) in version 7, the News Commentary corpus, and the Common Crawl corpus (Smith et al., 2013). Word alignments were created with fast align (Dyer et al., 2013). The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles. Language model scores are added to the search spaces using the cube pruning algorithm (Huang and Chiang, 2007) with poplimit = 200. SMT Model parameters were optimized using MIRA (Chiang et al., 2008) on the WMT2011 news test set (3003 Experimental Results. We first find a default weight v using grid search within v = [0, 3] and v = [0, 2] on the development sets for Wikipedia and patents, respectively. v controls the balance between the retrieval and translation features and with larger v, the model is more likely to produce query derivations diverging from the SMT 1-best translation. For Wikipedia, we sample 1,000 out of 10,000 queries to reduce the time of the grid search. For patents we use the full development set of 2,000 queries with 8,381 sentences. We combine rankings for single-s"
N15-1123,J07-2003,0,0.788867,"words representation of the target documents to be ranked. The SMT model is extended by retrieval-specific features that are optimized jointly with standard translation features for a ranking objective. We find significant gains over the state-of-the-art in a large-scale evaluation on cross-lingual search in the domains patents and Wikipedia. 1 Introduction Approaches to CLIR have been plentiful and diverse. While simple word translation probabilities are easily integrated into term-based retrieval models (Berger and Lafferty, 1999; Xu et al., 2001), state-of-the-art SMT systems (Koehn, 2010; Chiang, 2007) are complex statistical models on their own. The use of established translation models for context-aware translation of query strings, effectively reducing the problem of CLIR to a pipeline of translation and monolingual retrieval, has been shown to work well in the past (Chin et al., 2008). Only recently, approaches have been presented to include (weighted) translation alternatives into the query structure to allow a more generalized term Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or"
N15-1123,C14-1192,0,0.0225052,"ed term matching (Ture et al., 2012a; Ture et al., 2012b). Less work has been devoted to optimizing SMT towards a retrieval objective, for example in a re-ranking framework (Nikoulina et al., 2012) or by integrating a decomposable proxy for retrieval quality of query translations into discriminative ranking (Sokolov et al., 2014). The idea of forced decoding has been employed recently to select better perceptron updates from the 1173 full SMT search space for discriminative parameter tuning of SMT systems (Yu et al., 2013; Zhao et al., 2014). Most similar to our approach is the recent work of Dong et al. (2014) who use the Moses translation option lattices for translation retrieval, i.e., for mining comparable data. Their query lattices given by the translation options encode exponentially many queries and are used to retrieve the most probable translation candidate from a set of candidates. The approach is evaluated in the context of a parallel corpus mining system. We present a model that not only uses the full search space, including the language model and reordering information, but also evaluate the model specifically for the task of retrieval, rather than mate-finding only. We show that a forc"
N15-1123,P10-4002,0,0.343307,"es at retrieval time. However, they either use context-free word-based translation tables or select only terms from a small n-best fraction of the full search space. Dynamic Programming on Hypergraphs. Decoding in a hierarchical phrase-based SMT (Chiang, 2007) is usually understood as a two-step process: Initially, an input sentence is parsed using a Weighted Synchronous Context-Free Grammar (WSCFG) in a bottom-up manner to construct an initial hypergraph H that compactly encodes the full search space (“translation forest”) (Gallo et al., 1993; Klein and Manning, 2001; Huang and Chiang, 2005; Dyer et al., 2010). An ordered, directed hypergraph H is a tuple hV, E, g, Wi, consisting of a finite set of nodes V , a finite set of hyperedges E, and weight function W : E 7→ R assigning realvalued weights to e ∈ E. Language models are typically added in a second rescoring phase that is carried out by approximate solutions, such as cubepruning (Chiang, 2007; Huang and Chiang, 2007), limiting the number of derivations created at each node. A translation hypothesis h ∈ E corresponds to a sequence of nodes S ⊆ V connected via hyperedges e ending in goal node g. Each edge e is associated with a synchronous gramm"
N15-1123,N13-1073,0,0.0169121,"er et al., 2007). Significance levels are either indicated as superscripts, or provided in the captions of the respective tables. Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec (Dyer et al., 2010). For German-English cross-lingual article retrieval on Wikipedia, we built a system analogously to Schamoni et al. (2014) from parallel training data (over 104M words) consisting of the Europarl corpus (Koehn, 2005) in version 7, the News Commentary corpus, and the Common Crawl corpus (Smith et al., 2013). Word alignments were created with fast align (Dyer et al., 2013). The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles. Language model scores are added to the search spaces using the cube pruning algorithm (Huang and Chiang, 2007) with poplimit = 200. SMT Model parameters were optimized using MIRA (Chiang et al., 2008) on the WMT2011 news test set (3003 Experimental Results. We first find a default weight v using grid search within v = [0, 3] and v = [0, 2] on the development sets for Wikipedia and patents, respectively. v controls the balance between the r"
N15-1123,J99-4004,0,0.0174848,"sociated with a synchronous grammar rule r(e), and corresponding feature values Φ(r(e)). The weight of hyperedge e is defined as W(e; w) = wT Φ(r(e)). The quantity in (1) is efficiently computed using dynamic programming under the properL semiring. A N commutative semiring K is a tuple hK, , , ¯0, ¯1i, of a set K, Lan associative and commutative addition operator , an associative multiplication operator N , and their “neutral” elements ¯ 0 and ¯1, respectively (Dyer, 2010). The Inside algorithm over the topologically sorted, acyclic hypergraph H under the tropical hR, max, ×, −∞, 0i semiring (Goodman, 1999; Mohri, 2009) computes the inside score α of the Viterbi hypothesis, i.e. the weight of its sequence of nodes ending in goal node g: arg maxP (h|q) ≡ α(g) h∈Eq = M O W(e; wsmt ), h∈Hq e∈h T Φ where W(e; wsmt ) = wsmt smt (r(e)) assigns weights given parameters and features of the translation model. For Bag-of-Words Forced Decoding, we extend W with another set of parameters wir for local IR features Φir : arg maxP (h|q, d) ≡ α(g) h∈Eq = M O W 0 (e, d; wsmt , wir ), (4) h∈Hq e∈h T Φ with W 0 (e, d; wsmt , wir ) = wsmt smt (r(e)) + T wir Φir (r(e), d). Note that Φir depends on both translation"
N15-1123,W11-2123,0,0.172456,"vided in the captions of the respective tables. Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec (Dyer et al., 2010). For German-English cross-lingual article retrieval on Wikipedia, we built a system analogously to Schamoni et al. (2014) from parallel training data (over 104M words) consisting of the Europarl corpus (Koehn, 2005) in version 7, the News Commentary corpus, and the Common Crawl corpus (Smith et al., 2013). Word alignments were created with fast align (Dyer et al., 2013). The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles. Language model scores are added to the search spaces using the cube pruning algorithm (Huang and Chiang, 2007) with poplimit = 200. SMT Model parameters were optimized using MIRA (Chiang et al., 2008) on the WMT2011 news test set (3003 Experimental Results. We first find a default weight v using grid search within v = [0, 3] and v = [0, 2] on the development sets for Wikipedia and patents, respectively. v controls the balance between the retrieval and translation features and with larger v, the model is more likely t"
N15-1123,W05-1506,0,0.0159075,"d translation alternatives at retrieval time. However, they either use context-free word-based translation tables or select only terms from a small n-best fraction of the full search space. Dynamic Programming on Hypergraphs. Decoding in a hierarchical phrase-based SMT (Chiang, 2007) is usually understood as a two-step process: Initially, an input sentence is parsed using a Weighted Synchronous Context-Free Grammar (WSCFG) in a bottom-up manner to construct an initial hypergraph H that compactly encodes the full search space (“translation forest”) (Gallo et al., 1993; Klein and Manning, 2001; Huang and Chiang, 2005; Dyer et al., 2010). An ordered, directed hypergraph H is a tuple hV, E, g, Wi, consisting of a finite set of nodes V , a finite set of hyperedges E, and weight function W : E 7→ R assigning realvalued weights to e ∈ E. Language models are typically added in a second rescoring phase that is carried out by approximate solutions, such as cubepruning (Chiang, 2007; Huang and Chiang, 2007), limiting the number of derivations created at each node. A translation hypothesis h ∈ E corresponds to a sequence of nodes S ⊆ V connected via hyperedges e ending in goal node g. Each edge e is associated with"
N15-1123,P07-1019,0,0.112815,"nchronous Context-Free Grammar (WSCFG) in a bottom-up manner to construct an initial hypergraph H that compactly encodes the full search space (“translation forest”) (Gallo et al., 1993; Klein and Manning, 2001; Huang and Chiang, 2005; Dyer et al., 2010). An ordered, directed hypergraph H is a tuple hV, E, g, Wi, consisting of a finite set of nodes V , a finite set of hyperedges E, and weight function W : E 7→ R assigning realvalued weights to e ∈ E. Language models are typically added in a second rescoring phase that is carried out by approximate solutions, such as cubepruning (Chiang, 2007; Huang and Chiang, 2007), limiting the number of derivations created at each node. A translation hypothesis h ∈ E corresponds to a sequence of nodes S ⊆ V connected via hyperedges e ending in goal node g. Each edge e is associated with a synchronous grammar rule r(e), and corresponding feature values Φ(r(e)). The weight of hyperedge e is defined as W(e; w) = wT Φ(r(e)). The quantity in (1) is efficiently computed using dynamic programming under the properL semiring. A N commutative semiring K is a tuple hK, , , ¯0, ¯1i, of a set K, Lan associative and commutative addition operator , an associative multiplication oper"
N15-1123,W01-1812,0,0.0755109,"tion by keeping enumerated translation alternatives at retrieval time. However, they either use context-free word-based translation tables or select only terms from a small n-best fraction of the full search space. Dynamic Programming on Hypergraphs. Decoding in a hierarchical phrase-based SMT (Chiang, 2007) is usually understood as a two-step process: Initially, an input sentence is parsed using a Weighted Synchronous Context-Free Grammar (WSCFG) in a bottom-up manner to construct an initial hypergraph H that compactly encodes the full search space (“translation forest”) (Gallo et al., 1993; Klein and Manning, 2001; Huang and Chiang, 2005; Dyer et al., 2010). An ordered, directed hypergraph H is a tuple hV, E, g, Wi, consisting of a finite set of nodes V , a finite set of hyperedges E, and weight function W : E 7→ R assigning realvalued weights to e ∈ E. Language models are typically added in a second rescoring phase that is carried out by approximate solutions, such as cubepruning (Chiang, 2007; Huang and Chiang, 2007), limiting the number of derivations created at each node. A translation hypothesis h ∈ E corresponds to a sequence of nodes S ⊆ V connected via hyperedges e ending in goal node g. Each e"
N15-1123,P07-2045,0,0.00307459,"s task from a product of experts perspective (Hinton, 2002) and multiply scores score(·, d) of document d in all m sentence rankings, re-sorting the final output. If d is not in the top-k ranking of a sentence, we take the minimum score of that top-k ranking as a smoothing value to prevent the product to become zero. 2 bm25 parameters were fixed at k1 = 1.2 and b = 0.75 Implementation and Complexity Analysis.3 We implemented the above model on top of the hierarchical phrase-based decoder cdec (Dyer et al., 2010), but there are no limitations for applying this approach to phrase-based systems (Koehn et al., 2007). Procedurally, after cdec yields the translation forest, we compute the overlap of IR feature activations between edges in the forest and the document candidates. The Inside algorithm is only carried out for documents that activate at least one IR feature in the search space. For documents with no activation we can skip the computation of scores and assign the SMT Viterbi score, which constitutes a lower bound on the model score. For a single query q, forced decoding requires a single pass over the topologically sorted search space to find IR feature activations along hyperedges, yielding a c"
N15-1123,2005.mtsummit-papers.11,0,0.0293605,"riented measures. Differences in evaluation scores between two systems were tested for statistical significance using paired randomization tests (Smucker et al., 2007). Significance levels are either indicated as superscripts, or provided in the captions of the respective tables. Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec (Dyer et al., 2010). For German-English cross-lingual article retrieval on Wikipedia, we built a system analogously to Schamoni et al. (2014) from parallel training data (over 104M words) consisting of the Europarl corpus (Koehn, 2005) in version 7, the News Commentary corpus, and the Common Crawl corpus (Smith et al., 2013). Word alignments were created with fast align (Dyer et al., 2013). The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles. Language model scores are added to the search spaces using the cube pruning algorithm (Huang and Chiang, 2007) with poplimit = 200. SMT Model parameters were optimized using MIRA (Chiang et al., 2008) on the WMT2011 news test set (3003 Experimental Results. We first find a default weig"
N15-1123,J10-4005,0,0.0242986,"duce a bag-ofwords representation of the target documents to be ranked. The SMT model is extended by retrieval-specific features that are optimized jointly with standard translation features for a ranking objective. We find significant gains over the state-of-the-art in a large-scale evaluation on cross-lingual search in the domains patents and Wikipedia. 1 Introduction Approaches to CLIR have been plentiful and diverse. While simple word translation probabilities are easily integrated into term-based retrieval models (Berger and Lafferty, 1999; Xu et al., 2001), state-of-the-art SMT systems (Koehn, 2010; Chiang, 2007) are complex statistical models on their own. The use of established translation models for context-aware translation of query strings, effectively reducing the problem of CLIR to a pipeline of translation and monolingual retrieval, has been shown to work well in the past (Chin et al., 2008). Only recently, approaches have been presented to include (weighted) translation alternatives into the query structure to allow a more generalized term Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented"
N15-1123,C12-3040,0,0.0429763,"Missing"
N15-1123,N10-1069,0,0.0347599,"Missing"
N15-1123,E12-1012,0,0.412798,"statistical models on their own. The use of established translation models for context-aware translation of query strings, effectively reducing the problem of CLIR to a pipeline of translation and monolingual retrieval, has been shown to work well in the past (Chin et al., 2008). Only recently, approaches have been presented to include (weighted) translation alternatives into the query structure to allow a more generalized term Attempts to inform the SMT system about its use for retrieval by optimizing its parameters towards a retrieval objective have been presented in the form or re-ranking (Nikoulina et al., 2012) or ranking (Sokolov et al., 2014). In this paper, we take this idea a step further and directly integrate the task of scoring documents with respect to the query into the process of translation decoding. We make the full expressiveness of the translation search space available to the retrieval model, without enumerating all possible translation alternatives. This is done by augmenting the linear model of the SMT system with features that relate partial translation hypotheses to documents in the retrieval collection. These retrieval-specific features decompose over partial translation hypothes"
N15-1123,P14-2080,1,0.801586,"ures for words in the bagof-words (BOW) representation of documents force the decoder to prefer relevant documents with high probability, by a slight abuse of terminology, we call our approach BOW Forced Decoding. One of the key features of our approach is the use of context-sensitive information such as the language model and reordering information. We show that the use of such a translation-benign search space is crucial to outperform state-of-the-art CLIR approaches. Our experimental evaluation of retrieval performance is done on Wikipedia cross-lingual article retrieval (Bai et al., 2010; Schamoni et al., 2014) and patent prior art search (Fujii et al., 2009; Guo and Gomes, 2009; Sokolov et al., 2013; Schamoni et al., 2014). On both datasets, we show substantial improvements over the CLIR baselines of direct translation (Chin et al., 2008) or Probabilistic Structured Queries (Ture et al., 2012b), with and without further parameter tuning using learning-to-rank techniques and extended feature sets. From our results we conclude, that, in spite of algorithmic complexity, it is central to model translation and retrieval jointly to create more powerful CLIR models. 2 Related Work The framework of transla"
N15-1123,P13-1135,0,0.0138044,"or statistical significance using paired randomization tests (Smucker et al., 2007). Significance levels are either indicated as superscripts, or provided in the captions of the respective tables. Baseline SMT systems and BOW-FD share the hierarchical phrase-based SMT systems built with cdec (Dyer et al., 2010). For German-English cross-lingual article retrieval on Wikipedia, we built a system analogously to Schamoni et al. (2014) from parallel training data (over 104M words) consisting of the Europarl corpus (Koehn, 2005) in version 7, the News Commentary corpus, and the Common Crawl corpus (Smith et al., 2013). Word alignments were created with fast align (Dyer et al., 2013). The 4-gram language model was trained with the KenLM toolkit (Heafield, 2011) on the English side of the training data and the English Wikipedia articles. Language model scores are added to the search spaces using the cube pruning algorithm (Huang and Chiang, 2007) with poplimit = 200. SMT Model parameters were optimized using MIRA (Chiang et al., 2008) on the WMT2011 news test set (3003 Experimental Results. We first find a default weight v using grid search within v = [0, 3] and v = [0, 2] on the development sets for Wikiped"
N15-1123,D13-1175,1,0.897335,"er relevant documents with high probability, by a slight abuse of terminology, we call our approach BOW Forced Decoding. One of the key features of our approach is the use of context-sensitive information such as the language model and reordering information. We show that the use of such a translation-benign search space is crucial to outperform state-of-the-art CLIR approaches. Our experimental evaluation of retrieval performance is done on Wikipedia cross-lingual article retrieval (Bai et al., 2010; Schamoni et al., 2014) and patent prior art search (Fujii et al., 2009; Guo and Gomes, 2009; Sokolov et al., 2013; Schamoni et al., 2014). On both datasets, we show substantial improvements over the CLIR baselines of direct translation (Chin et al., 2008) or Probabilistic Structured Queries (Ture et al., 2012b), with and without further parameter tuning using learning-to-rank techniques and extended feature sets. From our results we conclude, that, in spite of algorithmic complexity, it is central to model translation and retrieval jointly to create more powerful CLIR models. 2 Related Work The framework of translation-model based retrieval has been introduced by Berger and Lafferty (1999). An extension"
N15-1123,C12-1164,0,0.229366,"tion such as the language model and reordering information. We show that the use of such a translation-benign search space is crucial to outperform state-of-the-art CLIR approaches. Our experimental evaluation of retrieval performance is done on Wikipedia cross-lingual article retrieval (Bai et al., 2010; Schamoni et al., 2014) and patent prior art search (Fujii et al., 2009; Guo and Gomes, 2009; Sokolov et al., 2013; Schamoni et al., 2014). On both datasets, we show substantial improvements over the CLIR baselines of direct translation (Chin et al., 2008) or Probabilistic Structured Queries (Ture et al., 2012b), with and without further parameter tuning using learning-to-rank techniques and extended feature sets. From our results we conclude, that, in spite of algorithmic complexity, it is central to model translation and retrieval jointly to create more powerful CLIR models. 2 Related Work The framework of translation-model based retrieval has been introduced by Berger and Lafferty (1999). An extension to the cross-lingual case using contextfree lexical translation tables has been given by Xu et al. (2001). While the industry standard to CLIR is a pipeline of SMT-based query translation feeding i"
N15-1123,D13-1112,0,0.0203012,"eighted) SMT translation alternatives into the query structure to allow a more generalized term matching (Ture et al., 2012a; Ture et al., 2012b). Less work has been devoted to optimizing SMT towards a retrieval objective, for example in a re-ranking framework (Nikoulina et al., 2012) or by integrating a decomposable proxy for retrieval quality of query translations into discriminative ranking (Sokolov et al., 2014). The idea of forced decoding has been employed recently to select better perceptron updates from the 1173 full SMT search space for discriminative parameter tuning of SMT systems (Yu et al., 2013; Zhao et al., 2014). Most similar to our approach is the recent work of Dong et al. (2014) who use the Moses translation option lattices for translation retrieval, i.e., for mining comparable data. Their query lattices given by the translation options encode exponentially many queries and are used to retrieve the most probable translation candidate from a set of candidates. The approach is evaluated in the context of a parallel corpus mining system. We present a model that not only uses the full search space, including the language model and reordering information, but also evaluate the model"
N15-1123,P14-2127,0,0.0123034,"slation alternatives into the query structure to allow a more generalized term matching (Ture et al., 2012a; Ture et al., 2012b). Less work has been devoted to optimizing SMT towards a retrieval objective, for example in a re-ranking framework (Nikoulina et al., 2012) or by integrating a decomposable proxy for retrieval quality of query translations into discriminative ranking (Sokolov et al., 2014). The idea of forced decoding has been employed recently to select better perceptron updates from the 1173 full SMT search space for discriminative parameter tuning of SMT systems (Yu et al., 2013; Zhao et al., 2014). Most similar to our approach is the recent work of Dong et al. (2014) who use the Moses translation option lattices for translation retrieval, i.e., for mining comparable data. Their query lattices given by the translation options encode exponentially many queries and are used to retrieve the most probable translation candidate from a set of candidates. The approach is evaluated in the context of a parallel corpus mining system. We present a model that not only uses the full search space, including the language model and reordering information, but also evaluate the model specifically for th"
N15-1123,1998.amta-tutorials.5,0,\N,Missing
N15-1149,P13-2009,0,0.180115,"test set of original English queries. We show that if false discovery rate on incorrect English queries is taken into account in model selection, the semantic parser that yields best results for response-based learning in SMT can be found reliably. 2 Related Work Our work is most closely related to Riezler et al. (2014). We extend their application of responsebased learning for SMT to a larger and lexically more diverse dataset and show how to perform model selection in the environment from which response signals are obtained. In contrast to their work where a monolingual SMT-based approach (Andreas et al., 2013) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013), Berant et al. (2013), Goldwasser and Roth (2013), inter alia). In these works, extrinsic responses in form of correct answers from a database are used to alleviate the problem of manual data annotation in semantic parsing. Saluja et al. (2012) integrate human binary feedback on the qu"
N15-1149,P05-1074,0,0.0768621,"Missing"
N15-1149,P14-1133,0,0.458719,"anslated database query is executed against the Freebase database. We view learning from such task-specific feedback as adaptation of SMT parameters to the task of translating opendomain database queries, thereby grounding SMT in the task of multilingual database access. The success criterion for this task is F1-score in returning the correct answer from a semantic parse of the translated query, rather than BLEU. Since the semantic parser In this paper, we compare different ways of scaling up state-of-the-art semantic parsers for Freebase by adding synonyms and paraphrases. First, we consider Berant and Liang (2014)’s own extension of the semantic parser of Berant et al. (2013) by using paraphrases. Second, we apply WordNet synonyms (Miller, 1995) for selected parts of speech to the queries in the Free917 dataset. The new pairs of queries and logical forms are added to the dataset on which the semantic parsers are retrained. We find that both techniques of enhancing the lexical coverage of the semantic parsers result in improved parsing performance, and that the improvements add up nicely. However, improved parsing performance does not correspond to improved F1-score in answer retrieval when using the re"
N15-1149,D13-1160,0,0.763799,"We view learning from such task-specific feedback as adaptation of SMT parameters to the task of translating opendomain database queries, thereby grounding SMT in the task of multilingual database access. The success criterion for this task is F1-score in returning the correct answer from a semantic parse of the translated query, rather than BLEU. Since the semantic parser In this paper, we compare different ways of scaling up state-of-the-art semantic parsers for Freebase by adding synonyms and paraphrases. First, we consider Berant and Liang (2014)’s own extension of the semantic parser of Berant et al. (2013) by using paraphrases. Second, we apply WordNet synonyms (Miller, 1995) for selected parts of speech to the queries in the Free917 dataset. The new pairs of queries and logical forms are added to the dataset on which the semantic parsers are retrained. We find that both techniques of enhancing the lexical coverage of the semantic parsers result in improved parsing performance, and that the improvements add up nicely. However, improved parsing performance does not correspond to improved F1-score in answer retrieval when using the respective parser in a response-based learning framework. We show"
N15-1149,P13-1042,0,0.137996,"two (P2) and first three (P3) synsets, evaluated on the F REE 917 test set of correct database queries for F1 and including the test set of incorrect database queries for FDR, and trained on #data training queries. Best results are indicated in bold face. discovery rate (FDR) (Murphy, 2012), defined as FP/FP+TP, i.e., as the ratio of false positives out of all positive answer retrieval events. 6 Experiments We use a data dump of Freebase1 which was has been indexed by the Virtuoso SPARQL engine2 as our knowledge base. The corpus used in the experiments is the F REE 917 corpus as assembled by Cai and Yates (2013) and consists of 614 training and 276 test queries in English and corresponding logical forms.3 The dataset of negative examples, i.e., incorrect English database queries that should receive incorrect answers, consists of 166 examples that were judged either grammatically or semantically incorrect by the authors. The translation of the English queries in F REE 917 into German, in order to provide a set of source sentences for SMT, was done by the authors. The SMT framework used is CDEC (Dyer et al., 2010) with standard dense features and additional sparse features as described in Simianer et a"
N15-1149,P10-4002,0,0.0405911,"wledge base. The corpus used in the experiments is the F REE 917 corpus as assembled by Cai and Yates (2013) and consists of 614 training and 276 test queries in English and corresponding logical forms.3 The dataset of negative examples, i.e., incorrect English database queries that should receive incorrect answers, consists of 166 examples that were judged either grammatically or semantically incorrect by the authors. The translation of the English queries in F REE 917 into German, in order to provide a set of source sentences for SMT, was done by the authors. The SMT framework used is CDEC (Dyer et al., 2010) with standard dense features and additional sparse features as described in Simianer et al. (2012)4 . Training of the baseline SMT system was performed on the C OMMON C RAWL5 (Smith 1 http://www.freebase.com/ http://virtuoso.openlinksw.com/ 3 Note that we filtered out 33 questions (21 from the training set and 12 from the test set) because their logical forms only returned an empty string as an answer. 4 https://github.com/pks/cdec-dtrain 5 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2 1342 et al., 2013) dataset consisting of 7.5M parallel English-German segments extracted"
N15-1149,N12-1023,0,0.0603097,"e, scores best under the FDR metric8 . Table 2 shows an evaluation of the use of different parsing models to retrieve correct answers from the F REE 917 test set of correct database queries. The systems are applied to translated queries, but evaluated in terms of standard parsing metrics. Statistical significance is measured using an Approximate Randomization test (Noreen, 1989; Riezler and Maxwell, 2005). The baseline system is CDEC as described above. It never sees the F REE 917 data during training. As a second baseline method we use a stochastic (sub)gradient descent variant of R AM PION (Gimpel and Smith, 2012). This system is 6 https://github.com/pks/rebol www-nlp.stanford.edu/software/sempre 8 Note that in case of FDR, smaller is better. 7 S P P1 P2 P3 R AMPION 1 CDEC 2 40.0 42.92 42.92 43.81 43.36 40.36 44.59 46.361 45.92 45.92 3 take advantage of parsers with good FDR score since learning to move away from translations dissimilar to the reference is helpful if they do not lead to correct answers. REBOL can make the best use of parsers with low FDR score since it can learn to prevent incorrect translations from hurting parsing performance at test time. R EBOL 42.9212 45.85 48.81 47.06 47.49 Table"
N15-1149,P12-1051,0,0.0131941,"ng as an answer. 4 https://github.com/pks/cdec-dtrain 5 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2 1342 et al., 2013) dataset consisting of 7.5M parallel English-German segments extracted from the web. Response-based learning for SMT uses the code described in Riezler et al. (2014)6 . For semantic parsing we use the SEMPRE and PARASEMPRE tools of Berant et al. (2013) and Berant and Liang (2014) which were trained on the training portion of the F REE 917 corpus7 . Further models use the training data enhanced with synonyms from WordNet as described in Section 4. Following Jones et al. (2012), we evaluate semantic parsers according to precision, defined as the percentage of correctly answered examples out of those for which a parse could be produced, recall, defined as the percentage of total examples answered correctly, and F1-score, defined as harmonic mean of precision and recall. Furthermore, we report false discovery rate (FDR) on the combined set of 276 correct and 166 incorrect database queries. Table 1 reports standard parsing evaluation metrics for the different parsers SEMPRE (S), PARASEMPRE (P), and extensions of the latter with synonyms from the first one (P1), first t"
N15-1149,D13-1161,0,0.0323381,"t al. (2014). We extend their application of responsebased learning for SMT to a larger and lexically more diverse dataset and show how to perform model selection in the environment from which response signals are obtained. In contrast to their work where a monolingual SMT-based approach (Andreas et al., 2013) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013), Berant et al. (2013), Goldwasser and Roth (2013), inter alia). In these works, extrinsic responses in form of correct answers from a database are used to alleviate the problem of manual data annotation in semantic parsing. Saluja et al. (2012) integrate human binary feedback on the quality of an SMT system output into a discriminative learner. Further work on learning from weak supervision signals has been presented in the machine learning community, e.g., in form of coactive learning (Shivaswamy and Joachims, 2012), reinforcement learning (Sutton and Barto, 1998), or online learning with li"
N15-1149,C12-1121,0,0.0815567,"Missing"
N15-1149,W05-0908,1,0.59276,"EM PRE improves F1 by nearly 10 points over SEMPRE . Another 0.5 points are added by extending the training data using two synsets. The third column shows that the system P1 that scored second-worst in terms of F1 score, scores best under the FDR metric8 . Table 2 shows an evaluation of the use of different parsing models to retrieve correct answers from the F REE 917 test set of correct database queries. The systems are applied to translated queries, but evaluated in terms of standard parsing metrics. Statistical significance is measured using an Approximate Randomization test (Noreen, 1989; Riezler and Maxwell, 2005). The baseline system is CDEC as described above. It never sees the F REE 917 data during training. As a second baseline method we use a stochastic (sub)gradient descent variant of R AM PION (Gimpel and Smith, 2012). This system is 6 https://github.com/pks/rebol www-nlp.stanford.edu/software/sempre 8 Note that in case of FDR, smaller is better. 7 S P P1 P2 P3 R AMPION 1 CDEC 2 40.0 42.92 42.92 43.81 43.36 40.36 44.59 46.361 45.92 45.92 3 take advantage of parsers with good FDR score since learning to move away from translations dissimilar to the reference is helpful if they do not lead to corr"
N15-1149,P14-1083,1,0.590455,"Missing"
N15-1149,2012.amta-papers.14,0,0.0145153,"where a monolingual SMT-based approach (Andreas et al., 2013) is used as semantic parser, our work builds on existing parsers for Freebase, with a focus on exploiting paraphrasing and synonym extension for scaling semantic parsers to open-domain database queries. Response-based learning has been applied in previous work to semantic parsing itself (Kwiatowski et al. (2013), Berant et al. (2013), Goldwasser and Roth (2013), inter alia). In these works, extrinsic responses in form of correct answers from a database are used to alleviate the problem of manual data annotation in semantic parsing. Saluja et al. (2012) integrate human binary feedback on the quality of an SMT system output into a discriminative learner. Further work on learning from weak supervision signals has been presented in the machine learning community, e.g., in form of coactive learning (Shivaswamy and Joachims, 2012), reinforcement learning (Sutton and Barto, 1998), or online learning with limited feedback (Cesa-Bianchi and Lugosi, 2006). 3 Response-based Online SMT Learning We denote by φ(x, y) a joint feature representation of input sentences x and output translations 1340 Algorithm 1 Response-based Online Learning repeat for i ="
N15-1149,P12-1002,1,0.852761,"Yates (2013) and consists of 614 training and 276 test queries in English and corresponding logical forms.3 The dataset of negative examples, i.e., incorrect English database queries that should receive incorrect answers, consists of 166 examples that were judged either grammatically or semantically incorrect by the authors. The translation of the English queries in F REE 917 into German, in order to provide a set of source sentences for SMT, was done by the authors. The SMT framework used is CDEC (Dyer et al., 2010) with standard dense features and additional sparse features as described in Simianer et al. (2012)4 . Training of the baseline SMT system was performed on the C OMMON C RAWL5 (Smith 1 http://www.freebase.com/ http://virtuoso.openlinksw.com/ 3 Note that we filtered out 33 questions (21 from the training set and 12 from the test set) because their logical forms only returned an empty string as an answer. 4 https://github.com/pks/cdec-dtrain 5 http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 2 1342 et al., 2013) dataset consisting of 7.5M parallel English-German segments extracted from the web. Response-based learning for SMT uses the code described in Riezler et al. (2014)6 . F"
N15-1149,P13-1135,0,0.0505928,"Missing"
N15-1149,N03-1033,0,0.00465808,"ueries: In their system, called PARASEMPRE, pairs of logical forms and utterances are generated from a given query and the database, and the pair whose utterance best paraphrases the input query is selected. These new pairs of queries and logical forms are added as ambiguous labels in training a model from queryanswer pairs. Following a similar idea of extending parser coverage by paraphrases, we extend the training set with synonyms from WordNet. This is done by iterating over the queries in the F REE 917 dataset. To ensure that the replacement is sensible, each sentence is first POS tagged (Toutanova et al., 2003) and WordNet lookups are restricted to matching POS between synonym and query words, for nouns, verbs, adjectives and adverbs. Lastly, in order to limit the number of retrieved words, a WordNet lookup is performed by carefully choosing from the first three synsets which are ordered from most common to least frequently used sense. Within a synset all words are taken. The new training queries are appended to the training portion of F REE 917. 5 Model Selection The most straightforward strategy to perform model selection for the task of response-based learning for SMT is to rely on parsing evalua"
N16-1088,P13-2009,0,0.402254,"l variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be trained from annotated data. OSM has previously been used by Boye et al. (2014) for pedestrian routing using a dialogue system, however, no details on semantic parsing and no resource are provided. Our SMT tuning experiment builds on the work of Riezler et al. (2014) and Haas and Riezler (2015) who applied response-based learning for SMT to the G EOQUERY and F REE 917 domains, respectively. press hierarchical structures. The Overpass API2 can be used to query the database made up of the aforementioned nodes, ways and relations. It can ef"
N16-1088,D15-1198,0,0.032068,"domain of US geography, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be traine"
N16-1088,D13-1160,0,0.288915,"57,159 Table 1: Statistics of OSM as of December 14th, 2015 2005) and F REE 917 (Cai and Yates, 2013). While G EOQUERY queries are restricted to the closed domain of US geography, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that"
N16-1088,P13-1042,0,0.0246245,"Missing"
N16-1088,P11-2031,0,0.0326383,"Missing"
N16-1088,P10-4002,0,0.257713,"ate-of-the-art performance on G EOQUERY. In this framework it is crucial that the MRL can be represented as a tree. A pre-order tree traversal can give a unique string in which each node is a word (i.e. surrounded by white space). Once the MRL has been converted into such a structure (for an example see Figure 2), a word aligner, here GIZA++ (Och and Ney, 2003), can be used to generate word-to-word alignments in both translation directions which can then be combined with various heuristics (Koehn, 2010, Chapter 4.5.3). From the next step onwards we use the freely available SMT framework CDEC (Dyer et al., 2010). After building a language model for the target (MRL) side, SCFG grammars for hierarchical phrases for tuning and testing were extracted. Experiments in n-gram order showed that 5-gram models are sufficient for language modelling. At test time, a critical issue is the fact that monolingual SMT does not ensure that the translations are valid MRL formulae. Thus a k-best list (sorted from most probable to least) is generated which needs to be traversed until a valid formula is found. Experimentation with the k-best list size showed that 100 is a good trade-off between speed and performance. A bi"
N16-1088,N12-1023,0,0.0211305,"questions were translated by the first author into German. As parser we chose to use number 3 (+intersect +stem +cdec +pass +cfg) from Table 2, deciding against the use of sparse features due speed reasons. The CDEC (Dyer et al., 2010) decoder was used for machine translation from German to English. Here we employ its standard features plus additional sparse features5 and the C OMMON C RAWL6 (Smith et al., 2013) corpus to built the baseline SMT system. R EBOL is compared to a baseline system without discriminative training (CDEC) and to a stochastic (sub)gradient descent variant of R AMPION (Gimpel and Smith, 2012). Both baseline systems do not make use of the feedback from the semantic parser. While both R EBOL and R AMPION assume the availability of both a reference translation and a gold parse, response-based learning can also succeed without any access to reference translations or even to gold standard parses. Riezler et al. (2014) introduced an algorithm, called E XEC, that only relies on task-based feedback and omits the cost function based on sentence-wise BLEU. Collecting real world data for this algorithm is realistic for an online interface to OSM since it only requires a user to pose a questi"
N16-1088,N15-1149,1,0.763053,"problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be trained from annotated data. OSM has previously been used by Boye et al. (2014) for pedestrian routing using a dialogue system, however, no details on semantic parsing and no resource are provided. Our SMT tuning experiment builds on the work of Riezler et al. (2014) and Haas and Riezler (2015) who applied response-based learning for SMT to the G EOQUERY and F REE 917 domains, respectively. press hierarchical structures. The Overpass API2 can be used to query the database made up of the aforementioned nodes, ways and relations. It can efficiently extract the correct subset of database objects that satisfy the entered constraints. The following constraints are most relevant for our corpus creation later on: • The simplest constraints require the database objects to have certain keys or key-value pairs. For example to search for hotels, one would use “node[tourism=‘hotel’];out;”. • Ov"
N16-1088,P03-1054,0,0.0141882,"t a comparison of the lexical and syntactic complexity of the three corpora. All statistics reported in Figure 5 are normalized by the number of sentences. Lexically, NL MAPS is more diverse than G EO QUERY , as can be seen by the average number of types, but less so compared to F REE 917 due to the fact that the OSM database is still a somewhat more closed domain compared to Freebase. Syntactically however, NL MAPS is with 3 more words on average per sentence more complex than G EOQUERY and F REE 917, which have nearly identical sentence length. As a further test, we ran the Stanford Parser (Klein and Manning, 2003) on the queries to generate syntactic parse trees. We then counted the number of non-terminals required to produce the parse tree. This result reaffirms what the simpler sentence length already reported: the language in NL MAPS is more complex than in the other two corpora, which have identical complexity. In Figure 4 we report the number of operators and 745 values needed to construct the different gold formulae. While there are a few questions that need a formula shorter than 10, the vast majority needs a length of around 15, followed by a long tail of sizes with decreasing frequency of up t"
N16-1088,J10-4005,0,0.0249498,"Missing"
N16-1088,D13-1161,0,0.424297,",232 1,259,132,137 76,204,309 57,159 Table 1: Statistics of OSM as of December 14th, 2015 2005) and F REE 917 (Cai and Yates, 2013). While G EOQUERY queries are restricted to the closed domain of US geography, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient"
N16-1088,C12-1121,0,0.0226987,"Missing"
N16-1088,J03-1002,0,0.00810864,"Missing"
N16-1088,P03-1021,0,0.0190848,"Missing"
N16-1088,P02-1040,0,0.0961205,"and then indicate if it was answered to their satisfaction. 5 https://github.com/pks/cdec-dtrain http://www.statmt.org/wmt13/ training-parallel-commoncrawl.tgz 6 method 1 CDEC 2 E XEC 3 R AMPION 4 R EBOL P R F1 BLEU 67.8 75.2 78.21 80.76 24.89 31.36 38.75 41.02 36.41 44.271 51.821,2 54.411,2,3 38.3 40.851 51.821,2 51.881,2 Table 3: SMT results on NL MAPS, reporting Precision (P), Recall (R) and their harmonic mean (F1). Best results are indicated in bold face. Statistical significance of result differences at p < 0.05 are indicated by algorithm number in superscript. While we do report BLEU (Papineni et al., 2002), the primary goal in our work is to achieve highest possible F1 score. This is vital because our ultimate aim is to give users asking German questions the correct answer, whereas the English translation from which BLEU is be calculated is only an intermediate result that is irrelevant for the task goal. To test significance of F1 and BLEU, we again use Approximate Randomization. Before training, we split of 200 sentences from the training set to use as held out data (dev set). R AMPION, R EBOL and E XEC ran for 50 epochs and then the dev set was used to pinpoint the best epoch for each algori"
N16-1088,P15-1142,0,0.0252688,"hy, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be trained from annotated data. OSM"
N16-1088,P14-1083,1,0.811626,"ingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be trained from annotated data. OSM has previously been used by Boye et al. (2014) for pedestrian routing using a dialogue system, however, no details on semantic parsing and no resource are provided. Our SMT tuning experiment builds on the work of Riezler et al. (2014) and Haas and Riezler (2015) who applied response-based learning for SMT to the G EOQUERY and F REE 917 domains, respectively. press hierarchical structures. The Overpass API2 can be used to query the database made up of the aforementioned nodes, ways and relations. It can efficiently extract the correct subset of database objects that satisfy the entered constraints. The following constraints are most relevant for our corpus creation later on: • The simplest constraints require the database objects to have certain keys or key-value pairs. For example to search for hotels, one would use “node["
N16-1088,P12-1002,1,0.87897,"Missing"
N16-1088,P13-1135,0,0.0230635,"hes the k-best list for the missing y − or y + , respectively, and performs an update that adds the feature vector of y + onto w, and subtracts the feature vector of y − . For our experiment, the NL MAPS questions were translated by the first author into German. As parser we chose to use number 3 (+intersect +stem +cdec +pass +cfg) from Table 2, deciding against the use of sparse features due speed reasons. The CDEC (Dyer et al., 2010) decoder was used for machine translation from German to English. Here we employ its standard features plus additional sparse features5 and the C OMMON C RAWL6 (Smith et al., 2013) corpus to built the baseline SMT system. R EBOL is compared to a baseline system without discriminative training (CDEC) and to a stochastic (sub)gradient descent variant of R AMPION (Gimpel and Smith, 2012). Both baseline systems do not make use of the feedback from the semantic parser. While both R EBOL and R AMPION assume the availability of both a reference translation and a gold parse, response-based learning can also succeed without any access to reference translations or even to gold standard parses. Riezler et al. (2014) introduced an algorithm, called E XEC, that only relies on task-b"
N16-1088,Q14-1042,0,0.0172208,"restricted to the closed domain of US geography, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic p"
N16-1088,P15-1129,0,0.0907683,"ic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translation problem in which natural language is translated into the machine readable language. This approach is convenient because one can make use of the efficient and robust decoders that are freely available for SMT. Despite the simplicity of the approach, Andreas et al. (2013) have shown that highly accurate semantic parsers can be trained from annotated data. OSM has previously been used by Boye et al. (2014) for pedestrian routing using a dialogue system, however, no details on semantic parsing and"
N16-1088,N06-1056,0,0.067276,"ate et al., 741 # users # objects # nodes # ways # relations # tags # distinct tags # distinct keys 2,389,374 3,464,399,738 3,139,787,926 320,775,580 3,836,232 1,259,132,137 76,204,309 57,159 Table 1: Statistics of OSM as of December 14th, 2015 2005) and F REE 917 (Cai and Yates, 2013). While G EOQUERY queries are restricted to the closed domain of US geography, the structural complexity of the questions is higher than for F REE 917, which focuses on open domain queries. Seminal work on building semantic parsers from the G EO QUERY meaning representations are Zettlemoyer and Collins (2005) or Wong and Mooney (2006). Later approaches try to learn semantic parsers from question-answer pairs only, for example, Liang et al. (2009) for G EOQUERY, or Kwiatkowski et al. (2013) or Berant et al. (2013) for F REE 917. Newer research attempts to close the gap between lexical variability and structural complexity (Vlachos and Clark, 2014; Artzi et al., 2015; Pasupat and Liang, 2015), however, answer retrieval accuracy is low if semantic parsers cannot be bootstrapped from a corpus of queries and MRLs (Wang et al., 2015; Pasupat and Liang, 2015). Our approach treats semantic parsing as a monolingual machine translat"
N18-3012,P15-1001,0,0.103239,"Missing"
N18-3012,D14-1181,0,0.00705475,"Missing"
N18-3012,P07-2045,0,0.00510222,"s a standard subword-based encoder-decoder architecture with attention (Bahdanau et al., 2015), implemented with TensorFlow (Abadi et al., 2015). The model is trained with MLE on 2.7M parallel sentences of out-of-domain data until the early stopping point which is determined on a small in-domain dev set of 1,619 product title translations. A beam of size 12 and length normalization (Wu et al., 2016) are used for beam search decoding. For significance tests we used approximate randomization (Clark et al., 2011), for BLEU score evaluation (lowercased) the multi-bleu script of the Moses decoder (Koehn et al., 2007), for TER computation the tercom tool (Snover et al., 2006). For MRT, DC and (W)MIX models we set k = 5, for (W-)MIX models λ = 0.5 and α = 0.05. For all NMT models involving random sampling, we report average results and standard deviation (in subscript) over two runs. Further details about training data and hyperparameters settings are described in Appendix D.  ) (7) As for MRT, the expectation over the full output space is approximated with a subset of k sample translations S(x) ⊂ Y(x). Relative Rewards. With the objectives as defined above, gradient steps are dependent on the magnitude of"
N18-3012,P17-1138,1,0.805866,"d constitutes the “bandit feedback” scenario where the name is inspired by “one-armed bandit” slot machines. 92 Proceedings of NAACL-HLT 2018, pages 92–105 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics 2 Related Work Sokolov et al. (2016a,b) introduced learning from bandit feedback for SMT models in an interactive online learning scenario: the MT model receives a source sentence from the user, provides a translation, receives feedback from the user for this translation, and performs a stochastic gradient update proportional to the feedback quality. Kreutzer et al. (2017) showed that the objectives proposed for log-linear models can be transferred to neural sequence learning and found that standard control variate techniques do not only reduce variance but also help to produce best BLEU results. Nguyen et al. (2017) proposed a very similar approach using a learned word-based critic in an advantage actor-critic reinforcement learning framework. A comparison of current approaches was recently performed in a shared task where participants had to build translation models that learn from the interaction with a service that provided e-commerce product descriptions a"
N18-3012,E17-2101,1,0.847328,"an feedback is usually only available for one translation per input, learning from direct user rewards requires the use of bandit learning algorithms. In our setup, human bandit feedback has been collected for translations of a historic MT system different from the target system to be optimized. This restricts the learning setup to offline learning from logged bandit feedback. 2. Reward Scoring Function: A possibility to use human bandit feedback to obtain rewards for more than a single translation per input is 2 Most titles consist of a sequence of keywords rather than a fluent sentence. See Calixto et al. (2017) for a fluency analysis of product titles. 94 Query Translated Query Title Translated Title Recall candado bicicleta bicycle lock New Bicycle Vibration Code Moped Lock Bike Cycling Security Alarm Sound Lock Nuevo c´odigo de vibraci´on Bicicleta Ciclomotor alarma de seguridad de bloqueo Bicicleta Ciclismo Cerradura De Sonido 0.5 Table 2: Example for query and product title translation. ‘candado’ is translated to ‘lock’ in the query, but then translated back to ‘cerradura’ in the title. The recall metric would prefer a title translation with ‘candado’, as it was specified by the user. NMT from r"
N18-3012,P11-2031,0,0.0616966,"el that has not seen in-domain data (i.e., no product title translations). The NMT baseline model (BL) is a standard subword-based encoder-decoder architecture with attention (Bahdanau et al., 2015), implemented with TensorFlow (Abadi et al., 2015). The model is trained with MLE on 2.7M parallel sentences of out-of-domain data until the early stopping point which is determined on a small in-domain dev set of 1,619 product title translations. A beam of size 12 and length normalization (Wu et al., 2016) are used for beam search decoding. For significance tests we used approximate randomization (Clark et al., 2011), for BLEU score evaluation (lowercased) the multi-bleu script of the Moses decoder (Koehn et al., 2007), for TER computation the tercom tool (Snover et al., 2006). For MRT, DC and (W)MIX models we set k = 5, for (W-)MIX models λ = 0.5 and α = 0.05. For all NMT models involving random sampling, we report average results and standard deviation (in subscript) over two runs. Further details about training data and hyperparameters settings are described in Appendix D.  ) (7) As for MRT, the expectation over the full output space is approximated with a subset of k sample translations S(x) ⊂ Y(x)."
N18-3012,D17-1272,1,0.4516,"research is the fact that we assume a counterfactual learning scenario where human feedback has been given to a historic system different from the target system. Learning is done offline from logged data, which is desirable in commercial settings where system updates need to be tested before deployment and the risk of showing inferior translations to users needs to be avoided. Our offline learning algorithms range from a simple bandit-to-supervised conversion (i.e., using translations with good feedback for supervised tuning) to transferring the counterfactual learning techniques presented by Lawrence et al. (2017b) from statistical machine translation (SMT) to NMT models. To our surprise, the bandit-to-supervised conversion proved to be very hard to beat, despite theoretical indications of poor generalization for exploration-free learning from logged data (Langford et al., 2008; Strehl et al., 2010). However, we show that we can further improve over this method by computing a task-specific reward scoring function, resulting in significant improvements in both BLEU and in task-specific metrics. Introduction In commercial scenarios of neural machine translation (NMT), the one-best translation of a text"
N18-3012,2015.iwslt-evaluation.11,0,0.254138,"Missing"
N18-3012,D17-1153,0,0.118132,"Missing"
N18-3012,W16-2323,0,0.0429844,"Missing"
N18-3012,1983.tc-1.13,0,0.520197,"Missing"
N18-3012,P16-1009,0,0.0712429,"Missing"
N18-3012,P16-1162,0,0.255495,"Missing"
N18-3012,P16-1159,0,0.034768,"red. The second option uses logged queries to obtain a matching score as in Equation 2. R RW-MRT (θ) = Maximum Likelihood Estimation by Banditto-Supervised Conversion. Most commonly, NMT models are trained with Maximum Likelihood Estimation (MLE, Equation 3) on a given parallel corpus of source and target sequences D = {(x(s) , y(s) )}Ss=1 s=1 log pθ (y(s) |x(s) ). X qθα (˜ y|x(s) ) ∆(˜ y), (4) where sample probabilities are renormalized over a subset of translation samples S(x) ⊂ Y(x): α qθα (˜ y|x) = P 0 pθ (˜y|x) The hyper0 |x)α . p (y y ∈S(x) θ parameter α controls the sharpness of q (see Shen et al. (2016)). With sequence-level rewards, all words of a translation of length T are reinforced to the same extent and are treated as if they contributed equally to the translation quality. A word-based reward function, such as the match with a given query (Equation 1), allows the words to have individual weights. The following modification of the sequence-level MRT objective (Equation 4) accounts for word-based rewards ∆(yt ): In the following, we present how rewards can be integrated in various objectives for NMT training. S X (θ) = S X s=1 y ˜∈S(x(s) ) 3. Estimated Reward: Another option to extend ba"
N18-3012,2006.amta-papers.25,0,0.121633,"th attention (Bahdanau et al., 2015), implemented with TensorFlow (Abadi et al., 2015). The model is trained with MLE on 2.7M parallel sentences of out-of-domain data until the early stopping point which is determined on a small in-domain dev set of 1,619 product title translations. A beam of size 12 and length normalization (Wu et al., 2016) are used for beam search decoding. For significance tests we used approximate randomization (Clark et al., 2011), for BLEU score evaluation (lowercased) the multi-bleu script of the Moses decoder (Koehn et al., 2007), for TER computation the tercom tool (Snover et al., 2006). For MRT, DC and (W)MIX models we set k = 5, for (W-)MIX models λ = 0.5 and α = 0.05. For all NMT models involving random sampling, we report average results and standard deviation (in subscript) over two runs. Further details about training data and hyperparameters settings are described in Appendix D.  ) (7) As for MRT, the expectation over the full output space is approximated with a subset of k sample translations S(x) ⊂ Y(x). Relative Rewards. With the objectives as defined above, gradient steps are dependent on the magnitude of the reward for the current training instance. In reinforce"
N18-3012,P16-1152,1,0.806228,"achine translation (NMT), the one-best translation of a text is shown to multiple users who can reinforce highquality (or penalize low-quality) translations by explicit feedback (e.g., on a Likert scale) or implicit feedback (by clicking on a translated page). In such settings this type of feedback can be easily collected in large amounts. While bandit feedback1 in form of user clicks on displayed ads is the standard learning signal for response prediction in online advertising (Bottou et al., 2013), bandit learning for machine translation has so far been restricted to simulation experiments (Sokolov et al., 2016b; Lawrence et al., 2017b; ∗ The work for this paper was done while the first author was an intern at eBay. 1 The fact that only feedback for a single translation is collected constitutes the “bandit feedback” scenario where the name is inspired by “one-armed bandit” slot machines. 92 Proceedings of NAACL-HLT 2018, pages 92–105 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics 2 Related Work Sokolov et al. (2016a,b) introduced learning from bandit feedback for SMT models in an interactive online learning scenario: the MT model receives a source sentence"
P00-1061,P99-1035,1,0.867771,"Missing"
P00-1061,P97-1003,0,0.125027,"Missing"
P00-1061,A94-1009,0,0.0611326,"Missing"
P00-1061,A00-2021,1,0.883041,"Missing"
P00-1061,P99-1069,1,0.492297,"for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large unicationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insuciency of EM for highly accurate statistical modeling. Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which is used to dene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG by Johnson et al. (1999), pseudolikelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999). We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models"
P00-1061,J93-2004,0,0.0541982,"Missing"
P00-1061,P92-1017,0,0.134096,"Missing"
P00-1061,C00-2094,1,0.846315,"Missing"
P00-1061,W97-0301,0,0.0177929,"Missing"
P02-1035,W01-0521,0,0.118829,"Missing"
P02-1035,P99-1069,1,0.740485,"ts have so far been confined to a relatively small scale for various reasons. Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999). Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994). The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis,"
P02-1035,H94-1020,0,0.0483752,"Missing"
P02-1035,J93-4001,1,0.513311,"on LFG parses and the discriminative statistical estimation technique. Experimental results are reported in section 4. A discussion of results is in section 5. 2 2.1 Robust Parsing using LFG A Broad-Coverage LFG The grammar used for this project was developed in the ParGram project (Butt et al., 1999). It uses LFG as a formalism, producing c(onstituent)-structures (trees) and f(unctional)-structures (attribute value matrices) as output. The c-structures encode constituency. F-structures encode predicate-argument relations and other grammatical information, e.g., number, tense. The XLE parser (Maxwell and Kaplan, 1993) was used to produce packed representations, specifying all possible grammar analyses of the input. The grammar has 314 rules with regular expression right-hand sides which compile into a collection of finite-state machines with a total of 8,759 states and 19,695 arcs. The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized. As such, most nouns, adjectives, and adverbs have no explicit lexical entry. The main verb lexicon contains 9,652 verb stems and 23,525 subcategorization fra"
P02-1035,P92-1017,0,0.21467,"Missing"
P02-1035,P00-1061,1,0.777157,"High versus low attachment is indicated by property functions counting the number of recursively embedded phrases. Other property functions are designed to refer to f-structure attributes, which correspond to grammatical functions in LFG, or to atomic attributevalue pairs in f-structures. More complex property functions are designed to indicate, for example, the branching behaviour of c-structures and the (non)parallelism of coordinations on both c-structure and f-structure levels. Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented in Riezler et al. (2000) are included in the model. Here tuples of head words, argument words, and grammatical relations are extracted from the training sections of the WSJ, and fed into a finite mixture model for clustering grammatical relations. The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses. 3.2 maximum likelihood estimation the joint probability of the training data to best describe observations is maximized. Since the discriminative task is kept in mind during estimation, discriminative methods can yield imp"
P07-1059,P05-1074,0,0.698508,"to a small number of key words. Furthermore, extraction of phrases was based on the intersection of alignments from both translation directions, thus favoring precision over recall also in phrase alignment. Table 2 shows unique translations of the query “how to live with cat allergies” on the phrase-level, with corresponding source and target phrases shown in brackets. Expansion terms are taken from phrase terms that have not been seen in the original query, and are highlighted in bold face. 467 4.2 SMT-Based Paraphrasing Our SMT-based paraphrasing system is based on the approach presented in Bannard and Callison-Burch (2005). The central idea in this approach is to identify paraphrases or synonyms at the phrase level by pivoting on another language. For example, given a table of Chinese-to-English phrase translations, phrasal synonyms in the target language are defined as those English phrases that are aligned to the same Chinese source phrases. Translation probabilities for extracted para-phrases can be inferred from bilingual translation probabilities as follows: Given an English para-phrase pair (trg, syn), the probability p(syn|trg) that trg translates into syn is defined as the joint probability that the Eng"
P07-1059,J93-2003,0,0.0122215,"sion techniques are based on a recent implementation of the phrasebased SMT framework (Koehn et al., 2003; Och and Ney, 2004). The probability of translating a foreign sentence f into English e is defined in the noisy channel model as arg max p(e|f) = arg max p(f|e)p(e) e e (1) This allows for a separation of a language model p(e), and a translation model p(f|e). Translation probabilities are calculated from relative frequencies of phrases, which are extracted via various heuristics as larger blocks of aligned words from best word alignments. Word alignments are estimated by models similar to Brown et al. (1993). For a sequence of I phrases, the translation probability in equation (1) can be decomposed into p(fiI |eIi ) = I Y p(fi |ei ) (2) i=1 Recent SMT models have shown significant improvements in translation quality by improved modeling of local word order and idiomatic expressions through the use of phrases, and by the deployment of large n-gram language models to model fluency and lexical choice. 4.1 Question-Answer Translation Our first approach to query expansion treats the questions and answers in the question-answer corpus as two distinct languages. That is, the 10 million question-answer p"
P07-1059,N06-2009,0,0.108702,"ues in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest to our models are the SMT approach of Radev et al. (2001) and the paraphrasing approach count web pages 4 billion FAQ pages 795,483 QA pairs 10,568,160 Table 1: Corpus statistics of QA pair data of Duboue and Chu-Carroll (2006). None of these approaches defines the problem of the lexical gap as a query expansion problem, and both approaches use much simpler SMT models than our systems, e.g., Radev et al. (2001) neglect to use a language model to aid disambiguation of translation choices, and Duboue and Chu-Carroll (2006) use SMT as black box altogether. In sum, our approach differs from previous work in QA and IR in the use SMT technology for query expansion, and should be applicable in both areas even though experimental results are only given for the restricted domain of retrieval from FAQ pages. 3 Question-Answer"
P07-1059,P03-1003,0,0.614359,"l language processing technology to close this gap. For example, syntactic information has been deployed to reformulate questions (Hermjakob et al., 2002) or to replace questions by syntactically similar ones (Lin 464 and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). Information retrieval (IR) is faced by a similar fundamental problem of “term mismatch” between queries and documents. A standard IR solution, query expansion, attempts to increase the chances of matching words in relevant documents by adding terms with similar statistical properties to those in the original query (Voorhees, 1994; Qiu and Frei, 1993; Xu and Croft, 1996). In this paper we will concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FA"
P07-1059,W04-2501,0,0.0272816,"Frei, 1993), into account, or by local expansion techniques that select expansion terms from the top ranked documents retrieved by the original query (Xu and Croft, 1996). A similar picture emerges for query expansion in QA: Mixed results have been reported for wordby-word expansion based on WordNet (Burke et al., 1997; Hovy et al., 2000; Prager et al., 2001; Harabagiu et al., 2001). Considerable improvements have been reported for the use of the local context analysis model of Xu and Croft (1996) in the QA system of Ittycheriah et al. (2001), or for the systems of Agichtein et al. (2004) or Harabagiu and Lacatusu (2004) that use FAQ data to learn how to expand query terms by answer terms. The SMT-based approaches presented in this paper can be seen as global query expansion techniques in that our question-answer translation model uses the whole question-answer corpus as information source, and our approach to paraphrasing deploys large amounts of bilingual phrases as highcoverage information source for synonym finding. Furthermore, both approaches take the entire query context into account when proposing to add new terms to the original query. The approaches that are closest to our models are the SMT approac"
P07-1059,P01-1037,0,0.0220049,"Missing"
P07-1059,N03-1017,0,0.0345335,"task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (Och and Ney, 2004; Koehn et al., 2003). Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages. 1 http://wordnet.princeton.edu Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such as word order. The abil"
P07-1059,J04-4002,0,0.0162244,"concentrate on the task of answer retrieval from FAQ pages, i.e., an IR problem where user queries are matched against documents consisting of question-answer pairs found in FAQ pages. Equivalently, this is a QA problem that concentrates on finding answers given FAQ documents that are known to contain the answers. Our approach to close the lexical gap in this setting attempts to marry QA and IR technology by deploying SMT methods for query expansion in answer retrieval. We present two approaches to SMT-based query expansion, both of which are implemented in the framework of phrase-based SMT (Och and Ney, 2004; Koehn et al., 2003). Our first query expansion model trains an endto-end phrase-based SMT model on 10 million question-answer pairs extracted from FAQ pages. 1 http://wordnet.princeton.edu Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics The goal of this system is to learn lexical correlations between words and phrases in questions and answers, for example by allowing for multiple unaligned words in automatic word alignment, and disregarding issues such as"
P07-1059,P03-1021,0,0.00433957,"rgies, allergies) (how, how) (to live, to live) (with cat, with cat) (allergies, allergens) (how, how) (to live, to live) (with cat, with cat) (allergies, allergen) Table 2: Unique n-best phrase-level translations of query “how to live with cat allergies”. 5 as follows: p(synI1 |trg1I ) = ( I Y λφ pφ (syni |trgi ) (4) i=1 × pφ0 (trgi |syni )λφ0 × pw (syni |trgi )λw × pw0 (trgi |syni )λw0 × pd (syni , trgi )λd ) × lw (synI1 )λl × cφ (synI1 )λc × pLM (synI1 )λLM For estimation of the feature weights ~λ defined in equation (4) we employed minimum error rate (MER) training under the BLEU measure (Och, 2003). Training data for MER training were taken from multiple manual English translations of Chinese sources from the NIST 2006 evaluation data. The first of four reference translations for each Chinese sentence was taken as source paraphrase, the rest as reference paraphrases. Discriminative training was conducted on 1,820 sentences; final evaluation on 2,390 sentences. A baseline paraphrase table consisting of 33 million English para-phrase pairs was extracted from 1 billion phrase pairs from three different languages, at a cutoff of para-phrase probabilities of 0.0025. Query expansion is done b"
P12-1002,P08-1024,0,0.0377061,"sizes on small development sets. Our software is freely available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to larg"
P12-1002,D08-1024,0,0.736009,"Missing"
P12-1002,N09-1025,0,0.49132,"Missing"
P12-1002,P05-1033,0,0.0405714,"8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Examples are rules like (1)-(3) shown in Figure 1. Local features are designed to be readable directly off the rule at decoding time. We use three rule templates in our work: Rule identifiers: These features identify each rule by a unique identifier. Such features correspond to the relative frequencies of rewrites rules used in standard models. Rule n-grams: These features identify n-grams of consecutive items in a rule. We use bigrams on source-sides of ru"
P12-1002,J07-2003,0,0.750409,"s on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Examples are rules like (1)-(3) shown in Figure 1. Local features are designed to be readable directly off the rule at decoding time. We use three rule templates in our work: Rule identifiers: These features identify each rule by a unique identifier. Such features correspond to the relative frequencies of rewrites rules used in standard models. Rule n-grams: These features identify n-grams of consecutive items in a rule. We use bigrams on source-sides of rules. Such featu"
P12-1002,W02-1001,0,0.0653264,"i)+ where (a)+ = max(0, a) , w ∈ IRD is a weight vector, and h·, ·i denotes the standard vector dot product. Instantiating SGD to the following stochastic 2 Similar “monolingual parse features” have been used in Dyer et al. (2011). 13 subgradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): ( −¯ xj if hw, x ¯j i ≤ 0, ∇lj (w) = 0 else. Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios. The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002) or for voting (Freund and Schapire, 1999). Algorithm 1 SGD: int I, T , float η Initialize w0,0,0 ← 0. for epochs t ← 0 . . . T − 1: do for all i ∈ {0 . . . I − 1}: do Decode ith input with wt,i,0 . for all pairs xj , j ∈ {0 . . . P − 1}: do wt,i,j+1 ← wt,i,j − η∇lj (wt,i,j ) end for wt,i+1,0 ← wt,i,P end for wt+1,0,0 ← wt,I,0 end for return 1 T T P wt,0,0 t=1 While stochastic learning exhibits a runtime behavior that is linear in sample size (Bottou, 2004), very large datasets can make sequential processing infeasible. Algorithm 2 (MixSGD) addresses this problem by parallelization in the fram"
P12-1002,P08-2010,0,0.0552385,"scriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative"
P12-1002,W10-1757,0,0.280326,"ms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n-best reranking. In contrast to our work, Duh et al. (2010) did not incorporate multitask learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools. 3 Local Features for Synchronous CFGs The work described in this paper is based on the SMT framework of hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG. Ex"
P12-1002,P10-4002,1,0.613628,"rithm 4 performs feature selection based on a choice of meta-parameter of K features instead of by thresholding a regularization meta-parameter λ, however, these techniques are equivalent and can be transformed into each other. 5 Experiments 5.1 Data, Systems, Experiment Settings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal"
P12-1002,W11-2139,1,0.852474,"a feature vector x ∈ IRD where preference pairs for training are prepared by sorting translations according to smoothed sentence-wise BLEU score (Liang et al., 2006a) against the reference. For a (1) (2) (1) preference pair xj = (xj , xj ) where xj is pre(2) (1) (2) ferred over xj , and x ¯j = xj − xj , we consider the following hinge loss-type objective function: lj (w) = (− hw, x ¯j i)+ where (a)+ = max(0, a) , w ∈ IRD is a weight vector, and h·, ·i denotes the standard vector dot product. Instantiating SGD to the following stochastic 2 Similar “monolingual parse features” have been used in Dyer et al. (2011). 13 subgradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): ( −¯ xj if hw, x ¯j i ≤ 0, ∇lj (w) = 0 else. Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios. The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002) or for voting (Freund and Schapire, 1999). Algorithm 1 SGD: int I, T , float η Initialize w0,0,0 ← 0. for epochs t ← 0 . . . T − 1: do for all i ∈ {0 . . . I − 1}: do Decode ith input with wt,i,0 . for all pairs xj ,"
P12-1002,N09-1046,1,0.762844,"e transformed into each other. 5 Experiments 5.1 Data, Systems, Experiment Settings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding,"
P12-1002,N12-1023,0,0.418187,"include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Our approach is inspired by Duh et al. (2010) who applied multi-task learning for improved generalization in n"
P12-1002,2009.iwslt-papers.3,0,0.0529315,"overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million fe"
P12-1002,W11-2123,0,0.112348,"Missing"
P12-1002,D11-1125,0,0.55458,"ork. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have"
P12-1002,N03-1017,0,0.0233465,"atures are the 3 templates described in Section 3. All feature weights were tuned together using algorithms 1-4. If not indicated otherwise, the perceptron was run for 10 epochs with learning rate η = 0.0001, started at zero weight vector, using deduplicated 100-best lists. The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets – BLEU result differences between tuning 12 default features 8 negative log relative frequency p(e|f ); log count(f ); log count(e, f ); lexical translation probability p(f |e) and p(e|f ) (Koehn et al., 2003); indicator variable on singleton phrase e; indicator variable on singleton phrase pair f, e; word penalty; language model weight; OOV count of language model; number of untranslated words; Hiero glue rules (Chiang, 2007). Alg. 1 4 Tuning set Features #Feats devtest-ep test-ep † Tuning set test-crawl10 dev-crawl 15.39 † test-crawl11 14.43† dev-ep default 12 25.62 26.42 dev-ep +id,ng,shape 300k 27.84 28.37 dev-crawl 17.84 16.834 train-ep +id,ng,shape 100k 28.0 @9 28.62 train-ep 19.121 17.331 Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl"
P12-1002,P09-1019,1,0.699806,"ers (id), rule n-gram (ng), and rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number. † indicates statistically significant differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs chosen on the devtest set. pergraph as is done in the cdec implementation of MIRA. We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hypergraph-MERT (Kumar et al., 2009) both of which depend on hypergraph sampling. In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set. Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.7 Table 2 shows the results of the experimental comparison of the 4 algorithms of Section 4. The 7 Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test"
P12-1002,P79-1022,0,0.29814,"Missing"
P12-1002,N06-1014,0,0.47942,"ted Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning s"
P12-1002,D07-1104,0,0.0967957,"unds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding, cdec loads the appropriate file immediately prior to translation of the sentence. The computational overhead is minimal compared to the expense of decoding. Also, deploying disk space instead of memory fits perfectly into the MapReduce framework we are working in. Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima’an, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. 3-gram (news-commentary) and 5-gram (Europarl) language models are trai"
P12-1002,N10-1069,0,0.0955785,"do for all i ∈ {0 . . . S − 1}: do Decode ith input with wz,t,i,0 . for all pairs xj , j ∈ {0 . . . P − 1}: do wz,t,i,j+1 ← wz,t,i,j − η∇lj (wz,t,i,j ) end for wz,t,i+1,0 ← wz,t,i,P end for wz,t+1,0,0 ← wz,t,S,0 end for end for Collect final weights from each   machine, return 1 Z Z P z=1 1 T T P wz,t,0,0 . t=1 3 Other loss functions lead to stochastic versions of SVMs (Collobert and Bengio, 2004; Shalev-Shwartz et al., 2007; Chapelle and Keerthi, 2010). Algorithm 2 is a variant of the SimuParallelSGD algorithm of Zinkevich et al. (2010) or equivalently of the parameter mixing algorithm of McDonald et al. (2010). The key idea of algorithm 2 is to partition the data into disjoint shards, then train SGD on each shard in parallel, and after training mix the final parameters from each shard by averaging. The algorithm requires no communication between machines until the end. McDonald et al. (2010) also present an iterative mixing algorithm where weights are mixed from each shard after training a single epoch of the perceptron in parallel on each shard. The mixed weight vector is re-sent to each shard to start another epoch of training in parallel on each shard. This algorithm corresponds to our algorithm"
P12-1002,P00-1056,0,0.0184572,"ttings The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1. The translation direction is German-to-English. The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007). We use the cdec decoder5 (Dyer et al., 2010) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). For word alignment of the news-commentary data, we used GIZA++ (Och and Ney, 2000); for aligning the Europarl data, we used the Berkeley aligner (Liang et al., 2006b). Before training, we collect all the grammar rules necessary to 4 Note that by definition of ||W||1,2 , standard `1 regularization is a special case of `1 /`2 regularization for a single task. 5 cdec metaparameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200. 15 translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007). When decoding, cdec loads the appropriate file immediately prior to translation of the sentence. T"
P12-1002,P02-1038,0,0.177134,"re sets of various sizes on small development sets. Our software is freely available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been"
P12-1002,P03-1021,0,0.120216,"MT that can be read off from rules at runtime, and present a learning algorithm that applies `1 /`2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets. 1 Introduction The standard SMT training pipeline combines scores from large count-based translation models and language models with a few other features and tunes these using the well-understood line-search technique for error minimization of Och (2003). If only a handful of dense features need to be tuned, minimum error rate training can be done on small tuning sets and is hard to beat in terms of accuracy and efficiency. In contrast, the promise of largescale discriminative training for SMT is to scale to arbitrary types and numbers of features and to provide sufficient statistical support by parameter estimation on large sample sizes. Features may be lexicalized and sparse, non-local and overlapping, or One possible reason why discriminative SMT has mostly been content with small tuning sets lies in the particular design of the features t"
P12-1002,2001.mtsummit-papers.68,0,0.0170286,"Missing"
P12-1002,W05-0908,1,0.875969,"Missing"
P12-1002,N04-1023,0,0.148515,"ly available as a part of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regul"
P12-1002,P06-1091,0,0.368282,"ly expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who"
P12-1002,E12-1083,1,0.861222,"Missing"
P12-1002,2006.iwslt-evaluation.14,0,0.099557,"art of the cdec1 framework. 1 https://github.com/redpony/cdec 12 2 Related Work The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. Howev"
P12-1002,D07-1080,0,0.504754,"t numbers. The focus of many approaches thus has been on feature engineering and on adaptations of machine learning algorithms to the special case of SMT (where gold standard rankings have to be created automatically). Examples for adapted algorithms include Maximum-Entropy Models (Och and Ney, 2002; Blunsom et al., 2008), Pairwise Ranking Perceptrons (Shen et al., 2004; Watanabe et al., 2006; Hopkins and May, 2011), Structured Perceptrons (Liang et al., 2006a), Boosting (Duh and Kirchhoff, 2008; Wellington et al., 2009), Structured SVMs (Tillmann and Zhang, 2006; Hayashi et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), and others. Adaptations of the loss functions underlying such algorithms to SMT have recently been described as particular forms of ramp loss optimization (McAllester and Keshet, 2011; Gimpel and Smith, 2012). All approaches have been shown to scale to large feature sets and all include some kind of regularization method. However, most approaches have been confined to training on small tuning sets. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, B"
P12-1002,P02-1040,0,\N,Missing
P12-1002,P06-1096,0,\N,Missing
P13-2058,J03-1002,0,0.00365921,") with the CLIR model described in section 3. It took 14 days to run 5.5M Arabic queries on 3.7M English documents. In contrast, our iterative approach completed a single iteration in less than 24 hours.1 In the absence of a Twitter data set for retrieval, we selected the parameters λ = 0.6 (eq.1), L = 0.005 and C = 0.95 in a mate-finding task on Wikipedia data. The n-best list size for Pnbest (t|q) was 1000. All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus Dtrg . Word alignments were created using GIZA++ (Och and Ney, 2003). Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table 1 reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests (Noreen, 1989; Riezler and Maxwell, 2005) showed that improvements of full-scale retrieval and task alternation over the baseline were statis5 Conclusion We pre"
P13-2058,W09-0432,0,0.0172637,"e n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ)Plex (t|q) Pnbest (t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak (t, q) be a function indicating an alignment of target term t to source term q in the k-th derivation of query Q. Then we can estimate Pnbest (t|q) as follows: Pn ak (t, q)D(k, Q) Pnbest (t|q) = Pk=1 n k=1 ak (·, q)D(k, Q) CLIR for Parallel Sentence Retrieval Context-Sensitive Translation for CLIR |Q| X 3.2 |Tq | X i=1 df (q) = |Tq | X i=1 Task Alternation in C"
P13-2058,W05-0908,1,0.597033,"he English side of the NIST data plus the English side of the Twitter corpus Dtrg . Word alignments were created using GIZA++ (Och and Ney, 2003). Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table 1 reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests (Noreen, 1989; Riezler and Maxwell, 2005) showed that improvements of full-scale retrieval and task alternation over the baseline were statis5 Conclusion We presented a method that makes translationbased CLIR feasible for mining parallel sentences from large amounts of comparable data. The key of our approach is a translation-based high-quality retrieval model which gradually adapts to the target domain by iteratively re-training the underlying SMT model on a few thousand parallel sentences retrieved in the step before. The number of new pairs added per iteration stabilizes to a few thousand after 7 iterations, yielding an SMT model"
P13-2058,J93-2003,0,0.030784,"mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time- and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences. 1 Introduction Statistical Machine Translation (SMT) crucially relies on large amounts of bilingual data (Brown et al., 1993). Unfortunately sentence-parallel bilingual data are not always available. Various approaches have been presented to remedy this problem by mining parallel sentences from comparable data, for example by using cross-lingual information retrieval (CLIR) techniques to retrieve a target language sentence for a source language sentence treated as a query. Most such approaches try to overcome the noise inherent in automatically extracted parallel data by sheer size. However, finding good quality parallel data from noisy resources like Twitter requires sophisticated retrieval methods. Running these m"
P13-2058,J07-2003,0,0.0348864,"from previous iterations. We have tried different variations of label persistency but did not find any improvements. A similar effect of preventing the SMT model to “forget” general-domain knowledge across iterations is achieved by mixing models from current and previous iterations. This is accomplished in two ways: First, by linearly interpolating the translation option weights P (t|q) from the current and bm25(tf (qj , D), df (qj )) j=1 tf (q, D) = (2) D(k, Q) is the model score of the k-th derivation in the n-best list for query Q. In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al., 2010). This allows us to extract word alignments between source and target text for Q from the SCFG rules used in the derivation. The concept of self-translation is covered by the decoder’s ability to use pass-through rules if words or phrases cannot be translated. Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. Th"
P13-2058,D08-1090,0,0.0281828,"13 Association for Computational Linguistics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ)Plex (t|q) Pnbest (t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak ("
P13-2058,P10-4002,0,0.0132348,"iations of label persistency but did not find any improvements. A similar effect of preventing the SMT model to “forget” general-domain knowledge across iterations is achieved by mixing models from current and previous iterations. This is accomplished in two ways: First, by linearly interpolating the translation option weights P (t|q) from the current and bm25(tf (qj , D), df (qj )) j=1 tf (q, D) = (2) D(k, Q) is the model score of the k-th derivation in the n-best list for query Q. In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al., 2010). This allows us to extract word alignments between source and target text for Q from the SCFG rules used in the derivation. The concept of self-translation is covered by the decoder’s ability to use pass-through rules if words or phrases cannot be translated. Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent,"
P13-2058,N12-1079,0,0.0116374,"tics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ)Plex (t|q) Pnbest (t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak (t, q) be a function indicating an alignm"
P13-2058,W12-3153,1,0.919801,"mputational Linguistics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ)Plex (t|q) Pnbest (t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak (t, q) be a function"
P13-2058,C12-1164,0,0.075782,"ductive learning in that candidate sentences used as queries are filtered for out-of-vocabulary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to proj"
P13-2058,W07-0733,0,0.0177451,"translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ)Plex (t|q) Pnbest (t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak (t, q) be a function indicating an alignment of target term t to source term q in the k-th derivation of query Q. Then we can estimate Pnbest (t|q) as follows: Pn ak (t, q)D(k, Q) Pnbest (t|q) = Pk=1 n k=1 ak (·, q)D(k, Q) CLIR for Parallel Sentence Retrieval Context-Sensitive Translation for CLIR |Q| X 3.2 |Tq | X i=1 df (q) = |Tq | X i=1 Task Alternation in CLIR The key idea of our approach is to iteratively alternate between the tasks of re"
P13-2058,C04-1072,0,0.0104801,"SMT model in each iteration. 4 4.1 4.2 Transductive Setup Our method can be considered transductive in two ways. First, all Twitter data were collected by keyword-based crawling. Therefore, we can expect a topical similarity between development, test and training data. Second, since our setup aims for speed, we created a small set of queries Qsrc , consisting of the source side of the evaluation data and similar Tweets. Similarity was defined by two criteria: First, we ranked all Arabic Tweets with respect to their term overlap with the development and test Tweets. Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. OOV-coverage served as a second criterion to remedy the problem of unknown words in Twitter translation. We first created a general list of all OOVs in the evaluation data under Mgen (3,069 out of 7,641 types). For each of the top 100 BLEU-ranked Tweets, we counted OOV-coverage with respect to the corresponding source Tweet and the general OOV list. We only kept Tweets Experiments Data We trained the general domain model Mgen on data from the NIST evaluation campaign, including UN reports, newswire, broadcast news and blogs. Since we were interested in relativ"
P13-2058,C10-1124,0,0.0136961,"Annual Meeting of the Association for Computational Linguistics, pages 323–327, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 3.1 P (t|q) = λPnbest (t|q) + (1 − λ"
P13-2058,P12-3005,0,0.0315586,"Missing"
P13-2058,J05-4003,0,0.0672704,"Missing"
P13-2058,P06-1011,0,0.0262005,"Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities Plex estimated on symmetrized word alignments, and (context-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain dev"
P13-2058,E09-1003,0,\N,Missing
P14-1083,1999.mtsummit-1.42,0,0.0509879,". Feedback is generated by executing the parse against the database of geographical facts. Positive feedback means that the correct answer is ? received, i.e., exec(pg ) = exec(ph ) indicates that the same answer is received from the gold standard parse pg and the parse for the hypothesis translation ph ; negative feedback results in case a different or no answer is received. The key advantage of response-based learning Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the"
P14-1083,N12-1023,0,0.341246,"on for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system. Such “unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), inter alia) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012). An alternative approach to alleviate the dependency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few ("
P14-1083,D13-1160,0,0.0967496,"T system. The proposed approach of response-based learning opens the doors for various extrinsic tasks 881 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 881–891, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013). The algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. in which SMT systems can be trained and evaluated. In this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment. Building on prior work in grounded sem"
P14-1083,P13-1042,0,0.0982837,"entation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the G EO QUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of F REEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action 882 Figure 1: Response-based learning cycle for grounding SMT in virtual trivia gameplay. 3 Grounding SMT in Semantic Parsing trast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from thes"
P14-1083,2010.amta-papers.21,0,0.0430724,"ing of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Mart´ınez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Mart´ınez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of"
P14-1083,P12-1051,0,0.138854,"ts 1234 denote a significant improvement over the respective method. method precision recall F1 BLEU 1 CDEC 2 E XEC R AMPION R EBOL 65.59 66.54 67.68 70.68 57.86 61.79 63.57 67.14 61.48 64.07 65.56 68.8612 46.53 46.00 55.6712 55.6712 3 4 Table 2: Experimental results using the original parser for returning answers from G EOQUERY (precision, recall, F1) and n-gram match to original English query (BLEU) on 280 re-translated test examples. et al. (2012).3 The dataset includes 880 English questions and their logical forms. The English strings were manually translated into German by the authors of Jones et al. (2012)), and corrected for typos by the authors of this paper. We follow the provided split into 600 training examples and 280 test examples. For response-based learning, we retrained the semantic parser of Andreas et al. (2013)4 on the full 880 G EOQUERY examples in order to reach full parse coverage. This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms. We used the hierarchical phrase-based variant of the parser. Note that we do not use G EOQUERY test data in SMT training"
P14-1083,W10-2903,0,0.0275351,"y the SMT system. Such “unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (Nakov et al. (2012), inter alia) and incorporates the current model score, leading to various ramp loss objectives described in Gimpel and Smith (2012). An alternative approach to alleviate the dependency on labeled training data is response-based learning. Clarke et al. (2010) or Goldwasser and Roth (2013) describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few (mostly only one) reference translations and guides the learner to promote translations that perform well with respect to the ex"
P14-1083,2009.mtsummit-papers.8,0,0.0132206,"slations by the additional use of a cost function in our approach. 2 A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Mart´ınez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al.,"
P14-1083,W02-1001,0,0.0120637,"the task. 1 http://www.clef-initiative.eu 883 is the possibility to receive positive feedback even from predictions that differ from gold standard reference translations, but yet receive the correct answer when parsed and matched against the database. Such structural and lexical variation broadens the learning capabilities in contrast to learning from fixed labeled data. For example, assume the following English query in the geographical domain, and assume positive feedback from executing the corresponding semantic parse against the geographical database: The structured perceptron algorithm (Collins, 2002) learns an optimal weight vector w by updating w on input x(i) by the following rule, in case the predicted translation yˆ is different from and scored higher than the reference translation y (i) : w = w + φ(x(i) , y (i) ) − φ(x(i) , yˆ). This stochastic structural update aims to demote weights of features corresponding to incorrect decisions, and to promote weights of features for correct decisions. An application of structured prediction to SMT involves more than a straightforward replacement of labeled output structures by reference translations. Firstly, update rules that require to comput"
P14-1083,D13-1161,0,0.265642,"nce translations by the SMT system. The proposed approach of response-based learning opens the doors for various extrinsic tasks 881 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 881–891, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination. Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013). The algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT. in which SMT systems can be trained and evaluated. In this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment. Building on prio"
P14-1083,W00-0507,0,0.0862032,"lity to boost similarity to human reference translations by the additional use of a cost function in our approach. 2 A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Mart´ınez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., usin"
P14-1083,E14-1042,0,0.0297383,"em learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Mart´ınez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Mart´ınez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of translations order with respect to retrieval performance. In conRelated Work The key idea of grounded language learning is to study natural language in the context of a no"
P14-1083,D07-1104,0,0.0193337,"translates the German input into the gold standard English query it should be rewarded by positive task feedback. To doublecheck whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experimental results using the original parser of Andreas et al. (2013) that is trained only on the 600 G EO QUERY training examples. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010)5 . We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data. We trained the SMT system on the English-German parallel web data provided in the C OMMON C RAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the C OMMON C RAWL data as described above. This system does not use any G EOQUERY data for training. Methods 2-4 use the 600 training examples from G EO QUERY for discriminative training only. Variants of the response-based learning algorit"
P14-1083,P13-1135,0,0.033567,"e also present experimental results using the original parser of Andreas et al. (2013) that is trained only on the 600 G EO QUERY training examples. The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder CDEC (Dyer et al., 2010)5 . We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data. We trained the SMT system on the English-German parallel web data provided in the C OMMON C RAWL6 (Smith et al., 2013) dataset. 5.2 Compared Systems Method 1 is the baseline system, consisting of the CDEC SMT system trained on the C OMMON C RAWL data as described above. This system does not use any G EOQUERY data for training. Methods 2-4 use the 600 training examples from G EO QUERY for discriminative training only. Variants of the response-based learning algorithm described above are implemented as a standalone tool that operates on CDEC n-best lists of 10,000 translations of the G EOQUERY training data. All variants use sparse features of CDEC as described in Simianer et al. (2012) that extract rule 3 http"
P14-1083,C12-1121,0,0.139765,"Missing"
P14-1083,2013.mtsummit-papers.2,1,0.843408,"Missing"
P14-1083,E12-1012,0,0.0175045,"after each interaction is too costly, online adaptation after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm (Ortiz-Mart´ınez et al., 2010; Hardt and Elming, 2010), or in discriminative SMT, e.g., using perceptron-type algorithms (Cesa-Bianchi et al., 2008; Mart´ınez-G´omez et al., 2012; W¨aschle et al., 2013; Denkowski et al., 2014). In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, Nikoulina et al. (2012) propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an n-best list of translations order with respect to retrieval performance. In conRelated Work The key idea of grounded language learning is to study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her underst"
P14-1083,N06-1056,0,0.0512956,"study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the G EO QUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of F REEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful"
P14-1083,P09-1110,0,0.021779,"in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting. Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the G EO QUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), inter alia), databases of simulated card games (Goldwasser and Roth (2013), inter alia), or the user-generated contents of F REEBASE (Cai and Yates (2013), inter alia). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action 882 Figure 1: Response-based learning cycle for grounding"
P14-1083,N10-1079,0,0.0486813,"Missing"
P14-1083,2001.mtsummit-papers.68,0,0.00982449,"are the populations of the states through which the mississippi river runs prediction: reference: what state borders california what is the adjacent state of california prediction: reference: what are the capitals of the states which have cities with the name durham what is the capital of states that have cities named durham prediction: reference: what rivers go through states with the least cities which rivers run through states with fewest cities Table 3: Predicted translations by response-based learning (R EBOL) leading to positive feedback versus gold standard references. We report BLEU (Papineni et al., 2001) of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the G EOQUERY database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization (Noreen, 1989) where result differences with a pvalue smaller than"
P14-1083,2013.mtsummit-papers.11,0,0.0129452,"hat the correct answer is ? received, i.e., exec(pg ) = exec(ph ) indicates that the same answer is received from the gold standard parse pg and the parse for the hypothesis translation ph ; negative feedback results in case a different or no answer is received. The key advantage of response-based learning Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using human reading comprehension testing (Pfafflin, 1965; Fuji, 1999; Jones et al., 2005), and more recently, using face-toface conversation mediated via machine translation (Sakamoto et al., 2013). However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to testing scenarios, but prevented a use for interactive training of SMT systems as in our work. Lastly, our work is related to cross-lingual natural language processing such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative.1 While these approaches focus on improvements of the respective natural language processing task, our goal is to"
P14-1083,2012.amta-papers.14,0,0.286047,"l and lexical variants of reference translations as positive examples in response-based learning. Furthermore, translations produced by responsebased learning are found to be grammatical. This is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach. 2 A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. Computer assisted translation (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction (Saluja et al., 2012), to human post-editing operations on a system prediction resulting in a reference translation (Cesa-Bianchi et al., 2008), to human acceptance or overriding of sentence completion predictions (Langlais et al., 2000; Barrachina et al., 2008; Koehn and Haddow, 2009). In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, online adaptation after each interaction has become the learning protocol o"
P14-1083,J09-1002,0,\N,Missing
P14-1083,P02-1040,0,\N,Missing
P14-1083,P13-2009,0,\N,Missing
P14-1083,P10-4002,0,\N,Missing
P14-1083,P12-1002,1,\N,Missing
P14-2080,W11-2123,0,0.0221671,"). For patents, vocabularies contained 60k and 365k words for JP and EN. Filtering special symbols and stopwords reduced the JP vocabulary size to 50k (small enough not to resort to CFH). To reduce the EN vocabulary 2 de.wikipedia.org/wiki/Brandungspfeiler www.wikidata.org/ 4 www.cl.uni-heidelberg.de/wikiclir 5 lintool.github.io/Cloud9/index.html 6 www.nltk.org/ 3 491 Experiments Experiment Settings. The SMT-based models use cdec (Dyer et al., 2010). Word alignments were created with mgiza (JP-EN) and fast align (Dyer et al., 2013) (DE-EN). Language models were trained with the KenLM toolkit (Heafield, 2011). The JP-EN system uses a 5-gram language model from the EN side of the training data. For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec’s MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008). PSQ on patents reuses settings found by Sokolov et al. (2013); settings for Wikipedia were adjusted on its dev set (n=1000, λ=0.4, L=0, C=1). Patent retrieval for DT was done by sentencewise translation and subsequent re-joining to form one query per pa"
P14-2080,D08-1024,0,0.0506072,"dex.html 6 www.nltk.org/ 3 491 Experiments Experiment Settings. The SMT-based models use cdec (Dyer et al., 2010). Word alignments were created with mgiza (JP-EN) and fast align (Dyer et al., 2013) (DE-EN). Language models were trained with the KenLM toolkit (Heafield, 2011). The JP-EN system uses a 5-gram language model from the EN side of the training data. For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec’s MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008). PSQ on patents reuses settings found by Sokolov et al. (2013); settings for Wikipedia were adjusted on its dev set (n=1000, λ=0.4, L=0, C=1). Patent retrieval for DT was done by sentencewise translation and subsequent re-joining to form one query per patent, which was ranked against the documents using BM25. For PSQ, BM25 is computed on expected term and document frequencies. For ranking-based retrieval, we compare several combinations of learners and features (Table 2). VW denotes a sparse model using word-based features trained with SGD. BM denotes a similar model trained using Boosting. D"
P14-2080,P03-1021,0,0.00752587,"o/Cloud9/index.html 6 www.nltk.org/ 3 491 Experiments Experiment Settings. The SMT-based models use cdec (Dyer et al., 2010). Word alignments were created with mgiza (JP-EN) and fast align (Dyer et al., 2013) (DE-EN). Language models were trained with the KenLM toolkit (Heafield, 2011). The JP-EN system uses a 5-gram language model from the EN side of the training data. For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec’s MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008). PSQ on patents reuses settings found by Sokolov et al. (2013); settings for Wikipedia were adjusted on its dev set (n=1000, λ=0.4, L=0, C=1). Patent retrieval for DT was done by sentencewise translation and subsequent re-joining to form one query per patent, which was ranked against the documents using BM25. For PSQ, BM25 is computed on expected term and document frequencies. For ranking-based retrieval, we compare several combinations of learners and features (Table 2). VW denotes a sparse model using word-based features trained with SGD. BM denotes a similar model tra"
P14-2080,P10-4002,0,0.0706561,"to lowercasing and punctuation removal, we applied Correlated Feature Hashing (CFH), that makes collisions more likely for words with close meaning (Bai et al., 2010). For patents, vocabularies contained 60k and 365k words for JP and EN. Filtering special symbols and stopwords reduced the JP vocabulary size to 50k (small enough not to resort to CFH). To reduce the EN vocabulary 2 de.wikipedia.org/wiki/Brandungspfeiler www.wikidata.org/ 4 www.cl.uni-heidelberg.de/wikiclir 5 lintool.github.io/Cloud9/index.html 6 www.nltk.org/ 3 491 Experiments Experiment Settings. The SMT-based models use cdec (Dyer et al., 2010). Word alignments were created with mgiza (JP-EN) and fast align (Dyer et al., 2013) (DE-EN). Language models were trained with the KenLM toolkit (Heafield, 2011). The JP-EN system uses a 5-gram language model from the EN side of the training data. For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec’s MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008). PSQ on patents reuses settings found by Sokolov et al. (2013); settings for Wikipedia were"
P14-2080,N13-1073,0,0.0255495,"that makes collisions more likely for words with close meaning (Bai et al., 2010). For patents, vocabularies contained 60k and 365k words for JP and EN. Filtering special symbols and stopwords reduced the JP vocabulary size to 50k (small enough not to resort to CFH). To reduce the EN vocabulary 2 de.wikipedia.org/wiki/Brandungspfeiler www.wikidata.org/ 4 www.cl.uni-heidelberg.de/wikiclir 5 lintool.github.io/Cloud9/index.html 6 www.nltk.org/ 3 491 Experiments Experiment Settings. The SMT-based models use cdec (Dyer et al., 2010). Word alignments were created with mgiza (JP-EN) and fast align (Dyer et al., 2013) (DE-EN). Language models were trained with the KenLM toolkit (Heafield, 2011). The JP-EN system uses a 5-gram language model from the EN side of the training data. For the DE-EN system, a 4-gram model was built on the EN side of the training data and the EN Wikipedia documents. Weights for the standard feature set were optimized using cdec’s MERT (JP-EN) and MIRA (DE-EN) implementations (Och, 2003; Chiang et al., 2008). PSQ on patents reuses settings found by Sokolov et al. (2013); settings for Wikipedia were adjusted on its dev set (n=1000, λ=0.4, L=0, C=1). Patent retrieval for DT was done"
P14-2080,N10-1063,0,0.0403058,"ranslation (SMT) to either produce a single most probable translation, or a weighted list of alternatives, that is used as search query to a standard search engine (Chin et al., 2008; Ture et al., 2012). This approach is advantageous if large amounts of indomain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not. The situation is different for CLIR in special domains such as patents or Wikipedia. Parallel data for translation have to be extracted with some effort from comparable or noisy parallel data (Utiyama and Isahara, 2007; Smith et al., 2010), however, relevance judgments are often straightforwardly encoded in special domains. For example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the invention claimed in the query patent. Since patent applicants and lawyers are required to list 488 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 488–494, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics features, and by evaluating both approach"
P14-2080,D13-1175,1,0.731025,"a number of further cues on relatedness that can be exploited as features in learning-to-rank approaches. For monolingual patent retrieval, Guo and Gomes (2009) and Oh et al. (2013) advocate the use of dense features encoding domain knowledge on inventors, assignees, location and date, together with dense similarity scores based on bag-of-word representations of patents. Bai et al. (2010) show that for the domain of Wikipedia, learning a sparse matrix of word associations between the query and document vocabularies from relevance rankings is useful in monolingual and cross-lingual retrieval. Sokolov et al. (2013) apply the idea of learning a sparse matrix of bilingual phrase associations from relevance rankings to cross-lingual retrieval in the patent domain. Both show improvements of learning-to-rank on relevance data over SMTbased approaches on their respective domains. The main contribution of this paper is a thorough evaluation of dense and sparse features for learning-to-rank that have so far been used only monolingually or only on either patents or Wikipedia. We show that for both domains, patents and Wikipedia, jointly learning bilingual sparse word associations and dense knowledgebased similar"
P14-2080,C12-1164,0,0.358124,"t search and cross-lingual retrieval in Wikipedia, our approach yields considerable improvements over learningto-rank with either only dense or only sparse features, and over very competitive baselines that combine state-of-the-art machine translation and retrieval. 1 Introduction Cross-Language Information Retrieval (CLIR) for the domain of web search successfully leverages state-of-the-art Statistical Machine Translation (SMT) to either produce a single most probable translation, or a weighted list of alternatives, that is used as search query to a standard search engine (Chin et al., 2008; Ture et al., 2012). This approach is advantageous if large amounts of indomain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not. The situation is different for CLIR in special domains such as patents or Wikipedia. Parallel data for translation have to be extracted with some effort from comparable or noisy parallel data (Utiyama and Isahara, 2007; Smith et al., 2010), however, relevance judgments are often straightforwardly encoded in special domains. For example, in patent prior art search, patents granted at any patent office worldwide are cons"
P14-2080,2007.mtsummit-papers.63,0,0.0490237,"e-art Statistical Machine Translation (SMT) to either produce a single most probable translation, or a weighted list of alternatives, that is used as search query to a standard search engine (Chin et al., 2008; Ture et al., 2012). This approach is advantageous if large amounts of indomain sentence-parallel data are available to train SMT systems, but relevance rankings to train retrieval models are not. The situation is different for CLIR in special domains such as patents or Wikipedia. Parallel data for translation have to be extracted with some effort from comparable or noisy parallel data (Utiyama and Isahara, 2007; Smith et al., 2010), however, relevance judgments are often straightforwardly encoded in special domains. For example, in patent prior art search, patents granted at any patent office worldwide are considered relevant if they constitute prior art with respect to the invention claimed in the query patent. Since patent applicants and lawyers are required to list 488 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 488–494, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics features, and by eva"
P14-2080,1998.amta-tutorials.5,0,\N,Missing
P16-1152,W10-1756,0,0.0141651,"convergence 1614 task CRF OCR Chunking SMT Text classification News (n-best, dense) News (h-graph, sparse) Algorithm 1 Algorithm 2 γt = 1.0 γt = 10−0.75 T0 = 0.4, γt = 10−3.5 T0 = 0.1, γt = 10−4 γt = 10−4 γt = 10−4 γt = 10−5 γt = 10−5 γt = 10−4.75 γt = 10−4 Algorithm 3 γt = 10−1 λ = 10−5 , k = 10−2 , γt = 10−6 λ = 10−6 , k = 10−2 , γt = 10−6 √ λ = 10−4 , µ = 0.99, γt = 10−6 / t λ = 10−6 , k = 5 · 10−3 , γt = 10−6 Table 1: Metaparameter settings determined on dev sets for constant learning rate γt , temperature co√ efficient T0 for annealing under the schedule T = T0 / 3 epoch + 1 (Rose, 1998; Arun et al., 2010), momentum coefficient min{1 − 1/(t/2 + 2), µ} (Polyak, 1964; Sutskever et al., 2013), clipping conyt |xt ), k} in line 7 of Algorithm 3 (Ionides, 2008), `2 yt |xt ) with max{pwt (˜ stant k used to replace pwt (˜ regularization constant λ. Unspecified parameters are set to zero. speeds across algorithms, we employ small constant learning rates in all experiments. The use of constant learning rates for Algorithms 1 and 2 is justified by the analysis of Ghadimi and Lan (2012). For Algorithm 3, the use of constant learning rates effectively compares convergence speed towards an area in close vici"
P16-1152,P09-1010,0,0.0388094,"erence learning. The picture of fastest empirical convergence of bandit pairwise preference learning is consistent across different tasks, both compared to bandit expected loss minimization and bandit cross-entropy minimization. Given the improved convergence and the ease of elicitability of relative feedback, the presented bandit pairwise preference learner is an attractive choice for interactive NLP tasks. 2 Related Work optimization of a parametric policy for action selection (Bertsekas and Tsitsiklis, 1996; Sutton et al., 2000). Policy gradient approaches have been applied to NLP tasks by Branavan et al. (2009), Chang et al. (2015) or Ranzato et al. (2016). Bandit learning operates in a similar scenario of maximizing the expected reward for selecting an arm of a multi-armed slot machine. Similar to our case, the models consist of a single state, however, arms are usually selected from a small set of options while structures are predicted over exponential output spaces. While bandit learning is mostly formalized as online regret minimization with respect to the best fixed arm in hindsight, we investigate asymptotic convergence of our algorithms. In the spectrum of stochastic (Auer et al., 2002a) vers"
P16-1152,E14-1042,0,0.0138339,"bility that an ad will be clicked (and the advertiser has to pay) is estimated by trading off exploration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term) in displaying ads to users. Similar to the online advertising scenario, there are many potential applications to interactive learning in NLP. For example, in interactive statistical machine translation (SMT), user feedback in form of post-edits of predicted translations is used for model adaptation (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Since post-editing feedback has a high cost and requires professional expertise of users, weaker forms of feedback are desirable. Sokolov et al. (2015) showed in a simulation experiment that partial information in form of translation quality judgements on predicted translations is sufficient for model adaptation in SMT. However, one drawback of their bandit expected loss minimization algorithm is the slow convergence speed, meaning that impractically many rounds of user feedback would be necessary for learning in real-world interactive SMT. Furthermore, their algorithms"
P16-1152,P10-4002,0,0.0151405,"ed to meet stopping criterion on development data. tion of smaller learning rates in Algorithm 3 by variance reduction and regularization. Discriminative ranking for SMT. Following Sokolov et al. (2015), we apply bandit learning to simulate personalized MT where a given SMT system is adapted to user style and domain based on feedback to predicted translations. We perform French-to-English domain adaptation from Europarl to NewsCommentary domains using the data of Koehn and Schroeder (2007). One difference of our experiment compared to Sokolov et al. (2015) is our use of the SCFG decoder cdec (Dyer et al., 2010) (instead of the phrase-based Moses decoder). Furthermore, in addition to bandit learning for re-ranking on unique 5,000-best lists, we perform ranking on hypergraphs with redecoding after each update. Sampling and computation of expectations on the hypergraph uses the Inside-Outside algorithm over the expectation semiring (Li and Eisner, 2009). The re-ranking model used 15 dense features (6 lexicalized reordering features, two (out-of- and in-domain) language models, 5 translation model features, distortion and word penalty). The hypergraph experiments used additionally lexicalized sparse fea"
P16-1152,W07-0733,0,0.0139395,"ion metric. News (n-best, dense) 3.8M News (h-graph, sparse) 370k 0.5M 1.2M 115k 1.1M 1.2M 281k Table 3: Number of iterations required to meet stopping criterion on development data. tion of smaller learning rates in Algorithm 3 by variance reduction and regularization. Discriminative ranking for SMT. Following Sokolov et al. (2015), we apply bandit learning to simulate personalized MT where a given SMT system is adapted to user style and domain based on feedback to predicted translations. We perform French-to-English domain adaptation from Europarl to NewsCommentary domains using the data of Koehn and Schroeder (2007). One difference of our experiment compared to Sokolov et al. (2015) is our use of the SCFG decoder cdec (Dyer et al., 2010) (instead of the phrase-based Moses decoder). Furthermore, in addition to bandit learning for re-ranking on unique 5,000-best lists, we perform ranking on hypergraphs with redecoding after each update. Sampling and computation of expectations on the hypergraph uses the Inside-Outside algorithm over the expectation semiring (Li and Eisner, 2009). The re-ranking model used 15 dense features (6 lexicalized reordering features, two (out-of- and in-domain) language models, 5 t"
P16-1152,D09-1005,0,0.0195149,"cted translations. We perform French-to-English domain adaptation from Europarl to NewsCommentary domains using the data of Koehn and Schroeder (2007). One difference of our experiment compared to Sokolov et al. (2015) is our use of the SCFG decoder cdec (Dyer et al., 2010) (instead of the phrase-based Moses decoder). Furthermore, in addition to bandit learning for re-ranking on unique 5,000-best lists, we perform ranking on hypergraphs with redecoding after each update. Sampling and computation of expectations on the hypergraph uses the Inside-Outside algorithm over the expectation semiring (Li and Eisner, 2009). The re-ranking model used 15 dense features (6 lexicalized reordering features, two (out-of- and in-domain) language models, 5 translation model features, distortion and word penalty). The hypergraph experiments used additionally lexicalized sparse features: rule-id features, rule source and target bigram features, and rule shape features. 1616 0.27 BLEU on dev 0.265 0.26 0.255 0.25 0 100000 1) expected-loss 200000 300000 #training samples 2) pairwise-ranking 400000 500000 600000 3) cross-entropy Figure 1: Learning curves for task loss BLEU on development data for SMT hypergraph re-decoding"
P16-1152,P03-1021,0,0.141086,"maximizing the expected reward for choosing an action at a given state in a Markov Decision Process (MDP) model, where rewards are received at each state or once at the final state. The algorithms in this paper can be seen as one-state MDPs where choosing an action corresponds to predicting a structured output. Most closely related are RL approaches that use gradient-based Probabilistic Structured Prediction Full Information vs. Bandit Feedback The objectives and algorithms presented in this paper are based on the well-known expected loss criterion for probabilistic structured prediction (see Och (2003), Smith and Eisner (2006), Gimpel and Smith (2010), Yuille and He (2012), He and Deng (2012), inter alia). The objective is defined as a minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs. This criterion 1611 has the form of a continuous, differentiable, and in general, non-convex objective function. More formally, let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y (y 0 ) suffered for predicting y 0 instead of the gold stand"
P16-1152,D14-1130,0,0.136882,"e clicked (and the advertiser has to pay) is estimated by trading off exploration (a new ad needs to be displayed in order to learn its click-through rate) and exploitation (displaying the ad with the current best estimate is better in the short term) in displaying ads to users. Similar to the online advertising scenario, there are many potential applications to interactive learning in NLP. For example, in interactive statistical machine translation (SMT), user feedback in form of post-edits of predicted translations is used for model adaptation (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Since post-editing feedback has a high cost and requires professional expertise of users, weaker forms of feedback are desirable. Sokolov et al. (2015) showed in a simulation experiment that partial information in form of translation quality judgements on predicted translations is sufficient for model adaptation in SMT. However, one drawback of their bandit expected loss minimization algorithm is the slow convergence speed, meaning that impractically many rounds of user feedback would be necessary for learning in real-world interactive SMT. Furthermore, their algorithms requires feedback in"
P16-1152,P12-1031,0,0.02781,"ecision Process (MDP) model, where rewards are received at each state or once at the final state. The algorithms in this paper can be seen as one-state MDPs where choosing an action corresponds to predicting a structured output. Most closely related are RL approaches that use gradient-based Probabilistic Structured Prediction Full Information vs. Bandit Feedback The objectives and algorithms presented in this paper are based on the well-known expected loss criterion for probabilistic structured prediction (see Och (2003), Smith and Eisner (2006), Gimpel and Smith (2010), Yuille and He (2012), He and Deng (2012), inter alia). The objective is defined as a minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs. This criterion 1611 has the form of a continuous, differentiable, and in general, non-convex objective function. More formally, let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y (y 0 ) suffered for predicting y 0 instead of the gold standard structure y; as a rule, ∆y (y 0 ) = 0 iff y = y 0 . In the full information setting, for"
P16-1152,N03-1028,0,0.418001,"h) is assumed as the ∆ function. We used their dataset of 6,876 handwritten words, from 150 human subjects, under a split where 5,546 examples (folds 2-9) were used as train set, 704 examples (fold 1) as dev, and 626 (fold 0) as test set. We assumed the classical linear-chain Conditional Random Field (CRF) (Lafferty et al., 2001) model with input images xi at every ith node, tabular state-transition probabilities between 28 possible labels of the (i − 1)th and ith node (Latin letters plus two auxiliary start and stop states).3 To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task 3 The feature set is composed of a 16 × 8 binary pixel representation for each character, yielding 28×16×8+282 = 4, 368 features for the training set. We based our code on the pystruct kit (M¨uller and Behnke, 2014). 1615 task CRF OCR (dense) Chunking (sparse) SMT Text classification News (n-best list, dense) News (hypergraph, sparse) gain/loss full information 0/1 ↓ percep., λ = 10−6 Hamming ↓ F1-score ↑ BLEU ↑ Alg. 1 partial information Alg. 2 Alg. 3 0.040 0.0306±0.0004 0.083±0.002 0.035±0.001 likelihood likelihood 0.099 0.935 0.261±0.003 0."
P16-1152,P12-1002,1,0.7049,"in data (news-commentary, 40,444 sent.) to simulate bandit feedback, by evaluating the sampled translation against the reference using as loss function ∆ a smoothed per-sentence 1 − BLEU (zero n-gram counts being replaced with 0.01). For pairwise preference learning we use binary feedback resulting from the comparison of the BLEU scores of the sampled translations. To speed up training for hypergraph re-decoding, the training instances were reduced to those with at most 60 words (38,350 sent.). Training is distributed across 38 shards using multitask-based feature selection for sparse models (Simianer et al., 2012), where after each epoch of distributed training, the top 10k features across all shards are selected, all other features are set to zero. The meta-parameters were adjusted on the in-domain dev sets (nc-devtest2007, 1,064 parallel sentences). The final results are obtained on separate in-domain test sets (nc-test2007, 2,007 sentences) by averaging three independent runs for the optimal dev set meta-parameters. The results for n-best re-ranking in Table 2 (4th row) show statistically significant improvements of 1-2 BLEU points over the out-of-domain SMT model (that includes an in-domain languag"
P16-1152,P06-2101,0,0.0618832,"he expected reward for choosing an action at a given state in a Markov Decision Process (MDP) model, where rewards are received at each state or once at the final state. The algorithms in this paper can be seen as one-state MDPs where choosing an action corresponds to predicting a structured output. Most closely related are RL approaches that use gradient-based Probabilistic Structured Prediction Full Information vs. Bandit Feedback The objectives and algorithms presented in this paper are based on the well-known expected loss criterion for probabilistic structured prediction (see Och (2003), Smith and Eisner (2006), Gimpel and Smith (2010), Yuille and He (2012), He and Deng (2012), inter alia). The objective is defined as a minimization of the expectation of a given task loss function with respect to the conditional distribution over structured outputs. This criterion 1611 has the form of a continuous, differentiable, and in general, non-convex objective function. More formally, let X be a structured input space, let Y(x) be the set of possible output structures for input x, and let ∆y : Y → [0, 1] quantify the loss ∆y (y 0 ) suffered for predicting y 0 instead of the gold standard structure y; as a rul"
P16-1152,2015.mtsummit-papers.13,1,0.918453,"ate) and exploitation (displaying the ad with the current best estimate is better in the short term) in displaying ads to users. Similar to the online advertising scenario, there are many potential applications to interactive learning in NLP. For example, in interactive statistical machine translation (SMT), user feedback in form of post-edits of predicted translations is used for model adaptation (Bertoldi et al., 2014; Denkowski et al., 2014; Green et al., 2014). Since post-editing feedback has a high cost and requires professional expertise of users, weaker forms of feedback are desirable. Sokolov et al. (2015) showed in a simulation experiment that partial information in form of translation quality judgements on predicted translations is sufficient for model adaptation in SMT. However, one drawback of their bandit expected loss minimization algorithm is the slow convergence speed, meaning that impractically many rounds of user feedback would be necessary for learning in real-world interactive SMT. Furthermore, their algorithms requires feedback in form of numerical assessments of translation quality. Such absolute feedback is arguably harder to elicit from human users than relative judgements. The"
P16-1227,J07-2003,0,0.0888918,"method described by Dyer (2009), as implemented by the cdec utility compound-split.pl. Table 1 gives an overview of the dataset. Our parallel development, development test and test data is publicly available.12 4.2 Translation Baselines We compare our approach to two baseline machine translation systems, one trained on out-ofdomain data exclusively and one domain-adapted system. Table 2 gives an overview of the training data for the machine translation systems. Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010). Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for the WMT15 workshop was used to train the translation model, with German as source and English as target language. Like the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same cdec tools. Sentences with lengths over 80 words in either the source or the target language were discarded before training. Source text compound splitting was performed using compound-split"
P16-1227,P10-4002,0,0.0225691,"the cdec utility compound-split.pl. Table 1 gives an overview of the dataset. Our parallel development, development test and test data is publicly available.12 4.2 Translation Baselines We compare our approach to two baseline machine translation systems, one trained on out-ofdomain data exclusively and one domain-adapted system. Table 2 gives an overview of the training data for the machine translation systems. Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010). Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for the WMT15 workshop was used to train the translation model, with German as source and English as target language. Like the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same cdec tools. Sentences with lengths over 80 words in either the source or the target language were discarded before training. Source text compound splitting was performed using compound-split.pl. Alignments were extracted bidirectionally using the"
P16-1227,N09-1046,0,0.0235511,"eriments, we used only the images and captions that were not included in the development, development test or test data, a total of 81,822 images with 5 English captions per image. All data was tokenized and converted to lower case using the cdec11 utilities tokenized-anything.pl and lowercase.pl. For the German data, we 10 We constructed our parallel dataset using only the training rather than the validation section of MS COCO so as to keep the latter pristine for future work based on this research. 11 https://github.com/redpony/cdec performed compound-splitting using the method described by Dyer (2009), as implemented by the cdec utility compound-split.pl. Table 1 gives an overview of the dataset. Our parallel development, development test and test data is publicly available.12 4.2 Translation Baselines We compare our approach to two baseline machine translation systems, one trained on out-ofdomain data exclusively and one domain-adapted system. Table 2 gives an overview of the training data for the machine translation systems. Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars (Chiang, 2007), as implemented by"
P16-1227,D15-1021,0,0.0389227,"conference.1 There is clearly also a practical demand for multilingual image captions, e.g., automatic translation of descriptions of art works would allow access to digitized art catalogues across language barriers and is thus of social and cultural interest; multilingual product descriptions are of high commercial interest since they would allow to widen e-commerce transactions automatically to international markets. However, while datasets of images and monolingual captions already include millions 1 http://www.statmt.org/wmt16/ multimodal-task.html riezler@cl.uni-heidelberg.de of tuples (Ferraro et al., 2015), the largest multilingual datasets of images and captions known to the authors contain 20,000 (Grubinger et al., 2006) or 30,0002 triples of images with German and English descriptions. In this paper, we want to address the problem of multilingual captioning from the perspective of statistical machine translation (SMT). In contrast to prior work on generating captions directly from images (Kulkarni et al. (2011), Karpathy and FeiFei (2015), Vinyals et al. (2015), inter alia), our goal is to integrate visual information into an SMT pipeline. Visual context provides orthogonal information that"
P16-1227,D15-1070,0,0.146464,"Missing"
P16-1227,P13-2121,0,0.0459203,"Missing"
P16-1227,W11-2123,0,0.0135563,"104 10 · 104 Table 3: Optimized hyperparameter values used in final evaluation. Table 2: Parallel and monolingual data used for training machine translation systems. Sentence counts are given for raw data without preprocessing. O/I: both out-of-domain and indomain system, I: in-domain system only. The target language model was trained on monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).13 We optimized the parameters of the translation system for translation quality as measured by IBM BLEU (Papineni et al., 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003). For tuning the translation models used for extraction of the hypothesis lists for final evaluation, MIRA was run for 20 iterations on the development set, and the best run was chosen for final testing. In-Domain Baseline. We also compared our models to a domain-adapted machine translation system. The domain-adapted system was identical to the out-of-domain system, except that it was suppli"
P16-1227,D15-1015,0,0.0862358,"Missing"
P16-1227,2005.mtsummit-papers.11,0,0.573345,"wm ∈typ(m) where δ is the Kronecker δ-function, Nfi is the set of the kn -best translation hypotheses for a source caption fi of image i by decoder score, typ(a) is a function yielding the set of types (unique tokens) contained in a caption a,6 tok(a) is a function yielding the tokens of caption a, idf (w) is the inverse document frequency (Sp¨arck Jones, 1 1972) of term w, and Zm = |typ(m)| is a normalization term introduced in order to avoid biasing the system towards long match candidates containing many low-frequency terms. Term frequencies were computed on monolingual data from Europarl (Koehn, 2005) and the News Commentary and News Discussions English datasets provided for the WMT15 workshop.7 Note that in this model, information from the image i is not used. Multimodal Target Side Retrieval using CNNs. In the TSR-CNN scenario, we supplement the textual target-side TSR model with visual similarity information from a deep convolutional neural network. We formalize this by introduction of the positive-semidefinite distance function v(ix , iy ) → [0, ∞) for images ix , iy (smaller values indicating more similar images). The relevance scoring function SCN N used in this model 6 The choice fo"
P16-1227,P02-1040,0,0.0981464,"d for training machine translation systems. Sentence counts are given for raw data without preprocessing. O/I: both out-of-domain and indomain system, I: in-domain system only. The target language model was trained on monolingual data from Europarl, as well as the News Crawl and News Discussions English datasets provided for the WMT15 workshop (the same data as was used for estimating term frequencies for the retrieval models) with the KenLM toolkit (Heafield et al., 2013; Heafield, 2011).13 We optimized the parameters of the translation system for translation quality as measured by IBM BLEU (Papineni et al., 2002) using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003). For tuning the translation models used for extraction of the hypothesis lists for final evaluation, MIRA was run for 20 iterations on the development set, and the best run was chosen for final testing. In-Domain Baseline. We also compared our models to a domain-adapted machine translation system. The domain-adapted system was identical to the out-of-domain system, except that it was supplied with additional parallel training data from the image caption domain. For this purpose, we used 29,000 parallel German-English"
P16-1227,W10-0721,0,0.643636,"em may use as input an image caption for image i in the source language fi , as well as the image i itself. The system may safely assume that fi is relevant to i, i.e., the identification of relevant captions for i (Hodosh et al., 2013) is not itself part of the task of caption translation. In contrast to the inference problem of finding eˆ = argmaxe p(e|f ) in text-based SMT, multimodal caption translation allows to take into consideration i as well as fi in finding eˆi : eˆi = argmax p(ei |fi , i) ei 2 The dataset used at the WMT16 shared task is based on translations of Flickr30K captions (Rashtchian et al., 2010). 2399 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2399–2409, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics In this paper, we approach caption translation by a general crosslingual reranking framework where for a given pair of source caption and image, monolingual captions in the target language are used to rerank the output of the SMT system. We present two approaches to retrieve target language captions for reranking by pivoting on images that are similar to the input image. One approach calculates imag"
P16-1227,W05-0908,1,0.65392,"Missing"
P16-1227,P14-1068,0,0.0269283,"ed the problem of caption translation from the perspective of neural machine translation.3 Their approach uses a model which is considerably more involved than ours and relies exclusively on the availability of parallel captions as training data. Both approaches crucially rely on neural networks, where they use a visually enriched neural encoder-decoder SMT approach, while we follow a retrieval paradigm for caption translation, using CNNs to compute similarity in visual space. Integration of multimodal information into NLP problems has been another active area of recent research. For example, Silberer and Lapata (2014) show that distributional word embeddings grounded in visual representations outperform competitive baselines on term similarity scoring and word categorization tasks. The orthogonality of visual feedback has previously been exploited in a multilingual setting by Kiela et al. (2015) (relying on previous work by Bergsma and Van Durme (2011)), who induce a bilingual lexicon using term-specific multimodal representations obtained by querying the Google image 3 We replicated the results of Elliott et al. (2015) on the IAPR TC-12 data. However, we decided to not include their model as baseline in t"
P16-1227,P13-1135,0,0.0246162,"ent, development test and test data is publicly available.12 4.2 Translation Baselines We compare our approach to two baseline machine translation systems, one trained on out-ofdomain data exclusively and one domain-adapted system. Table 2 gives an overview of the training data for the machine translation systems. Out-of-Domain Baseline. Our baseline SMT framework is hierarchical phrase-based translation using synchronous context free grammars (Chiang, 2007), as implemented by the cdec decoder (Dyer et al., 2010). Data from the Europarl (Koehn, 2005), News Commentary and Common Crawl corpora (Smith et al., 2013) as provided for the WMT15 workshop was used to train the translation model, with German as source and English as target language. Like the retrieval dataset, training, development and test data was tokenized and converted to lower case, using the same cdec tools. Sentences with lengths over 80 words in either the source or the target language were discarded before training. Source text compound splitting was performed using compound-split.pl. Alignments were extracted bidirectionally using the fast-align utility of cdec and symmetrized with the atools utility (also part of cdec) using the gro"
P16-1227,Q14-1017,0,0.0546531,"mages alone has only recently come into the scope of realistically solvable problems in image processing (Kulkarni et al. (2011), Karpathy and Fei-Fei (2015), Vinyals et al. (2015), inter alia). Recent approaches also employ reranking of image captions by measuring similarity between image and text using deep representations (Fang et al., 2015). The tool of choice in these works are neural networks whose deep representations have greatly increased the quality of feature representations of images, enabling robust and semantically salient analysis of image content. We rely on the CNN framework (Socher et al., 2014; Simonyan and Zisserman, 2015) to solve semantic classification and disambiguation tasks in NLP with the help of supervision signals from visual feedback. However, we consider image captioning as a different task than caption translation since it is not given the information of the source language string. Therefore we do not compare our work to caption generation models. In the area of SMT, W¨aschle and Riezler (2015) presented a framework for integrating a large, indomain, target-side monolingual corpus into machine translation by making use of techniques from crosslingual information retrie"
P16-1227,W15-4922,1,0.912394,"Missing"
P16-1227,W10-0707,0,\N,Missing
P16-1227,P11-2031,0,\N,Missing
P16-1227,2015.eamt-1.23,1,\N,Missing
P17-1138,P12-1031,0,0.0128401,"es from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum´e et al., 2009) to learn from feedback to the model’s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedback to only a single sample per sentence is available, making the learning problem much har"
P17-1138,W15-3014,0,0.031873,"p to a certain n. Hence, for our experiments ∆(˜ y) = −gGLEU(˜ y, y), where y ˜ is a sample translation and y is the reference translation. Unknown words. One drawback of NMT models is their limitation to a fixed source- and target vocabulary. In a domain adaptation setting, this limitation has a critical impact to the translation quality. The larger the distance between old and new domain, the more words in the new domain are unknown to the models trained on the old domain (represented with a special UNK token). We consider two strategies for this problem for our experiments: 1. UNK-Replace: Jean et al. (2015) and Luong et al. (2015b) replace generated UNK tokens with aligned source words or their lexical translations in a post-processing step. Freitag and Al-Onaizan (2016) and Hashimoto et al. (2016) demonstrated that this technique is beneficial for NMT domain adaptation. 2. BPE: Sennrich et al. (2016) introduce byte pair encoding (BPE) for word segmentation to build translation models on sub-word units. Rare words are decomposed into subword units, while the most frequent words remain single vocabulary items. For UNK-Replace we use fast align to generate lexical translations on the EP training d"
P17-1138,P16-1002,0,0.0153782,"up to 5.89 BLEU points for domain adaptation from simulated bandit feedback. 1 Introduction Many NLP tasks involve learning to predict a structured output such as a sequence, a tree or a graph. Sequence-to-sequence learning with neural networks has recently become a popular approach that allows tackling structured prediction as a mapping problem between variable-length sequences, e.g., from foreign language sentences into target-language sentences (Sutskever et al., 2014), or from natural language input sentences into linearized versions of syntactic (Vinyals et al., 2015) or semantic parses (Jia and Liang, 2016). A known bottleneck in structured prediction is the requirement of large amounts of gold-standard structures for supervised learning of model parameters, especially for data-hungry neural network models. Sokolov et al. (2016a,b) presented a framework for stochastic structured prediction under bandit feedback that alleviates the need for labeled output structures in learning: Following an online learning protocol, on each iteration the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.1 They “band"
P17-1138,W10-2903,0,0.0224231,"thms with simulated bandit feedback on various NLP tasks. We show how to lift linear structured prediction under bandit feedback to non-linear models for sequence-to-sequence learning with attentionbased recurrent neural networks (Bahdanau et al., 2015). Our framework is applicable to sequenceto-sequence learning from various types of weak feedback. For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). Our work can build the basis for neural semantic parsing from weak feedback. In this paper, we focus on the application of machine translation via neural sequence-to-sequence learning. The standard procedure of training neural machine translation (NMT) models is to compare their output to human-generated translations and to infer model updates from this comparison. However, the creation of reference translations or post-edits requires professional expertise of users. Our framework allows NMT models to learn from feedback that is weaker than human references or post-edits"
P17-1138,P11-1060,0,0.0168076,"andit feedback on various NLP tasks. We show how to lift linear structured prediction under bandit feedback to non-linear models for sequence-to-sequence learning with attentionbased recurrent neural networks (Bahdanau et al., 2015). Our framework is applicable to sequenceto-sequence learning from various types of weak feedback. For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). Our work can build the basis for neural semantic parsing from weak feedback. In this paper, we focus on the application of machine translation via neural sequence-to-sequence learning. The standard procedure of training neural machine translation (NMT) models is to compare their output to human-generated translations and to infer model updates from this comparison. However, the creation of reference translations or post-edits requires professional expertise of users. Our framework allows NMT models to learn from feedback that is weaker than human references or post-edits. One could imagine a"
P17-1138,W16-2361,0,0.0701708,"Missing"
P17-1138,2015.iwslt-evaluation.11,0,0.052113,"fine-tuned on NC. The EP→TED is first trained on EP, then finetuned on TED. NC, but the in-domain EP→NC (Table 3) baselines outperform the linear baseline by more than 3 BLEU points. Continuing training of a pre-trained out-of-domain model on a small amount of in domain data is very hence effective, whilst the performance of the models solely trained on small indomain data is highly dependent on the size of this training data set. For TED, the in-domain dataset is almost four times as big as the NC training set, so the in-domain baselines perform better. This effect was previously observed by Luong and Manning (2015) and Freitag and Al-Onaizan (2016). Bandit Learning. The NMT bandit models that optimize the EL objective yield generally a much higher improvement over the out-of-domain models than the corresponding linear models: As listed in Table 4, we find improvements of between 2.33 and 2.89 BLEU points on the NC domain, and between 4.18 and 5.18 BLEU points on the TED domain. In contrast, the linear models with sparse features and hypergraph re-decoding achieved a maximum improvement of 0.82 BLEU points on NC. Optimization of the PR objective shows improvements of up to 1.79 BLEU points on NC (compare"
P17-1138,D15-1166,0,0.706423,"encoded source. The input to the encoder is a sequence of vectors x = (x1 , . . . , xTx ) representing a sequence of source words of length Tx . In the approach of Sutskever et al. (2014), they are encoded into a single vector c = q({h1 , . . . , hTx }), where ht = f (xt , ht−1 ) is the hidden state of the RNN at time t. Several choices are possible for the non-linear functions f and q: Here we are using a Gated Recurrent Unit (GRU) (Chung et al., 2014) for f , and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states (Bahdanau et al., 2015; Luong et al., 2015a). The decoder RNN predicts the next target word yt at time t given the context vector c and the previous target words y&lt;t = {y1 , . . . , yt−1 } from a probability distribution over the target vocabulary V . This distribution is the result of a softmax transformation of the decoder outputs o = {o1 , . . . , oTy }, such that exp(owi ) pθ (yt = wi |y&lt;t , c) = PV . v=1 exp(owv ) pθ (y|x) = t=1 pθ (yt |y&lt;t , c). Since this encoder-decoder architecture is fully differentiable, it can be trained with gradient descent methods. Given a parallel training set of S source sentences and their reference"
P17-1138,P15-1002,0,0.126265,"Missing"
P17-1138,P03-1021,0,0.159565,"erence words, not on their own predictions. Ranzato et al. (2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum´e et al., 2009) to learn from feedback to the model’s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedbac"
P17-1138,P16-1162,0,0.0917582,"a critical impact to the translation quality. The larger the distance between old and new domain, the more words in the new domain are unknown to the models trained on the old domain (represented with a special UNK token). We consider two strategies for this problem for our experiments: 1. UNK-Replace: Jean et al. (2015) and Luong et al. (2015b) replace generated UNK tokens with aligned source words or their lexical translations in a post-processing step. Freitag and Al-Onaizan (2016) and Hashimoto et al. (2016) demonstrated that this technique is beneficial for NMT domain adaptation. 2. BPE: Sennrich et al. (2016) introduce byte pair encoding (BPE) for word segmentation to build translation models on sub-word units. Rare words are decomposed into subword units, while the most frequent words remain single vocabulary items. For UNK-Replace we use fast align to generate lexical translations on the EP training data. When an UNK token is generated, we look up the attention weights and find the source token that receives most attention in this step. If possible, we replace the UNK token by its lexical translation. If it is not included in the lexical translations, it is replaced by the source token. The main"
P17-1138,P16-1159,0,0.30463,"output words based on the history of given reference words, not on their own predictions. Ranzato et al. (2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum´e et al., 2009) to learn from feedback to the model’s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their r"
P17-1138,P06-2101,0,0.0806261,"s, not on their own predictions. Ranzato et al. (2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum´e et al., 2009) to learn from feedback to the model’s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedback to only a single sampl"
P17-1138,P16-1152,1,0.0783362,"ith neural networks has recently become a popular approach that allows tackling structured prediction as a mapping problem between variable-length sequences, e.g., from foreign language sentences into target-language sentences (Sutskever et al., 2014), or from natural language input sentences into linearized versions of syntactic (Vinyals et al., 2015) or semantic parses (Jia and Liang, 2016). A known bottleneck in structured prediction is the requirement of large amounts of gold-standard structures for supervised learning of model parameters, especially for data-hungry neural network models. Sokolov et al. (2016a,b) presented a framework for stochastic structured prediction under bandit feedback that alleviates the need for labeled output structures in learning: Following an online learning protocol, on each iteration the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.1 They “banditize” several objective functions for linear structured predictions, and evaluate the resulting algorithms with simulated bandit feedback on various NLP tasks. We show how to lift linear structured prediction under bandit fe"
P18-1165,P11-2031,0,0.0262529,"bidirectional encoder and a singlelayer decoder with 1,024 GRUs each, and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte-pair merges (Sennrich et al., 2016). For model selection we use greedy decoding, for test set evaluation beam search with a beam of width 10. We sample k = 5 translations for RL models and set the softmax temperature τ = 0.5. Further hyperparameters are given in the supplementary material. Evaluation Method. Trained models are evaluated with respect to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) using MULTEVAL (Clark et al., 2011) and BEER (Stanojevi´c and Sima’an, 2014) to cover a diverse set of automatic measures for translation quality.6 We test for statistical significance with approximate randomization (Noreen, 1989). 4 Pre-processing and data splits as described in https: //github.com/rizar/actor-critic-public/ tree/master/exp/ted. 5 The code is available in the Neural Monkey fork https://github.com/juliakreutzer/ bandit-neuralmonkey/tree/acl2018. 6 Since tendencies of improvement turn out to be consistent across metrics, we only discuss BLEU in the text. 1784 Model Rewards BLEU METEOR BEER Baseline - - 27.0 30.7"
P18-1165,W11-2107,0,0.0314014,"elcl and Libovick´y, 2017).5 The NMT has a bidirectional encoder and a singlelayer decoder with 1,024 GRUs each, and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte-pair merges (Sennrich et al., 2016). For model selection we use greedy decoding, for test set evaluation beam search with a beam of width 10. We sample k = 5 translations for RL models and set the softmax temperature τ = 0.5. Further hyperparameters are given in the supplementary material. Evaluation Method. Trained models are evaluated with respect to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) using MULTEVAL (Clark et al., 2011) and BEER (Stanojevi´c and Sima’an, 2014) to cover a diverse set of automatic measures for translation quality.6 We test for statistical significance with approximate randomization (Noreen, 1989). 4 Pre-processing and data splits as described in https: //github.com/rizar/actor-critic-public/ tree/master/exp/ted. 5 The code is available in the Neural Monkey fork https://github.com/juliakreutzer/ bandit-neuralmonkey/tree/acl2018. 6 Since tendencies of improvement turn out to be consistent across metrics, we only discuss BLEU in the text. 1784 Model Rewards BLE"
P18-1165,D14-1130,0,0.0639249,"of the machine learning notion of learnability (Shalev-Shwartz et al., 2010) as the question how well reward estimates can approximate human rewards. Our experiments reveal that rank correlation of reward estimates with TER against human references is higher for regression models trained on standardized cardinal rewards than for Bradley-Terry models trained on pairwise preferences. This emphasizes the influence of the reliability of human feedback signals on the quality of reward estimates learned from them. Lastly, we investigate machine learnability of the overall NMT task, in the sense of Green et al. (2014) who posed the question of how well an MT system can be tuned on post-edits. We use an RL approach for tuning, where a crucial difference of our work to previous work on RL from human rewards (Knox and Stone, 2009; Christiano et al., 2017) is that our RL scenario is not interactive, but rewards are collected in an offline log. RL then can proceed either by off-policy learning using logged single-shot human rewards directly, or by using estimated rewards. An expected advantage of estimating rewards is to tackle a simpler problem first — learning a reward estimator instead of a full RL task for"
P18-1165,P82-1020,0,0.825676,"Missing"
P18-1165,W17-4763,0,0.0203046,"d the reward estimation in our work: sQE usually has more training data, often from more than one machine translation model. Its gold labels are inferred from post-edits, i.e. corrections of the machine translation output, while we learn from weaker bandit feedback. Although this would in principle be possible, sQE predictions have not (yet) been used to directly reinforce predictions of MT systems, mostly because their primary purpose is to predict post-editing effort, i.e. give guidance how to further process a translation. State-of-the-art models for sQE such as (Martins et al., 2017) and (Kim et al., 2017) are unsuitable for the direct use in this task since they rely on linguistic input features, stacked architecFigure 1: Rating interface for 5-point ratings. Figure 2: Rating interface for pairwise ratings. tures or post-edit or word-level supervision. Similar to approaches for generative adversarial NMT (Yu et al., 2017; Wu et al., 2017) we prefer a simpler convolutional architecture based on word embeddings for the human reward estimation. 3 3.1 Human MT Rating Task Data We translate a subset of the TED corpus with a general-domain and a domain-adapted NMT model (see §6.2 for NMT and data),"
P18-1165,D14-1181,0,0.00288552,"e experiments paux was tuned to 0.8. Architecture. We choose the following neural architecture for the reward estimation (details see supplementary material): Inputs are padded source and target subword embeddings, which are each processed with a biLSTM (Hochreiter and Schmidhuber, 1997). Their outputs are concatenated for each time step, further fed to a 1Dconvolution with max-over-time pooling and subsequently a leaky ReLU (Maas et al., 2013) output layer. This architecture can be seen as a biLSTMenhanced bilingual extension to the convolutional model for sentence classification proposed by Kim (2014). It has the advantage of not requiring any feature extraction but still models n-gram features on an abstract level. Evaluation Method. The quality of the reward estimation models is tested by measuring Spearman’s ρ with TER on a held-out test set of 1,314 translations following the standard in sQE evaluations. Hyperparameters are tuned on another 1,200 TED translations. Results. Table 2 reports the results of reward estimators trained on simulated and human rewards. When trained from cardinal rewards, the model of simulated scores performs slightly better than the model of human ratings. Thi"
P18-1165,N18-3012,1,0.898948,"es one displayed translation, but cannot be expected to rate an alternative translation, let alone large amounts of alternatives. In this paper we will show that despite the fact that human feedback is ambiguous and partial in nature, a catalyst for successful learning from human reinforcements is the reliability of the feedback signals. The first deployment of bandit NMT in an e-commerce translation scenario conjectured lacking reliability of user judgments as the reason for disappointing results when learning from 148k user-generated 5-star ratings for around 70k product title translations (Kreutzer et al., 2018). We thus raise the question of how human feedback can be gathered in the most reliable way, and what effect reliability will have in downstream tasks. In order to answer these questions, we measure intra- and inter-annotator agreement for two feedback tasks for bandit NMT, using cardinal feedback (on a 5-point scale) and ordinal feedback (by pairwise preferences) for 800 translations, conducted by 16 and 14 human raters, respectively. Perhaps surprisingly, while relative feedback is often considered easier for humans to provide (Thurstone, 1927), our investigation shows that α-reliability (Kr"
P18-1165,P17-1138,1,0.926891,"f a clearly specified reward function, e.g., defined by winning or losing a game, or by computing an automatic sequence-level evaluation metric. Secondly, both RL applications rely on a sufficient exploration of the action space, e.g., by evaluating multiple game moves for the same game state, or various sequence predictions for the same input. The goal of this paper is to advance the stateof-the-art of sequence-to-sequence RL, exemplified by bandit learning for neural machine translation (NMT). Our aim is to show that successful learning from simulated bandit feedback (Sokolov et al., 2016b; Kreutzer et al., 2017; Nguyen et al., 2017; Lawrence et al., 2017) does in fact carry over to learning from actual human bandit feedback. The promise of bandit NMT is that human feedback on the quality of translations is easier to obtain in large amounts than human references, thus compensating the weaker nature of the signals by their quantity. However, the human factor entails several differences to the above sketched simulation scenarios of RL. Firstly, human rewards are not well-defined functions, but complex and inconsistent signals. For example, in general every input sentence has a multitude of correct tran"
P18-1165,D17-1272,1,0.924625,"defined by winning or losing a game, or by computing an automatic sequence-level evaluation metric. Secondly, both RL applications rely on a sufficient exploration of the action space, e.g., by evaluating multiple game moves for the same game state, or various sequence predictions for the same input. The goal of this paper is to advance the stateof-the-art of sequence-to-sequence RL, exemplified by bandit learning for neural machine translation (NMT). Our aim is to show that successful learning from simulated bandit feedback (Sokolov et al., 2016b; Kreutzer et al., 2017; Nguyen et al., 2017; Lawrence et al., 2017) does in fact carry over to learning from actual human bandit feedback. The promise of bandit NMT is that human feedback on the quality of translations is easier to obtain in large amounts than human references, thus compensating the weaker nature of the signals by their quantity. However, the human factor entails several differences to the above sketched simulation scenarios of RL. Firstly, human rewards are not well-defined functions, but complex and inconsistent signals. For example, in general every input sentence has a multitude of correct translations, each of which humans may judge diff"
P18-1165,P17-1003,0,0.0161213,"level (Mnih et al., 2015) or even superhuman performance (Silver et al., 2016). This success and the ability of RL to circumvent the data annotation bottleneck in supervised learning has led to renewed interest in RL in sequenceto-sequence learning problems with exponential ∗ The work for this paper was done while the second author was an intern in Heidelberg. output spaces. A typical approach is to combine REINFORCE (Williams, 1992) with policies based on deep sequence-to-sequence learning (Bahdanau et al., 2015), for example, in machine translation (Bahdanau et al., 2017), semantic parsing (Liang et al., 2017), or summarization (Paulus et al., 2017). These RL approaches focus on improving performance in automatic evaluation by simulating reward signals by evaluation metrics such as BLEU, F1-score, or ROUGE, computed against gold standards. Despite coming from different fields of application, RL in games and sequence-to-sequence learning share firstly the existence of a clearly specified reward function, e.g., defined by winning or losing a game, or by computing an automatic sequence-level evaluation metric. Secondly, both RL applications rely on a sufficient exploration of the action space, e.g., b"
P18-1165,2015.iwslt-evaluation.11,0,0.0151823,"d direct (D) rewards from simulation (S), humans (H) and filtered (F) human ratings. Significant (p ≤ 0.05) differences to the baseline are marked with ? . For RL experiments we show three runs with different random seeds, mean and standard deviation in subscript. The out-of-domain model is trained with MLE on WMT. The task is now to improve the generalization of this model to the TED domain. Table 3 compares the out-of-domain baseline with domain-adapted models that were further trained on TED in a fully-supervised manner (supervised fine-tuning as introduced by Freitag and AlOnaizan (2016); Luong and Manning (2015)). The supervised domain-adapted model serves as an upper bound for domain adaptation with human rewards: if we had references, we could improve up to 7 BLEU. What if references are not available, but we can obtain rewards for sample translations? Results for RL from Simulated Rewards. First we simulate “clean” and deterministic rewards by comparing sample translations to references using GLEU (Wu et al., 2016) for RL, and smoothed sBLEU for estimated rewards and OPL. Table 4 lists the results for this simulation experiment in rows 2-5 (S). If unlimited clean feedback was given (RL with direct"
P18-1165,D17-1153,0,0.168388,"Missing"
P18-1165,P02-1040,0,0.100773,"L objectives in Neural Monkey (Helcl and Libovick´y, 2017).5 The NMT has a bidirectional encoder and a singlelayer decoder with 1,024 GRUs each, and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte-pair merges (Sennrich et al., 2016). For model selection we use greedy decoding, for test set evaluation beam search with a beam of width 10. We sample k = 5 translations for RL models and set the softmax temperature τ = 0.5. Further hyperparameters are given in the supplementary material. Evaluation Method. Trained models are evaluated with respect to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) using MULTEVAL (Clark et al., 2011) and BEER (Stanojevi´c and Sima’an, 2014) to cover a diverse set of automatic measures for translation quality.6 We test for statistical significance with approximate randomization (Noreen, 1989). 4 Pre-processing and data splits as described in https: //github.com/rizar/actor-critic-public/ tree/master/exp/ted. 5 The code is available in the Neural Monkey fork https://github.com/juliakreutzer/ bandit-neuralmonkey/tree/acl2018. 6 Since tendencies of improvement turn out to be consistent across metrics, we only discuss BLEU"
P18-1165,W15-3049,0,0.0649946,"Missing"
P18-1165,P16-1162,0,0.0307582,"and the test data 6,750 parallel sentences. Architecture. Our NMT model is a standard subword-based encoder-decoder architecture with attention (Bahdanau et al., 2015). An encoder Recurrent Neural Network (RNN) reads in the source sentence and a decoder RNN generates the target sentence conditioned on the encoded source. We implemented RL and OPL objectives in Neural Monkey (Helcl and Libovick´y, 2017).5 The NMT has a bidirectional encoder and a singlelayer decoder with 1,024 GRUs each, and subword embeddings of size 500 for a shared vocabulary of subwords obtained from 30k byte-pair merges (Sennrich et al., 2016). For model selection we use greedy decoding, for test set evaluation beam search with a beam of width 10. We sample k = 5 translations for RL models and set the softmax temperature τ = 0.5. Further hyperparameters are given in the supplementary material. Evaluation Method. Trained models are evaluated with respect to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) using MULTEVAL (Clark et al., 2011) and BEER (Stanojevi´c and Sima’an, 2014) to cover a diverse set of automatic measures for translation quality.6 We test for statistical significance with approximate randomization"
P18-1165,P16-1159,0,0.0378057,"ation model (estimated reward) or be computed with respect to a reference in a simulation setting (simulated direct reward). In order to counteract high variance in the gradient updates, the running average of rewards is subtracted from r for learning. In practice, Equation 1 is approximated with k samples from pθ (y|x) (see Equation 2). When k = 1, this is equivalent to the expected loss minimization in Sokolov et al. (2016a,b); Kreutzer et al. (2017), where the system interactively learns from online bandit feedback. For k > 1 this is similar to the minimum-risk training for NMT proposed in Shen et al. (2016). Adding a temperature hyper-parameter τ ∈ (0.0, ∞] to the softmax over the model output o allows us to control the sharpness of the sampling distribution pτθ (y|x) = softmax(o/τ ), i.e. the amount of exploration during training. With temperature τ < 1, the model’s entropy decreases and samples closer to the onebest output are drawn. We seek to keep the exploration low to prevent the NMT to produce samples that lie far outside the training domain of the reward estimator. Off-Policy Learning from Direct Rewards. When rewards can not be obtained for samples from a learning system, but were colle"
P18-1165,P16-1152,1,0.929141,"firstly the existence of a clearly specified reward function, e.g., defined by winning or losing a game, or by computing an automatic sequence-level evaluation metric. Secondly, both RL applications rely on a sufficient exploration of the action space, e.g., by evaluating multiple game moves for the same game state, or various sequence predictions for the same input. The goal of this paper is to advance the stateof-the-art of sequence-to-sequence RL, exemplified by bandit learning for neural machine translation (NMT). Our aim is to show that successful learning from simulated bandit feedback (Sokolov et al., 2016b; Kreutzer et al., 2017; Nguyen et al., 2017; Lawrence et al., 2017) does in fact carry over to learning from actual human bandit feedback. The promise of bandit NMT is that human feedback on the quality of translations is easier to obtain in large amounts than human references, thus compensating the weaker nature of the signals by their quantity. However, the human factor entails several differences to the above sketched simulation scenarios of RL. Firstly, human rewards are not well-defined functions, but complex and inconsistent signals. For example, in general every input sentence has a m"
P18-1165,D14-1025,0,0.0446598,"Missing"
P18-1165,Q17-1015,0,\N,Missing
P18-1169,Q13-1005,0,0.0955704,"s. This is done by reweighting target model probabilities over the logged data under a one-step-late model that decouples the normalization from gradient estimation and is thus applicable in stochastic (minibatch) gradient optimization. 2 Related Work Semantic parsers have been successfully trained using neural sequence-to-sequence models with a cross-entropy objective and question-parse pairs (Jia and Liang, 2016; Dong and Lapata, 2016)) or question-answer pairs (Neelakantan et al., 2017). Improving semantic parsers using weak feedback has previously been studied (Goldwasser and Roth (2013); Artzi and Zettlemoyer (2013); inter alia). More recently, several works have applied policy gradient techniques such as REINFORCE (Williams, 1992) to train neural semantic parsers (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia). However, they assume the existence of the true target answers that can be used to obtain a reward for any number of output queries suggested by the system. It thus differs from a bandit setup where we assume that a reward is available for only one structure. Our work most closely resembles the work of 1821 Iyer et al. (2017) who do make the assumption of only being able t"
P18-1169,P16-1004,0,0.0367416,"n counterfactual learning by lifting the multiplicative control variate technique (Swaminathan and Joachims, 2015b; Lawrence et al., 2017b,a) to stochastic learning for neural models. This is done by reweighting target model probabilities over the logged data under a one-step-late model that decouples the normalization from gradient estimation and is thus applicable in stochastic (minibatch) gradient optimization. 2 Related Work Semantic parsers have been successfully trained using neural sequence-to-sequence models with a cross-entropy objective and question-parse pairs (Jia and Liang, 2016; Dong and Lapata, 2016)) or question-answer pairs (Neelakantan et al., 2017). Improving semantic parsers using weak feedback has previously been studied (Goldwasser and Roth (2013); Artzi and Zettlemoyer (2013); inter alia). More recently, several works have applied policy gradient techniques such as REINFORCE (Williams, 1992) to train neural semantic parsers (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia). However, they assume the existence of the true target answers that can be used to obtain a reward for any number of output queries suggested by the system. It thus differs from a bandit s"
P18-1169,N16-1088,1,0.822109,"ferent and will not hamper convergence. Despite losing the formal justification from the perspective of control variates, we found empirically that the OSL update schedule for reweighting is sufficient and does not deteriorate performance. The gradient for learning with OSL updates is given in Table 1. the world. A point of interest consists of one or more associated GPS points. Further relevant information may be added at the discretion of the volunteer in the form of tags. Each tag consists of a key and an associated value, for example “tourism : hotel”. The NL MAPS corpus was introduced by Haas and Riezler (2016) as a basis to create a natural language interface to the OSM database. It pairs English questions with machine readable parses, i.e. queries that can be executed against OSM. Human Feedback Collection. The task of creating a natural language interface for OSM demonstrates typical difficulties that make it expensive to collect supervised data. The machine readable language of the queries is based on the OVERPASS query language which was specifically Token-Level Rewards. For our application of designed for the OSM database. It is thus not eascounterfactual learning to human bandit feedback, ily"
P18-1169,P17-1089,0,0.0347675,"studied (Goldwasser and Roth (2013); Artzi and Zettlemoyer (2013); inter alia). More recently, several works have applied policy gradient techniques such as REINFORCE (Williams, 1992) to train neural semantic parsers (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia). However, they assume the existence of the true target answers that can be used to obtain a reward for any number of output queries suggested by the system. It thus differs from a bandit setup where we assume that a reward is available for only one structure. Our work most closely resembles the work of 1821 Iyer et al. (2017) who do make the assumption of only being able to judge one output. They improve their parser using simulated and real user feedback. Parses with negative feedback are given to experts to obtain the correct parse. Corrected queries and queries with positive feedback are added to the training corpus and learning continues with a cross-entropy objective. We show that this bandit-to-supervision approach can be outperformed by offline bandit learning from partially correct queries. Yih et al. (2016) proposed a user interface for the Freebase database that enables a fast and easy creation of parses"
P18-1169,P16-1002,0,0.0315324,"degenerate behavior in counterfactual learning by lifting the multiplicative control variate technique (Swaminathan and Joachims, 2015b; Lawrence et al., 2017b,a) to stochastic learning for neural models. This is done by reweighting target model probabilities over the logged data under a one-step-late model that decouples the normalization from gradient estimation and is thus applicable in stochastic (minibatch) gradient optimization. 2 Related Work Semantic parsers have been successfully trained using neural sequence-to-sequence models with a cross-entropy objective and question-parse pairs (Jia and Liang, 2016; Dong and Lapata, 2016)) or question-answer pairs (Neelakantan et al., 2017). Improving semantic parsers using weak feedback has previously been studied (Goldwasser and Roth (2013); Artzi and Zettlemoyer (2013); inter alia). More recently, several works have applied policy gradient techniques such as REINFORCE (Williams, 1992) to train neural semantic parsers (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia). However, they assume the existence of the true target answers that can be used to obtain a reward for any number of output queries suggested by the system. It thus"
P18-1169,C16-2002,1,0.864011,"Missing"
P18-1169,D17-1272,1,0.891588,"ted freely and from any user interacting with the system. From a machine learning perspective, related work can be found in the areas of counterfactual bandit learning (Dudik et al., 2011; Swaminathan and Joachims, 2015a), or equivalently, off-policy reinforcement learning (Precup et al., 2000; Jiang and Li, 2016). Our contribution is to modify the self-normalizing estimator (Kong, 1992; Precup et al., 2000; Swaminathan and Joachims, 2015b; Joachims et al., 2018) to be applicable to neural networks. Our work is similar to the counterfactual learning setup for machine translation introduced by Lawrence et al. (2017b). Following their insight, we also assume the logs were created deterministically, i.e. the logging policy always outputs the most likely sequence. Their framework was applied to statistical machine translation using linear models. We show how to generalize their framework to neural models and how to apply it to the task of neural semantic parsing in the OSM domain. 3 Neural Semantic Parsing → − sively computing f (xi , h i−1 ) where f is a Gated Recurrent Unit (GRU) (Chung et al., 2014), and ← − h i is computed analogously. The input sequence is reduced to a single vector c = g({h1 , . . ."
P18-1169,P17-1003,0,0.169113,"tion. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data. 1 Introduction In semantic parsing, natural language utterances are mapped to machine readable parses which are complex and often tailored specifically to the underlying task. The cost and difficulty of manually preparing large amounts of such parses thus is a bottleneck for supervised learning in semantic parsing. Recent work (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia) has applied reinforcement learning to address the annotation bottleneck as follows: Given a question, the existence of a corresponding gold answer is assumed. A semantic parser produces multiple parses per question and corresponding answers are obtained. These answers are then compared against the gold answer and a positive reward is recorded if there is an overlap. The parser is then guided towards correct parses using the REINFORCE algorithm (Williams, 1992) which scales the gradient for the various parses by their obtained reward (see the"
P18-1169,D17-1252,0,0.0778855,"users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data. 1 Introduction In semantic parsing, natural language utterances are mapped to machine readable parses which are complex and often tailored specifically to the underlying task. The cost and difficulty of manually preparing large amounts of such parses thus is a bottleneck for supervised learning in semantic parsing. Recent work (Liang et al. (2017); Mou et al. (2017); Peng et al. (2017); inter alia) has applied reinforcement learning to address the annotation bottleneck as follows: Given a question, the existence of a corresponding gold answer is assumed. A semantic parser produces multiple parses per question and corresponding answers are obtained. These answers are then compared against the gold answer and a positive reward is recorded if there is an overlap. The parser is then guided towards correct parses using the REINFORCE algorithm (Williams, 1992) which scales the gradient for the various parses by their obtained reward (see the left half of Figure 1). However, learn"
P18-1169,E17-3017,0,0.0843255,"Missing"
P18-1169,P16-2033,0,0.0254978,"hat a reward is available for only one structure. Our work most closely resembles the work of 1821 Iyer et al. (2017) who do make the assumption of only being able to judge one output. They improve their parser using simulated and real user feedback. Parses with negative feedback are given to experts to obtain the correct parse. Corrected queries and queries with positive feedback are added to the training corpus and learning continues with a cross-entropy objective. We show that this bandit-to-supervision approach can be outperformed by offline bandit learning from partially correct queries. Yih et al. (2016) proposed a user interface for the Freebase database that enables a fast and easy creation of parses. However, in their setup the worker still requires expert knowledge about the Freebase database, whereas in our approach feedback can be collected freely and from any user interacting with the system. From a machine learning perspective, related work can be found in the areas of counterfactual bandit learning (Dudik et al., 2011; Swaminathan and Joachims, 2015a), or equivalently, off-policy reinforcement learning (Precup et al., 2000; Jiang and Li, 2016). Our contribution is to modify the self-"
P19-1029,J09-1002,0,0.077784,"Missing"
P19-1029,P18-1008,0,0.0141146,"ector: The tokens yˆt that receive δt = 1 are part of the correct output y ∗ , so the model receives a hint how a corrected output should look like. Although the likelihood of the incorrect parts of the sequence does not weigh into the sum, they are contained in the context of the correct parts (in y&lt;t ). pθ (yt |x; y&lt;t ) = softmax(NNθ (x; y&lt;t )). There are various options for building the architecture of the neural model NNθ , such as recurrent (Sutskever et al., 2014), convolutional (Gehring et al., 2017) or attentional (Vaswani et al., 2017) encoder-decoder architectures (or a mix thereof (Chen et al., 2018)). Regardless of their architecture, there are multiple ways of interactive learning that can be applied to neural Seq2Seq learners. Self-Supervision (S ELF). Instead of querying the teacher for feedback, the learner can also choose to learn from its own output, that is, to learn from self-supervision. The simplest option is to treat the learner’s output as if it was correct, but that quickly leads to overconfidence and degeneration. Clark et al. (2018) proposed a cross-view training method: the learner’s original prediction is used as a target for a weaker model that shares parameters with th"
P19-1029,P18-1165,1,0.894826,"Missing"
P19-1029,D18-1217,0,0.207405,"ever et al., 2014), convolutional (Gehring et al., 2017) or attentional (Vaswani et al., 2017) encoder-decoder architectures (or a mix thereof (Chen et al., 2018)). Regardless of their architecture, there are multiple ways of interactive learning that can be applied to neural Seq2Seq learners. Self-Supervision (S ELF). Instead of querying the teacher for feedback, the learner can also choose to learn from its own output, that is, to learn from self-supervision. The simplest option is to treat the learner’s output as if it was correct, but that quickly leads to overconfidence and degeneration. Clark et al. (2018) proposed a cross-view training method: the learner’s original prediction is used as a target for a weaker model that shares parameters with the original model. We adopt this strategy by first producing a target sequence yˆ with beam search and then weaken the decoder through attention dropout with probability patt . The objective is to minimize the negative likelihood of the original target under the weakened model 1 X J S ELF (θ) = − log ppθatt (ˆ y |x), |D| Learning from Corrections (F ULL). Under full supervision, i.e., when the learner receives a fully corrected output y ∗ for an input x,"
P19-1029,K18-1033,0,0.0977445,"NMT) system functions as a student which receives feedback simulated from a human reference translation or supervises itself. The intended real-world application is a machine translation personalization scenario where the goal of the human translator is to teach the NMT system 303 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 303–315 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics strategy (Fang et al., 2017), or to learn a curriculum to order noisy examples (Kumar et al., 2019), or to the approach of Liu et al. (2018) who use imitation learning to select batches of data to be labeled. However, the action space these approaches consider is restricted to the decision whether or not to select particular data and is designed for a fixed budget, neither do they incorporate feedback cost in their frameworks. As we will show, our self-regulation strategy outperforms active learning based on uncertainty sampling (Settles and Craven, 2008; Peris and Casacuberta, 2018) and our reinforcement learner is rewarded in such a way that it will produce the best system as early as possible. Research that addresses the choice"
P19-1029,D15-1166,0,0.061667,"ario where cost-free pre-training on a general domain data is possible, but each feedback generated by the human translator in the personalization step incurs a specific cost. In our experiment, we use human-generated reference translations to simulate both the cost of human feedback and to measure the performance gain achieved by model updates. 4.1 Seq2Seq Architecture. Both the Seq2Seq learner and the regulator are based on LSTMs (Hochreiter and Schmidhuber, 1997). The Seq2Seq has four bi-directional encoder and four decoder layers with 1024 units each, embedding layers of size 512. It uses Luong et al. (2015)’s input feeding and output layer, and global attention with a single feed forward layer (Bahdanau et al., 2015). basis of the feedback and mini-batch of stochastic gradients computed as summarized in Figure 2. In order to reinforce the regulator, the Seq2Seq model’s improvement (line 9) is assessed, and the parameters of the regulator are updated (line 10, Eq. 3). Training ends when the data stream or the provision of feedback ends. The intermediate Seq2Seq evaluations can be re-used for model selection (early stopping). In practice, these evaluations can either be performed by validation on"
P19-1029,E17-3017,0,0.0324702,"Missing"
P19-1029,W18-6319,0,0.0135516,"ction (Tiedemann, 2012) for testing the regulator on another domain. Data pre-processing details and splits are given in §A.1. The joint vocabulary for Seq2Seq and the regulator consists of 32k BPE sub-words (Sennrich et al., 2016) trained on WMT. surement of cost to the effort of the human teacher, and neglecting the effort on the learner’s side. Table 1 illustrates the costs per feedback type on a randomly selected set of examples. We measure the model improvement by evaluating the held-out set translation quality of the learned model at various time steps with corpus BLEU (cased SacreBLEU (Post, 2018)) and measure the accumulated costs. The best model is considered the one that delivers the highest quality at the lowest cost. This trade-off is important to bear in mind since it differs from the standard evaluation of machine translation models, where the overall best-scoring model, regardless of the supervision cost, is considered best. Finally, we evaluate the strategy learned by the regulator on an unseen domain, where the regulator decides which type of feedback the learner gets, but is not updated itself. Training. The Seq2Seq model is first trained on WMT with Adam (Kingma and Ba, 201"
P19-1029,P16-1162,0,0.0211056,"training the regulator with simulated feedback, and the Books 1. Which strategies does the regulator develop? 2. How well does a trained regulator transfer across domains? 1 https://github.com/joeynmt/joeynmt Code: https://github.com/juliakreutzer/ joeynmt/tree/acl19 3. How do these strategies compare against (active) learning from a single feedback type? 2 307 corpus from the OPUS collection (Tiedemann, 2012) for testing the regulator on another domain. Data pre-processing details and splits are given in §A.1. The joint vocabulary for Seq2Seq and the regulator consists of 32k BPE sub-words (Sennrich et al., 2016) trained on WMT. surement of cost to the effort of the human teacher, and neglecting the effort on the learner’s side. Table 1 illustrates the costs per feedback type on a randomly selected set of examples. We measure the model improvement by evaluating the held-out set translation quality of the learned model at various time steps with corpus BLEU (cased SacreBLEU (Post, 2018)) and measure the accumulated costs. The best model is considered the one that delivers the highest quality at the lowest cost. This trade-off is important to bear in mind since it differs from the standard evaluation of"
P19-1029,D08-1112,0,0.774583,"August 2, 2019. 2019 Association for Computational Linguistics strategy (Fang et al., 2017), or to learn a curriculum to order noisy examples (Kumar et al., 2019), or to the approach of Liu et al. (2018) who use imitation learning to select batches of data to be labeled. However, the action space these approaches consider is restricted to the decision whether or not to select particular data and is designed for a fixed budget, neither do they incorporate feedback cost in their frameworks. As we will show, our self-regulation strategy outperforms active learning based on uncertainty sampling (Settles and Craven, 2008; Peris and Casacuberta, 2018) and our reinforcement learner is rewarded in such a way that it will produce the best system as early as possible. Research that addresses the choice and the combination of different types of feedback is situated in the area between reinforcement and imitation learning (Ranzato et al., 2016; Cheng et al., 2018). Instead of learning how to mix different supervision signals, these approaches assume fixed schedules. Further connections between our work on learning with multiple feedback types can be drawn to various extensions of reinforcement learning by multiple t"
P99-1014,P99-1035,1,0.760478,"2 0.365586 0.365374 0.292716 0.280183 0.238182 drop grow vary improve climb flow cut mount 0.741467 0.720221 0.693922 0.656021 0.438486 0.375039 0.316081 0.215156 0.160317 0.154633 diminish ansteigen steigen absinken sinken schrumpfen zuriickgehen anwachsen stagnieren wachsen hinzukommen (go up) (rise) (sink) (go down) (shrink) (decrease) (increase) (stagnate) (grow) (be added) Figure 8: Scalar motion verbs corpus of German subordinate clauses, yielding 418290 tokens (318086 types) of pairs of verbs or adjectives and nouns. The lexicalized probabilistic grammar for German used is described in Beil et al. (1999). We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of &quot;einfache Anderungsverben&quot; (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there Figure 9: German intransitive scalar motion verbs increase (8, 17) development - pressure fat - risk communication - awareness supplementation - concentration increase- number 0.3097650 2.3055 2.11807 2.04227 1.98918 1.80559 Figure 10: Transitive increase with"
P99-1014,W98-1505,1,0.770586,"nguages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical meaning. Both of these considerations suggest the relevance of inductive and experimental approaches to the construction of lexicons with semantic information. This paper presents a method for automatic induction of semantically annotated subcategorization frames from unannotated corpora. We use a statistical subcat-induction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame (Carroll and Rooth, 1998). The statistical parser can also collect frequencies for the nominal fillers of slots in a subcat frame. The induction of labels for slots in a frame is based upon estimation of a probability distribution over tuples consisting of a class label, a selecting head, a grammatical relation, and a filler head. The class label is treated as hidden data in the EMframework for statistical estimation. 2 EM-Based Clustering In our clustering approach, classes are derived directly from distributional d a t a - - a sample of pairs of verbs and nouns, gathered by parsing an unannotated corpus and extracti"
P99-1014,P93-1024,0,0.979918,"e two main tasks of EM-based clustering are i) the induction of a smooth probability model on the data, and ii) the automatic discovery of class-structure in the data. Both of these aspects are respected in our application of lexicon induction. The basic ideas of our EM-based clustering approach were presented in Rooth (Ms). Our approach constrasts with the merely heuristic and empirical justification of similarity-based approaches to clustering (Dagan et al., to appear) for which so far no clear probabilistic interpretation has been given. The probability model we use can be found earlier in Pereira et al. (1993). However, in contrast to this ap104 Class PROB 17 0.0265 o~~ 0.0437 0.0302 0.0344 0.0337 0.0329 0.0257 0.0196 0.0177 0.0169 0.0156 ~ increase.aso:o fall.as:s pay.aso:o reduce.aso:o rise.as:s exceed.aso:o exceed.aso:s affect.aso:o grow.as:s • • • • • • s • • • • • • s • • • s • s s • • • • • • • • • s • • • s • • • • • s • • • • • • s • • • • • s • • • • • • • s • s • s • • • • • • • 0.0134 include.aso:s • • • • • • • 10.0129 reach.aso:s • • • • • • • • • 0.0102 decline.as:s lose.aso:o 0.0099 act.aso:s 0.0120 • • • • • • • 0.0099 improve.aso:o • • 0.0088 include.aso:o • s 0.0088 cut.aso:o 0.00"
P99-1014,C94-2123,0,0.01482,"tegorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. 1 Introduction An important challenge in computational linguistics concerns the construction of large-scale computational lexicons for the numerous natural languages where very large samples of language use are now available. Resnik (1993) initiated research into the automatic acquisition of semantic selectional restrictions. Ribas (1994) presented an approach which takes into account the syntactic position of the elements whose semantic relation is to be acquired. However, those and most of the following approaches require as a prerequisite a fixed taxonomy of semantic relations. This is a problem because (i) entailment hierarchies are presently available for few languages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical meaning. Both of these considerations suggest the relevance of inductive and experimental approaches to t"
P99-1014,W97-0309,0,0.0123898,"• s • • • • • • • • • • • • s • • • • • s • • • • • • • • • • s . • • • • s • • s . • •. • • • • s s • s s s • • • • • • • • • • • s • • • • • • s • • • • • • • s s • • .• . • • • s • • • • • • • • • • • • • • s • • • • • • • • • • . s • s • s • • . • •. • • • • . • •. :. • • • s : • s • • ~ • s • • • • • • . s • • • s o : : &quot;:&apos;:: s • • • • • • • • 1:&apos;11:1 • ~ ::: ::: • • • • • Figure 1: Class 17: scalar change proach, our statistical inference m e t h o d for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998). There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998). We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c E C, where the classes are given no prior interpretation. The semantically smoothed probability of a p"
P99-1035,W98-1117,0,0.0660895,"Missing"
P99-1035,W98-1505,1,0.764283,"e E s t i m a t i o n o f a L e x i c a l i z e d P C F G for G e r m a n Franz Beil, G l e n n Carroll, D e t l e f Prescher, Stefan Riezler and Mats R o o t h I n s t i t u t ffir Maschinelle Sprachverarbeitung, University of S t u t t g a r t Abstract The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verbfinal clauses. Grammar and formalism features which make the experiment feasible are described. Successive models are evaluated on precision and recall of phrase markup. 1 Introduction Charniak (1995) and Carroll and Rooth (1998) present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates. The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order"
P99-1069,J96-1002,0,0.0368621,"udo-likelihood: (7) i=l,...,n Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992). Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a &quot;pseudolikelihood&quot; estimator to address these difficulties (Besag, 1974; Besag,"
P99-1069,J97-4005,0,\N,Missing
P99-1069,C90-3029,0,\N,Missing
Q19-1015,D14-1179,0,0.0160968,"Missing"
Q19-1015,D16-1025,0,0.0612677,"Missing"
Q19-1015,P11-2031,0,0.0347277,"from weak feedback. We showed that MRT can be turned into a bipolar objective by defining a metric that assigns negative values to bad outputs. This improves the performance of MRT objectives. However, the ramp loss objective is still superior as it is easy to implement and efficient to compute. Furthermore, on weakly supervised tasks our novel token-level ramp loss objective RAMP-T can obtain further improvements over its sequence-level counterpart Table 8: BLEU scores for fully supervised MT experiments. Boldfaced results are significantly better than MLE at p &lt; 0.01 according to multeval (Clark et al., 2011). ∗ marks a significant difference to MRT and PERC2, and ∗∗ marks a difference to RAMP1. MLE baseline and PERC1, but perform on a par with MRT and each other. Both RAMP and RAMP1 are able to outperform MRT, PERC2, and RAMP2, with the bipolar objective RAMP also outperforming RAMP1 by a narrow margin. The main difference between RAMP and RAMP1, compared to PERC2 and RAMP2, is the fact that the latter objectives use yˆ as y − , whereas the former use a fear translation with high probability and low BLEU+1 . We surmise that for this fully supervised task, selecting a y − which has some known nega"
Q19-1015,D13-1160,0,0.0718578,"Missing"
Q19-1015,W10-2903,0,0.111447,"Missing"
Q19-1015,2012.eamt-1.60,0,0.0153892,"s are averaged over the runs. using sequence-level training towards a reference can lead to degenerate solutions where the model gives low probability to all its predictions (Shen et al., 2016). PERC2 addresses this problem by replacing y¯ by a surrogate translation that achieves the highest BLEU+1 score in K(x). This approach is also used by Edunov et al. (2018) for the loss functions which require an oracle. PERC1 corresponds to equation (9), PERC2 to equation (10) of Gimpel and Smith (2012). Experimental Setup. We conduct experiments on the IWSLT 2014 German–English task, which is based on Cettolo et al. (2012) in the same way as Edunov et al. (2018). The training set contains 160K sentence pairs. We set the maximum sentence length to 50 and use BPE with 14,000 merge operations. Edunov et al. (2018) sample 7K sentences from the training set as heldout data. We do the same, but only use one tenth of the data as heldout set to be able to validate often. Our baseline system (MLE) is a BiLSTM encoder-decoder with attention, which is trained using the MLE objective. Word embedding and hidden layer dimensions are set to 256. We use batches of 64 sentences for baseline training and batches of 40 inputs for"
Q19-1015,W02-1001,0,0.230146,"ectives. For fully supervised MT we assume access to one or more reference translations y¯ for each input x. The reward BLEU+1 (y, y¯) is a per-sentence approximation of the BLEU score.11 Table 7 shows the different definitions of y + and y − , which give rise to different ramp losses. RAMP, RAMP1, and RAMP2 are defined analogously to the other tasks. We again include a hyperparameter α &gt; 0 interpolating cost function and model score when searching for y + and y − . Gimpel and Smith (2012) also include the perceptron loss in their analysis. PERC1 is a re-formulation of the Collins perceptron (Collins, 2002) where the reference is used as y + and yˆ is used as y − . A comparison with PERC1 is not possible for the weakly supervised tasks in the previous sections, as gold structures are not available for these tasks. With neural MT and subword methods we are able to compute this loss for any reference without running into the problem of reachability that was faced by phrase-based MT (Liang et al., 2006). However, Fully Supervised Machine Translation Our work focuses on weakly supervised tasks, but we also conduct experiments using a fully supervised MT task. These experiments are motivated on the o"
Q19-1015,P18-2008,0,0.0281211,"Missing"
Q19-1015,W14-3346,0,0.0129083,"ections, as gold structures are not available for these tasks. With neural MT and subword methods we are able to compute this loss for any reference without running into the problem of reachability that was faced by phrase-based MT (Liang et al., 2006). However, Fully Supervised Machine Translation Our work focuses on weakly supervised tasks, but we also conduct experiments using a fully supervised MT task. These experiments are motivated on the one hand by adapting the findings of Gimpel and Smith (2012) to the neural MT 11 We use the BLEU score with add-1 smoothing for n &gt; 1, as proposed by Chen and Cherry (2014). 242 Loss RAMP RAMP1 RAMP2 PERC1 PERC2 y+ y− arg maxy πw (y |x) − α(1 − BLEU+1 (y, y¯)) yˆ arg maxy πw (y |x) − α(1 − BLEU+1 (y, y¯)) y¯ arg maxy BLEU+1 (y, y¯) arg maxy πw (y |x) + α(1 − BLEU+1 (y, y¯)) arg maxy πw (y |x) + α(1 − BLEU+1 (y, y¯)) yˆ yˆ yˆ Table 7: Configurations for y + and y − for fully supervised MT. yˆ is the highest-probability model output, y¯ is a gold standard reference. πw (y |x) is the probability of y according to the model. The arg maxy is taken over the k -best list K(x). BLEU+1 is smoothed per-sentence BLEU and α is a scaling factor. use a learning rate of 0.001."
Q19-1015,eisele-chen-2010-multiun,0,0.0105965,"he probability of y under the model. The arg maxy is taken over the k -best list K(x). α is a scaling factor regulating the influence of the metric compared to the model probability. δ1 and δ2 are metrics defined with respect to relevant and irrelevant documents d+ and d− (see Eq. 8 and 9). are removed from the training set. Our neural MT model uses 500-dimensional word embeddings and hidden layer dimension of 1,024. Encoder and decoder use GRU units. An out-of-domain model is trained on 2.1 million sentence pairs from Europarl v7 (Koehn, 2005), News Commentary v10, and the MultiUN v1 corpus (Eisele and Chen, 2010). The baseline (MLE) is trained using the MLE objective and ADADELTA (Zeiler, 2012) for 20 epochs. We train on batches of 64 and use dropout for regularization, with a dropout rate of 0.2 for embedding and hidden layers and 0.1 for source and target layers. Gradients are clipped if their norm exceeds 1.0. The metric-augmented objectives are trained using SGD. All hyperparameters are chosen on the development set. For the ramp loss objectives, we use a learning rate of 0.005, α = 10, and a k -best size of 16. We compare ramp loss to MRT using both δ1 (y, d+ ) and δ2 (y, d+ , d− ) as the externa"
Q19-1015,C16-1297,1,0.858099,"e average n-gram precision between a hypothesis and a document, multiplied by a brevity penalty. As we do not have a reference length, we include a brevity penalty term that compares the output length to the input length. This ratio can be modified by a factor r that represents the average length difference between source and target language and which can be computed over the training data: N  1  un c(un , y ) · 11un ∈d  δ1 (y, d) = · BP , N n=1 un c ( u n , y ) (8) Experimental Setup. We test our objectives on a weakly supervised English–German Wikipedia translation task first proposed in Jehl and Riezler (2016). In-domain training data are 10,000 English sentences with relevant German documents sampled from the WikiCLIR corpus (Schamoni et al., 2014).8 The task includes a small in-domain development and test set (dev: 1,712 sentences, test: 1,526 sentences), each consisting of four Wikipedia articles on diverse subjects. Irrelevant documents d− are sampled from the German side of the News Commentary9 data set, which contains document boundary information. Byte-pair encoding (Sennrich et al., 2016) with 30,000 merge operations is applied to all source and target data. Sentences longer than 80 words w"
Q19-1015,2005.mtsummit-papers.11,0,0.147664,"on. yˆ is the highest-probability model output. πw (y |x) is the probability of y under the model. The arg maxy is taken over the k -best list K(x). α is a scaling factor regulating the influence of the metric compared to the model probability. δ1 and δ2 are metrics defined with respect to relevant and irrelevant documents d+ and d− (see Eq. 8 and 9). are removed from the training set. Our neural MT model uses 500-dimensional word embeddings and hidden layer dimension of 1,024. Encoder and decoder use GRU units. An out-of-domain model is trained on 2.1 million sentence pairs from Europarl v7 (Koehn, 2005), News Commentary v10, and the MultiUN v1 corpus (Eisele and Chen, 2010). The baseline (MLE) is trained using the MLE objective and ADADELTA (Zeiler, 2012) for 20 epochs. We train on batches of 64 and use dropout for regularization, with a dropout rate of 0.2 for embedding and hidden layers and 0.1 for source and target layers. Gradients are clipped if their norm exceeds 1.0. The metric-augmented objectives are trained using SGD. All hyperparameters are chosen on the development set. For the ramp loss objectives, we use a learning rate of 0.005, α = 10, and a k -best size of 16. We compare ram"
Q19-1015,D16-1116,0,0.0634518,"Missing"
Q19-1015,N12-1023,0,0.130113,"on batch: 1/2019; Published 5/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  a more reliable signal can be produced by not just encouraging outputs that are good according to weak positive feedback, but also by actively discouraging bad structures. In this way, a system can more effectively learn what distinguishes good outputs from bad ones. We call an objective that incorporates this idea a bipolar objective. The bipolar idea is naturally captured by the structured ramp loss objective (Chapelle et al., 2009), especially in the formulation by Gimpel and Smith (2012) and Chiang (2012), who use ramp loss to separate a hope from a fear output in a linear structured prediction model. We employ several ramp loss objectives for two weak supervision tasks, and adapt them to neural models. First, we turn to the task of semantic parsing in a setup where only question-answer pairs, but no gold semantic parses, are given. We assume a baseline system has been trained using a small supervised data set of question-parse pairs under the MLE objective. The goal is to improve this system by leveraging a larger data set of questionanswer pairs. During learning, the semant"
Q19-1015,P18-1169,1,0.877554,"Missing"
Q19-1015,P17-1097,0,0.0156337,"ly update the weights of features that differ between hope and fear. Finally, the token-level objective allows us to capture token-level errors in a setup where MLE training is not available. Using this objective, we obtain additional gains on top of the sequence-level ramp loss for weakly supervised tasks. 2 Related Work Training neural models with metric-augmented objectives has been explored for various NLP tasks in supervised and weakly supervised scenarios. MRT for neural models has previously been used for machine translation (Shen et al., 2016) and semantic parsing (Liang et al., 2017; Guu et al., 2017).1 Other objectives based on classical 1 Note that Liang et al. (2017) refer to their objective as an instantiation of REINFORCE, however they build an average over several outputs for one input and thus the objective more accurately falls under the heading of MRT. 234 structured prediction losses have been used for both machine translation and summarization (Edunov et al., 2018), as well as semantic parsing (Iyyer et al., 2017; Misra et al., 2018). Objectives inspired by REINFORCE have, for example, been applied to machine translation (Ranzato et al., 2016; Norouzi et al., 2016), semantic par"
Q19-1015,P17-1003,0,0.114203,"ear models, which only update the weights of features that differ between hope and fear. Finally, the token-level objective allows us to capture token-level errors in a setup where MLE training is not available. Using this objective, we obtain additional gains on top of the sequence-level ramp loss for weakly supervised tasks. 2 Related Work Training neural models with metric-augmented objectives has been explored for various NLP tasks in supervised and weakly supervised scenarios. MRT for neural models has previously been used for machine translation (Shen et al., 2016) and semantic parsing (Liang et al., 2017; Guu et al., 2017).1 Other objectives based on classical 1 Note that Liang et al. (2017) refer to their objective as an instantiation of REINFORCE, however they build an average over several outputs for one input and thus the objective more accurately falls under the heading of MRT. 234 structured prediction losses have been used for both machine translation and summarization (Edunov et al., 2018), as well as semantic parsing (Iyyer et al., 2017; Misra et al., 2018). Objectives inspired by REINFORCE have, for example, been applied to machine translation (Ranzato et al., 2016; Norouzi et al.,"
Q19-1015,N16-1088,1,0.890407,"Missing"
Q19-1015,P06-1096,0,0.188059,"Missing"
Q19-1015,D18-1266,0,0.0788432,"vised scenarios. MRT for neural models has previously been used for machine translation (Shen et al., 2016) and semantic parsing (Liang et al., 2017; Guu et al., 2017).1 Other objectives based on classical 1 Note that Liang et al. (2017) refer to their objective as an instantiation of REINFORCE, however they build an average over several outputs for one input and thus the objective more accurately falls under the heading of MRT. 234 structured prediction losses have been used for both machine translation and summarization (Edunov et al., 2018), as well as semantic parsing (Iyyer et al., 2017; Misra et al., 2018). Objectives inspired by REINFORCE have, for example, been applied to machine translation (Ranzato et al., 2016; Norouzi et al., 2016), semantic parsing (Liang et al., 2017; Mou et al., 2017; Guu et al., 2017), and reading comprehension (Choi et al., 2017; Yang et al., 2017).2 Misra et al. (2018) are the first to compare several objectives for neural semantic parsing. For semantic parsing, they find that objectives employing structured prediction losses perform best. Edunov et al. (2018) compare different classical structured prediction objectives including MRT on a fully supervised machine tr"
Q19-1015,P14-2080,1,0.8259,"de a brevity penalty term that compares the output length to the input length. This ratio can be modified by a factor r that represents the average length difference between source and target language and which can be computed over the training data: N  1  un c(un , y ) · 11un ∈d  δ1 (y, d) = · BP , N n=1 un c ( u n , y ) (8) Experimental Setup. We test our objectives on a weakly supervised English–German Wikipedia translation task first proposed in Jehl and Riezler (2016). In-domain training data are 10,000 English sentences with relevant German documents sampled from the WikiCLIR corpus (Schamoni et al., 2014).8 The task includes a small in-domain development and test set (dev: 1,712 sentences, test: 1,526 sentences), each consisting of four Wikipedia articles on diverse subjects. Irrelevant documents d− are sampled from the German side of the News Commentary9 data set, which contains document boundary information. Byte-pair encoding (Sennrich et al., 2016) with 30,000 merge operations is applied to all source and target data. Sentences longer than 80 words where un are the n-grams present in y , c() counts the occurrences of an n-gram in y , and N is the maximum order of n-grams used. The brevity"
Q19-1015,E17-3017,0,0.0597854,"Missing"
Q19-1015,P16-1162,0,0.0339229,"test our objectives on a weakly supervised English–German Wikipedia translation task first proposed in Jehl and Riezler (2016). In-domain training data are 10,000 English sentences with relevant German documents sampled from the WikiCLIR corpus (Schamoni et al., 2014).8 The task includes a small in-domain development and test set (dev: 1,712 sentences, test: 1,526 sentences), each consisting of four Wikipedia articles on diverse subjects. Irrelevant documents d− are sampled from the German side of the News Commentary9 data set, which contains document boundary information. Byte-pair encoding (Sennrich et al., 2016) with 30,000 merge operations is applied to all source and target data. Sentences longer than 80 words where un are the n-grams present in y , c() counts the occurrences of an n-gram in y , and N is the maximum order of n-grams used. The brevity penalty term is   r · |y | BP = min 1, . |x| δ2 (y, d+ , d− ) is defined as the difference between δ1 (y, d+ ) and δ1 (y, d− ), subject to a linear transformation to allow values to lie between 0 and 1: δ2 (y, d+ , d− ) = 0.5 · (δ1 (y, d+ ) − δ1 (y, d− ) + 1) . 8 WikiCLIR annotates both a stronger mate relation when there is a direct cross-lingual li"
Q19-1015,2001.mtsummit-papers.68,0,0.0464902,"Missing"
Q19-1015,P16-1159,0,0.372457,"ual data collections. In this paper we investigate methods where a supervision signal for output structures can be extracted from weak feedback. In the following, we use learning from weak feedback, or weakly supervised learning, to refer to a scenario where output structures generated by the model are judged according to an external metric, and this feedback is used to extract a supervision signal that guides the learning process. Metric-augmented sequence-level objectives from reinforcement learning (Williams, 1992; Ranzato et al., 2016), minimum risk training (MRT) (Smith and Eisner, 2006; Shen et al., 2016) or margin-based structured prediction objectives (Taskar et al., 2005; Edunov et al., 2018) can be seen as instances of such algorithms. In natural language processing applications, such algorithms have mostly been used in combination with full supervision tasks, allowing to compute a feedback score from metrics such as BLEU or F-score that measure the similarity of output structures against gold structures. Our main interest is in weak supervision tasks where the calculation of a feedback score cannot fall back onto gold structures. For example, matching proposed answers to a gold answer can"
Q19-1015,P06-2101,0,0.0606575,"esent for many multilingual data collections. In this paper we investigate methods where a supervision signal for output structures can be extracted from weak feedback. In the following, we use learning from weak feedback, or weakly supervised learning, to refer to a scenario where output structures generated by the model are judged according to an external metric, and this feedback is used to extract a supervision signal that guides the learning process. Metric-augmented sequence-level objectives from reinforcement learning (Williams, 1992; Ranzato et al., 2016), minimum risk training (MRT) (Smith and Eisner, 2006; Shen et al., 2016) or margin-based structured prediction objectives (Taskar et al., 2005; Edunov et al., 2018) can be seen as instances of such algorithms. In natural language processing applications, such algorithms have mostly been used in combination with full supervision tasks, allowing to compute a feedback score from metrics such as BLEU or F-score that measure the similarity of output structures against gold structures. Our main interest is in weak supervision tasks where the calculation of a feedback score cannot fall back onto gold structures. For example, matching proposed answers"
Q19-1015,P15-1142,0,0.0704371,"Missing"
Q19-1015,D16-1264,0,0.0411225,"Missing"
Q19-1015,P17-1096,0,0.01673,"iation of REINFORCE, however they build an average over several outputs for one input and thus the objective more accurately falls under the heading of MRT. 234 structured prediction losses have been used for both machine translation and summarization (Edunov et al., 2018), as well as semantic parsing (Iyyer et al., 2017; Misra et al., 2018). Objectives inspired by REINFORCE have, for example, been applied to machine translation (Ranzato et al., 2016; Norouzi et al., 2016), semantic parsing (Liang et al., 2017; Mou et al., 2017; Guu et al., 2017), and reading comprehension (Choi et al., 2017; Yang et al., 2017).2 Misra et al. (2018) are the first to compare several objectives for neural semantic parsing. For semantic parsing, they find that objectives employing structured prediction losses perform best. Edunov et al. (2018) compare different classical structured prediction objectives including MRT on a fully supervised machine translation task. They find MRT to perform best. However, they only obtain larger gains by interpolating MRT with the MLE loss. Neither Misra et al. (2018) nor Edunov et al. (2018) investigate objectives that correspond to the bipolar ramp loss that is central in our work. The"
Q19-1015,D16-1137,0,0.0541945,"Missing"
Q19-1015,P02-1040,0,\N,Missing
Q19-1015,P17-1167,0,\N,Missing
Q19-1015,P17-1020,0,\N,Missing
Q19-1015,N18-1033,0,\N,Missing
W03-2401,H94-1020,0,\N,Missing
W03-2401,J93-4001,1,\N,Missing
W03-2401,P02-1035,1,\N,Missing
W04-3223,P99-1069,1,0.634543,"be seen as a built-in regularization mechanism that avoids overfitting the training data. However, it is only a weak regularizer that cannot avoid overfitting in situations where the number of training examples is significantly smaller than the number of features. In such situations some features will occur zero times on the training set and receive negative infinity weights, causing the assignment of zero probabilities for inputs including those features. Similar assignment of (negative) infinity weights happens to features that are pseudominimal (or pseudo-maximal) on the training set (see Johnson et al. (1999)), that is, features whose value on correct parses always is less (or greater) 1 This research has been funded in part by contract MDA904-03-C-0404 of the Advanced Research and Development Activity, Novel Intelligence from Massive Data program. than or equal to their value on all other parses. Also, if large features sets are generated automatically from conjunctions of simple feature tests, many features will be redundant. Besides overfitting, large feature sets also create the problem of increased time and space complexity. Common techniques to deal with these problems are regularization and"
W04-3223,N04-1013,1,0.540398,"1. 6 Experiments 6.1 Train and Test Data In the experiments presented in this paper, we evaluate `2 , `1 , and `0 regularization on the task of stochastic parsing with maximum-entropy models For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). For estimation and best-parse searching, efficient dynamicprogramming techniques over features forests are employed (see Kaplan et al. (2004)). For the setup of discriminative estimation from partially labeled data, we found that a restriction of the training data to sentences with a relatively low ambiguity rate was possible at no loss in accuracy compared to training from all sentences. Furthermore, data were restricted to sentences of which a discriminative learner can possibly take advantage, i.e. sentences where the set of parses assigned to the labeled string is a proper subset of the parses assigned to the unlabeled string. Together with a restriction to examples that could be parsed by the full grammar and did not have to u"
W04-3223,W03-1018,0,0.333078,"ion about inclusion or deletion of features can be done in the same framework. Feature sparsity is produced by the polyhedral structure of the `1 norm which exhibits a gradient discontinuity at zero that tends to force a subset of parameter values to be exactly zero at the optimum. Since this discontinuity makes optimization a hard numerical problem, standard gradient-based techniques for estimation cannot be applied directly. Tibshirani (1996) presents a specialized optimization algorithm for `1 regularization for linear least-squares regression called the Lasso algorithm. Goodman (2003) and Kazama and Tsujii (2003) employ standard iterative scaling and conjugate gradient techniques, however, for regularization a simplified one-sided exponential prior is employed which is non-zero only for non-negative parameter values. In these approaches the full feature space is considered in estimation, so savings in computational complexity are gained only in applications of the resulting sparse models. Perkins et al. (2003) presented an approach that combines `1 based regularization with incremental feature selection. Their basic idea is to start with a model in which almost all weights are zero, and iteratively de"
W04-3223,W02-2018,0,0.73659,"ted features is used that exhibits only a moderate amount of redundancy. We will see that for such cases, n-best feature selection considerably improves computational complexity, and also achieves slightly better generalization performance. After adding n ≥ 1 features to the model in a grafting step, the model is optimized with respect to all parameters corresponding to currently included features. This optimization is done by calling a gradient-based general purpose optimization routine for the regularized objective function. We use a conjugate gradient routine for this purpose (Minka, 2001; Malouf, 2002)2 . The gradient of our criterion with respect to a parameter λi is: ∂C(λ) ∂λi 2 m = 1 X ∂L(λ) + γ sign(λi ) m ∂λi k=1 Note that despite gradient feature testing, the parameters for some features can be driven to zero in conjugate gradient optimization of the `1 -regularized objective function. Care has to be taken to catch those features and prune them explicitly to avoid numerical instability. The sign of λi decides if γ is added or subtracted from the gradient for feature fi . For a feature that is newly added to the model and thus has weight λi = 0, we use the feature gradient test to dete"
W04-3223,J93-2004,0,0.0442512,"re gradient test to determine ∂C(λ) the sign. If ∂L(λ) ∂λi > γ, we know that ∂λi > 0, thus we let sign(λi ) = −1 in order to decrease C. Following the same rationale, if ∂L(λ) < −γ we ∂λi set sign(λi ) = +1. An outline of an n-best grafting algorithm is given in Fig. 1. 6 Experiments 6.1 Train and Test Data In the experiments presented in this paper, we evaluate `2 , `1 , and `0 regularization on the task of stochastic parsing with maximum-entropy models For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). For estimation and best-parse searching, efficient dynamicprogramming techniques over features forests are employed (see Kaplan et al. (2004)). For the setup of discriminative estimation from partially labeled data, we found that a restriction of the training data to sentences with a relatively low ambiguity rate was possible at no loss in accuracy compared to training from all sentences. Furthermore, data were restricted to sentences of which a discriminative learner c"
W04-3223,J93-4001,0,0.0305317,"ss sparse in our case is that most of our feature templates are instantiated to linguistically motivated cases, and only a few feature templates encode all possible conjunctions of simple feature tests. Redundant features are introduced mostly by the latter templates, whereas the former features are generalizations over possible combinations of grammar constants. We conjecture that feature sets like this are typical for natural language applications. Efficient feature detection is achieved by a combination of hashing and dynamic programming on the packed representation of c- and f-structures (Maxwell and Kaplan, 1993). Features can be described as local and non-local, depending on the size of the graph that has to be traversed in their computation. For each local template one of the parameters is selected as a key for hashing. Non-local features are treated as two (or more) local sub-features. Packed structures are traversed depth-first, visiting each node only once. Only the features keyed on the label of the current node are considered for matching. For each non-local feature, the contexts of matching subfeatures are stored at the respective nodes, propagated upward in dynamic programing fashion, and con"
W04-3223,P02-1035,1,0.879079,"to decrease C. Following the same rationale, if ∂L(λ) < −γ we ∂λi set sign(λi ) = +1. An outline of an n-best grafting algorithm is given in Fig. 1. 6 Experiments 6.1 Train and Test Data In the experiments presented in this paper, we evaluate `2 , `1 , and `0 regularization on the task of stochastic parsing with maximum-entropy models For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). For estimation and best-parse searching, efficient dynamicprogramming techniques over features forests are employed (see Kaplan et al. (2004)). For the setup of discriminative estimation from partially labeled data, we found that a restriction of the training data to sentences with a relatively low ambiguity rate was possible at no loss in accuracy compared to training from all sentences. Furthermore, data were restricted to sentences of which a discriminative learner can possibly take advantage, i.e. sentences where the set of parses assigned to the labeled string is a proper subset of the"
W04-3223,W03-1020,0,0.0778951,"Missing"
W05-0908,J93-3001,0,0.148086,"hod is that the null hypothesis may be rejected because the shape of the sampling distribution is not well-approximated by the shape of the bootstrap sampling distribution rather than because the expected value of the test statistic differs from the value that is hypothesized.”(Noreen (1989), p. 89). Below we describe these two test procedures in more detail, and compare them in our experimental setup. 4.1 Approximate Randomization An excellent introduction to the approximate randomization test is Noreen (1989). Applications of this test to natural language processing problems can be found in Chinchor et al. (1993). In our case of assessing statistical significance of result differences between SMT systems, the test statistic of interest is the absolute value of the difference in BLEU, NIST, or F-scores produced by two systems on the same test set. These test statistics are computed by accumulating certain count variables over the sentences in the test set. For example, in case of BLEU and NIST, variables for the length of reference translations and system translations, and for n-gram matches and n-gram counts are accumulated over the test corpus. In case of F-score, variable tuples consisting of the nu"
W05-0908,1993.eamt-1.1,0,0.170153,"2S different ways to shuffle the variable tuples between the two systems. Approximate randomization produce shuffles by random assignments instead of evaluating all 2S possible assignments. Significance levels are computed as the percentage of trials where the pseudo statistic, i.e., the test statistic computed on the shuffled data, is greater than or equal to the actual statistic, i.e., the test statistic computed on the test data. A sketch of an algorithm for approximate randomization testing is given in Fig. 1. 4.2 The Bootstrap An excellent introduction to the technique is the textbook by Efron and Tibshirani (1993). In contrast to approximate randomization, the bootstrap method makes the assumption that the sample is a representative “proxy” for the population. The shape of the sampling distribution is estimated by repeatedly sampling (with replacement) from the sample itself. A sketch of a procedure for bootstrap testing is given in Fig. 2. First, the test statistic is computed on the test data. Then, the sample mean of the pseudo statistic is computed on the bootstrapped data, i.e., the test statistic is computed on bootstrap samples of equal size and averaged over bootstrap samples. In order to compu"
W05-0908,N03-1017,0,0.0140452,"Missing"
W05-0908,koen-2004-pharaoh,0,0.0278711,"Missing"
W05-0908,W04-3250,0,0.15829,"tandard in the MT literature. This is especially important in situations where multiple pairwise comparisons are conducted, and small result differences are expected. 58 2 The Experimental Setup: Discriminative Reranking for Phrase-Based SMT The experimental setup we employed to compare evaluation measures and significance tests is a discriminative reranking experiment on 1000-best lists of a phrase-based SMT system. Our system is a re-implementation of the phrase-based system described in Koehn (2003), and uses publicly available components for word alignment (Och and Ney, 2003)1 , decoding (Koehn, 2004a)2 , language modeling (Stolcke, 2002)3 and finite-state processing (Knight and Al-Onaizan, 1999)4 . Training and test data are taken from the Europarl parallel corpus (Koehn, 2002)5 . Phrase-extraction follows Och et al. (1999) and was implemented by the authors: First, the word aligner is applied in both translation directions, and the intersection of the alignment matrices is built. Then, the alignment is extended by adding immediately adjacent alignment points and alignment points that align previously unaligned words. From this many-to-many alignment matrix, phrases are extracted accordi"
W05-0908,N04-1022,0,0.014776,"Missing"
W05-0908,P02-1038,0,0.0409508,"ty, word This gradient test is applied to each feature and at penalty) as provided by PHARAOH, together with a each step the features that pass the test with maximultitude of overlapping phrase features. For exam- mum magnitude are added to the model. This prople, for a phrase-table of phrases consisting of max- vides both efficient and accurate estimation with imally 3 words, we allow all 3-word phrases and 2- large feature sets. word phrases as features. Since bigram features can Work on discriminative reranking has been reoverlap, information about trigrams can be gathered ported before by Och and Ney (2002), Och et al. by composing bigram features even if the actual tri- (2004), and Shen et al. (2004). The main purpose of gram is not seen in the training data. our reranking experiments is to have a system that Feature selection makes it possible to employ and can easily be adjusted to yield system variants that evaluate a large number of features, without con- differ at controllable amounts. For quick experimencerns about redundant or irrelevant features hamper- tal turnaround we selected the training and test data ing generalization performance. The `1 regularizer is from sentences with 5 to 15"
W05-0908,J03-1002,0,0.00322332,"ction levels than is currently standard in the MT literature. This is especially important in situations where multiple pairwise comparisons are conducted, and small result differences are expected. 58 2 The Experimental Setup: Discriminative Reranking for Phrase-Based SMT The experimental setup we employed to compare evaluation measures and significance tests is a discriminative reranking experiment on 1000-best lists of a phrase-based SMT system. Our system is a re-implementation of the phrase-based system described in Koehn (2003), and uses publicly available components for word alignment (Och and Ney, 2003)1 , decoding (Koehn, 2004a)2 , language modeling (Stolcke, 2002)3 and finite-state processing (Knight and Al-Onaizan, 1999)4 . Training and test data are taken from the Europarl parallel corpus (Koehn, 2002)5 . Phrase-extraction follows Och et al. (1999) and was implemented by the authors: First, the word aligner is applied in both translation directions, and the intersection of the alignment matrices is built. Then, the alignment is extended by adding immediately adjacent alignment points and alignment points that align previously unaligned words. From this many-to-many alignment matrix, phra"
W05-0908,W99-0604,0,0.0111316,"r Phrase-Based SMT The experimental setup we employed to compare evaluation measures and significance tests is a discriminative reranking experiment on 1000-best lists of a phrase-based SMT system. Our system is a re-implementation of the phrase-based system described in Koehn (2003), and uses publicly available components for word alignment (Och and Ney, 2003)1 , decoding (Koehn, 2004a)2 , language modeling (Stolcke, 2002)3 and finite-state processing (Knight and Al-Onaizan, 1999)4 . Training and test data are taken from the Europarl parallel corpus (Koehn, 2002)5 . Phrase-extraction follows Och et al. (1999) and was implemented by the authors: First, the word aligner is applied in both translation directions, and the intersection of the alignment matrices is built. Then, the alignment is extended by adding immediately adjacent alignment points and alignment points that align previously unaligned words. From this many-to-many alignment matrix, phrases are extracted according to a contiguity requirement that states that words in the source phrase are aligned only with words in the target phrase, and vice versa. Discriminative reranking on a 1000-best list of translations of the SMT system uses an `"
W05-0908,P03-1021,0,0.073368,"Missing"
W05-0908,2001.mtsummit-papers.68,0,0.119997,"some pitfalls that arise in automatic evaluation and statistical significance testing in MT research. The first pitfall concerns the discriminatory power of automatic evaluation measures. In the following, we compare the sensitivity of three intrinsic evaluation measures that differ with respect to their focus on different aspects 57 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 57–64, Ann Arbor, June 2005. 2005 Association for Computational Linguistics of translation. We consider the well-known BLEU score (Papineni et al., 2001) which emphasizes fluency by incorporating matches of high n-grams. Furthermore, we consider an F-score measure that is adapted from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al., 2003). This measure matches grammatical dependency relations of parses for system output and reference translations, and thus emphasizes semantic aspects of translational adequacy. As a third measure we consider NIST (Doddington, 2002), which favors lexical choice over word order and does not take structural information into account. On an experimental evaluation on a rerank"
W05-0908,N03-1026,1,0.868119,"ity of three intrinsic evaluation measures that differ with respect to their focus on different aspects 57 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 57–64, Ann Arbor, June 2005. 2005 Association for Computational Linguistics of translation. We consider the well-known BLEU score (Papineni et al., 2001) which emphasizes fluency by incorporating matches of high n-grams. Furthermore, we consider an F-score measure that is adapted from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al., 2003). This measure matches grammatical dependency relations of parses for system output and reference translations, and thus emphasizes semantic aspects of translational adequacy. As a third measure we consider NIST (Doddington, 2002), which favors lexical choice over word order and does not take structural information into account. On an experimental evaluation on a reranking experiment we found that only NIST was sensitive enough to detect small result differences, whereas BLEU and Fscore produced result differences that were statistically not significant. A second pitfall addressed in this pape"
W05-0908,N04-1023,0,0.019927,"ogether with a each step the features that pass the test with maximultitude of overlapping phrase features. For exam- mum magnitude are added to the model. This prople, for a phrase-table of phrases consisting of max- vides both efficient and accurate estimation with imally 3 words, we allow all 3-word phrases and 2- large feature sets. word phrases as features. Since bigram features can Work on discriminative reranking has been reoverlap, information about trigrams can be gathered ported before by Och and Ney (2002), Och et al. by composing bigram features even if the actual tri- (2004), and Shen et al. (2004). The main purpose of gram is not seen in the training data. our reranking experiments is to have a system that Feature selection makes it possible to employ and can easily be adjusted to yield system variants that evaluate a large number of features, without con- differ at controllable amounts. For quick experimencerns about redundant or irrelevant features hamper- tal turnaround we selected the training and test data ing generalization performance. The `1 regularizer is from sentences with 5 to 15 words, resulting in a defined by the weighted `1 -norm of the parameters training set of 160,00"
W05-0908,zhang-etal-2004-interpreting,0,0.0407538,"Missing"
W05-0908,P02-1040,0,\N,Missing
W05-0908,N04-1021,0,\N,Missing
W05-0908,W04-3223,1,\N,Missing
W12-3153,D09-1030,0,0.0303191,"een pioneered at the 2011 Workshop on Statistical Machine Translation that featured a translation task of Haitian Creole emergency SMS messages4 . This task is very similar to the problem of Twitter translation since SMS contain noisy, abbreviated language. The research papers related to the featured translation task deploy several approaches to domain adaptation, including crowdsourcing (Hu et al., 2011) or extraction of parallel sentences from comparable data (Hewavitharana et al., 2011). The use of crowdsourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and 4 http://www.statmt.org/wmt11/ featured-translation-task.html Callison-Burch (2009). Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillman"
W12-3153,W09-0432,0,0.200856,"mé and Jagarlamudi (2011). Both approaches show improvements by adding new phrase tables; however, both approaches rely on techniques that require larger comparable texts for mining unseen words. Since in our case documents are very short (they consist of 140 character sequences), these techniques are not applicable. However, the advantage of the fact that microblog messages resemble sentences is that we can apply standard word- and phrase-alignment techniques directly to the retrieval results. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation 5 411 http://www.turk.com using self-translations of in-domain source language texts (Ueffing et al., 2007). In our experiments we compare our approach to these domain adaptation techniques. 3 Cross-Lingual Retrieval via Statistical Translation 3.1 Retrieval Model In our approach, comparable candidates for domain adaptation are selected via cross-lingual retrieval. In a probabilistic retrieval framework, we estimate the probability of a relevant document microblog message D given"
W12-3153,P11-2071,0,0.0216355,"s from the retrieval results using a clean bilingual lexicon and an averaging filter. In this approach, filtering and cleaning techniques in alignment and phrase extraction have to compensate for low-quality retrieval results. In our approach, the focus is on high-quality retrieval. As our experimental results show, the main improvement of our technique is a decrease in out-ofvocabulary (OOV) rate at an increase of the percentage of correctly translated unigrams and bigrams. Similar work on solving domain adaptation for SMT by mining unseen words has been presented by Snover et al. (2008) and Daumé and Jagarlamudi (2011). Both approaches show improvements by adding new phrase tables; however, both approaches rely on techniques that require larger comparable texts for mining unseen words. Since in our case documents are very short (they consist of 140 character sequences), these techniques are not applicable. However, the advantage of the fact that microblog messages resemble sentences is that we can apply standard word- and phrase-alignment techniques directly to the retrieval results. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 20"
W12-3153,W04-3208,0,0.0123982,"development sets was pioneered by Callison-Burch (2009) and Zaidan and 4 http://www.statmt.org/wmt11/ featured-translation-task.html Callison-Burch (2009). Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and ming Xu, 2009) and on finding parallel phrases in non-parallel sentences (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010; Vogel and Hewavitharana, 2011). The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. In this approach, filtering and cleaning techniques in alignment and phrase extraction have to compensate fo"
W12-3153,P05-1071,0,0.0349939,"Besides removal of Twitter markup, several additional preprocessing steps such as digit normalization were applied to the data. We also decided to apply the Buckwalter Arabic transliteration scheme13 to avoid encoding difficulties. Habash and Sadat (2006) have shown that tokenization is helpful for translating Arabic. We therefore decided to apply a more involved tokenization scheme than simple whitespace splitting to our data. As the retrieval relies on translation tables, all data need to be tokenized the same way. We are aware of the MADA+TOKAN Arabic morphological analyzer and tokenizer (Habash and Rambow, 2005), however, this toolkit produces very in-depth analyses of the data and thus led to difficulties when we tried to scale it to millions of sentences/microblog messages. That is why we only used MADA for transliteration and chose to implement the simpler approach by Lee et al. (2003) for tokenization. This approach only requires a small set of annotated data to obtain a list of prefixes and suffixes and uses n13 http://www.qamus.org/transliteration. htm R EFERENCE T RANSLATION 1 T RANSLATION 2 T RANSLATION 3 breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous a"
W12-3153,N06-2013,0,0.0121792,"nce, forcing them to complete an entire batch. In order to account for translation variants we decided to use all three translations obtained via Mechanical Turk as multiple references instead of just keeping the top translation. We randomly split our small parallel corpus, using half of the microblog messages for development and half for testing. 4.3 Preprocessing Besides removal of Twitter markup, several additional preprocessing steps such as digit normalization were applied to the data. We also decided to apply the Buckwalter Arabic transliteration scheme13 to avoid encoding difficulties. Habash and Sadat (2006) have shown that tokenization is helpful for translating Arabic. We therefore decided to apply a more involved tokenization scheme than simple whitespace splitting to our data. As the retrieval relies on translation tables, all data need to be tokenized the same way. We are aware of the MADA+TOKAN Arabic morphological analyzer and tokenizer (Habash and Rambow, 2005), however, this toolkit produces very in-depth analyses of the data and thus led to difficulties when we tried to scale it to millions of sentences/microblog messages. That is why we only used MADA for transliteration and chose to i"
W12-3153,W11-2146,0,0.0185912,"echniques. This corpus is used for development and testing in our experiments. 2 Related Work SMT for user-generated noisy data has been pioneered at the 2011 Workshop on Statistical Machine Translation that featured a translation task of Haitian Creole emergency SMS messages4 . This task is very similar to the problem of Twitter translation since SMS contain noisy, abbreviated language. The research papers related to the featured translation task deploy several approaches to domain adaptation, including crowdsourcing (Hu et al., 2011) or extraction of parallel sentences from comparable data (Hewavitharana et al., 2011). The use of crowdsourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and 4 http://www.statmt.org/wmt11/ featured-translation-task.html Callison-Burch (2009). Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on ext"
W12-3153,W11-2148,0,0.0289712,"manual English translations each, which were created using crowdsourcing techniques. This corpus is used for development and testing in our experiments. 2 Related Work SMT for user-generated noisy data has been pioneered at the 2011 Workshop on Statistical Machine Translation that featured a translation task of Haitian Creole emergency SMS messages4 . This task is very similar to the problem of Twitter translation since SMS contain noisy, abbreviated language. The research papers related to the featured translation task deploy several approaches to domain adaptation, including crowdsourcing (Hu et al., 2011) or extraction of parallel sentences from comparable data (Hewavitharana et al., 2011). The use of crowdsourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and 4 http://www.statmt.org/wmt11/ featured-translation-task.html Callison-Burch (2009). Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around"
W12-3153,W07-0733,0,0.040559,"ables; however, both approaches rely on techniques that require larger comparable texts for mining unseen words. Since in our case documents are very short (they consist of 140 character sequences), these techniques are not applicable. However, the advantage of the fact that microblog messages resemble sentences is that we can apply standard word- and phrase-alignment techniques directly to the retrieval results. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation 5 411 http://www.turk.com using self-translations of in-domain source language texts (Ueffing et al., 2007). In our experiments we compare our approach to these domain adaptation techniques. 3 Cross-Lingual Retrieval via Statistical Translation 3.1 Retrieval Model In our approach, comparable candidates for domain adaptation are selected via cross-lingual retrieval. In a probabilistic retrieval framework, we estimate the probability of a relevant document microblog message D given a query microblog message Q, P (D|Q). Following Bayes rule, this can be simplifie"
W12-3153,P07-2045,0,0.0153518,"ea is to crawl a large set of topically related Arabic and English microblogging messages, and use Arabic microblog messages as search queries in a cross-lingual information retrieval (CLIR) setup. We use the probabilistic translation-based retrieval technique of Xu et al. (2001) that naturally integrates translation tables for cross-lingual retrieval. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al., 2007). We investigate several filtering techniques for retrieval and phrase extraction (Munteanu and Marcu, 2006; Snover et al., 2008) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal. Furthermore, we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques. Finally, a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English translations each, which were created using crowdsourcing techniques. This corpus is used"
W12-3153,P03-1051,0,0.00952163,"helpful for translating Arabic. We therefore decided to apply a more involved tokenization scheme than simple whitespace splitting to our data. As the retrieval relies on translation tables, all data need to be tokenized the same way. We are aware of the MADA+TOKAN Arabic morphological analyzer and tokenizer (Habash and Rambow, 2005), however, this toolkit produces very in-depth analyses of the data and thus led to difficulties when we tried to scale it to millions of sentences/microblog messages. That is why we only used MADA for transliteration and chose to implement the simpler approach by Lee et al. (2003) for tokenization. This approach only requires a small set of annotated data to obtain a list of prefixes and suffixes and uses n13 http://www.qamus.org/transliteration. htm R EFERENCE T RANSLATION 1 T RANSLATION 2 T RANSLATION 3 breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers. and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier. breaking the silence is a group of is"
W12-3153,J05-4003,0,0.0565247,"ioneered by Callison-Burch (2009) and Zaidan and 4 http://www.statmt.org/wmt11/ featured-translation-task.html Callison-Burch (2009). Crowdsourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and ming Xu, 2009) and on finding parallel phrases in non-parallel sentences (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010; Vogel and Hewavitharana, 2011). The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. In this approach, filtering and cleaning techniques in alignment and phrase extraction have to compensate for low-quality retrieval re"
W12-3153,P06-1011,0,0.236922,"bic microblog messages as search queries in a cross-lingual information retrieval (CLIR) setup. We use the probabilistic translation-based retrieval technique of Xu et al. (2001) that naturally integrates translation tables for cross-lingual retrieval. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al., 2007). We investigate several filtering techniques for retrieval and phrase extraction (Munteanu and Marcu, 2006; Snover et al., 2008) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal. Furthermore, we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques. Finally, a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English translations each, which were created using crowdsourcing techniques. This corpus is used for development and testing in our experiments. 2 Related Work SMT for user-generated noisy data has been"
W12-3153,P00-1056,0,0.18886,"of the Arab spring were published in parallel in Arabic and in English. The central idea is to crawl a large set of topically related Arabic and English microblogging messages, and use Arabic microblog messages as search queries in a cross-lingual information retrieval (CLIR) setup. We use the probabilistic translation-based retrieval technique of Xu et al. (2001) that naturally integrates translation tables for cross-lingual retrieval. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al., 2007). We investigate several filtering techniques for retrieval and phrase extraction (Munteanu and Marcu, 2006; Snover et al., 2008) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal. Furthermore, we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques. Finally, a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English t"
W12-3153,J04-4002,0,0.0875855,"allel in Arabic and in English. The central idea is to crawl a large set of topically related Arabic and English microblogging messages, and use Arabic microblog messages as search queries in a cross-lingual information retrieval (CLIR) setup. We use the probabilistic translation-based retrieval technique of Xu et al. (2001) that naturally integrates translation tables for cross-lingual retrieval. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al., 2007). We investigate several filtering techniques for retrieval and phrase extraction (Munteanu and Marcu, 2006; Snover et al., 2008) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal. Furthermore, we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques. Finally, a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English translations each, which were created usin"
W12-3153,2001.mtsummit-papers.68,0,0.0243796,"nts We conducted a series of experiments to evaluate our strategy of using CLIR and phrase-extraction to extract comparable data in the Twitter domain. We also explored more standard ways of domain adaptation such as using English microblog messages to build an in-domain language model, or generating synthetic bilingual corpora from monolingual data. All experiments were conducted using the Moses machine translation system15 (Koehn et al., 2007) with standard settings. Language models were built using the SRILM toolkit16 (Stolcke, 2002). For all experiments, we report lowercased BLEU4 scores (Papineni et al., 2001) as calculated by Moses’ multi-bleu script. For assessing significance, we apply the approximate randomization test (Noreen, 1989; Riezler and Maxwell, 2005). We consider pairwise differing results scoring a p-value &lt; 0.05 as significant. Our baseline model was trained using 5,823,363 million parallel sentences in Modern Standard Arabic (MSA) (198,500,436 tokens) and English (193,671,201 tokens) from the NIST evaluation campaign. This data contains parallel text from different domains, including UN reports, newsgroups, newswire, broadcast news and weblogs. 5.1 Domain Adaption using Monolingual"
W12-3153,2007.mtsummit-papers.50,0,0.0183657,"sourcing has its limits when it comes to generating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and ming Xu, 2009) and on finding parallel phrases in non-parallel sentences (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010; Vogel and Hewavitharana, 2011). The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. In this approach, filtering and cleaning techniques in alignment and phrase extraction have to compensate for low-quality retrieval results. In our approach, the focus is on high-quality retrieval. As our experimental results show, the main improvement of our techniq"
W12-3153,W05-0908,1,0.776268,"e also explored more standard ways of domain adaptation such as using English microblog messages to build an in-domain language model, or generating synthetic bilingual corpora from monolingual data. All experiments were conducted using the Moses machine translation system15 (Koehn et al., 2007) with standard settings. Language models were built using the SRILM toolkit16 (Stolcke, 2002). For all experiments, we report lowercased BLEU4 scores (Papineni et al., 2001) as calculated by Moses’ multi-bleu script. For assessing significance, we apply the approximate randomization test (Noreen, 1989; Riezler and Maxwell, 2005). We consider pairwise differing results scoring a p-value &lt; 0.05 as significant. Our baseline model was trained using 5,823,363 million parallel sentences in Modern Standard Arabic (MSA) (198,500,436 tokens) and English (193,671,201 tokens) from the NIST evaluation campaign. This data contains parallel text from different domains, including UN reports, newsgroups, newswire, broadcast news and weblogs. 5.1 Domain Adaption using Monolingual Resources As a first step, we used the available in-domain data for a combination of domain adaptation tech14 The n-gram-model required for tokenization was"
W12-3153,D08-1090,0,0.323284,"search queries in a cross-lingual information retrieval (CLIR) setup. We use the probabilistic translation-based retrieval technique of Xu et al. (2001) that naturally integrates translation tables for cross-lingual retrieval. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al., 2007). We investigate several filtering techniques for retrieval and phrase extraction (Munteanu and Marcu, 2006; Snover et al., 2008) and find a straightforward application of phrase extraction from symmetrized alignments to be optimal. Furthermore, we compare our approach to related domain adaptation techniques for SMT and find our approach to yield large improvements over all related techniques. Finally, a side-product of our research is a corpus of around 1,000 Arabic Twitter messages with 3 manual English translations each, which were created using crowdsourcing techniques. This corpus is used for development and testing in our experiments. 2 Related Work SMT for user-generated noisy data has been pioneered at the 2011"
W12-3153,N09-2024,0,0.0243186,"Missing"
W12-3153,P07-1004,0,0.0373577,"ery short (they consist of 140 character sequences), these techniques are not applicable. However, the advantage of the fact that microblog messages resemble sentences is that we can apply standard word- and phrase-alignment techniques directly to the retrieval results. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation 5 411 http://www.turk.com using self-translations of in-domain source language texts (Ueffing et al., 2007). In our experiments we compare our approach to these domain adaptation techniques. 3 Cross-Lingual Retrieval via Statistical Translation 3.1 Retrieval Model In our approach, comparable candidates for domain adaptation are selected via cross-lingual retrieval. In a probabilistic retrieval framework, we estimate the probability of a relevant document microblog message D given a query microblog message Q, P (D|Q). Following Bayes rule, this can be simplified to ranking documents according to the likelihood P (Q|D) if we assume a uniform prior over documents. score(Q, D) = P (D|Q) = P (D)P (Q|D)"
W12-3153,W11-1209,0,0.0130537,"enerating parallel training data on the scale of millions of parallel sentences. In our work, we use crowdsourcing via Amazon Mechanical Turk5 to create a development and test corpus that includes 3 English translations for each of around 1,000 Arabic microblog messages. There is a substantial amount of previous work on extracting parallel sentences from comparable data such as newswire text (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Tillmann and ming Xu, 2009) and on finding parallel phrases in non-parallel sentences (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010; Vogel and Hewavitharana, 2011). The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. In this approach, filtering and cleaning techniques in alignment and phrase extraction have to compensate for low-quality retrieval results. In our approach, the focus is on high-quality retrieval. As our experimental results show, the main improvement of our technique is a decrease in out-ofvocabulary (OOV) rate at an"
W12-3153,D09-1006,0,0.0339094,"Missing"
W12-3153,P11-2007,0,0.211434,"oving retweets - compared to English users, a smaller number of Arabic users produced a much larger number of retweets. Interestingly, 56,087 users tweet a substantial amount in both languages. This suggests that users spread messages simultaneously in Arabic and English. 4.2 Creating a Small Parallel Twitter Corpus using Crowdsourcing For the evaluation of our method, a small amount of parallel in-domain data was required. Since there are no corpora of translated microblog messages, we decided to use Amazon Mechanical Turk12 to create our own evaluation set, following the exploratory work of Zaidan and Callison-Burch (2011b). We randomly selected 2,000 Arabic microblog messages. Hashtags, user mentions and URLs were removed from each microblog message beforehand, because they do not need to be translated and would just artificially inflate scores at test time. The microblog messages were then manually cleaned and pruned. We discarded messages which contained almost no text or large portions of other languages and removed remaining Twitter markup. In the end, 1,022 microblog messages were used in the Mechanical Turk task. We split the data into batches of ten sentences which comprised one HIT (human intelligence"
W12-3153,P11-1122,0,0.185195,"oving retweets - compared to English users, a smaller number of Arabic users produced a much larger number of retweets. Interestingly, 56,087 users tweet a substantial amount in both languages. This suggests that users spread messages simultaneously in Arabic and English. 4.2 Creating a Small Parallel Twitter Corpus using Crowdsourcing For the evaluation of our method, a small amount of parallel in-domain data was required. Since there are no corpora of translated microblog messages, we decided to use Amazon Mechanical Turk12 to create our own evaluation set, following the exploratory work of Zaidan and Callison-Burch (2011b). We randomly selected 2,000 Arabic microblog messages. Hashtags, user mentions and URLs were removed from each microblog message beforehand, because they do not need to be translated and would just artificially inflate scores at test time. The microblog messages were then manually cleaned and pruned. We discarded messages which contained almost no text or large portions of other languages and removed remaining Twitter markup. In the end, 1,022 microblog messages were used in the Mechanical Turk task. We split the data into batches of ten sentences which comprised one HIT (human intelligence"
W12-3153,C04-1059,0,0.0216412,"n campaign. This data had previously been tokenized with the same method, trained to match the Penn Arabic Treebank, v3. 15 http://statmt.org/moses/ 16 http://www.speech.sri.com/projects/ srilm/ 415 niques similar to Bertoldi and Federico (2009). There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding metaparameters, as introduced by Koehn and Schroeder (2007). Second, the English microblog messages in our crawl were used to build an in-domain language model. This adaptation technique was first proposed by Zhao et al. (2004). Third, the Arabic portion of our crawl was used to synthetically generate additional parallel training data. This was accomplished by machine-translating the Arabic microblog messages with the best system after performing the first two adaptation steps. Since decoding is very timeintensive, only 1 million randomly selected Arabic microblog messages were used to generate synthetic parallel data. This new data was then used to train another phrase table. Such self-translation techniques have been introduced by Ueffing et al. (2007). All results were evaluated against a baseline of using only N"
W12-3153,P02-1040,0,\N,Missing
W12-3153,2010.iwslt-papers.3,0,\N,Missing
W13-2236,2011.eamt-1.5,0,0.162404,"Missing"
W13-2236,J07-2003,0,0.0468694,"sting, we use data extracted from the PatTR3 corpus for the experiments in W¨aschle and Riezler (2012b). Training data consists of about 1.2 million German-English parallel sentences. We translate from German into English. German compound words were split using the technique of Koehn and Knight (2003). We use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). We built a modified KneserNey smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)7 . The International Patent Classification (IPC) categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, down to 70,000 subgroups at the leaf level. The eight top classes (called sections) are listed in Table 1. Typically, a patent belongs to more than one section, with one section chosen as main classification. Our development and test sets for each of the classes, A to H, comprise 2,000 sentences each, originati"
W13-2236,P11-2031,0,0.0264791,"o optimize the weights of 12 dense default features; eight translation model features, a word penalty, the passthrough weight, the language model (LM) score, and an LM out-of-vocabulary penalty. Perceptron training allows to add millions of sparse features which are directly derived from grammar rules: rule shape, rule identifier, bigrams in rule source and target. For a further explanation of these features see Simianer et al. (2012). For testing we measured IBM BLEU4 on tokenized and lowercased data. Significance results were obtained by approximate randomization tests using the approach of Clark et al. (2011)9 to account for optimizer instability. Tuning methods with a random component (MERT, randomized experiments) were repeated three times, scores reported in the tables are averaged over optimizer runs. Experimental Results In single-task tuning mode, systems are tuned on the eight independent data sets separately, the pooled data set, and the independent data sets con9 pooled 1 pooled test Table 2: BLEU4 results of MERT baseline using dense features for three different tuning sets: independent (separate tuning sets for each IPC class), pooled and pooled-cat (concatenated independent sets). Sign"
W13-2236,P07-1033,0,0.123408,"Missing"
W13-2236,W10-1757,0,0.429079,"eatures that are most important across all tasks are kept in the model. Early research on multi-task learning for SMT has investigated pooling of IPC sections, with larger pools improving results (Utiyama and Isahara, 2007; Tinsley et al., 2010; Ceaus¸u et al., 2011). W¨aschle and Riezler (2012b) apply multitask learning to tasks defined as IPC sections and compare patent translation on independent tasks, pooled tasks, and multi-task learning, using samesized training data. They show small but statistically significant improvements for multi-task learning over independent and pooled training. Duh et al. (2010) introduce random tasks as n-best lists of translations and showed significant improvements by applying various multi-task learning techniques to discriminative reranking. Song et al. (2011) define tasks as bootstrap samples from the development set and show significant improvements for a bagging-based system combina3 Multi-task Learning for Discriminative Training in SMT In multi-task learning, we have data points {(xiz , yzi ), i = 1, . . . , Nz , z = 1, . . . , Z}, sampled from a distribution Pz on X × Y . The subscript z indexes tasks and the superscript i indexes i.i.d. data for each task"
W13-2236,P10-4002,0,0.0585662,"man Necessities Performing Operations, Transporting Chemistry, Metallurgy Textiles, Paper Fixed Constructions Mechanical Engineering, Lighting, Heating, Weapons Physics Electricity Table 1: IPC top level sections. Data & System Setup For training, development and testing, we use data extracted from the PatTR3 corpus for the experiments in W¨aschle and Riezler (2012b). Training data consists of about 1.2 million German-English parallel sentences. We translate from German into English. German compound words were split using the technique of Koehn and Knight (2003). We use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). We built a modified KneserNey smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)7 . The International Patent Classification (IPC) categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, down to 70,000 subgroups at the leaf level. The eig"
W13-2236,P02-1040,0,0.0920986,"a patent with the respective class. These sets were built so that there is no overlap of development sets and test sets, and no overlap between sets of different classes. These eight test sets are referred to as independent test sets. Furthermore, we test on a combined set, called pooled-cat, that is constructed by concatenating the independent sets. Additionally we use two pooled sets for development and testing, each containing 2,000 sentences with all classes evenly represented. Our tuning baseline is an implementation of hypergraph MERT (Kumar et al., 2009), directly optimizing IBM BLEU4 (Papineni et al., 2002). Furthermore, we present a regularization baseline by applying `1 regularization with clipping (Carpenter, 2008; Tsuruoka et al., 2009) to the standard pairwise ranking perceptron. All pairwise ranking methods use a smoothed sentence-wise BLEU+1 score (Nakov et al., 2012) to create gold standard rankings. Our multi-task learning experiments are based on pairwise ranking perceptrons that differ in their objective, corresponding either to the original perceptron or to the margin-perceptron. Both versions of the perceptron are used for single-task tuning and multi-task tuning. In the multi-task"
W13-2236,N09-1068,0,0.0484708,"r multitask learning improves by nearly 2 BLEU points over the standard MERT baseline. 2 Related Work Multi-task learning is an active area in machine learning, dating back at least to Caruana (1997). A regularization perspective was introduced by Evgeniou and Pontil (2004), who formalize the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models in an SVM framework. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., `1 /`2 regularization (Obozinski et al., 2010) or `1 /`∞ regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. Early resea"
W13-2236,W11-2123,0,0.0174659,"-English parallel sentences. We translate from German into English. German compound words were split using the technique of Koehn and Knight (2003). We use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). We built a modified KneserNey smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)7 . The International Patent Classification (IPC) categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, down to 70,000 subgroups at the leaf level. The eight top classes (called sections) are listed in Table 1. Typically, a patent belongs to more than one section, with one section chosen as main classification. Our development and test sets for each of the classes, A to H, comprise 2,000 sentences each, originating from a patent with the respective class. These sets were built so that there is no overlap of development sets and test sets, and no overlap between sets"
W13-2236,E03-1076,0,0.0902702,"nd effective learning machine. 4 G H Experiments 4.1 Human Necessities Performing Operations, Transporting Chemistry, Metallurgy Textiles, Paper Fixed Constructions Mechanical Engineering, Lighting, Heating, Weapons Physics Electricity Table 1: IPC top level sections. Data & System Setup For training, development and testing, we use data extracted from the PatTR3 corpus for the experiments in W¨aschle and Riezler (2012b). Training data consists of about 1.2 million German-English parallel sentences. We translate from German into English. German compound words were split using the technique of Koehn and Knight (2003). We use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). We built a modified KneserNey smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)7 . The International Patent Classification (IPC) categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, d"
W13-2236,P12-1002,1,0.621304,"improves learning results over learning independent or pooled models. The research question we ask in this paper is as follows: Is multi-task learning dependent on a “natural” task structure in the data, where shared and individual knowledge is properly balanced? Or can multi-task learning be seen as a general regularization technique that prevents overfitting irrespective of the task structure in the data? We investigate this research question on the example of discriminative training for patent translation, using the algorithm for multi-task learning with `1 /`2 regularization presented by Simianer et al. (2012). We compare multi-task learning on “natural” tasks given by IPC sections to multitask learning on “random” tasks given by random shards and to baseline models trained on independent tasks and pooled tasks. We find that both versions of multi-task learning improve over independent or pooled training. However, differences between multi-task learning on IPC tasks and random tasks are small. This points to a more general regularization effect of multi-task learning and indicates a broad applicability of multi-task learning techniques. Another advantage of the `1 /`2 regMulti-task learning has bee"
W13-2236,P09-1019,0,0.0128733,"H, comprise 2,000 sentences each, originating from a patent with the respective class. These sets were built so that there is no overlap of development sets and test sets, and no overlap between sets of different classes. These eight test sets are referred to as independent test sets. Furthermore, we test on a combined set, called pooled-cat, that is constructed by concatenating the independent sets. Additionally we use two pooled sets for development and testing, each containing 2,000 sentences with all classes evenly represented. Our tuning baseline is an implementation of hypergraph MERT (Kumar et al., 2009), directly optimizing IBM BLEU4 (Papineni et al., 2002). Furthermore, we present a regularization baseline by applying `1 regularization with clipping (Carpenter, 2008; Tsuruoka et al., 2009) to the standard pairwise ranking perceptron. All pairwise ranking methods use a smoothed sentence-wise BLEU+1 score (Nakov et al., 2012) to create gold standard rankings. Our multi-task learning experiments are based on pairwise ranking perceptrons that differ in their objective, corresponding either to the original perceptron or to the margin-perceptron. Both versions of the perceptron are used for singl"
W13-2236,2011.mtsummit-papers.33,0,0.603002,"Missing"
W13-2236,D07-1104,0,0.0421731,"nical Engineering, Lighting, Heating, Weapons Physics Electricity Table 1: IPC top level sections. Data & System Setup For training, development and testing, we use data extracted from the PatTR3 corpus for the experiments in W¨aschle and Riezler (2012b). Training data consists of about 1.2 million German-English parallel sentences. We translate from German into English. German compound words were split using the technique of Koehn and Knight (2003). We use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6 . SCFG models use the same settings as described in Chiang (2007). We built a modified KneserNey smoothed 5-gram language model using the English side of the training data and performed querying with KenLM (Heafield, 2011)7 . The International Patent Classification (IPC) categorizes patents hierarchically into 8 sections, 120 classes, 600 subclasses, down to 70,000 subgroups at the leaf level. The eight top classes (called sections) are listed in Table 1. Typically, a patent belongs to more than one section,"
W13-2236,2010.amta-commercial.14,0,0.0302359,"Missing"
W13-2236,D11-1139,0,0.0431377,"Missing"
W13-2236,P09-1054,0,0.0519392,"between sets of different classes. These eight test sets are referred to as independent test sets. Furthermore, we test on a combined set, called pooled-cat, that is constructed by concatenating the independent sets. Additionally we use two pooled sets for development and testing, each containing 2,000 sentences with all classes evenly represented. Our tuning baseline is an implementation of hypergraph MERT (Kumar et al., 2009), directly optimizing IBM BLEU4 (Papineni et al., 2002). Furthermore, we present a regularization baseline by applying `1 regularization with clipping (Carpenter, 2008; Tsuruoka et al., 2009) to the standard pairwise ranking perceptron. All pairwise ranking methods use a smoothed sentence-wise BLEU+1 score (Nakov et al., 2012) to create gold standard rankings. Our multi-task learning experiments are based on pairwise ranking perceptrons that differ in their objective, corresponding either to the original perceptron or to the margin-perceptron. Both versions of the perceptron are used for single-task tuning and multi-task tuning. In the multi-task setting, we compare three different methods for defining a task: “natural” tasks given by IPC sections where each independent data set i"
W13-2236,2007.mtsummit-papers.63,0,0.0435109,"arning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., `1 /`2 regularization (Obozinski et al., 2010) or `1 /`∞ regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. Early research on multi-task learning for SMT has investigated pooling of IPC sections, with larger pools improving results (Utiyama and Isahara, 2007; Tinsley et al., 2010; Ceaus¸u et al., 2011). W¨aschle and Riezler (2012b) apply multitask learning to tasks defined as IPC sections and compare patent translation on independent tasks, pooled tasks, and multi-task learning, using samesized training data. They show small but statistically significant improvements for multi-task learning over independent and pooled training. Duh et al. (2010) introduce random tasks as n-best lists of translations and showed significant improvements by applying various multi-task learning techniques to discriminative reranking. Song et al. (2011) define tasks a"
W13-2236,N10-1069,0,0.0326516,"tire groups of features jointly. For example, such groups can be defined as non-overlapping subsets of features (Yuan and Lin, 2006), or as hierarchical groups of features (Zhao et al., 2009), or they can be grouped by the general structure of the prediction problem (Martins et al., 2011). However, these approaches are concerned with grouping features within a single prediction problem whereas multitask learning adds an orthogonal layer of multiple task-specific prediction problems. By virtue of averaging selected weights after each epoch, the algorithm of Simianer et al. (2012) is related to McDonald et al. (2010)’s iterative mixing procedure. This algorithm is itself related to the bagging procedure of Breiman (1996), if random shards are considered from the perspective of random samples. In both cases averaging helps to reduce the variance of the per-sample classifiers. ularization technique of Simianer et al. (2012) is a considerable efficiency gain due to parallelization and iterative feature selection that makes the algorithm suitable for big data applications and for large-scale training with millions of sparse features. Last but not least, our best result for multitask learning improves by nearl"
W13-2236,C12-1121,0,0.0465186,"Missing"
W13-2236,E12-1083,1,0.907271,"Missing"
W15-3037,N13-1090,0,0.523341,"d classification task. Furthermore, most approaches rely on manually designed features, including source and target contexts, alignments, and generalizations by linguistic categories (POS, syntactic dependency links, WordNet senses) as reported by Bojar et al. (2014), similar to the 25 feature templates provided by the organizers. We apply the framework of Collobert et al. (2011) to learn bilingual correspondences “from scratch”, i.e. from raw input words. To this aim, a continuous space deep neural network is pre-trained by initializing the lookup-table with distributed word representations (Mikolov et al., 2013b), and fine-tuned for the QE classification task by back-propagating word-level prediction errors using stochastic gradient descent (Rumelhart et al., 1986). Moreover, we train a linear combination of the manually defined baseline features provided by the task organizers. A combination of the orthogonal information based on the continuous space features and the manually chosen baseline features shows significant improvements over the combined systems, and produces very competitive F1 scores for predicting word-level translation quality. This paper describes the system submitted by the Univers"
W15-3037,P10-4002,0,0.0138789,"sh to Spanish translations (en-es) but also German to English (de-en) and vice versa. Since the plain QUETCH system does not rely on language-specific features, we simply use the same deep learning architecture for all of these language pairs. QUETCH is trained on the WMT14 training set, with a source and target window size of 3, a lookup-table dimensionality of 10, 300 hidden units, and a constant learning rate of 0.001. Test and training data were lowercased. The alignments used for positioning the target window as described in Section 3.1 were created with fast align from the cdec toolkit (Dyer et al., 2010). The collection of corpora provided with WMT13’s translation task3 is utilized as source for unsupervised pre-training: Europarl v7 (Koehn, 2005), Common Crawl corpus, and News Commentary. Note that we did not use these corpora because of their parallel structure, but because they are large, multilingual, and are commonly used in WMT submissions. Following the WMT14 evaluation (Bojar et al., 2014), we report on accuracy and BAD F1 -score, the latter being the task’s primary evaluation metric. The WMT14 baselines trivially predict either only BAD or only OK labels. Table 1 presents the best F1"
W15-3037,D14-1162,0,0.0818468,"grams by vectors in continuous space, allowing similar words to have similar representations, and avoiding data sparsity issues. These advantages have been demonstrated experimentally by showcasing meaningful structure in vector space 1 A factor of 4.22 on WMT15 train, and 4.21 on WMT15 dev, as opposed to 1.84 for WMT14 train and 1.81 for WMT14 test for the same language pair. 316 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 316–322, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. representations (Mikolov et al. (2013c), Pennington et al. (2014) inter alia), or by producing stateof-the-art performance in applications such as language modeling (Bengio et al. (2003), Mikolov et al. (2010), inter alia) or statistical machine translation (Kalchbrenner and Blunsom (2013), Bahdanau et al. (2015), inter alia). The property that makes these models most attractive for various applications is the ability to learn continuous space representations “from scratch”(Collobert et al., 2011), and to infuse the representation with non-linearity. The deep layers of the neural network capture these representations – even a single hidden layer is sufficie"
W15-3037,reese-etal-2010-wikicorpus,0,0.0104822,"consistent across languages proves that QUETCH is capable of language-independent quality estimation. Table 2: Winning submissions of the WMT14 Quality Estimation Task 2 (Bojar et al., 2014). 5.2 WMT15 With the insights from the experiments on the WMT14 data we proceed to the experiments on the WMT15 en-es data. We introduce a weight w for BAD training samples, such that QUETCH is trained on each BAD sample w times. In this way, we easily counterbalance the skewed distribution of labels, without modifying the classifier’s loss function. Also, we utilize the larger and nonparallel Wikicorpus (Reese et al., 2010) in English configuration BAD F1 Accuracy (v) (a) (p) (a), (p) † (a), (p), (w) (a), (p), (w) ‡ (a), (p), (w) 0.2535 0.2628 0.2535 0.2793 0.3527 0.3876 0.2985 0.7104 0.7099 0.7668 0.7716 0.7508 0.6031 0.7888 ‡ VW 0.4084 0.7335 † QUETCH+ 0.4305 0.6977 Table 3: QUETCH results on en-es WMT15 task 2 test data under different configuration setting: (v)anilla model vs. models using (p)re-training, (a)lignments from an SMTSystem, and (w)eighting of the BAD-instances. Submitted systems are preceded by † , components of the final QUETCH+ system are marked with ‡ . Although proceeding in the same manner"
W15-3037,W11-2117,0,0.0153826,"yer. Since the lookup-table representations of words are updated during training, QUETCH learns representations of words in bilingual contexts that are optimized for QE. 3.2 25 features for each target word in the context window, the input to the first linear layer would grow by 25 ∗ |wintgt |∗ dwrd dimensions. For these reasons, we decided to design a system combination that treats the QUETCH system and the baseline features individually and independently. For many complex applications, system combination has proven to be effective strategy to boost performance. In machine translation tasks, Heafield and Lavie (2011) and Karakos et al. (2008), inter alia, increased overall performance by cleverly combining the outputs of several MT systems. In cross-lingual information retrieval, Schamoni and Riezler (2015) empirically showed that it is more beneficial to combine systems that are most dissimilar than those that have highest single scores. Our approach is to train separate systems, one based on the deep learning approach described in Section 3, and one based solely on the baseline features provided for the shared task. In a final step, we combine both systems together with binarized versions of selected ba"
W15-3037,D13-1176,0,0.0182945,"in vector space 1 A factor of 4.22 on WMT15 train, and 4.21 on WMT15 dev, as opposed to 1.84 for WMT14 train and 1.81 for WMT14 test for the same language pair. 316 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 316–322, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. representations (Mikolov et al. (2013c), Pennington et al. (2014) inter alia), or by producing stateof-the-art performance in applications such as language modeling (Bengio et al. (2003), Mikolov et al. (2010), inter alia) or statistical machine translation (Kalchbrenner and Blunsom (2013), Bahdanau et al. (2015), inter alia). The property that makes these models most attractive for various applications is the ability to learn continuous space representations “from scratch”(Collobert et al., 2011), and to infuse the representation with non-linearity. The deep layers of the neural network capture these representations – even a single hidden layer is sufficient (Hornik et al., 1989). We present an approach to address the challenges of word-level translation quality estimation by learning these continuous space bilingual representations instead of relying on manual feature enginee"
W15-3037,P08-2021,0,0.0117384,"presentations of words are updated during training, QUETCH learns representations of words in bilingual contexts that are optimized for QE. 3.2 25 features for each target word in the context window, the input to the first linear layer would grow by 25 ∗ |wintgt |∗ dwrd dimensions. For these reasons, we decided to design a system combination that treats the QUETCH system and the baseline features individually and independently. For many complex applications, system combination has proven to be effective strategy to boost performance. In machine translation tasks, Heafield and Lavie (2011) and Karakos et al. (2008), inter alia, increased overall performance by cleverly combining the outputs of several MT systems. In cross-lingual information retrieval, Schamoni and Riezler (2015) empirically showed that it is more beneficial to combine systems that are most dissimilar than those that have highest single scores. Our approach is to train separate systems, one based on the deep learning approach described in Section 3, and one based solely on the baseline features provided for the shared task. In a final step, we combine both systems together with binarized versions of selected baseline features. From this"
W15-3037,P14-2080,1,0.808383,"up-tables for each feature class (Collobert et al., 2011). While this certainly works for wordsimilar features like POS-tags, this is not suitable for continuous numerical features. Preliminary tests of extending QUETCH with a lookup-table for POS-tags did not result in better F1 scores. Also, training took considerably longer, because of (1) the additional lookup-table to train and (2) the larger dimensionality of the vector representing a target word with its context. If we added all > f (p, q) = p W q = Q P X X pi Wij qj , i=1 j=1 where W ∈ RP ×Q encodes a feature matrix (Bai et al., 2010; Schamoni et al., 2014). The value of f (·, ·) is the prediction of the classifier given a target vector p and a vector of related features q. To address the problem of data sparsity, we reduced the number of possible feature pairs by restricting the feature expansion to two groups: (1) target words are combined with target context 2 All words are indexed within a vocabulary V . The vocabulary contains the entire training, development and test data of the QE task and is realized as a gensim dictionary ˇ uˇrek and Sojka, 2010). (Reh˚ 318 5 words and source aligned words, and (2) target POS tags are combined with sour"
W15-3037,2005.mtsummit-papers.11,0,0.016471,"eatures, we simply use the same deep learning architecture for all of these language pairs. QUETCH is trained on the WMT14 training set, with a source and target window size of 3, a lookup-table dimensionality of 10, 300 hidden units, and a constant learning rate of 0.001. Test and training data were lowercased. The alignments used for positioning the target window as described in Section 3.1 were created with fast align from the cdec toolkit (Dyer et al., 2010). The collection of corpora provided with WMT13’s translation task3 is utilized as source for unsupervised pre-training: Europarl v7 (Koehn, 2005), Common Crawl corpus, and News Commentary. Note that we did not use these corpora because of their parallel structure, but because they are large, multilingual, and are commonly used in WMT submissions. Following the WMT14 evaluation (Bojar et al., 2014), we report on accuracy and BAD F1 -score, the latter being the task’s primary evaluation metric. The WMT14 baselines trivially predict either only BAD or only OK labels. Table 1 presents the best F1 -scores during training and the according accuracies for QUETCH under different configurations. The plain QUETCH system yields an acceptable accu"
W15-3037,2006.amta-papers.25,0,0.162445,"015 Workshop on Statistical Machine Translation (WMT15). The task consists of predicting the word-level quality level (“OK”/“BAD”) of English-to-Spanish machine translations, without the use of human references, and without insight into the translation derivations, that is, by treating the Machine Translation (MT) system that produced the translations as a black box. The task organizers provided training and development data comprising tokenized MT outputs that were automatically annotated for errors as edit operations (replacements, insertions, or deletions) with respect to human post-edits (Snover et al., 2006). Furthermore, a set of 25 baseline features that operate on source and target translation, but do not use features of the SMT pipeline that produced the translations, was provided. Even though the distribution of binary labels is skewed towards 2 Deep Learning for Quality Estimation Continuous space neural network models are credited with the advantage of superior modeling power by replacing discrete units such as words or n-grams by vectors in continuous space, allowing similar words to have similar representations, and avoiding data sparsity issues. These advantages have been demonstrated e"
W15-3037,W14-3302,0,\N,Missing
W15-4922,E14-1022,0,0.031671,"Missing"
W15-4922,J07-2003,0,0.11329,"rce side si,j of the TM pair, and returning its target side ti,j . 2 ti,best = CLIR(qi , ti,j ) = max p∈Hg(qi ) � e∈p wSMT ·φSMT (e(qi )) + wn-gr · φn-gr (e(qi ), ti,j )) where p is a path through the hypergraph, e the set of edges on the path, φ are feature values of an edge, w the corresponding weights, and · denotes the vector dot product. We explore two different ways to incorporate n-gram features φn-gr in addition to the SMT feature set φSMT . Unigram oracle. Since n-gram features are nonlocal and the size of the hypergraph grows when adding n-gram features for orders higher than n = 1 (Chiang, 2007), we restrict our ﬁrst model to unigram precision and a brevity penalty feature; the latter is only active at goal state. In this way, two additional features are inserted into the log-linear model, using the TM match candidate as an oracle. possible translations, but found that using the 1-best translation prediction of the baseline system yielded superior results. 171 Additional language model. To be able to include higher-order n-gram matches, we add the match candidates as an additional language model to the decoder.This approach makes use of the fact that cdec handles the extension of the"
W15-4922,P11-2031,0,0.25315,"Missing"
W15-4922,C14-1192,0,0.13585,"of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integrated model uses a coarse-to-ﬁne approach for integrating TM information into an SMT system: First, efﬁcient retrieval is done using locality-sensitive hashing on large corpora. Second, a more ﬁne-grained search for the best match is performed for a given sentence. Lastly, a reranking step uses this info"
W15-4922,N13-1073,0,0.0791992,"Missing"
W15-4922,P10-4002,0,0.064898,"irect translation baseline in cross-language information retrieval. In addition to this simple model, we explore two methods that operate on the full translation hypergraph of the query. Both techniques are similar to the translation retrieval technique presented by Dong et al. (2014). They perform Viterbi search on a translation lattice of the input sentence that is enriched, besides the default SMT features, with n-gram features that indicate the overlap status between the current state in the lattice and a given TM match. We adopt this approach for the hypergraph built by the cdec decoder (Dyer et al., 2010). As a cross-lingual similarity measure we then compute the Viterbi score on the query hypergraph Hg(qi ) for each match candidate ti,j , i.e. Fine-grained matching In the standard bilingual case, choosing the best TM match amounts to selecting the sentence pair (s, t) from the coarse candidate set LSH(qi ) that achieves the highest fuzzy match score FMS of the (source) query qi against the source side si,j of the TM pair, and returning its target side ti,j . 2 ti,best = CLIR(qi , ti,j ) = max p∈Hg(qi ) � e∈p wSMT ·φSMT (e(qi )) + wn-gr · φn-gr (e(qi ), ti,j )) where p is a path through the hy"
W15-4922,P10-1064,0,0.055631,"Missing"
W15-4922,C10-2043,0,0.0254533,"Missing"
W15-4922,D11-1125,0,0.147865,"erior results. 171 Additional language model. To be able to include higher-order n-gram matches, we add the match candidates as an additional language model to the decoder.This approach makes use of the fact that cdec handles the extension of the hypergraph to accommodate for the non-local higher order ngrams. Cube pruning (Chiang, 2007) is used to make the search feasible. In both cases, we keep the weights of the SMT features ﬁxed, which have been optimized for translation performance on a development set, and only adjust the additional weights in relation. This is done by pairwise ranking (Hopkins and May, 2011). The gold standard ranking of the TM candidates is given by FMS(ti,j , ri ) with respect to the reference ri for qi . The learning goal is to adjust the weights of the n-gram features so as to rank the TM match highest that has the smallest distance to the reference. Note, that we do not optimize the translation performance of the derivation, which corresponds to the Viterbi path. This could potentially replace the re-ranking step and we plan to explore this option in the future. 3.3 on the n-best list of SMT outputs by TER match against the reference. domain acquis (en-fr) oo3 (en-zh) ntcir"
W15-4922,2010.jec-1.4,0,0.0190744,"only be available in the target language. We show consistent and signiﬁcant improvements on different domains (IT, legal, patents) for different language pairs (including Chinese, Japanese, English, French, and German), achieving results compara169 ble to or better than using a target-language reference of source-side matches. 2 Related Work Work on integrating MT and SMT can be divided into approaches at the sentence level that decide whether to pass SMT or TM output to the user (He et al., 2010a,b), and approaches that merge both techniques at a sub-sentential level (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Wang et al., 2013). While the goal of the former is to improve human translation effort in a CAT environment, the second line of research aims to improve SMT performance. Bic¸ici and Dymetman (2008) were among the ﬁrst to propose a combined system. They start by identifying matching subsequences between the current sentence and a fuzzy match retrieved from a translation memory. Source and target of the match together with the corresponding alignment are used to construct a non-contiguous bi-phrase, which is added to the SMT grammar with a strong weight. The de"
W15-4922,2014.amta-researchers.19,0,0.0175267,"the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integrated model uses a coa"
W15-4922,P11-1124,0,0.0351753,"Missing"
W15-4922,P02-1040,0,0.0950671,"Missing"
W15-4922,tiedemann-2012-parallel,0,0.0698167,"Missing"
W15-4922,I05-3027,0,0.044667,"Missing"
W15-4922,2007.mtsummit-papers.63,0,0.103123,"Missing"
W15-4922,P13-1002,0,0.0602926,"nt and signiﬁcant improvements on different domains (IT, legal, patents) for different language pairs (including Chinese, Japanese, English, French, and German), achieving results compara169 ble to or better than using a target-language reference of source-side matches. 2 Related Work Work on integrating MT and SMT can be divided into approaches at the sentence level that decide whether to pass SMT or TM output to the user (He et al., 2010a,b), and approaches that merge both techniques at a sub-sentential level (Smith and Clark, 2009; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Wang et al., 2013). While the goal of the former is to improve human translation effort in a CAT environment, the second line of research aims to improve SMT performance. Bic¸ici and Dymetman (2008) were among the ﬁrst to propose a combined system. They start by identifying matching subsequences between the current sentence and a fuzzy match retrieved from a translation memory. Source and target of the match together with the corresponding alignment are used to construct a non-contiguous bi-phrase, which is added to the SMT grammar with a strong weight. The decoder is then run as usual using the augmented gramm"
W15-4922,2009.mtsummit-papers.14,0,0.0420021,"rt (2010), Zhechev and van Genabith (2010), and Ma et al. (2011), force the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integr"
W15-4922,2006.amta-papers.25,0,0.0679347,"Missing"
W15-4922,steinberger-etal-2006-jrc,0,0.117089,"Missing"
W15-4922,C14-1039,0,0.0258691,"Ma et al. (2011), force the SMT system to translate only the unmatching segments of the source, either by restricting translation or by adding a very high feature weight to rules or biphrases extracted from the TM match. While all presented approaches make use of the alignment between source and target of the fuzzy match, our approach uses only the target side to restrict the translation, making it possible to use matches that can be found in a target-only corpus. The use of TM matches to generate additional features for SMT has been explored by Simard and Isabelle (2009), Wang et al. (2013), Wang et al. (2014) and Li et al. (2014). Our re-ranking approach is very similar, with the novelty of using not only matches found by querying the source side of the corpus, but also the target. The idea of directly searching for translations in a monolingual target language corpus has been explored by Dong et al. (2014). They retrieve target side translation candidates using a lattice representation of possible translations of a source sentence. The system is successfully applied to the task of identifying parallel sentences, but no SMT experiments are reported. 3 Integrating monolingual TM into SMT Our integr"
W15-4922,D07-1080,0,0.0325862,"00, 0x9FFF]. 6 https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 7 http://www.cl.uni-heidelberg.de/ statnlpgroup/pattr/ 8 http://research.nii.ac.jp/ntcir/ ntcir-10/ 9 We merged, shufﬂed and again split up the data into three sets, to generate a devtest set. 5 genre parallel train (en-de) dev/devtest/test (en-de) LM-train (de) TM (de) cl. desc. cl.+descr. cl.+desc. size (sent.) 6M 1K (each) 16.2M 16.2M Table 4: Data for domain adaptation scenario. trained with SRILM (Stolcke, 2002) on the target side of the training data. The weights of the loglinear model were optimized with MIRA (Watanabe et al., 2007) on a held-out development set reserved for this purpose (dev). We employed the baseline model to produce query translations and hypergraphs for the cross-lingual retrieval of target matches as well as to produce 500-best lists, which we re-ranked according to our model given the best match found after ﬁne-grained retrieval. Retrieval and re-ranking parameters were optimized on an additional held-out (devtest) set. All presented results were obtained on a third (test) data set. To compare source and different target retrieval methods in a fair setting, we used the bilingual data from training"
W15-4922,W10-3806,0,0.0377949,"Missing"
W15-4922,2014.amta-researchers.13,0,\N,Missing
W17-4756,D11-1033,0,0.0575876,"ue (running average of rewards) that has been previously found to improve both variance and generalization for NMT bandit training (Kreutzer et al., 2017). exp(owv /T ) 519 5.3 Domain adaptation and reinforcement learning based on NMT (University of Maryland). model ‘translate’ by copying source NMT SMT UMD-domain-adaptation. The UMD team’s systems were based on an attention-based encoder-decoder translation model. The models use the BPE technique for subword encoding, which helps addressing the rare word problem and enlarges vocabulary. A further addition is the domain adaptation approach of Axelrod et al. (2011) to select training data after receiving in-domain source-side data and selecting the most similar out-of-domain data from the WMT 2016 training set for re-training. UMD-reinforce. Another type of models submitted by UMD uses reinforcement learning techniques to learn from feedback and improve the update of the translation model to optimize the reward, based on Bahdanau et al. (2016) and Ranzato et al. (2016). 64,481.8 SMT-oracle SMT-static 499,578.0 229,621.7 SMT-EL-CV-ADADELTA SMT-EL-CV-ADAM SMT-SZO-CV-ADAM 214,398.8 225,535.3 208,464.7 BNMT-oracle BNMT-static WMT16-static 780,580.4 222,066."
W17-4756,N13-1073,0,0.0463397,"elihood objective, Adam (α = 1 × 10−4 , β1 = 0.9, β2 = 0.999) and dropout (Srivastava et al., 2014) with probability 0.2. Bandit learning starts from this pretrained model and continues with stochastic gradient descent (initial learning rate γ0 = 1 × 10−5 , annealing starts at kST ART = 700, 000, dropout with probability 0.5, gradient norm clipping when the norm exceeds 1.0 (Pascanu et al., 2013)), where the model was updated as soon as a feedback is received. As described above, UNK replacement was applied to the output on the basis of an IBM2 lexical translation model built with fast align (Dyer et al., 2013) on out-of-domain training data. If the aligned source word for a generated UNK token is not in the dictionary of the lexical translation model, the UNK token was simply replaced by the source word. SMT-SZO-CV-ADAM. As a novel contribution, we adapted the two-point stochastic zerothorder approach by (Sokolov et al., 2015) that required two quality evaluation per iteration to a one-point feedback scenario. In a nutshell, on each step of the SZO algorithm, the model parameters w are perturbed with an additive standard Gaussian noise , and the Viterbi translation is sent to the service. Such alg"
W17-4756,P10-4002,0,0.0323442,") allows for a representation within the vocabulary. We generate a BNMT system using a BPE vocabulary from 30k merge operations on all tokens and all single characters of the training data, including the UNK token. If unknown characters occur, they are copied from source to target. As baseline systems, we used SMT and NMT models that were trained on out-of-domain data, but did not perform online learning on in-domain data. We further present oracle systems that were trained in batch on in-domain data. 4.1 Static SMT baselines. SMT-static. We based our SMT submissions on the SCFG decoder cdec (Dyer et al., 2010) with on-the-fly grammar extraction with suffix arrays (Lopez, 2007). Training was done in batch on the parallel out-of-domain data; tuning was done on newstest2016-deen. During the development phase we evaluated MERT (on 14 default dense features) and MIRA (on additional lexicalized sparse features: rule-id features, rule source and target bigram features, and rule shape features), and found no significant difference in results. We chose MERT with dense features as the seed system for the training phase for its speed and smaller memory footprint. 4.2 4.3 Oracle SMT and NMT systems To simulate"
W17-4756,P16-1162,0,0.110316,"alancing exploration and exploitation to achieve high scores over the full data sequence. Unlike in these competitions, where environments (i.e., action spaces and context features) were fixed, in our task the environment is heterogeneous due to the use of different underlying MT architectures. Thus, systems that start out with a well-performing pretrained 517 4 Baselines expected to work reasonable well for tokens that occur in the training data and those that are copied from source to target. However, the NMT model does not learn anything about these words as such in contrast to BPE models (Sennrich et al., 2016b) where the decomposition by byte pair encoding (BPE) allows for a representation within the vocabulary. We generate a BNMT system using a BPE vocabulary from 30k merge operations on all tokens and all single characters of the training data, including the UNK token. If unknown characters occur, they are copied from source to target. As baseline systems, we used SMT and NMT models that were trained on out-of-domain data, but did not perform online learning on in-domain data. We further present oracle systems that were trained in batch on in-domain data. 4.1 Static SMT baselines. SMT-static. We"
W17-4756,W15-3014,0,0.0188715,"el that implements stochastic zeroth-order (SZO) optimization for online bandit learning. Cube pruning limit (up to 600), learning rate adaptation schedules (constant vs. Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014)), as well as the initial learning rates (for Adam), were tuned during the development phase. The best configurations were selected for the training phase. The running average of rewards as an additive control variate (CV)6 was found helpful for stochastic policy gradient updates (Williams, 1992) for all online learning systems. BNMT-static. The UNK replacement strategy of Jean et al. (2015) and Luong et al. (2015) is SMT-EL-CV-ADADELTA. We used the EL minimization approach of Sokolov et al. (2016a), adding Adadelta’s learning rate scheduling, and a control variate (effectively, replacing the received 5 From data.statmt.org/rsennrich/wmt16_ systems/de-en/ 6 Called a baseline in RL literature; here we use a term from statistics not to confuse it with baseline MT models. 518 P feedback ∆(tk ) with ∆(tk ) − k1 kk0 =1 ∆(tk0 )). Sampling and computation of expectations on the hypergraph used the Inside-Outside algorithm (Li and Eisner, 2009). probability of each word wi of the target"
W17-4756,P16-1152,1,0.824956,"ich et al., 2016a)). Final predictions are made with an ensemble formed of the four last training checkpoints and beam search with width 12. It was trained on a different corpus than allowed for this shared task – the WMT 2016 news training data (Europarl v7, News Commentary v11, CommonCrawl) and additional synthetic parallel data generated by translating the monolingual news crawl corpus with a EN-DE NMT model. 5.1 Submitted Systems Online bandit learners based on SMT. Online bandit learners based on SMT were following the existing approaches to adapting an SMT model from weak user feedback (Sokolov et al., 2016b,a) by stochastically optimizing expected loss (EL) for a log-linear model. Furthermore, we present a model that implements stochastic zeroth-order (SZO) optimization for online bandit learning. Cube pruning limit (up to 600), learning rate adaptation schedules (constant vs. Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014)), as well as the initial learning rates (for Adam), were tuned during the development phase. The best configurations were selected for the training phase. The running average of rewards as an additive control variate (CV)6 was found helpful for stochastic policy gradie"
W17-4756,P17-1138,1,0.863904,"l is to elicit binary or real-valued judgments of translation quality from laymen users (for example, Graham et al. (2016) show that consistent assessments of real-valued translation quality can be provided by crowdsourcing), or to infer feedback signals from user interactions with the translated content on a web page (for example, by interpreting a copy-paste action of the MT output as positive quality signal, and a correction as a negative quality signal). The goal of this shared task is to evaluate existing algorithms for learning MT systems from weak feedback (Sokolov et al., 2015, 2016a; Kreutzer et al., 2017) on real-world data and compare them to new algorithms, with a focus on performing online learning efficiently and effectively from bandit feedback, i.e. the best algorithms are those that perform fast online learning and, simultaneously, achieve high translation quality. In the following, we present a description of the protocol and infrastructure of our online learning task, and of the data for pretraining, online training, and evaluation (Section 2). We introduce the online and batch evaluation metrics used in the shared task (Section 3), and describe static baseline systems (Section 4) and"
W17-4756,D17-1272,1,0.900928,"Missing"
W17-4756,2015.mtsummit-papers.13,1,0.928258,"translator. Instead, the goal is to elicit binary or real-valued judgments of translation quality from laymen users (for example, Graham et al. (2016) show that consistent assessments of real-valued translation quality can be provided by crowdsourcing), or to infer feedback signals from user interactions with the translated content on a web page (for example, by interpreting a copy-paste action of the MT output as positive quality signal, and a correction as a negative quality signal). The goal of this shared task is to evaluate existing algorithms for learning MT systems from weak feedback (Sokolov et al., 2015, 2016a; Kreutzer et al., 2017) on real-world data and compare them to new algorithms, with a focus on performing online learning efficiently and effectively from bandit feedback, i.e. the best algorithms are those that perform fast online learning and, simultaneously, achieve high translation quality. In the following, we present a description of the protocol and infrastructure of our online learning task, and of the data for pretraining, online training, and evaluation (Section 2). We introduce the online and batch evaluation metrics used in the shared task (Section 3), and describe static b"
W17-4756,D09-1005,0,0.0324242,"BNMT-static. The UNK replacement strategy of Jean et al. (2015) and Luong et al. (2015) is SMT-EL-CV-ADADELTA. We used the EL minimization approach of Sokolov et al. (2016a), adding Adadelta’s learning rate scheduling, and a control variate (effectively, replacing the received 5 From data.statmt.org/rsennrich/wmt16_ systems/de-en/ 6 Called a baseline in RL literature; here we use a term from statistics not to confuse it with baseline MT models. 518 P feedback ∆(tk ) with ∆(tk ) − k1 kk0 =1 ∆(tk0 )). Sampling and computation of expectations on the hypergraph used the Inside-Outside algorithm (Li and Eisner, 2009). probability of each word wi of the target vocabulary V to be sampled in timestep t. The annealing schedule for this temperature T is defined as Tk = 0.99max(k−kSTART ,0) , i.e. decreases from iteration kSTART on. The same decay is applied to the learning rate, such that γk = γk−1 ·Tk . This schedule was proven successful during tuning with the leaderboard. SMT-EL-CV-ADAM. This system uses the same approach as above except for using Adam to adapt the learning rates, with tuning of the initial learning rate on the development service. WNMT-EL. Using the implementation of Kreutzer et al. (2017)"
W17-4756,W16-2361,0,0.0794738,"Missing"
W17-4756,P04-1077,0,0.153774,"Missing"
W17-4756,D07-1104,0,0.0149259,"stem using a BPE vocabulary from 30k merge operations on all tokens and all single characters of the training data, including the UNK token. If unknown characters occur, they are copied from source to target. As baseline systems, we used SMT and NMT models that were trained on out-of-domain data, but did not perform online learning on in-domain data. We further present oracle systems that were trained in batch on in-domain data. 4.1 Static SMT baselines. SMT-static. We based our SMT submissions on the SCFG decoder cdec (Dyer et al., 2010) with on-the-fly grammar extraction with suffix arrays (Lopez, 2007). Training was done in batch on the parallel out-of-domain data; tuning was done on newstest2016-deen. During the development phase we evaluated MERT (on 14 default dense features) and MIRA (on additional lexicalized sparse features: rule-id features, rule source and target bigram features, and rule shape features), and found no significant difference in results. We chose MERT with dense features as the seed system for the training phase for its speed and smaller memory footprint. 4.2 4.3 Oracle SMT and NMT systems To simulate full-information systems (oracles) for regret calculation, we train"
W17-4756,W17-4779,0,0.0241763,"ved by learning to predict the reward a translation hypothesis will get. This model can then be used to score hypotheses of the search space and translate source sentences while taking into account the specificities of the in-domain data. Second, three variants of the UCB1 (Auer et al., 2002) algorithm (vanilla UCB1, a UCB1-sampling variant encouraging more exploration, and a UCB1 with selecting only the examples not used to train the regression model) chose which of the ‘adapted’ or ‘seed’ systems should be used to translate a given source sentence in order to maximize the cumulative reward (Wisniewski, 2017). 520 0.31 0.30 0.29 0.28 0.27 regret 0.26 0.25 0.24 0.23 0.22 0.21 0.20 0.19 1000 10000 100000 1e+06 iteration SMT-static SMT-EL-CV-ADADELTA SMT-EL-CV-ADAM SMT-SZO-CV-ADAM WMT16-static BNMT-static WNMT-EL BNMT-EL BNMT-EL-CV UMD-dom-adapt UMD-reinforce1 UMD-reinforce2 UMD-reinforce3 LIMSI-UCB1 LIMSI-UCB1-sampl LIMSI-UCB1-select Figure 1: Evolution of regret plotted against log-scaled number of iterations during training. The steeper is the decrease of a curve, the better learning capability has the corresponding algorithm. 0.18 0.16 corpus-BLEU 0.14 0.12 0.10 0.08 0.06 0 1 2 3 check points Lex"
W17-4756,P15-1002,0,0.0171151,"hastic zeroth-order (SZO) optimization for online bandit learning. Cube pruning limit (up to 600), learning rate adaptation schedules (constant vs. Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014)), as well as the initial learning rates (for Adam), were tuned during the development phase. The best configurations were selected for the training phase. The running average of rewards as an additive control variate (CV)6 was found helpful for stochastic policy gradient updates (Williams, 1992) for all online learning systems. BNMT-static. The UNK replacement strategy of Jean et al. (2015) and Luong et al. (2015) is SMT-EL-CV-ADADELTA. We used the EL minimization approach of Sokolov et al. (2016a), adding Adadelta’s learning rate scheduling, and a control variate (effectively, replacing the received 5 From data.statmt.org/rsennrich/wmt16_ systems/de-en/ 6 Called a baseline in RL literature; here we use a term from statistics not to confuse it with baseline MT models. 518 P feedback ∆(tk ) with ∆(tk ) − k1 kk0 =1 ∆(tk0 )). Sampling and computation of expectations on the hypergraph used the Inside-Outside algorithm (Li and Eisner, 2009). probability of each word wi of the target vocabulary V to be sampl"
W17-4756,E17-3017,0,0.0485718,"Missing"
W17-4756,W16-2323,0,0.141346,"alancing exploration and exploitation to achieve high scores over the full data sequence. Unlike in these competitions, where environments (i.e., action spaces and context features) were fixed, in our task the environment is heterogeneous due to the use of different underlying MT architectures. Thus, systems that start out with a well-performing pretrained 517 4 Baselines expected to work reasonable well for tokens that occur in the training data and those that are copied from source to target. However, the NMT model does not learn anything about these words as such in contrast to BPE models (Sennrich et al., 2016b) where the decomposition by byte pair encoding (BPE) allows for a representation within the vocabulary. We generate a BNMT system using a BPE vocabulary from 30k merge operations on all tokens and all single characters of the training data, including the UNK token. If unknown characters occur, they are copied from source to target. As baseline systems, we used SMT and NMT models that were trained on out-of-domain data, but did not perform online learning on in-domain data. We further present oracle systems that were trained in batch on in-domain data. 4.1 Static SMT baselines. SMT-static. We"
W17-4756,W16-2301,0,\N,Missing
W18-1802,D11-1033,0,0.0383729,"e to include corpus (or in-domain/out-of-domain) identiﬁers, which they ﬁnd beneﬁcial for domain adaptation. However, they do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make"
W18-1802,2011.iwslt-evaluation.18,0,0.0743171,"Missing"
W18-1802,2012.eamt-1.60,0,0.0580572,"Missing"
W18-1802,W17-3205,0,0.0115843,"he more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario, called dynamic adaptation, the target domain is unknown and possibly shifting. No in-domain sample is available. Data from the target domain are only provided at test time, and their domain may change with"
W18-1802,K16-1031,0,0.0131863,"training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario, called dynamic adaptation, the target domain is unknown and possibly shifting. No in-domain sample is available. Data from the target domain are on"
W18-1802,P13-1126,0,0.0126192,"identiﬁers, which they ﬁnd beneﬁcial for domain adaptation. However, they do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the mode"
W18-1802,2016.amta-researchers.10,0,0.266787,"Missing"
W18-1802,P17-2061,0,0.0391349,"i-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario, called dynamic adaptation, the target domain is unknown and possibly shifting. No in-domain sample is available. Data from the target domain are only provided at test"
W18-1802,P11-2031,0,0.0959841,"Missing"
W18-1802,P16-5005,0,0.0564009,"Missing"
W18-1802,P10-4002,0,0.0280829,"s per feature for IPC1 and IPC2 were set to 2 and 3, the embedding sizes were set to 5 and 20. In order to avoid improvements from merely increasing the number of parameters, we used 475-dimensional source word embeddings when adding word-attached features. The concatenated embedding vectors then have the same length (500) as the original word embedding vectors. We trained all NMT models using early stopping based on training cost on heldout data with a patience of 10. Results are reported on the ﬁnal model. For the PBMT experiments, we trained and tested a hierarchical PBMT model using cdec (Dyer et al., 2010). The baseline used 21 built-in dense features. A 5-gram target-side language 3 taku910.github.io/mecab/ 4 github.com/moses-smt/mosesdecoder 5 github.com/EdinburghNLP/nematus Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 6 BLEU↑ TER↓ PBMT Baseline 5 phrase features 27.2 27.2 57.8 58.3 NMT Baseline sentence, IPC1, front sentence, IPC1, back sentence, IPC1, front/back sentence, IPC2, front sentence, COMP, front word, IPC1/IPC2 36.9 37.4 37.5∗ 37.6∗ 37.2 37 37.2 49.3 49.1 48.5∗ 48.3∗ 49 48.9 49.2 Table 3: Japanese-English translation results. BLEU↑ TER↓ PBM"
W18-1802,P12-2023,0,0.0207589,"hey ﬁnd beneﬁcial for domain adaptation. However, they do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better m"
W18-1802,W14-3358,0,0.0170231,"y do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario,"
W18-1802,P13-2121,0,0.0130442,"2 57.8 58.3 NMT Baseline sentence, IPC1, front sentence, IPC1, back sentence, IPC1, front/back sentence, IPC2, front sentence, COMP, front word, IPC1/IPC2 36.9 37.4 37.5∗ 37.6∗ 37.2 37 37.2 49.3 49.1 48.5∗ 48.3∗ 49 48.9 49.2 Table 3: Japanese-English translation results. BLEU↑ TER↓ PBMT Baseline 5 phrase features 41.7 41.7 45 45.2 NMT Baseline sentence, IPC1, front sentence, IPC1, back sentence, IPC1, front/back sentence, IPC2, front/back word, IPC1/IPC2 42.5 43.5∗ 43.2∗ 42.7 43.9∗ 43.5∗ 47.2 45.6∗ 46.3∗ 46.9 45.3∗ 46.3∗ Table 4: German-English translation results. model was built with lmplz (Heaﬁeld et al., 2013). Feature weights were trained with dtrain (Simianer et al., 2012) for 15 epochs. Results are reported on the ﬁnal epoch. All ﬁve annotations categories (IPC section, class, and subclass, company, inventor) are used to compute 5 phrase level features. We report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) on tokenized output, as computed by multeval (Clark et al., 2011). 5 Results Table 3 shows results for Japanese-English patent translation, German-English results are shown in Table 4. For both language pairs, adding side constraints to PBMT did not improve the baseline. Results"
W18-1802,P13-2122,0,0.0156018,"main adaptation. However, they do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution."
W18-1802,W15-3014,0,0.0261942,"ct translation, given the constraints. In this paper, we focus on patent translation. Patent translation lends itself particularly well to our endeavor since patent documents are annotated with different types of information, from hierarchical categorization to information about individual inventors. We are interested in seeing whether patent translation can be improved by this information and if so, which kind of information and which model integration is most useful. Since 2014, neural machine translation (NMT) has become the state of the art in machine translation (Luong and Manning, 2015; Jean et al., 2015). We hypothesize that NMT is well-suited to the integration of document information. Since the model works on sentence representations, it can decide whether or not to pay attention to this information for each particular 1 Sennrich et al. (2016a) explore politeness as a side constraint for translation. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 1 translation in context. What is more, annotations could be highly correlated. For example, a patent document’s subclass label contains its class label. Deep models are capable of learning these correlations."
W18-1802,2015.iwslt-evaluation.11,0,0.0322209,"model to select the correct translation, given the constraints. In this paper, we focus on patent translation. Patent translation lends itself particularly well to our endeavor since patent documents are annotated with different types of information, from hierarchical categorization to information about individual inventors. We are interested in seeing whether patent translation can be improved by this information and if so, which kind of information and which model integration is most useful. Since 2014, neural machine translation (NMT) has become the state of the art in machine translation (Luong and Manning, 2015; Jean et al., 2015). We hypothesize that NMT is well-suited to the integration of document information. Since the model works on sentence representations, it can decide whether or not to pay attention to this information for each particular 1 Sennrich et al. (2016a) explore politeness as a side constraint for translation. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 1 translation in context. What is more, annotations could be highly correlated. For example, a patent document’s subclass label contains its class label. Deep models are capable of learning"
W18-1802,D09-1074,0,0.0181413,"in-domain/out-of-domain) identiﬁers, which they ﬁnd beneﬁcial for domain adaptation. However, they do not use more ﬁne-grained information. For phrase-based patent translation, W¨aschle and Riezler (2012b) use patent section labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain samp"
W18-1802,2010.eamt-1.29,0,0.0602978,"Missing"
W18-1802,P02-1040,0,0.102292,"Missing"
W18-1802,E17-3017,0,0.0601418,"Missing"
W18-1802,W16-2209,0,0.0903358,"atures has also been applied in other work: Originally, this method was proposed by Sennrich et al. (2016a) to model politeness as a side constraint. Johnson et al. (2016) have used it to indicate the desired target language for multilingual NMT. Chu et al. (2017) apply it to neural domain adaptation in combination with ﬁne tuning methods (Luong and Manning, 2015). Passing additional information to a neural network via word-attached features was ﬁrst introduced by Collobert et al. (2011) as a way to add linguistic annotation for various NLP tasks using feed-forward and convolutional networks. Sennrich and Haddow (2016) transferred this idea to neural translation models. The word-attached features used by Kobus et al. (2016) were ﬁrst presented by Crego et al. (2016), where they were used to encode case information. The idea of leveraging document information as a side constraint for translation was recently investigated by Chen et al. (2016). They focus on integrating product category information for translation of product descriptions in e-commerce, and also apply their method to online lecture translation (Cettolo et al., 2012), where the lectures are annotated with topic keywords. They also experimented"
W18-1802,N16-1005,0,0.0856576,"Missing"
W18-1802,P16-1162,0,0.64057,"n as additional phrase features for phrasebased translation, but did not ﬁnd any improvements. 1 Introduction Document information beyond the text is readily available in many data sets, but is rarely used when building translation systems. Such information could comprise the document’s origin (time, place, author), its topic, or its connections to other documents. It exists, for example, in patents, textual content on the web, or e-commerce data. We aim to investigate the usefulness of document information as side constraints for translation. We use the term side constraints as it is used in Sennrich et al. (2016a) to mean information that is not present in a source string, but can inﬂuence translation choice in the target string.1 For example, patents are assigned to a hierarchical classiﬁcation system indicating their topic(s) in various degrees of granularity. Depending on the topic, different translation choices may be required. The correct choice will not always be apparent from the sentence context. By providing the classiﬁcation information to the translation model, we enable the model to select the correct translation, given the constraints. In this paper, we focus on patent translation. Paten"
W18-1802,P12-1002,1,0.842138,"ck sentence, IPC1, front/back sentence, IPC2, front sentence, COMP, front word, IPC1/IPC2 36.9 37.4 37.5∗ 37.6∗ 37.2 37 37.2 49.3 49.1 48.5∗ 48.3∗ 49 48.9 49.2 Table 3: Japanese-English translation results. BLEU↑ TER↓ PBMT Baseline 5 phrase features 41.7 41.7 45 45.2 NMT Baseline sentence, IPC1, front sentence, IPC1, back sentence, IPC1, front/back sentence, IPC2, front/back word, IPC1/IPC2 42.5 43.5∗ 43.2∗ 42.7 43.9∗ 43.5∗ 47.2 45.6∗ 46.3∗ 46.9 45.3∗ 46.3∗ Table 4: German-English translation results. model was built with lmplz (Heaﬁeld et al., 2013). Feature weights were trained with dtrain (Simianer et al., 2012) for 15 epochs. Results are reported on the ﬁnal epoch. All ﬁve annotations categories (IPC section, class, and subclass, company, inventor) are used to compute 5 phrase level features. We report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) on tokenized output, as computed by multeval (Clark et al., 2011). 5 Results Table 3 shows results for Japanese-English patent translation, German-English results are shown in Table 4. For both language pairs, adding side constraints to PBMT did not improve the baseline. Results which improve signiﬁcantly over the NMT baseline at p ≤ 0.05 are"
W18-1802,2006.amta-papers.25,0,0.0801645,"Missing"
W18-1802,2007.mtsummit-papers.63,0,0.135105,"Missing"
W18-1802,D17-1155,0,0.0182411,"t do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario, called dynamic adaptation, the target domain is unknown and possibly shifting. No in-domain sample is available. Data from the target domain are only provided at test time, and their dom"
W18-1802,E12-1083,1,0.903453,"Missing"
W18-1802,C16-1170,0,0.0189831,"n labels to partition training data for multi-task learning, but do not look into the more ﬁne-grained classiﬁcation information. 2.2 Relation to Domain Adaptation This work is related to the problem of domain adaptation in machine translation, which has been researched extensively for phrase-based translation, among others by Foster and Kuhn (2007); Axelrod et al. (2011); Matsoukas et al. (2009); Chen et al. (2013); Eidelman et al. (2012); Hewavitharana et al. (2013); Hasler et al. (2014), and is currently also being explored for neural machine translation (see Freitag and Al-Onaizan (2016); Zhang et al. (2016); Chen and Huang (2016); Chu et al. (2017); Wang et al. (2017); Chen et al. (2017) inter alia). There are two main scenarios for domain adaptation in the literature: In the ﬁrst scenario, a translation model is adapted to a known, ﬁxed target domain. A sample from the target domain is usually available. The aim of adaptation in this scenario is to make use of the in-domain sample to shift the model parameters to better match the target distribution. In a second scenario, called dynamic adaptation, the target domain is unknown and possibly shifting. No in-domain sample is available. Data from t"
W18-1814,E17-2101,0,0.0231767,"Missing"
W18-1814,W14-3346,0,0.0188595,"ure 2: Absolute frequencies of lowercased words against their rank in Multi30k, MS COCO and Wikimedia Commons data, illustrating the differences in corpus size and vocabulary of the three corpora. The retrieval score deﬁned in Equation (1) can make use of different top-k caption lists, Lfi or Mi , based on textual or visual similarity, respectively. The relevance function gm (x, y) can be instantiated to any retrieval score or similarity function that suits our needs. In our experiments, we applied the standard TFIDF metric from information retrieval and smoothed sentence-based BLEU (S-BLEU) (Chen and Cherry, 2014). The normalization parameter Ngm depends on the relevance function and is in our case either the number of top-k captions for S-BLEU or the number of words for TFIDF. Based on this formulation we combine up to 36 ranking functions deﬁned on different top-k sequences, relevance functions, and cutoff levels. This setup can be easily extended by additional relevance score functions. 3.4 Learning to Rank Captions Our ﬁnal ranking score function REθ , θ ∈ R37 is deﬁned as linear combination of up to 36 retrieval scores plus 1 translation model score as additional feature as follows:  REθ (h) = αm"
W18-1814,P11-2031,0,0.0558164,"6 33.78 26.96 33.88 28.47 33.27 30.83 32.97 27.54 33.92 30.59 32.46 62.74 62.93 62.37 62.29 63.01 61.70 – ‡ 31.04 34.03 62.90 † – – 30.79 34.30 30.67 34.21 30.76 34.40 – 62.91 62.86 63.00 Table 5: BLEU and Character F-scores (ChF) on de-en MS COCO test data from Hitschler et al. (2016) for their cdec in-domain and TSR-CNN systems, the new reranker applied to the cdec hypothesis lists, and our new system on different combinations of rerankers. Signiﬁcant improvements over the baseline system are indicated by preceding † (p < 0.03) and ‡ (p < 0.003) as reported by MultEval’s randomization test (Clark et al., 2011). 5.4 Results Our experiments revealed a strong connection between certain properties of retrieval data and the performance of our approach. On very clean, manually constructed data of limited size and complexity like the Multi30k dataset, our retrieval-based method fails to extract additional useful information. As the data available for retrieval grows, we observe small gains like in the experiments on MS COCO. The biggest improvements, however, can be found on the much larger and inherently diverse Wikimedia Commons dataset we described before. We discuss the results of an ablation experime"
W18-1814,W17-4718,0,0.144893,"ranslations of crowdsourced English captions was presented (Specia et al., 2016). However, this dataset has limitations: The captions were created by human annotators that were guided to produce “conceptual” descriptions that identify the objects depicted in the image (Hodosh et al., 2013). This leads to relatively short captions amounting to a comparatively easy translation task with little room for improvement by incorporating visual information. This is conﬁrmed by recent results showing that improvements over a text-only MT baseline are inconsistent and hard to achieve (Lala et al., 2017; Elliott et al., 2017). While caption translation in previous work has been conducted solely on clean, manually labeled captions based on MS COCO (Lin et al., 2014), Flickr30k (Rashtchian et al., 2010), or its multilingual variant Multi30k (Elliott et al., 2016), the goal of our work is to lift multimodal caption translation to a more realistic setup. For this purpose, we extracted a dataset of 4M “capProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 140 tions in the wild” as they appear in the user-generated Wikimedia Commons database. This new dataset is very different to previ"
W18-1814,W16-3210,0,0.0323142,"Missing"
W18-1814,D15-1021,0,0.0255442,"rong dependency of our approach’s performance on the type and size of retrieval data. The experiments are described in Section 5. 2 Related Work The dataset presented in this paper is to our knowledge the ﬁrst publicly available resource of user-generated image captions at the size of 4M image-caption pairs. The dataset that is closest to ours is the SBU captioned photo dataset (Ordonez et al., 2011) that contains 1M images and captions. However, this dataset was ﬁltered to include speciﬁc terms and to limit description lengths, resulting in an average sentence length of around 13 tokens. See Ferraro et al. (2015) for an overview over image-caption datasets. Multimodal caption translation on parallel caption data (see the approaches described in Specia et al. (2016)) incorporate visual information directly into the sequence-to-sequence caption translation model or into a reranking component, or into both (see for example the attentionbased LSTM approach of Huang et al. (2016)), or they use back-translation to generate synthetic parallel data (see for example Calixto et al. (2017)). However, obtaining parallel captioning data or retraining NMT models on large synthetic datasets is either ﬁnancially or c"
W18-1814,P16-1227,1,0.141326,"ions in this dataset are around 34 tokens long, compared to 11 and 14 for MS COCO and Multi30k, respectively. Caption translation of Wikimedia Commons data thus contrasts to previous image-caption translation tasks. However, we ﬁnd the new dataset to provide a lot of room for improvement by incorporating visual information into the translation process. The dataset is described in Section 4. Since the dataset only contains very small subsets of parallel captions (which we use for tuning and testing), the proper way to integrate visual information is to leverage monolingual image-caption pairs. Hitschler et al. (2016) presented an approach based on a crosslingual reranking framework where monolingual captions in the target language are used to rerank translation hypotheses given a source caption and the corresponding image. In order to retrieve captions for reranking, they pivot on target language image-captions pairs in two ways: A list of monolingual captions is obtained by a joint textual and visual similarity model by comparison between the hypotheses and the captions in the target language. To calculate the visual similarity component of their joint model, they use rich image feature representations f"
W18-1814,W16-2360,0,0.0239845,"o dataset (Ordonez et al., 2011) that contains 1M images and captions. However, this dataset was ﬁltered to include speciﬁc terms and to limit description lengths, resulting in an average sentence length of around 13 tokens. See Ferraro et al. (2015) for an overview over image-caption datasets. Multimodal caption translation on parallel caption data (see the approaches described in Specia et al. (2016)) incorporate visual information directly into the sequence-to-sequence caption translation model or into a reranking component, or into both (see for example the attentionbased LSTM approach of Huang et al. (2016)), or they use back-translation to generate synthetic parallel data (see for example Calixto et al. (2017)). However, obtaining parallel captioning data or retraining NMT models on large synthetic datasets is either ﬁnancially or computationally expensive. We thus opt for a way that does not require large amounts of parallel captions to improve translation quality. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 141 Dataset Retrieval Reranker MT-System Languages Hitschler et al. (2016) Our work MS COCO: clean, limited vocabulary multimodal joint model inte"
W18-1814,P02-1040,0,0.100364,"Missing"
W18-1814,W15-3049,0,0.0328735,"Missing"
W18-1814,W10-0721,0,0.0475478,"guided to produce “conceptual” descriptions that identify the objects depicted in the image (Hodosh et al., 2013). This leads to relatively short captions amounting to a comparatively easy translation task with little room for improvement by incorporating visual information. This is conﬁrmed by recent results showing that improvements over a text-only MT baseline are inconsistent and hard to achieve (Lala et al., 2017; Elliott et al., 2017). While caption translation in previous work has been conducted solely on clean, manually labeled captions based on MS COCO (Lin et al., 2014), Flickr30k (Rashtchian et al., 2010), or its multilingual variant Multi30k (Elliott et al., 2016), the goal of our work is to lift multimodal caption translation to a more realistic setup. For this purpose, we extracted a dataset of 4M “capProceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 140 tions in the wild” as they appear in the user-generated Wikimedia Commons database. This new dataset is very different to previous image-caption data, as it contains highly diverse types of user-generated texts associated with images. The English captions in this dataset are around 34 tokens long, compare"
W18-1814,E17-3017,1,0.821959,"Missing"
W18-1814,P16-1162,0,0.0119018,"used in our experiments. Examples for image-caption pairs from our dev and test sets together with a script to retrieve the full data set is available for download.5 The data is released in accordance to the respective licenses. 5.2 Translation Systems We trained our baseline translation system with Nematus (Sennrich et al., 2017), a state of the art toolkit for neural machine translation.6 We tokenized and converted all training data to lower case using the same cdec utilities as were used for pre-processing of the retrieval data. In addition, we performed 20,000 steps of byte pair encoding (Sennrich et al., 2016) on the input and output vocabularies, giving our systems an open output vocabulary in principle. We used default parameters for learning, measured the cross-entropy of a held-out validation set after processing every 10,000 training samples and stopped training accordingly. For training and validation data, we ﬁltered out sentences longer than 70 words (before byte pair encoding). 4 https://github.com/redpony/cdec 5 http://www.cl.uni-heidelberg.de/wikicaps 6 https://github.com/rsennrich/nematus, 54be147dc363603d69643c35b700ae5d9de2ad93 Proceedings of AMTA 2018, vol. 1: MT Research Track git c"
W18-1814,W16-2346,0,0.0306365,"Missing"
W18-1814,W15-4922,1,0.908691,"Missing"
W19-6610,1997.mtsummit-papers.1,0,0.928469,"s via constrained beam search. In simulation experiments on two language pairs our systems get close to the performance of supervised training with much less human effort. 1 Introduction Despite recent success reports on neural machine translation (NMT) reaching human parity (Wu et al., 2016; Hassan et al., 2018), professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs (Wuebker et al., 2018; Michel and Neubig, 2018). In this paper, we will focus on the paradigm of interactivepredictive machine translation (Foster et al., 1997; Barrachina et al., 2008) which has been shown to fit easily into the sequence-to-sequence model of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which is again corrected or accepted by the human user. Recent work showed in simulation experiments that human effort can be reduced by asking"
W19-6610,W02-1020,0,0.165451,"s from prior work by allowing model updates based on partial translations. Our experiments show that weak feedback in form of keep/delete rewards on translation outputs yields consistent improvements of between 2.6 and 4.3 BLEU points over the pre-trained baseline. On one language pair, it even matches the improvements gained by forcing word substitutions from reference translations into the re-decoded output. Furthermore, both feedback scenarios considerably reduce human effort. 2 Related Work Interactive-predictive translation goes back to early approaches for IBM-type (Foster et al., 1997; Foster et al., 2002) and phrase-based machine translation (Barrachina et al., 2008; Green et al., Dublin, Aug. 19-23, 2019 |p. 96 2014). Knowles and Koehn (2016) and Wuebker et al. (2016) presented neural interactive translation prediction — a translation scenario where translators interact with an NMT system by accepting or correcting subsequent target tokens suggested by the NMT system in an auto-complete style. However, in their work the system parameters are not updated based on the prefix. This idea is implemented in Turchi et al. (2017), Michel and Neubig (2018), Wuebker et al. (2018), Karimova et al. (2018"
W19-6610,P07-2045,0,0.011074,"Missing"
W19-6610,P17-1138,1,0.722914,"ile we integrate constraints derived from diverse human feedback to interactively improve decoding. Additionally, we try to reduce human effort by minimizing the number of feedback requests and by frequent model updates. Several recent approaches to reinforcement learning from human feedback implement the idea of reinforcing/penalizing a targeted set of actions. Kreutzer et al. (2018) presented an approach were ratings from human users on full translations are used successfully for NMT domain adaptation. Simulations of NMT systems interacting with human feedback have been presented firstly by Kreutzer et al. (2017), Nguyen et al. (2017), or Bahdanau et al. (2017), who apply different policy gradient algorithms, William’s REINFORCE (Williams, 1992) or advantage-actor-critic methods (Mnih et al., 2016), respectively. In this paper, we use REINFORCE update strategies for simulated bandit feedback on the sub-sentence level. Gonz´alez-Rubio et al. (2011; 2012) apply active learning for interactive machine translation, where a user interactively finishes translations of a statistical MT system. Their active learning component decides which sentences to sample for translation and receive supervision for, and t"
W19-6610,P18-1165,1,0.859419,"Our work is also closely related to approaches for interactive pre-post-editing (Marie and Max, 2015; Domingo et al., 2017). The core idea is to ask the translator to mark good segments and use these for a more informed re-decoding, while we integrate constraints derived from diverse human feedback to interactively improve decoding. Additionally, we try to reduce human effort by minimizing the number of feedback requests and by frequent model updates. Several recent approaches to reinforcement learning from human feedback implement the idea of reinforcing/penalizing a targeted set of actions. Kreutzer et al. (2018) presented an approach were ratings from human users on full translations are used successfully for NMT domain adaptation. Simulations of NMT systems interacting with human feedback have been presented firstly by Kreutzer et al. (2017), Nguyen et al. (2017), or Bahdanau et al. (2017), who apply different policy gradient algorithms, William’s REINFORCE (Williams, 1992) or advantage-actor-critic methods (Mnih et al., 2016), respectively. In this paper, we use REINFORCE update strategies for simulated bandit feedback on the sub-sentence level. Gonz´alez-Rubio et al. (2011; 2012) apply active lear"
W19-6610,E12-1025,0,0.507229,"Missing"
W19-6610,D18-1397,0,0.119009,"l of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which is again corrected or accepted by the human user. Recent work showed in simulation experiments that human effort can be reduced by asking humans for reward signals or validations of partial system outputs instead of for corrections (Lam et al., 2018; Domingo et al., 2017). Our goal is to combine both feedback modes — corrections and rewards — by treating them as expert demonstrations and reward values in an interactive protocol that combines imitation learning (IL) (Ross et al., 2011) and reinforcement learning (RL) (Sutton and Barto, 2018), respectively, using only limited human edits. A further difference of our framework to standard interactive-predictive NMT is our use of an uncertainty criterion that reduces the amount of feedback requests to the tokens where the entropy of the policy distribution is highest. This idea has been used"
W19-6610,D14-1130,0,0.272115,"Missing"
W19-6610,D15-1120,0,0.281769,"ted by the NMT system in an auto-complete style. However, in their work the system parameters are not updated based on the prefix. This idea is implemented in Turchi et al. (2017), Michel and Neubig (2018), Wuebker et al. (2018), Karimova et al. (2018), or Peris et al. (2017). In contrast to our work, these approaches use complete post-edited sentences to update their system, while we update our model based on partial translations. Furthermore, our approach employs techniques to reduce the number of interactions. Our work is also closely related to approaches for interactive pre-post-editing (Marie and Max, 2015; Domingo et al., 2017). The core idea is to ask the translator to mark good segments and use these for a more informed re-decoding, while we integrate constraints derived from diverse human feedback to interactively improve decoding. Additionally, we try to reduce human effort by minimizing the number of feedback requests and by frequent model updates. Several recent approaches to reinforcement learning from human feedback implement the idea of reinforcing/penalizing a targeted set of actions. Kreutzer et al. (2018) presented an approach were ratings from human users on full translations are"
W19-6610,P17-1141,0,0.0206963,"pective word, or “action” (Bahdanau et al., 2017). We instantiate rewards and demonstrations to the feedback types in interactive-predictive translation as follows: In the first case, uncertain words predicted by the system receive a positive or negative reward based on “keep” or “delete” feedback respectively. In the second case, uncertain words can additionally be corrected based on an expert policy in the form of “substitute” feedback associated with a positive reward. This feedback is integrated in context of the model’s own predictions by adding rules to constrained beam search decoding (Hokamp and Liu, 2017; Post and Vilar, 2018).1 3.1 Learning Objective We formalize the objective of interactivepredictive NMT as maximizing the value function V of a parametrized policy πθ , i.e., we seek to maximize the expected (future) reward obtainable from interactions of the NMT system with a human translator who, by editing translations, implicitly assigns rewards R(ˆ y) to system predictions y ˆ given source sentences x: max Vπθ (ˆ y; x) = max Eyˆ∼πθ (·|x) [R(ˆ y)] θ θ (1) 1 We observe that the distinction between weak feedback and expert feedback is difficult to make in the “keep” feedback case: on the on"
W19-6610,P18-2050,0,0.131539,"in the form of “substitute” edits. Conditioning on the collected feedback, the system creates alternative translations via constrained beam search. In simulation experiments on two language pairs our systems get close to the performance of supervised training with much less human effort. 1 Introduction Despite recent success reports on neural machine translation (NMT) reaching human parity (Wu et al., 2016; Hassan et al., 2018), professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs (Wuebker et al., 2018; Michel and Neubig, 2018). In this paper, we will focus on the paradigm of interactivepredictive machine translation (Foster et al., 1997; Barrachina et al., 2008) which has been shown to fit easily into the sequence-to-sequence model of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which is again corrected or acc"
W19-6610,D17-1153,0,0.0321987,"Missing"
W19-6610,2016.amta-researchers.9,0,0.517764,"pervised training with much less human effort. 1 Introduction Despite recent success reports on neural machine translation (NMT) reaching human parity (Wu et al., 2016; Hassan et al., 2018), professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs (Wuebker et al., 2018; Michel and Neubig, 2018). In this paper, we will focus on the paradigm of interactivepredictive machine translation (Foster et al., 1997; Barrachina et al., 2008) which has been shown to fit easily into the sequence-to-sequence model of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which is again corrected or accepted by the human user. Recent work showed in simulation experiments that human effort can be reduced by asking humans for reward signals or validations of partial system outputs instead of for corrections (Lam et al., 2018; Domingo et al.,"
W19-6610,N10-1079,0,0.0409607,"Missing"
W19-6610,P02-1040,0,0.106205,"Missing"
W19-6610,K18-1015,0,0.0875679,"e both feedback modes — corrections and rewards — by treating them as expert demonstrations and reward values in an interactive protocol that combines imitation learning (IL) (Ross et al., 2011) and reinforcement learning (RL) (Sutton and Barto, 2018), respectively, using only limited human edits. A further difference of our framework to standard interactive-predictive NMT is our use of an uncertainty criterion that reduces the amount of feedback requests to the tokens where the entropy of the policy distribution is highest. This idea has been used successfully before in Lam et al. (2018) and Peris and Casacuberta (2018) and connects our work to the area of active learning (Settles and Craven, 2008). Lastly, our framework differs from prior work by allowing model updates based on partial translations. Our experiments show that weak feedback in form of keep/delete rewards on translation outputs yields consistent improvements of between 2.6 and 4.3 BLEU points over the pre-trained baseline. On one language pair, it even matches the improvements gained by forcing word substitutions from reference translations into the re-decoded output. Furthermore, both feedback scenarios considerably reduce human effort. 2 Rel"
W19-6610,W15-3049,0,0.0246003,"Missing"
W19-6610,N18-1119,0,0.0181688,"on” (Bahdanau et al., 2017). We instantiate rewards and demonstrations to the feedback types in interactive-predictive translation as follows: In the first case, uncertain words predicted by the system receive a positive or negative reward based on “keep” or “delete” feedback respectively. In the second case, uncertain words can additionally be corrected based on an expert policy in the form of “substitute” feedback associated with a positive reward. This feedback is integrated in context of the model’s own predictions by adding rules to constrained beam search decoding (Hokamp and Liu, 2017; Post and Vilar, 2018).1 3.1 Learning Objective We formalize the objective of interactivepredictive NMT as maximizing the value function V of a parametrized policy πθ , i.e., we seek to maximize the expected (future) reward obtainable from interactions of the NMT system with a human translator who, by editing translations, implicitly assigns rewards R(ˆ y) to system predictions y ˆ given source sentences x: max Vπθ (ˆ y; x) = max Eyˆ∼πθ (·|x) [R(ˆ y)] θ θ (1) 1 We observe that the distinction between weak feedback and expert feedback is difficult to make in the “keep” feedback case: on the one hand, this type of fe"
W19-6610,P16-1007,0,0.392633,"ch less human effort. 1 Introduction Despite recent success reports on neural machine translation (NMT) reaching human parity (Wu et al., 2016; Hassan et al., 2018), professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs (Wuebker et al., 2018; Michel and Neubig, 2018). In this paper, we will focus on the paradigm of interactivepredictive machine translation (Foster et al., 1997; Barrachina et al., 2008) which has been shown to fit easily into the sequence-to-sequence model of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which is again corrected or accepted by the human user. Recent work showed in simulation experiments that human effort can be reduced by asking humans for reward signals or validations of partial system outputs instead of for corrections (Lam et al., 2018; Domingo et al., 2017). Our goal is to"
W19-6610,D18-1104,0,0.08066,"expert demonstrations in the form of “substitute” edits. Conditioning on the collected feedback, the system creates alternative translations via constrained beam search. In simulation experiments on two language pairs our systems get close to the performance of supervised training with much less human effort. 1 Introduction Despite recent success reports on neural machine translation (NMT) reaching human parity (Wu et al., 2016; Hassan et al., 2018), professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs (Wuebker et al., 2018; Michel and Neubig, 2018). In this paper, we will focus on the paradigm of interactivepredictive machine translation (Foster et al., 1997; Barrachina et al., 2008) which has been shown to fit easily into the sequence-to-sequence model of NMT (Knowles and Koehn, 2016; Wuebker et al., 2016). The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 which"
W19-6610,D08-1112,0,0.0316244,"trations and reward values in an interactive protocol that combines imitation learning (IL) (Ross et al., 2011) and reinforcement learning (RL) (Sutton and Barto, 2018), respectively, using only limited human edits. A further difference of our framework to standard interactive-predictive NMT is our use of an uncertainty criterion that reduces the amount of feedback requests to the tokens where the entropy of the policy distribution is highest. This idea has been used successfully before in Lam et al. (2018) and Peris and Casacuberta (2018) and connects our work to the area of active learning (Settles and Craven, 2008). Lastly, our framework differs from prior work by allowing model updates based on partial translations. Our experiments show that weak feedback in form of keep/delete rewards on translation outputs yields consistent improvements of between 2.6 and 4.3 BLEU points over the pre-trained baseline. On one language pair, it even matches the improvements gained by forcing word substitutions from reference translations into the re-decoded output. Furthermore, both feedback scenarios considerably reduce human effort. 2 Related Work Interactive-predictive translation goes back to early approaches for I"
W19-6610,J09-1002,0,\N,Missing
