2020.aacl-main.81,W06-0901,0,0.173113,"ic English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Tradit"
2020.aacl-main.81,D17-1209,0,0.0554986,"Missing"
2020.aacl-main.81,D14-1199,0,0.038221,"Missing"
2020.aacl-main.81,P17-1038,1,0.848953,"08; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish can"
2020.aacl-main.81,P15-1017,1,0.877144,"relevant sources. • We propose an edge-enriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However,"
2020.aacl-main.81,D18-1158,1,0.821423,"nriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensiv"
2020.aacl-main.81,W04-1000,0,0.595724,"a graph into rich vector representations to facilitate event region identification. The experimental results on two datasets of two languages show that our approach yields new state-of-the-art performance for the challenging event extraction task. 1 Event Template Event Extraction (EE), a challenging task in Natural Language Processing, aims to extract key types of information (aka event roles, e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004). Existing research on EE mostly focused on sentence-level, such as the evaluation in Automatic Content Extraction (ACE) 20051 . However, an event is usually described in Most of the work was done when the first author was a research engineer in the Institute of Automation, CAS. 1 http://projects.ldc.upenn.edu/ace/ Role Fillers PerpInd TERRORISTS, HOODED INDIVIDUALS PerpOrg SHINING PATH Victim DOLORES HINOSTROZA, HINOSTROZA Figure 1: An example of document-level event extraction. We need to extract noun phrases from the document as role fillers for the event roles in the predefined event templ"
2020.aacl-main.81,P11-1114,1,0.959214,"intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish candidate role fillers from texts and achieved better performance. Until recent years, researchers (Hsi, 2018; Yang et al., 2018; Zheng et al., 2019) began to utilize multiple neuralbased methods to solve the task. Notably, among the document-level EE research, some works (Patwardhan and Riloff, 2009; Huang and Riloff, 2012; Yang et al., 2018) have noticed the importance of identifying event regions to improve performance. Traditional neural networks such as Convolutional Neural Networks and Recursive Neural Network"
2020.aacl-main.81,P08-1030,0,0.140204,"MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based"
2020.aacl-main.81,P10-1081,0,0.195906,"ge-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based meth"
2020.aacl-main.81,D17-1159,0,0.0728135,"Missing"
2020.aacl-main.81,N16-1034,0,0.0729328,"We propose an edge-enriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised meth"
2020.aacl-main.81,D07-1075,0,0.0602653,", these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish candidate role fillers from texts and achieved better performance. Until recent years, researchers (Hsi, 2018; Yang et al., 2018; Zheng et al., 2019) began to utilize multiple neuralbased methods to solve the task. Notably, among the document-level EE research, some works (Patwardhan and Riloff, 2009; Huang and Riloff, 2012; Yang et al., 2018) have noticed the importance of identifying event regions to improve performance. Traditional neural networks such as Convolutional Neural Network"
2020.aacl-main.81,D09-1016,0,0.768247,"mentions the target event twice in two regions. The correct role fillers are crowding in the first event region S1, S2, S3 and the second one S5, S6 respectively. Nevertheless, the sentence-level extractor will extract noise from both the event regions like HOUSE from S3 and irrelevant sentence like FATHER in S4, destroying the layout of the original regions. Many previous efforts try to avoid aggregating the noisy candidates by detecting such event regions. The popular approach is to apply sentential classification to filter the sentences and recognize role fillers from the chosen sentences (Patwardhan and Riloff, 2009; Huang and Riloff, 2012). However, these approaches only detect regions at single sentence-level and ignore the crowding of relevant sentences. Also, they also suffer from the accumulative error of sentential classification. For example, they may identify S2 as a relevant event region but S3 as irrelevant because they fail to take into account the similarity of S2 and S3. Another solution proposed by Yang et al. (2018) tries to detect the primary event description sentence and supplement the missing event roles with fillers from adjacent sentences. This method considers the multiple sentences"
2020.aacl-main.81,N12-1008,0,0.0667316,"taset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this"
2020.aacl-main.81,P18-4009,1,0.243235,"detecting such event regions. The popular approach is to apply sentential classification to filter the sentences and recognize role fillers from the chosen sentences (Patwardhan and Riloff, 2009; Huang and Riloff, 2012). However, these approaches only detect regions at single sentence-level and ignore the crowding of relevant sentences. Also, they also suffer from the accumulative error of sentential classification. For example, they may identify S2 as a relevant event region but S3 as irrelevant because they fail to take into account the similarity of S2 and S3. Another solution proposed by Yang et al. (2018) tries to detect the primary event description sentence and supplement the missing event roles with fillers from adjacent sentences. This method considers the multiple sentences in an event region but is limited to one region per document. For instance, it may detect S1 as the primary sentence and supplement it with S2, missing the valid items like SHINING PATH from region 2. Moreover, it also suffers from the errors selecting primary sentence, and the supplementing strategy is coarse-grained and fails to take into account every candidate filler individually. We build a graph for each document"
2020.aacl-main.81,D19-1032,0,0.443645,"Missing"
2020.acl-demos.33,D15-1180,0,0.028667,"d by (Mullenbach et al., 2018), assigning the importance value for each label to the discharge summaries to assists in explaining the model prediction process. 2.2 Dilated Convolution Dilated convolution is designed for image classification to aggregate multi-scale contextual information without losing resolution in computer vision (Yu and Koltun, 2016). It inserts “holes” in the standard convolution map to increase the reception field. The hole-structure brings a breakthrough improvement to the semantic segmentation task. Similarly, several hole-structured convolution neural networks (CNNs) (Lei et al., 2015; Guo et al., 2017) are designed to handle natural language processing tasks. In the text, there exists noncontinuous semantic where useless information may be interspersed among the sentences. Holes in the dilated convolution can ignore the extra word between the non-continuous words and well adapt to match non-continuous semantic. Since the semantic infomation is crutial when understanding natural language(Zuo et al., 2019), we apply the dilated convolution to encode the text, capturing the non-continuous semantic information. 3 3.1 Clinical Coder System Method We propose a Dilated Convoluti"
2020.acl-demos.33,N18-1100,0,0.388356,"bstract is no doubt that the increased granularity increases the difficulty of manual coding. Existing studies came up with several approaches of automatic coding prediction to replace the repetitive manual work, from the traditional machine learning methods (Perotte et al., 2013; Koopman et al., 2015), to neural network methods (Shi et al., 2017; Yu et al., 2019). Although these methods achieve great success, they are still confronted with a critical challenge, which is the interpretability of predicted codes. Explainable model and results are essential for clinical medicine decision making (Mullenbach et al., 2018). Thus, the practical approach is supposed to predict correct codes and simultaneously give the reason why each code is predicted. In this paper, we introduce Clinical-Coder, an online system aiming to assign ICD codes to Chinese clinical notes. ICD coding has been a research hotspot of clinical medicine, but the interpretability of prediction hinders its practical application. We exploit a Dilated Convolutional Attention network with N-gram Matching Mechanism (DCANM) to capture semantic features for non-continuous words and continuous n-gram words, concentrating on explaining the reason why e"
2020.acl-demos.33,D18-1352,0,0.0143544,"Related Work Automatic ICD coding Automatic ICD coding has recently been a research hotspot in the field of clinical medicine, where neural network architecture methods show promising results than traditional machine learning methods. Most studies treat automatic ICD coding as a multi-label classification problem and use only the free-text in summaries to predict codes (Subotin and Davis, 2015; Kavuluru et al., 2015; Yu et al., 2019), while many methods benefit from extra information. Shi et al. (2017) encode label description with character-level and word-level long shortterm memory network. Rios and Kavuluru (2018) encode label description with averaging words embedding. Furthermore, adversarial learning is employed to unify writing styles of diagnosis descriptions and ICD code descriptions (Xie et al., 2018). Besides code descriptions, Wikipedia comes to be regarded as an external knowledge source (Prakash et al., 2017; Bai and Vucetic, 2019). Additionally, inferring interpretability is a crucial challenge and obstacle for practical automatic coding, since professionals are willing to be con295 Figure 3: The whole architecture of the model. The input is the clinical text, and output is the ICD codes. T"
2020.acl-demos.33,P18-1098,0,0.0279916,"nal machine learning methods. Most studies treat automatic ICD coding as a multi-label classification problem and use only the free-text in summaries to predict codes (Subotin and Davis, 2015; Kavuluru et al., 2015; Yu et al., 2019), while many methods benefit from extra information. Shi et al. (2017) encode label description with character-level and word-level long shortterm memory network. Rios and Kavuluru (2018) encode label description with averaging words embedding. Furthermore, adversarial learning is employed to unify writing styles of diagnosis descriptions and ICD code descriptions (Xie et al., 2018). Besides code descriptions, Wikipedia comes to be regarded as an external knowledge source (Prakash et al., 2017; Bai and Vucetic, 2019). Additionally, inferring interpretability is a crucial challenge and obstacle for practical automatic coding, since professionals are willing to be con295 Figure 3: The whole architecture of the model. The input is the clinical text, and output is the ICD codes. The yellow dotted box indicates how to use attention-based dilated convolution to capture the implicit semantic of noncontinuous words. The green dotted box indicates how to use n-gram matching mecha"
2020.acl-main.282,D17-1209,0,0.0178045,"been successfully applied to question answering (Tay et al., 2018), machine translation (Gulcehre et al., 2018) and sentence representation (Dhingra et al., 2018). To our knowledge, this is the first work to apply hyperbolic representation method to the automatic ICD coding task. Graph Convolutional Networks. GCN (Kipf and Welling, 2016) is a powerful neural network, which operates on graph data. It yields substantial improvements over various NLP tasks such as semantic role labeling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and machine translation (Bastings et al., 2017). Veliˇckovi´c et al. (2017) propose graph attention networks (GAT) to summarize neighborhood features by using masked self-attentional layers. We are the first to capture the code co-occurrence characteristic via the GCN for the automatic ICD coding task. 3 Method We propose a hyperbolic and co-graph representation (HyperCore) model for automatic ICD coding. Firstly, to capture the code hierarchy, we learn the code hyperbolic representations and measure the similarities between document and codes in the hyperbolic space. Secondly, to exploit code cooccurrence, we exploit the GCN to learn code"
2020.acl-main.282,W18-1708,0,0.248355,"rchy and code co-occurrence. Hyperbolic Representation. Hyperbolic space has been applied to modeling complex networks (Krioukov et al., 2010). Recent research on representation learning demonstrates that the hyperbolic space is more suitable for representing symbolic data with hierarchical structures than the Euclidean space (Nickel and Kiela, 2017, 2018; Hamann, 2018). In the field of natural language processing (NLP), the hyperbolic representation has been successfully applied to question answering (Tay et al., 2018), machine translation (Gulcehre et al., 2018) and sentence representation (Dhingra et al., 2018). To our knowledge, this is the first work to apply hyperbolic representation method to the automatic ICD coding task. Graph Convolutional Networks. GCN (Kipf and Welling, 2016) is a powerful neural network, which operates on graph data. It yields substantial improvements over various NLP tasks such as semantic role labeling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and machine translation (Bastings et al., 2017). Veliˇckovi´c et al. (2017) propose graph attention networks (GAT) to summarize neighborhood features by using masked self-attentional layer"
2020.acl-main.282,W19-4319,0,0.123564,"Missing"
2020.acl-main.282,D17-1159,0,0.028195,"17, 2018; Hamann, 2018). In the field of natural language processing (NLP), the hyperbolic representation has been successfully applied to question answering (Tay et al., 2018), machine translation (Gulcehre et al., 2018) and sentence representation (Dhingra et al., 2018). To our knowledge, this is the first work to apply hyperbolic representation method to the automatic ICD coding task. Graph Convolutional Networks. GCN (Kipf and Welling, 2016) is a powerful neural network, which operates on graph data. It yields substantial improvements over various NLP tasks such as semantic role labeling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and machine translation (Bastings et al., 2017). Veliˇckovi´c et al. (2017) propose graph attention networks (GAT) to summarize neighborhood features by using masked self-attentional layers. We are the first to capture the code co-occurrence characteristic via the GCN for the automatic ICD coding task. 3 Method We propose a hyperbolic and co-graph representation (HyperCore) model for automatic ICD coding. Firstly, to capture the code hierarchy, we learn the code hyperbolic representations and measure the similarities between document and c"
2020.acl-main.282,P18-1098,0,0.290404,"tical for the automatic ICD coding. Code Hierarchy: Based on ICD taxonomy, ICD codes are organized under a tree-like hierarchical structure as shown in Figure 2, which indicates the parent-child and sibling relations between codes. In the hierarchical structure, the upper level nodes represent more generic disease categories and the lower level nodes represent more specific diseases. The code hierarchy can capture the mutual exclusion of some codes. If code X and Y are both children of Z (i.e., X and Y are the siblings), it is unlikely to simultaneously assign X and Y to a patient in general (Xie and Xing, 2018). For example in Figure 2, if code “464.00 (acute laryngitis without mention of obstruction)” is assigned to a patient, it is unlikely to assign the code “464.01 (acute laryngitis with obstruction)” to the patient at the same time. If automatic ICD coding models ignore such a characteristic, they are prone to giving inconsistent predictions. Thus, a challenging problem is how to model the code hierarchy and use it to capture the mutual exclusion of codes. Code Co-occurrence: Since some diseases are concurrent or have a causal relationship with each other, their codes usually co-occur in the cl"
2020.acl-main.282,K17-1045,0,0.0287217,"cessing (NLP), the hyperbolic representation has been successfully applied to question answering (Tay et al., 2018), machine translation (Gulcehre et al., 2018) and sentence representation (Dhingra et al., 2018). To our knowledge, this is the first work to apply hyperbolic representation method to the automatic ICD coding task. Graph Convolutional Networks. GCN (Kipf and Welling, 2016) is a powerful neural network, which operates on graph data. It yields substantial improvements over various NLP tasks such as semantic role labeling (Marcheggiani and Titov, 2017), multi-document summarization (Yasunaga et al., 2017) and machine translation (Bastings et al., 2017). Veliˇckovi´c et al. (2017) propose graph attention networks (GAT) to summarize neighborhood features by using masked self-attentional layers. We are the first to capture the code co-occurrence characteristic via the GCN for the automatic ICD coding task. 3 Method We propose a hyperbolic and co-graph representation (HyperCore) model for automatic ICD coding. Firstly, to capture the code hierarchy, we learn the code hyperbolic representations and measure the similarities between document and codes in the hyperbolic space. Secondly, to exploit cod"
2020.acl-main.282,N18-1100,0,0.245413,"alized ICD coding skills can handle the task. However, it is hard to train such an eligible ICD coder. Second, it is difficult to correctly assign proper codes to the input document even for professional coders, because one document can be assigned multiple ICD codes and the number of codes in the taxonomy of ICD is large. For example, there are over 15,000 and 60,000 codes respectively in the ninth version (ICD-9) and the tenth version (ICD-10) of ICD taxonomies. To reduce human labor and coding errors, many methods have been carefully designed for automatic ICD coding (Perotte et al., 2013; Mullenbach et al., 2018). For example in Figure 1, given the clinical text of a patient, the ICD coding model needs to automatically predict the corresponding ICD codes. The automatic ICD coding task can be modeled as a multi-label classification task since each clinical text is usually accompanied by mul3105 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3105–3114 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ICD-9 Descriptor Hierarchical Structure 460-519 - DISEASES OF THE RESPIRATORY SYSTEM 460-519 460 - Acute nasopharyngitis 461 - Acute sinusit"
2020.acl-main.282,D18-1352,0,0.0115341,"sopharyngitis 461 - Acute sinusitis 461.0 - Maxillary 460 461 462 463 464 461.1 - Frontal 464 - Acute laryngitis and tracheitis 464.0 - Acute laryngitis 461.0 464.0 461.1 464.1 464.00 - Without mention of obstruction 464.01 - With obstruction 464.00 464.01 464.1 - Acute tracheitis Figure 2: An example of ICD-9 descriptors and the derived hierarchical structure. tiple codes. Most of the previous methods handle each code in isolation and convert the multi-label problem into a set of binary classification problems to predict whether each code of interest presents or not (Mullenbach et al., 2018; Rios and Kavuluru, 2018). Though effective, they ignore two important characteristics: Code Hierarchy and Code Co-occurrence, which can be leveraged to improve coding accuracy. In the following, we will introduce the two characteristics and the reasons why they are critical for the automatic ICD coding. Code Hierarchy: Based on ICD taxonomy, ICD codes are organized under a tree-like hierarchical structure as shown in Figure 2, which indicates the parent-child and sibling relations between codes. In the hierarchical structure, the upper level nodes represent more generic disease categories and the lower level nodes re"
2020.acl-main.572,N18-1020,0,0.154092,"mechanisms of entity type inference with local typing knowledge and global triple knowledge. Adam1679/ConnectE 1 Introduction The past decade has witnessed great thrive in building web-scale knowledge graphs (KGs), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), Google Knowledge Graph (Dong et al., 2014), which usually consists of a huge amount of triples in the form of (head entity, relation, tail entity) (denoted (e, r, e˜)). KGs usually suffer from incompleteness and miss important facts, jeopardizing their usefulness in downstream tasks such as question answering (Elsahar et al., 2018), semantic parsing (Berant et al., 2013), relation classification (Zeng et al., 2014). Hence, the task of ∗ Equal Contribution. Corresponding author: Y. Zhao (zhaoyu@swufe.edu.cn). knowledge graph completion (KGC, i.e. completing knowledge graph entries) is extremely significant and attracts wide attention. This paper concentrates on KG entity typing, i.e. inferring missing entity type instances in KGs, which is an important sub-problem of KGC. Entity type instances, each of which is in the formed of (entity, entity type) (denoted (e, t)), are essential entries of KGs and widely used in many N"
2020.acl-main.572,D17-1284,0,0.0610199,"Missing"
2020.acl-main.572,D13-1029,0,0.0547621,"Missing"
2020.acl-main.572,E17-1075,0,0.0180144,"can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity type instances N/A entity type instance N/A mixed triple knowledge syn. entity type inst./ triple know. asyn. mixed triple knowledge syn. PEM (Neelakantan et al., 2015) e UV t N/A e ∈ Rκ , t ∈ R` , U ∈ Rκ×d ,V ∈ R`×d RESCAL (Nickel et al., 2011) N/A e> Mr e ˜ e, e ˜ ∈ Rκ , Mr ∈ Rκ×κ RESCAL-ET (Moon et al., 2017) HOLE (Nickel et al., 2016) HOLE-ET (Moon et al., 2017) TransE (Bordes et al."
2020.acl-main.572,D13-1160,0,0.150146,"Missing"
2020.acl-main.572,P18-1009,0,0.0567397,"and relationships from KGs. A combination of both models are utilized to conduct entity type inference. • We conduct empirical experiments on two real-world datasets for entity type inference, which demonstrate our model can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity type instances N/A entity type instance N/A mixed triple knowledge syn. entity type inst./ triple know. asyn. mixed triple knowledge syn. PEM (Neelakantan et al.,"
2020.acl-main.572,P18-1011,0,0.0462672,"Missing"
2020.acl-main.572,P18-2013,0,0.0303936,"Missing"
2020.acl-main.572,C18-1024,0,0.13223,"Missing"
2020.acl-main.572,D18-1222,0,0.078398,"Missing"
2020.acl-main.572,P19-1466,0,0.0388457,"Missing"
2020.acl-main.572,N15-1054,0,0.151895,"Missing"
2020.acl-main.572,D18-1231,0,0.104015,"chanisms, we propose two novel embedding-based models: one for predicting entity types given entities and another one to encode the interactions among entity types and relationships from KGs. A combination of both models are utilized to conduct entity type inference. • We conduct empirical experiments on two real-world datasets for entity type inference, which demonstrate our model can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity t"
2020.acl-main.572,P12-1059,0,0.0293083,"experiments on two real-world datasets for entity type inference, which demonstrate our model can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity type instances N/A entity type instance N/A mixed triple knowledge syn. entity type inst./ triple know. asyn. mixed triple knowledge syn. PEM (Neelakantan et al., 2015) e UV t N/A e ∈ Rκ , t ∈ R` , U ∈ Rκ×d ,V ∈ R`×d RESCAL (Nickel et al., 2011) N/A e> Mr e ˜ e, e ˜ ∈ Rκ , Mr ∈ Rκ×κ RESCAL-ET"
2020.acl-main.572,N18-1002,0,0.034425,"from KGs. A combination of both models are utilized to conduct entity type inference. • We conduct empirical experiments on two real-world datasets for entity type inference, which demonstrate our model can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity type instances N/A entity type instance N/A mixed triple knowledge syn. entity type inst./ triple know. asyn. mixed triple knowledge syn. PEM (Neelakantan et al., 2015) e UV t N/A e ∈ R"
2020.acl-main.572,P15-2048,0,0.0166316,"ions among entity types and relationships from KGs. A combination of both models are utilized to conduct entity type inference. • We conduct empirical experiments on two real-world datasets for entity type inference, which demonstrate our model can successfully take into account global triple information to improve KG entity typing. 2 Related Works Entity typing is valuable for many NLP tasks (Yaghoobzadeh et al., 2018), such as knowledge base population (Zhou et al., 2018), question answering (Elsahar et al., 2018), etc. In recent years, researchers attempt to mine fine-grained entity types (Yogatama et al., 2015; Choi et al., 2018; Xu and Barbosa, 2018; Yuan and Downey, 2018) with external text information, such as web search query logs (Pantel et al., 2012), the textual surface patterns (Yao et al., 2013), context representation (Abhishek et al., 2017), Wikipedia (Zhou et al., 6420 Table 1: Entity type embedding models. Models Energy function Se2t (e, t) Striple (·) e> t LM (Neelakantan et al., 2015) Parameters Sources Training strategy N/A e, t ∈ Rκ entity type instances N/A entity type instance N/A mixed triple knowledge syn. entity type inst./ triple know. asyn. mixed triple knowledge syn. PEM (N"
2020.acl-main.572,C14-1220,1,0.811426,"Missing"
2020.acl-main.576,P19-1087,0,0.283045,"tors. Furthermore, EMRs allow medical researchers to investigate the implicit contents included, such as epidemiologic study and patient cohorts finding. ⇤ Contribution during internship at Institute of Automation, Chinese Academy of Sciences. 1 Data and codes are available at https://github. com/nlpir2020/MIE-ACL-2020. Extracting information from medical dialogues is an emerging research field, and there are only few previous attempts. Finley et al. (2018) proposed an approach that consists of five stages to convert a clinical conversation to EMRs, but they do not describe the detail method. Du et al. (2019) also focused on extracting information from medical dialogues, and successfully defined a new task of extracting 186 symptoms and their corresponding status. The symptoms were relatively comprehensive, but they did not concern other key information like surgeries or tests. Lin et al. (2019) collected online medical dialogues to perform symptom recognition and symptom inference, i.e., inference the status of the recognized symptoms. They also used the sequential labeling method, incorporated global attention and introduced a static symptom graph. There are two main distinctive challenges for t"
2020.acl-main.576,N18-5003,0,0.121723,"round the world. Compared with conventional medical records, EMRs are easy to save and retrieve, which bring considerable convenience for both patients and doctors. Furthermore, EMRs allow medical researchers to investigate the implicit contents included, such as epidemiologic study and patient cohorts finding. ⇤ Contribution during internship at Institute of Automation, Chinese Academy of Sciences. 1 Data and codes are available at https://github. com/nlpir2020/MIE-ACL-2020. Extracting information from medical dialogues is an emerging research field, and there are only few previous attempts. Finley et al. (2018) proposed an approach that consists of five stages to convert a clinical conversation to EMRs, but they do not describe the detail method. Du et al. (2019) also focused on extracting information from medical dialogues, and successfully defined a new task of extracting 186 symptoms and their corresponding status. The symptoms were relatively comprehensive, but they did not concern other key information like surgeries or tests. Lin et al. (2019) collected online medical dialogues to perform symptom recognition and symptom inference, i.e., inference the status of the recognized symptoms. They als"
2020.acl-main.576,D19-1508,0,0.193145,"b. com/nlpir2020/MIE-ACL-2020. Extracting information from medical dialogues is an emerging research field, and there are only few previous attempts. Finley et al. (2018) proposed an approach that consists of five stages to convert a clinical conversation to EMRs, but they do not describe the detail method. Du et al. (2019) also focused on extracting information from medical dialogues, and successfully defined a new task of extracting 186 symptoms and their corresponding status. The symptoms were relatively comprehensive, but they did not concern other key information like surgeries or tests. Lin et al. (2019) collected online medical dialogues to perform symptom recognition and symptom inference, i.e., inference the status of the recognized symptoms. They also used the sequential labeling method, incorporated global attention and introduced a static symptom graph. There are two main distinctive challenges for tackling doctor-patient dialogues: a) Oral expres6460 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6460–6469 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Dialogue Window Annotated Labels Symptom: Premature beat (doctor-p"
2020.acl-main.576,P16-1101,0,0.0284819,"nt dialogues from a Chinese medical consultation website, Chunyu-Doctor. The dialogues are already in text format. We select cardiology topic consultations, since there are more inquiries, while dialogues of other topics often depend more on tests. A typical consultation dialogue is illustrated in Figure 1. The principle of the annotation is to label useful information as comprehensive as possible. A commonly utilized annotation paradigm is sequential labeling, where the medical entities are labeled using BIO tags (Du et al., 2019; Lin et al., 2019; Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016). However, such annotation methods cannot label information that a) expressed by multiple turns and b) not explicitly or not consecutively expressed. Such situations are not rare in spoken dialogues, as can be seen in Figure 1. To this end, we use a window-to-information annotation method instead of sequential labeling. As listed in Table 1, we define four main categories, and for each category, we further define frequent items. The item quantity of symptom, surgery, test and other info is 45, 4, 16 and 6, respectively. In medical dialogues, status is quite The most similar work to ours is (Li"
2020.acl-main.576,P17-1163,0,0.0404605,"Missing"
2020.acl-main.576,P17-1046,0,0.0272872,"of the windows that belong to the same dialogue. For labels that are mutually exclusive, we update the old labels with the latest ones. Then we evaluate the results of each dialogue, and finally report the micro-average of all the test dialogues. 4 Our Approach In this section, we will elaborate the proposed MIE model, a novel deep matching neural network model. Deep matching models are widely used in multiple natural language processing tasks such as machine reading comprehension (Seo et al., 2017; Yu et al.), question answering (Yang et al., 2016) and dialogue generation (Zhou et al., 2018; Wu et al., 2017). Compared with classification models, matching models are able to introduce more information of the candidate side and promote interaction between both ends. The architecture of MIE is shown in Figure 2. There are four main components, namely encoder module, matching module, aggregate module and scorer module. The input of MIE is a doctor-patient 6463 Medical Dialogue I can&apos;t breathe out. It seems that there is phlegm in my throat. Utterance Encoder Has cardiac ultrasound been done? 1 1 ? 1 ? 1 ? ? 2 2 ? 2 ? 2 Category Encoder Aggregate Module ... Matching Module ... No, what medicine should"
2020.acl-main.576,P18-1103,0,0.0153525,"merge the results of the windows that belong to the same dialogue. For labels that are mutually exclusive, we update the old labels with the latest ones. Then we evaluate the results of each dialogue, and finally report the micro-average of all the test dialogues. 4 Our Approach In this section, we will elaborate the proposed MIE model, a novel deep matching neural network model. Deep matching models are widely used in multiple natural language processing tasks such as machine reading comprehension (Seo et al., 2017; Yu et al.), question answering (Yang et al., 2016) and dialogue generation (Zhou et al., 2018; Wu et al., 2017). Compared with classification models, matching models are able to introduce more information of the candidate side and promote interaction between both ends. The architecture of MIE is shown in Figure 2. There are four main components, namely encoder module, matching module, aggregate module and scorer module. The input of MIE is a doctor-patient 6463 Medical Dialogue I can&apos;t breathe out. It seems that there is phlegm in my throat. Utterance Encoder Has cardiac ultrasound been done? 1 1 ? 1 ? 1 ? ? 2 2 ? 2 ? 2 Category Encoder Aggregate Module ... Matching Module ... No, wha"
2020.ccl-1.84,D17-1209,0,0.0358136,"Missing"
2020.ccl-1.84,D18-1307,0,0.0127475,"an modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), a"
2020.ccl-1.84,D18-1017,1,0.713794,"dding vector of each word sn (ddm ) as sn (ddm ) from the pre-trained embedding. Finally, we obtain Proceedings of the 19th China National Conference on Computational Linguistics, pages 903-914, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics the word representation sequence s = {s1 , ..., sn } of message s and d = {dd1 , ..., ddm } of discourse d corresponding to s. 3.1.3 Word Encoding Inspired by the application of self-attention to multiple tasks (Tan et al., 2018; Cao et al., 2018), we exploit multi-head self-attention encoder to encode input words. The scaled dot-product attention can be described as follows:   QK T √ (Q, K, V ) = softmax V (1) d 20 where Q ∈ RN ×2dimh , K ∈ RN ×2dimh and V ∈ RN ×2dimh are query matrices, keys matrices and value matrices, respectively. In our setting, Q = K = V = s for encoding sentence, and Q = K = V = d for encoding discourse. Multi-head attention first projects the queries, keys, and values h times by using different linear projections. The results of attention are concatenated and once again projected to get the final representat"
2020.ccl-1.84,D11-1027,0,0.0194132,"iscourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interact"
2020.ccl-1.84,Q17-1009,0,0.0191957,"ent network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM framework (Xia and Ding, 2019) to e"
2020.ccl-1.84,W17-0812,0,0.0242857,"ent network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM framework (Xia and Ding, 2019) to e"
2020.ccl-1.84,N19-1179,0,0.173756,"planation detection (CED) which is the fundamental and important subtask of CEA. Syntactic Dependency with Graph Network: Syntactic dependency is a vital linguistic feature for natural language processing (NLP). There are some researches employ syntactic dependency such as retrieving question answering passage assisted with syntactic dependency (Cui et al., 2005), mining opinion with syntactic dependency (Wu et al., 2009) and so on. For tasks related to causal semantics extraction from relevant texts, dependency syntactic information may evoke causal relations between discourse units in text (Gao et al., 2019). And recently, there are some researches (Marcheggiani and Titov, 2017; Zhang et al., 2018) convert the syntactic dependency into a graph with graph convolutional network (GCN) (Kipf and Welling, 2016) to effectively capture the syntactic dependency semantics between words in context, such as a semantic role model with GCN (Marcheggiani and Titov, 2017), a GCN-based model assisted with a syntactic dependency to improving relation extraction (Zhang et al., 2018). In this paper, we capture the salient explanatory semantics based on the syntactic-centric graph. Self-attention Mechanism: Self-att"
2020.ccl-1.84,P16-1135,0,0.0159136,"coherent semantics (Jurafsky, 2010). As shown in Figure 1, M1 can be divided into three discourses, and D2 is the explanation that expresses the reason why it is advantageous for the equipment to operate at these temperatures. CED is important for tasks that require an understanding of textual expression (Son et al., 2018). For example, for question answering, the answers of questions are most likely to be in a group of sentences that contains causal explanations (Oh et al., 2013). Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences (Hidey and McKeown, 2016). Therefore, CED is a problem worthy of further study. The existing methods mostly regard this task as a classification problem (Son et al., 2018). At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing (Nejat et al., 2017; Jia et al., 2018; Soricut and Marcu, 2003). The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances whic"
2020.ccl-1.84,P18-2070,0,0.0283788,"of sentences that contains causal explanations (Oh et al., 2013). Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences (Hidey and McKeown, 2016). Therefore, CED is a problem worthy of further study. The existing methods mostly regard this task as a classification problem (Son et al., 2018). At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing (Nejat et al., 2017; Jia et al., 2018; Soricut and Marcu, 2003). The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure 1, D2 lacks explicit features such as because of, due to, or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model (Wang et al., 2017) and hierarchical Bi-LSTM model (Son et al., 2018). The Tree-LSTM models learn the relations between words to capture the semantics of discours"
2020.ccl-1.84,D15-1278,0,0.0307231,"Missing"
2020.ccl-1.84,D16-1035,0,0.0279858,"to keywords, the common way is using attention mechanisms to increase the attention weight of them. However, this implicitly learned attention is not very interpretable. Inspired by previous researches (Vashishth et al., 2019; Bastings et al., 2017), we propose a bottom graph-based word-level salient network which merges the syntactic dependency to capture the salient semantics of discourses contained in their keywords. Finally, how to consider the correlation at the discourse level and pay more attention to the discourses that are key to the explanatory semantics? Inspired by previous work (Li et al., 2016), we propose a top attention-based discourse-level salient network to focus on the key discourses in terms of explanatory semantics. In summary, the contributions of this paper are as follows: • We design a Pyramid Salient-Aware Network (PSAN) to detect causal explanations of messages which can effectively learn the pivotal relations between keywords at word level and further filter the key information at discourse level in terms of explanatory semantics. 20 • PSAN can assist in causal explanation detection via capturing the salient semantics of discourses contained in their keywords with a bo"
2020.ccl-1.84,W17-5535,0,0.012278,"ly to be in a group of sentences that contains causal explanations (Oh et al., 2013). Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences (Hidey and McKeown, 2016). Therefore, CED is a problem worthy of further study. The existing methods mostly regard this task as a classification problem (Son et al., 2018). At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing (Nejat et al., 2017; Jia et al., 2018; Soricut and Marcu, 2003). The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure 1, D2 lacks explicit features such as because of, due to, or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model (Wang et al., 2017) and hierarchical Bi-LSTM model (Son et al., 2018). The Tree-LSTM models learn the relations between words to capture the sem"
2020.ccl-1.84,P13-1170,0,0.537816,"Missing"
2020.ccl-1.84,W14-4322,0,0.024862,"ntion-based discourse-level salient network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM fram"
2020.ccl-1.84,D16-1014,0,0.0137524,"semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM framework (Xia and Ding, 2019) to extract emotion-cause pair in mess"
2020.ccl-1.84,P17-2103,0,0.0264943,"the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM framework (Xia and Ding, 2019) to extract emotion-cause pair in messages. Furthermore, Son (2018) defines the causal explanation analysis task (CEA) to extract causal explanatory semantics in messages and annotates a dataset for other downstream tasks. In this paper, we focus on causal explanation detection (CED) which is the fundamental and important subtask of CEA. Syntactic Dependency with"
2020.ccl-1.84,D18-1372,0,0.117529,"relations in messages which explain how the meaning of different textual units can combine to jointly build a discourse meaning for the larger unit. The explanation is an important relation of coherence which refers to the textual unit (e.g. discourse) in a message that expresses explanatory coherent semantics (Jurafsky, 2010). As shown in Figure 1, M1 can be divided into three discourses, and D2 is the explanation that expresses the reason why it is advantageous for the equipment to operate at these temperatures. CED is important for tasks that require an understanding of textual expression (Son et al., 2018). For example, for question answering, the answers of questions are most likely to be in a group of sentences that contains causal explanations (Oh et al., 2013). Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences (Hidey and McKeown, 2016). Therefore, CED is a problem worthy of further study. The existing methods mostly regard this task as a classification problem (Son et al., 2018). At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse"
2020.ccl-1.84,N03-1030,0,0.129378,"contains causal explanations (Oh et al., 2013). Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences (Hidey and McKeown, 2016). Therefore, CED is a problem worthy of further study. The existing methods mostly regard this task as a classification problem (Son et al., 2018). At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing (Nejat et al., 2017; Jia et al., 2018; Soricut and Marcu, 2003). The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure 1, D2 lacks explicit features such as because of, due to, or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model (Wang et al., 2017) and hierarchical Bi-LSTM model (Son et al., 2018). The Tree-LSTM models learn the relations between words to capture the semantics of discourses more accurately but lac"
2020.ccl-1.84,P19-1320,0,0.0132091,"f the 19th China National Conference on Computational Linguistics, pages 903-914, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics Next, we need to consider how to make better use of the information of keywords contained in the syntactic structure. To pay more attention to keywords, the common way is using attention mechanisms to increase the attention weight of them. However, this implicitly learned attention is not very interpretable. Inspired by previous researches (Vashishth et al., 2019; Bastings et al., 2017), we propose a bottom graph-based word-level salient network which merges the syntactic dependency to capture the salient semantics of discourses contained in their keywords. Finally, how to consider the correlation at the discourse level and pay more attention to the discourses that are key to the explanatory semantics? Inspired by previous work (Li et al., 2016), we propose a top attention-based discourse-level salient network to focus on the key discourses in terms of explanatory semantics. In summary, the contributions of this paper are as follows: • We design a Pyr"
2020.ccl-1.84,I17-1050,0,0.0168256,"milar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing (Nejat et al., 2017; Jia et al., 2018; Soricut and Marcu, 2003). The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure 1, D2 lacks explicit features such as because of, due to, or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model (Wang et al., 2017) and hierarchical Bi-LSTM model (Son et al., 2018). The Tree-LSTM models learn the relations between words to capture the semantics of discourses more accurately but lack further understanding of the semantics between discourses. The hierarchical Bi-LSTM models can employ sequence structure to implicitly learn the relations between words and discourses. However, previous work shows that compared with Tree-LSTM, Bi-LSTM lacks a direct understanding of the dependency relations between words. Therefore, the method of implicit learning of inter-word relations is not prominent in the tasks related"
2020.ccl-1.84,D09-1159,0,0.00806823,"defines the causal explanation analysis task (CEA) to extract causal explanatory semantics in messages and annotates a dataset for other downstream tasks. In this paper, we focus on causal explanation detection (CED) which is the fundamental and important subtask of CEA. Syntactic Dependency with Graph Network: Syntactic dependency is a vital linguistic feature for natural language processing (NLP). There are some researches employ syntactic dependency such as retrieving question answering passage assisted with syntactic dependency (Cui et al., 2005), mining opinion with syntactic dependency (Wu et al., 2009) and so on. For tasks related to causal semantics extraction from relevant texts, dependency syntactic information may evoke causal relations between discourse units in text (Gao et al., 2019). And recently, there are some researches (Marcheggiani and Titov, 2017; Zhang et al., 2018) convert the syntactic dependency into a graph with graph convolutional network (GCN) (Kipf and Welling, 2016) to effectively capture the syntactic dependency semantics between words in context, such as a semantic role model with GCN (Marcheggiani and Titov, 2017), a GCN-based model assisted with a syntactic depend"
2020.ccl-1.84,P19-1096,0,0.0195044,"tz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on movie reviews (Ji and Smith, 2017), and a two-step interactive hierarchical Bi-LSTM framework (Xia and Ding, 2019) to extract emotion-cause pair in messages. Furthermore, Son (2018) defines the causal explanation analysis task (CEA) to extract causal explanatory semantics in messages and annotates a dataset for other downstream tasks. In this paper, we focus on causal explanation detection (CED) which is the fundamental and important subtask of CEA. Syntactic Dependency with Graph Network: Syntactic dependency is a vital linguistic feature for natural language processing (NLP). There are some researches employ syntactic dependency such as retrieving question answering passage assisted with syntactic depen"
2020.ccl-1.84,D17-1004,0,0.0245705,"nt network. Furthermore, PSAN can modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. 2 Related Works 20 • Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. Our experiments also prove the effectiveness of each module. CC L Causal Semantic Detection: Recently, causality detection which detects specific causes and effects and the relations between them has received more attention, such as the researches proposed by Li (Li and Mao, 2019), Zhang (Zhang et al., 2017), Bekoulis (Bekoulis et al., 2018), Do (Do et al., 2011), Riaz (Riaz and Girju, 2014), Dunietz (Dunietz et al., 2017a) and Sharp (Sharp et al., 2016). Specifically, to extract the causal explanation semantics from the messages in a general level, some researches capture the causal semantics in messages from the perspective of discourse structure, such as capturing counterfactual conditionals from a social message with the PDTB discourse relation parsing (Son et al., 2017), a pre-trained model with Rhetorical Structure Theory Discourse Treebank (RSTDT) for exploiting discourse structures on mov"
2020.ccl-1.84,D18-1244,0,0.0936203,"Dependency with Graph Network: Syntactic dependency is a vital linguistic feature for natural language processing (NLP). There are some researches employ syntactic dependency such as retrieving question answering passage assisted with syntactic dependency (Cui et al., 2005), mining opinion with syntactic dependency (Wu et al., 2009) and so on. For tasks related to causal semantics extraction from relevant texts, dependency syntactic information may evoke causal relations between discourse units in text (Gao et al., 2019). And recently, there are some researches (Marcheggiani and Titov, 2017; Zhang et al., 2018) convert the syntactic dependency into a graph with graph convolutional network (GCN) (Kipf and Welling, 2016) to effectively capture the syntactic dependency semantics between words in context, such as a semantic role model with GCN (Marcheggiani and Titov, 2017), a GCN-based model assisted with a syntactic dependency to improving relation extraction (Zhang et al., 2018). In this paper, we capture the salient explanatory semantics based on the syntactic-centric graph. Self-attention Mechanism: Self-attention has been introduced to machine translation by Vaswani (Vaswani et al., 2017) for capt"
2020.ccl-1.86,D18-1017,1,0.770613,"the LSTM. Peters et al. (2017) leverage a character language model to enhance the input of the model. For Chinese NER, character-based methods have been the dominant approaches (Lu et al., 2016; Dong et al., 2016). These methods only focus on character sequence information, ignoring word boundaries information, which can cause errors of predicting entity boundaries. Thus, how to better exploit lexical knowledge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as well as words that match a lexicon. However, the lattice model cannot exploit all matched words and only processes the matched words once. Recently, graph-based models have been proposed for Chinese NER (Gui et al., 2019; Sui et al., 2019). Based on the lattice structure, Sui et al. (2019) propose a graph neural network to encode word information. Tag dependencies is also a challenging problem, but few attention has been pa"
2020.ccl-1.86,N13-1006,0,0.0660884,"Missing"
2020.ccl-1.86,W06-0130,0,0.0670738,"Missing"
2020.ccl-1.86,P15-1017,1,0.880678,"Missing"
2020.ccl-1.86,D19-1096,0,0.0157827,"it lexical knowledge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as well as words that match a lexicon. However, the lattice model cannot exploit all matched words and only processes the matched words once. Recently, graph-based models have been proposed for Chinese NER (Gui et al., 2019; Sui et al., 2019). Based on the lattice structure, Sui et al. (2019) propose a graph neural network to encode word information. Tag dependencies is also a challenging problem, but few attention has been paid to tackling the problem. Zhang et al. (2018) leverages LSTM as decoder for sequence labeling task. However, the unidirectional LSTM decoder only exploits the past predicted tags information, ignoring the future un-predicted tags. Hence, we propose a hierarchical tagging mechanism to capture bidirectional tag dependencies in the whole sentence. To our best knowledge, we are the first to i"
2020.ccl-1.86,C02-1054,0,0.432472,"ethods. CC L The task of named entity recognition (NER) is to recognize the named entities from a plain text and classify them into pre-defined types. NER is a fundamental and preliminary task in natural language processing (NLP) area and is beneficial for many downstream NLP tasks such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yahya et al., 2013). In recent years, numerous methods have been carefully studied for NER task, including Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002). Currently, with the development of deep learning methods, neural networks have been introduced for the NER task. In particular, sequence labeling neural network models have achieved state-of-the-art performance (Lample et al., 2016; Zhang and Yang, 2018). Though sequence labeling neural network methods have achieved great success for Chinese NER task, some challenging issues still have not been well addressed. One significant drawback is that previous methods usually fail to correctly predict entity boundaries. To conduct a quantitative analysis, we perform a BiLSTM+CRF model proposed by Hua"
2020.ccl-1.86,N16-1030,0,0.147027,"is beneficial for many downstream NLP tasks such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yahya et al., 2013). In recent years, numerous methods have been carefully studied for NER task, including Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002). Currently, with the development of deep learning methods, neural networks have been introduced for the NER task. In particular, sequence labeling neural network models have achieved state-of-the-art performance (Lample et al., 2016; Zhang and Yang, 2018). Though sequence labeling neural network methods have achieved great success for Chinese NER task, some challenging issues still have not been well addressed. One significant drawback is that previous methods usually fail to correctly predict entity boundaries. To conduct a quantitative analysis, we perform a BiLSTM+CRF model proposed by Huang et al. (2015), which is the most representative Chinese NER sequence labeling system, on WeiboNER dataset (Peng and Dredze, 2015; He and Sun, 2016), OntoNotes 4 dataset (Weischedel et al., 2011) and MSRA dataset (Levow, 2006). The"
2020.ccl-1.86,W06-0115,0,0.105309,"Missing"
2020.ccl-1.86,L16-1138,0,0.0137114,"long short term memory (BiLSTM) for feature extraction and the CRF for decoding. The model is trained via the end-to-end paradigm. After that, the BiLSTM+CRF model is usually exploited as the baseline model for NER task. Ma and Hovy (2016) use a character convolutional neural network (CNN) to represent spelling characteristic. Then the charcter representation vector is concatenated with word embedding as the input of the LSTM. Peters et al. (2017) leverage a character language model to enhance the input of the model. For Chinese NER, character-based methods have been the dominant approaches (Lu et al., 2016; Dong et al., 2016). These methods only focus on character sequence information, ignoring word boundaries information, which can cause errors of predicting entity boundaries. Thus, how to better exploit lexical knowledge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as wel"
2020.ccl-1.86,P18-1230,0,0.153362,"g passes on the matched words to better learn lexical knowledge in complex sentences intuitively. Take the sentence “南京市长江大桥 (Nanjing Yangtze River Bridge)” for example, it is more complicated than the sentence in Figure 1 because it is prone to be misunderstood as “南京市长/江大桥 (The mayor of Nanjing is Jiang Daqiao)”. Thus, it needs more reasoning passes to learn the lexical knowledge for recognizing the entity “长江大桥 (Yangtze River Bridge)” than the entity “北海道 (Hokkaido)” in Figure 1. However, if the reasoning passes are too many, the performance will decrease in word sense disambiguation task (Luo et al., 2018). We argue that the problem also exists in Chinese NER task. Hence, how to exploit all matched words and perform flexible multi-pass reasoning according to the complexity of sentences should be well investigated. Another issue is that most of the existing methods cannot efficiently capture tag dependencies. In sequence labeling neural network models, CRF is usually used as a decoding layer. Although the CRF decoder has achieved improvements, the transition matrix in CRF layer only learns the neighboring tag dependencies, which are typically first order dependencies (Zhang et al., 2018). Thus,"
2020.ccl-1.86,P16-1101,0,0.0346172,") and SVMs (Isozaki and Kazawa, 2002). These methods rely heavily on feature engineering. However, the designed features may be not appropriate for the task, which can lead to error propagation problem. Currently, neural network methods have been introduced into NER task and achieved state-of-the-art performance (Lample et al., 2016). Huang et al. (2015) use the bidirectional long short term memory (BiLSTM) for feature extraction and the CRF for decoding. The model is trained via the end-to-end paradigm. After that, the BiLSTM+CRF model is usually exploited as the baseline model for NER task. Ma and Hovy (2016) use a character convolutional neural network (CNN) to represent spelling characteristic. Then the charcter representation vector is concatenated with word embedding as the input of the LSTM. Peters et al. (2017) leverage a character language model to enhance the input of the model. For Chinese NER, character-based methods have been the dominant approaches (Lu et al., 2016; Dong et al., 2016). These methods only focus on character sequence information, ignoring word boundaries information, which can cause errors of predicting entity boundaries. Thus, how to better exploit lexical knowledge has"
2020.ccl-1.86,D15-1064,0,0.165739,"sentation vector is concatenated with word embedding as the input of the LSTM. Peters et al. (2017) leverage a character language model to enhance the input of the model. For Chinese NER, character-based methods have been the dominant approaches (Lu et al., 2016; Dong et al., 2016). These methods only focus on character sequence information, ignoring word boundaries information, which can cause errors of predicting entity boundaries. Thus, how to better exploit lexical knowledge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as well as words that match a lexicon. However, the lattice model cannot exploit all matched words and only processes the matched words once. Recently, graph-based models have been proposed for Chinese NER (Gui et al., 2019; Sui et al., 2019). Based on the lattice structure, Sui et al. (2019) propose a graph neural network to encode word information. Tag dependen"
2020.ccl-1.86,P16-2025,0,0.0178452,"embedding as the input of the LSTM. Peters et al. (2017) leverage a character language model to enhance the input of the model. For Chinese NER, character-based methods have been the dominant approaches (Lu et al., 2016; Dong et al., 2016). These methods only focus on character sequence information, ignoring word boundaries information, which can cause errors of predicting entity boundaries. Thus, how to better exploit lexical knowledge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as well as words that match a lexicon. However, the lattice model cannot exploit all matched words and only processes the matched words once. Recently, graph-based models have been proposed for Chinese NER (Gui et al., 2019; Sui et al., 2019). Based on the lattice structure, Sui et al. (2019) propose a graph neural network to encode word information. Tag dependencies is also a challenging problem, but few"
2020.ccl-1.86,D19-1396,1,0.790402,"ge has received much research attention. Word segmentation information is used as extra features for Chinese NER task (Peng and Dredze, 2015; He and Sun, 2016). Peng and Dredze (2016) and Cao et al. (2018) propose a joint model for Chinese NER, which is jointly trained with CWS task. Zhang and Yang (2018) investigate a lattice LSTM to encode a sequence of input characters as well as words that match a lexicon. However, the lattice model cannot exploit all matched words and only processes the matched words once. Recently, graph-based models have been proposed for Chinese NER (Gui et al., 2019; Sui et al., 2019). Based on the lattice structure, Sui et al. (2019) propose a graph neural network to encode word information. Tag dependencies is also a challenging problem, but few attention has been paid to tackling the problem. Zhang et al. (2018) leverages LSTM as decoder for sequence labeling task. However, the unidirectional LSTM decoder only exploits the past predicted tags information, ignoring the future un-predicted tags. Hence, we propose a hierarchical tagging mechanism to capture bidirectional tag dependencies in the whole sentence. To our best knowledge, we are the first to introduce the hierar"
2020.ccl-1.86,N16-1174,0,0.0318534,"ed tagging vector. 20 Tagging Attention Module: T-Attention Tagging attention aims to dynamically leverage the hidden ˆ 1, h ˆ 2, . . . , h ˆ n } and Traw = {Tb1 , Tb2 , . . . , Tbn } b = {h states and preliminary predictions of the TLSTM. H denote the hidden states and preliminary predictions of the TLSTM, respectively. The attention is expressed as follows: ˆ di = [h ˆ i : Tbi ] h ˆ di + bda ) mi = uT tanh(Wda h d exp(mi ) αi = Pn j=1 exp(mj ) Xn ˆ dj ) ri = tanh( αj h (10) CC L j=1 Rdda where ud ∈ is the context vector, which is randomly initialized and learned during the training process (Yang et al., 2016b). ri denotes the representation of the hidden states and preliminary predictions of the TLSTM. The Second Tagging Module: CRF H = {h1 , h2 , . . . , hn } and R = {r1 , r2 , . . . , rn } denote the outputs of BiLSTM encoding layer and tagging attention module, respectively, which are concatenated as the input of the CRF module, denoted as Hc = {hc1 , hc2 , . . . , hcn }. Given a sentence s = {x1 , x2 , . . . , xn } with a final predicted tag sequence y = {y1 , y2 , . . . , yn }, the CRF tagging process is formalized as follows: oi = Wo hci + bo Xn s(s, y) = (oi,yi + Tyi−1 ,yi ) i=1 (11) y ∗ ="
2020.ccl-1.86,P18-1144,0,0.19628,"” in Figure 1. To reduce the errors of predicting entity boundaries, some works (Peng and Dredze, 2016; Cao et al., 2018) try to jointly perform Chinese NER with Chinese word segmentation (CWS) for using word boundaries information. However, the joint model requires additional annotated training data for CWS task. Fortunately, existing lexicons can provide information on word boundaries and we refer to the information as lexical knowledge. In addition, the cost of obtaining lexicon is low and almost all fields have their lexicons, such as biomedical, social science fields and so on. Recently, Zhang and Yang (2018) propose a lattice LSTM model capable of leveraging lexicon for Chinese NER. Though effective, the lattice LSTM Proceedings of the 19th China National Conference on Computational Linguistics, pages 927-938, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics English Translation: Chinese Sentence: Matched Words: Gold Label: Hokkaido has a variable climate 北 海 道 气 候 多 北海 北海道 海道 气候 多变 North Sea Hokkaido Seaway Climate Change B-LOC I-LOC I-LOC BiLSTM+CRF: B-LOC I-LOC O 变 O O O"
2020.ccl-1.86,W06-0126,0,0.0696592,"Missing"
2020.ccl-1.86,W06-0140,0,0.102478,"Missing"
2020.coling-main.135,2020.acl-main.499,0,0.0216175,"tation ei and ej encoded by BERT. Then, we take the stitching of manual designed feature vector (same lexical, causal potential, and syntactic features representation as Gao et al. (Gao et al., 2019)) f , ei and ej as the input of top MLP classifier. Finally, the output is a binary vector to indicate the causality of the input event pair eij . We employ relabeling and annealing strategies to make better use of distantly labeled data for training. (1) Relabeling: We pre-train a detector on annotated data and employ it to relabel the refined distantly labeled training data Dr via self-training (Asai and Hajishirzi, 2020). Then, we collect the sentences that are relabeled as causal sentences to obtain the distantly relabeled training data Drr which are more casual and informative for the training of ECD task. (2) Annealing: Distantly labeled training data may not be appropriate at the beginning of training for building an effective detector due to noises. Therefore, we employ the annealing training strategy (Kirkpatrick et al., 1983) to maximize the effectiveness of distantly labeled training data. In the beginning, we only employ annotated data for training, and with the increase of epochs, we added Drr for t"
2020.coling-main.135,P98-1013,0,0.388273,"Missing"
2020.coling-main.135,W17-2711,0,0.297622,"hortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks, such as relation extraction (Mintz et al., 2009), event detection (Chen et al., 2017), and so on. Therefore, we investigate a distant data augmentation framework for solving the data lacking problem on the ECD task, dubbed as Knowledge Enhanced Distant Data Augmentation (KnowDis), to au"
2020.coling-main.135,P17-1038,1,0.831098,"hieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks, such as relation extraction (Mintz et al., 2009), event detection (Chen et al., 2017), and so on. Therefore, we investigate a distant data augmentation framework for solving the data lacking problem on the ECD task, dubbed as Knowledge Enhanced Distant Data Augmentation (KnowDis), to automatically label available data. We argue that a sentence contains an event pair with a high probability of causality and expresses its causal semantic can be labeled as training data for the ECD task. To automatically label a large number of training data, we need to solve the following three challenges. (1) How to collect a large number of event pairs with a high probability of causality and"
2020.coling-main.135,P17-2001,0,0.232749,"Missing"
2020.coling-main.135,D17-1190,0,0.14897,"Missing"
2020.coling-main.135,P89-1010,0,0.359682,"Missing"
2020.coling-main.135,N19-1423,0,0.0707252,"Missing"
2020.coling-main.135,N19-1179,0,0.674643,"ju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks, such as relation"
2020.coling-main.135,W03-1210,0,0.525738,"s). Experimental results on two benchmark datasets EventStoryLine corpus and CausalTimeBank show that 1) KnowDis can augment available training data assisted with the lexical and causal commonsense knowledge for ECD via distant supervision, and 2) our method outperforms previous methods by a large margin assisted with automatically labeled training data. 1 Introduction Event causality detection (ECD) aims to identify causal relations between events from texts, which may provide crucial clues for many NLP tasks, such as information extraction, logical reasoning, question answering, and others (Girju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et a"
2020.coling-main.135,P14-1093,0,0.43486,"easoning, question answering, and others (Girju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label trainin"
2020.coling-main.135,W17-5540,0,0.109412,"ring, and others (Girju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks"
2020.coling-main.135,P09-1113,0,0.127371,"2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks, such as relation extraction (Mintz et al., 2009), event detection (Chen et al., 2017), and so on. Therefore, we investigate a distant data augmentation framework for solving the data lacking problem on the ECD task, dubbed as Knowledge Enhanced Distant Data Augmentation (KnowDis), to automatically label available data. We argue that a sentence contains an event pair with a high probability of causality and expresses its causal semantic can be labeled as training data for the ECD task. To automatically label a large number of training data, we need to solve the following three challenges. (1) How to collect a large number of event pairs with"
2020.coling-main.135,C14-1198,0,0.332249,"training data. In the beginning, we only employ annotated data for training, and with the increase of epochs, we added Drr for training incrementally in a proportion of β. 3 Experiments Datasets. (1) ESC: We use the same way to partition dataset as the SOTA method on ESC (Gao et al., 2019). Same as it, we use the last two topics as a development set. (2) Causal-TB: This dataset only contains 318 causal links which can further prove effectiveness of the proposed framework for solving the problem of data lacking. We use the same development set as ESC because of the SOTA method on this dataset (Mirza and Tonelli, 2014) does not partition the development set. Specifically, we conduct 5-fold cross-validation on the two datasets3 . We tune the augmented proportion, α, and β on the development set. All the results are the average of three independent experiments. 3 For each fold, we add extra distantly labeled data based on the annotated event pairs corresponding to this fold for training. 1546 Parameters Setting. We apply the base-uncase-bert as the pre-trained BERT model. We set the learning rate of detector as 1e-5. Specifically, the dimension of the causal semantic space is 100. We set the α and β as 0.5 an"
2020.coling-main.135,C16-1007,0,0.0730174,"asks, such as information extraction, logical reasoning, question answering, and others (Girju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision hav"
2020.coling-main.135,P13-1170,0,0.356303,"Missing"
2020.coling-main.135,W14-0707,0,0.474301,"extraction, logical reasoning, question answering, and others (Girju, 2003; Oh et al., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be ef"
2020.coling-main.135,D19-1670,0,0.063133,"Missing"
2020.coling-main.135,2020.ccl-1.84,1,0.747803,"., 2013; Oh et al., 2017). For example, the causal relation that Kimani Gray was killed because of a police attack is needed to be detected in the following sentence: ”Kimani Gray, a young man who likes football, was killed in a police attack shortly after a tight match.” This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. To this end, most existing methods adopt a supervised learning paradigm (Mirza and Tonelli, 2016; Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019; Zuo et al., 2020). Although these methods have achieved good performance, they usually need large-scale annotated training data. However, existing event causality detection datasets are relatively small. For example, the EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. These small datasets are in low coverage of causal expressions and obstacle NLP applications deployed on large-scale data. Recent improvements of distant supervision have been proven to be effective to label training data for some tasks, such as relation extraction (Mintz"
2020.coling-main.219,D18-1454,0,0.0211812,"idering its diversity in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) combines a pre-trained language model (Devlin et al., 2019) with an utterance-level pre-training strategy. Knowledge Incorporation for MRC. Integrating background knowledge to enhance machine reading is a longstanding goal of artificial intelligence. In the task of MRC, previous studies (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Qiu et al., 2019) have exploited 2426 Figure 1: The overview of our approach, which structures the dialogue as a relational graph and integrates co-reference and relation knowledge for reasoning. external knowledge. While, such work may not be applied to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate different types of knowledge. Graph Representa"
2020.coling-main.219,K17-1023,0,0.150496,"can be divided as HD and HQ to indicate dialogue-specific and question-specific representations. HD is used to initialize the node representations in the relational graph. 3.2 Graph-Based Knowledge Integration Graph-based knowledge integration involves relational graph construction, knowledge integration via graph convolution, and representation fusion. Relational Graph Construction. We first organize dialogue contexts as a “relational graph”, where the nodes correspond to words in D, and the edges reflect their relationships. We consider two types of relationships: 1) co-reference knowledge (Chen et al., 2017), which designates expressions referring to the same entity, and 2) relation knowledge (Yu et al., 2020), which reflects semantic relations between two entities (We refer to § 4.1 for how we obtain such knowledge). A heterogeneous graph is proposed to model the knowledge, which uses different types of edges to indicate different types of knowledge. We also add self-loop edges in the graph to facilitate effective computation (Schlichtkrull et al., 2017). Thus, the total number of different types of edges is 1 + 1 + Nr (self-loop + co-reference + number of semantic relation). Knowledge Integrati"
2020.coling-main.219,N19-1423,0,0.0636018,"ltiple-choice question answering over multi-turn dialogues. Yang and Choi (2019) extend the work of Ma et al. (2018) and propose FriendsQA, a dataset annotated open-domain questions and answers. QA over dialogue is recognized more challenging than general MRC tasks. In our study, we chose FriendsQA as the testbed, considering its diversity in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) combines a pre-trained language model (Devlin et al., 2019) with an utterance-level pre-training strategy. Knowledge Incorporation for MRC. Integrating background knowledge to enhance machine reading is a longstanding goal of artificial intelligence. In the task of MRC, previous studies (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Qiu et al., 2019) have exploited 2426 Figure 1: The overview of our approach, which structures the dialogue as a relational graph and integrates co-reference and relation knowledge for reasoning. external knowledge. While, such work may not be applied to QA over dialogue, whose c"
2020.coling-main.219,D19-1096,0,0.0154815,"d to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate different types of knowledge. Graph Representation Learning. Graph neural networks (GNNs) (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017; Schlichtkrull et al., 2017) provide an effective way to model graph-structure data and show promising results in many NLP problems (Vashishth et al., 2019; Gui et al., 2019; Liu et al., 2019; Qiu et al., 2019). Among all GNNs, Relational Graph Convolution Networks (R-GCNs) (Schlichtkrull et al., 2017) are variations of Graph Convolution Networks (GCNs) that are designed for modeling multi-relation data. To our knowledge, this is the first work introducing R-GCNs to model co-reference and relation knowledge for the task of QA over dialogue. 3 Approach Figure 1 schematically visualizes our approach, which involves three major steps: • Joint dialogue-question representation. In this step, the dialogue and question are jointly encoded to build their representations,"
2020.coling-main.219,D17-1215,0,0.0401468,"ter co-reference knowledge that “I” refers to Joey (the speaker) and “her” refers to Kathy. While, how to effectively integrate the background knowledge in this task remains an open question. Existing approaches for this task (Yang and Choi, 2019; Li and Choi, 2020) did not consider background knowledge and only learned reasoning patterns from plain texts. This may expose them at risk of achieving sub-optimal results and becoming vulnerable facing adversarial attacks (i.e., models only learn shallow patterns for reasoning is fragile facing adversarial examples by adding distracting sentences (Jia and Liang, 2017)). In this paper, we propose a new approach for QA over dialogue, featured by its novelty in structuring dialogue and integrating background knowledge — specifically, co-reference and relation knowledge — for reasoning. Different from previous “structure-less” methods, our approach structures the dialogue as a “relational graph”, where nodes correspond to words in contexts and edges designate their relationships. The graph uses different types of edges to indicate different types of relations and thus is a heterogeneous graph. To encode this graph, we devise a model based on relational graph c"
2020.coling-main.219,2020.acl-main.505,0,0.224897,"ommons.org/licenses/by/4.0/. License details: http:// 2425 Proceedings of the 28th International Conference on Computational Linguistics, pages 2425–2435 Barcelona, Spain (Online), December 8-13, 2020 example. To find the correct answer, a QA system should not only locate the evidence sentence “I’am having a late dinner with her” (in U4) but also master co-reference knowledge that “I” refers to Joey (the speaker) and “her” refers to Kathy. While, how to effectively integrate the background knowledge in this task remains an open question. Existing approaches for this task (Yang and Choi, 2019; Li and Choi, 2020) did not consider background knowledge and only learned reasoning patterns from plain texts. This may expose them at risk of achieving sub-optimal results and becoming vulnerable facing adversarial attacks (i.e., models only learn shallow patterns for reasoning is fragile facing adversarial examples by adding distracting sentences (Jia and Liang, 2017)). In this paper, we propose a new approach for QA over dialogue, featured by its novelty in structuring dialogue and integrating background knowledge — specifically, co-reference and relation knowledge — for reasoning. Different from previous “s"
2020.coling-main.219,D19-1068,1,0.739977,"gue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate different types of knowledge. Graph Representation Learning. Graph neural networks (GNNs) (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017; Schlichtkrull et al., 2017) provide an effective way to model graph-structure data and show promising results in many NLP problems (Vashishth et al., 2019; Gui et al., 2019; Liu et al., 2019; Qiu et al., 2019). Among all GNNs, Relational Graph Convolution Networks (R-GCNs) (Schlichtkrull et al., 2017) are variations of Graph Convolution Networks (GCNs) that are designed for modeling multi-relation data. To our knowledge, this is the first work introducing R-GCNs to model co-reference and relation knowledge for the task of QA over dialogue. 3 Approach Figure 1 schematically visualizes our approach, which involves three major steps: • Joint dialogue-question representation. In this step, the dialogue and question are jointly encoded to build their representations, and the dialogue"
2020.coling-main.219,N18-1185,0,0.0220403,"logue. To our best knowledge, this is the first work introducing R-GCN to QA over dialogue. • We set up a new state-of-the-art performance on the benchmark dataset. Moreover, results of robustness testing suggest that our method is robust against adversarial examples. 2 Related Work QA over Dialogue. QA over dialogue is a specified MRC task (Hermann et al., 2015), which requires a system to answer questions regarding a dialogue. Many recent studies have benchmarked and advanced this task. To name a few, Reddy et al. (2019) introduce CoQA corpus, which measures MRC over oneto-one conversation. Ma et al. (2018) introduce a corpus based on transcripts of a TV show friends and focus on questions whose answers are PERSON entities. Sun et al. (2019) propose DREAM, which focuses on multiple-choice question answering over multi-turn dialogues. Yang and Choi (2019) extend the work of Ma et al. (2018) and propose FriendsQA, a dataset annotated open-domain questions and answers. QA over dialogue is recognized more challenging than general MRC tasks. In our study, we chose FriendsQA as the testbed, considering its diversity in different types of questions. Moreover, the extractive QA style is more suitable th"
2020.coling-main.219,P18-1076,0,0.0162509,"tudy, we chose FriendsQA as the testbed, considering its diversity in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) combines a pre-trained language model (Devlin et al., 2019) with an utterance-level pre-training strategy. Knowledge Incorporation for MRC. Integrating background knowledge to enhance machine reading is a longstanding goal of artificial intelligence. In the task of MRC, previous studies (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Qiu et al., 2019) have exploited 2426 Figure 1: The overview of our approach, which structures the dialogue as a relational graph and integrates co-reference and relation knowledge for reasoning. external knowledge. While, such work may not be applied to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate differe"
2020.coling-main.219,D19-1602,1,0.836923,"y in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) combines a pre-trained language model (Devlin et al., 2019) with an utterance-level pre-training strategy. Knowledge Incorporation for MRC. Integrating background knowledge to enhance machine reading is a longstanding goal of artificial intelligence. In the task of MRC, previous studies (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Qiu et al., 2019) have exploited 2426 Figure 1: The overview of our approach, which structures the dialogue as a relational graph and integrates co-reference and relation knowledge for reasoning. external knowledge. While, such work may not be applied to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate different types of knowledge. Graph Representation Learning. Grap"
2020.coling-main.219,Q19-1016,0,0.0543372,"multi-relation data characteristics of a heterogeneous relational graph representing a dialogue. To our best knowledge, this is the first work introducing R-GCN to QA over dialogue. • We set up a new state-of-the-art performance on the benchmark dataset. Moreover, results of robustness testing suggest that our method is robust against adversarial examples. 2 Related Work QA over Dialogue. QA over dialogue is a specified MRC task (Hermann et al., 2015), which requires a system to answer questions regarding a dialogue. Many recent studies have benchmarked and advanced this task. To name a few, Reddy et al. (2019) introduce CoQA corpus, which measures MRC over oneto-one conversation. Ma et al. (2018) introduce a corpus based on transcripts of a TV show friends and focus on questions whose answers are PERSON entities. Sun et al. (2019) propose DREAM, which focuses on multiple-choice question answering over multi-turn dialogues. Yang and Choi (2019) extend the work of Ma et al. (2018) and propose FriendsQA, a dataset annotated open-domain questions and answers. QA over dialogue is recognized more challenging than general MRC tasks. In our study, we chose FriendsQA as the testbed, considering its diversit"
2020.coling-main.219,P16-1162,0,0.0301898,"learn their joint representations. We adopt the BERT based QA architecture (Devlin et al., 2019) considering its effectiveness. Specifically, given 2427 D and Q, we first construct an input sequence to concatenate them: U1 U2 [CLS], q1 , q2 , . . . , qL , [SEP], [S], s1 , [/S], w11 , w12 , . . . , [S],s2 , [/S], w21 , w22 , . . ., ..., [SEP] | {z } | {z } Q (1) D where [CLS] and [SEP] are special tokens used in BERT; [S] and [/S] are special tokens used by our approach to indicate speakers’ positions. We then split the above sequence into sub-word pieces according to Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and adopt BERT to encode the sequence1 . We take the last hidden layer of BERT as the joint representation of D and Q, denoted as H ∈ RT ×d , where T is the length of the extended input sequence (regarding sub-word pieces), and d is the hidden dimension of BERT. H can be divided as HD and HQ to indicate dialogue-specific and question-specific representations. HD is used to initialize the node representations in the relational graph. 3.2 Graph-Based Knowledge Integration Graph-based knowledge integration involves relational graph construction, knowledge integration via graph convolution, and r"
2020.coling-main.219,Q19-1014,0,0.0592056,"ce on the benchmark dataset. Moreover, results of robustness testing suggest that our method is robust against adversarial examples. 2 Related Work QA over Dialogue. QA over dialogue is a specified MRC task (Hermann et al., 2015), which requires a system to answer questions regarding a dialogue. Many recent studies have benchmarked and advanced this task. To name a few, Reddy et al. (2019) introduce CoQA corpus, which measures MRC over oneto-one conversation. Ma et al. (2018) introduce a corpus based on transcripts of a TV show friends and focus on questions whose answers are PERSON entities. Sun et al. (2019) propose DREAM, which focuses on multiple-choice question answering over multi-turn dialogues. Yang and Choi (2019) extend the work of Ma et al. (2018) and propose FriendsQA, a dataset annotated open-domain questions and answers. QA over dialogue is recognized more challenging than general MRC tasks. In our study, we chose FriendsQA as the testbed, considering its diversity in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) comb"
2020.coling-main.219,P19-1320,0,0.0213463,"h work may not be applied to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous graph to incorporate different types of knowledge. Graph Representation Learning. Graph neural networks (GNNs) (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017; Schlichtkrull et al., 2017) provide an effective way to model graph-structure data and show promising results in many NLP problems (Vashishth et al., 2019; Gui et al., 2019; Liu et al., 2019; Qiu et al., 2019). Among all GNNs, Relational Graph Convolution Networks (R-GCNs) (Schlichtkrull et al., 2017) are variations of Graph Convolution Networks (GCNs) that are designed for modeling multi-relation data. To our knowledge, this is the first work introducing R-GCNs to model co-reference and relation knowledge for the task of QA over dialogue. 3 Approach Figure 1 schematically visualizes our approach, which involves three major steps: • Joint dialogue-question representation. In this step, the dialogue and question are jointly encoded to build thei"
2020.coling-main.219,P17-1018,0,0.0249959,"the original paper. dimension of R-GCN is set as 60, chosen from 50 to 100. The layers of R-GCN, k, is set as 3, chosen from 1 to 5. The learning rate is set as 1.0 × 10−5 . The balance factor is set as 0.5, chosen from 0.1 to 0.9. We use Deep Graph Library (DGL)4 to build the graph and implement graph model. 4.3 Baseline Models We compare our model with the following baseline models: BERT, the standard BERT MRC model. BERTpre is a model that uses dialog contexts to pre-train BERT (Li and Choi, 2020), which corresponds to the best reported method (denoted as SoTA). We also compare with R-Net (Wang et al., 2017), the earlier SoTA model achieving the 1st place on the SQuAD leaderboard, which builds representations for questions and evidence passages via a self-matching mechanism. Our model is denoted as + Graph (e.g., BERTpre + Graph indicates using BERTpre as basic encoding model). 5 Experimental Results The results of comparing our approach with baselines models are shown in Table 3. Here we adopt golden co-reference and relation knowledge to build the relational graph (The results of using system predicted results are shown in § 7.1). From the results, our approach consistently outperforms models w"
2020.coling-main.219,W19-5923,0,0.108698,"tonight?”, the task requires a system to give the correct answer “having a late dinner”. U1 U2 U3 U4 Chandler: Joey: Chandler: Joey: Q1 Q2 Hey-Hey-Hey! Who was that? That would be Casey. We’re going out :::::: tonight. Goin’ out, huh? Wow! Wow! So things didn’t work out with Kathy, huh? Bummer. No. Things are fine with Kathy. [I’m having a late dinner with her :::::: tonight], right after my early dinner with Casey. What is Joey going to do with Kathy tonight? When will Joey have dinner with Casey? ::::: A1 A2 having a late dinner tonight :::::: Table 1: Up: a dialogue from FriendsQA corpus (Yang and Choi, 2019). Down: two related questions with their answers. An evidence sentence for inferring A1 is given in []. Compared with other MRC tasks, QA over dialogue is more challenging (Yang and Choi, 2019) owing to that conversations often involve complex relationships and background knowledge. In detail, studies show that a dialogue with 12 turns contains 6.1 co-reference chains (Zhou and Choi, 2018) and expresses 4.5 relationships (Yu et al., 2020) on the average. Therefore, to excel in this task, a QA system must master background knowledge for reasoning. Let us consider the reasoning process of Q1 in"
2020.coling-main.219,P17-1132,0,0.0303006,"neral MRC tasks. In our study, we chose FriendsQA as the testbed, considering its diversity in different types of questions. Moreover, the extractive QA style is more suitable than the multiple-choice style for building practical QA applications. On this benchmark, the best reported method (Li and Choi, 2020) combines a pre-trained language model (Devlin et al., 2019) with an utterance-level pre-training strategy. Knowledge Incorporation for MRC. Integrating background knowledge to enhance machine reading is a longstanding goal of artificial intelligence. In the task of MRC, previous studies (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Qiu et al., 2019) have exploited 2426 Figure 1: The overview of our approach, which structures the dialogue as a relational graph and integrates co-reference and relation knowledge for reasoning. external knowledge. While, such work may not be applied to QA over dialogue, whose contexts are dynamic. It also worth noting that Qiu et al. (2019) adopt graph structure to model external knowledge, but in their work, the relation is not discerned, with a general “related to” relation. By contrast, our approach uses a heterogeneous gr"
2020.coling-main.219,2020.acl-main.444,0,0.126655,"with Kathy tonight? When will Joey have dinner with Casey? ::::: A1 A2 having a late dinner tonight :::::: Table 1: Up: a dialogue from FriendsQA corpus (Yang and Choi, 2019). Down: two related questions with their answers. An evidence sentence for inferring A1 is given in []. Compared with other MRC tasks, QA over dialogue is more challenging (Yang and Choi, 2019) owing to that conversations often involve complex relationships and background knowledge. In detail, studies show that a dialogue with 12 turns contains 6.1 co-reference chains (Zhou and Choi, 2018) and expresses 4.5 relationships (Yu et al., 2020) on the average. Therefore, to excel in this task, a QA system must master background knowledge for reasoning. Let us consider the reasoning process of Q1 in the above ∗ Equal Contribution This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2425 Proceedings of the 28th International Conference on Computational Linguistics, pages 2425–2435 Barcelona, Spain (Online), December 8-13, 2020 example. To find the correct answer, a QA system should not only locate the evidence sentence “I’am having a late d"
2020.coling-main.219,C18-1003,0,0.0691561,"my early dinner with Casey. What is Joey going to do with Kathy tonight? When will Joey have dinner with Casey? ::::: A1 A2 having a late dinner tonight :::::: Table 1: Up: a dialogue from FriendsQA corpus (Yang and Choi, 2019). Down: two related questions with their answers. An evidence sentence for inferring A1 is given in []. Compared with other MRC tasks, QA over dialogue is more challenging (Yang and Choi, 2019) owing to that conversations often involve complex relationships and background knowledge. In detail, studies show that a dialogue with 12 turns contains 6.1 co-reference chains (Zhou and Choi, 2018) and expresses 4.5 relationships (Yu et al., 2020) on the average. Therefore, to excel in this task, a QA system must master background knowledge for reasoning. Let us consider the reasoning process of Q1 in the above ∗ Equal Contribution This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2425 Proceedings of the 28th International Conference on Computational Linguistics, pages 2425–2435 Barcelona, Spain (Online), December 8-13, 2020 example. To find the correct answer, a QA system should not only"
2020.emnlp-main.128,W06-0901,0,0.357252,"emplates to generate questions, our method can generate questions that are both topic-relevant and context-dependent, which can better instruct an MRC model for question-answering. • We report on state-of-the-art performance on the benchmark EE dataset. Our method also demonstrate promising results in addressing data-low and zero-shot scenarios. 2 Related Work Event Extraction. EE is a crucial IE task that aims to extract event information in texts, which has attracted extensive attention among researchers. Traditional EE methods employ manual-designed features, such as the syntactic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Rec"
2020.emnlp-main.128,W14-3001,0,0.0410834,"Missing"
2020.emnlp-main.128,D14-1159,0,0.0822613,"Missing"
2020.emnlp-main.128,P16-1223,0,0.08691,"Missing"
2020.emnlp-main.128,N19-1423,0,0.0466044,"extraction of role-filler of Instrument is semantically equivalent to the following questionanswering process (as shown in Figure 1 (b)): Q1: What Instrument did the protester use According to the ACE event ontology. 1641 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1641–1651, c November 16–20, 2020. 2020 Association for Computational Linguistics to stab the officer? A1: a paper cutter. 2 This implies new ways to tackle EE, which come with two major advantages: First, by framing EE as MRC, we can leverage the recent advances in MRC (e.g., BERT (Devlin et al., 2019)) to boost EE task, which may greatly strengthen the reasoning process in the model. Second, we may directly leverage the abundant MRC datasets to boost EE, which may relieve the data scarcity problem (This is referred to as cross-domain data augmentation). The second advantage also opens a door for zero-shot EE: for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme"
2020.emnlp-main.128,2020.emnlp-main.49,0,0.114784,"guyen, 2019; Zhang et al., 2019). Despite many advances, as mentioned in Introduction, most previous approaches formulate EE as a classification problem, which usually suffer from the data scarcity problem, and they generally cannot deal with new event types never seen at the training time. MRC for Other Tasks. Our work also relates to works connecting MRC and other tasks, such as relation extraction (Levy et al., 2017; Li et al., 2019b), semantic role labeling (FitzGerald et al., 2018), named entity recognition (Li et al., 2019a), and others (Wu et al., 2019; Gao et al., 2019). Particularly, Du and Cardie (2020) adopt a similar idea to frames EE as MRC. But different from our work, most of the above methods (Levy et al., 2017; Li et al., 2019b; FitzGerald et al., 2018; Du and Cardie, 2020) adopt human-designed, context-independent questions, which may not provide enough contextual evidence for question-answering. Some works indeed do not adopt question-style queries (Li et al., 2019a; Gao et al., 2019). For example, Li et al. (2019a) use “Find organizations in the text” as a query command to find ORGANIZATION entity. The discrepancy between such non-natural “queries” and natural questions in MRC data"
2020.emnlp-main.128,D17-1090,0,0.029054,"directly leverage the abundant MRC datasets to boost EE, which may relieve the data scarcity problem (This is referred to as cross-domain data augmentation). The second advantage also opens a door for zero-shot EE: for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation pro"
2020.emnlp-main.128,N18-1020,0,0.0245017,"tasets to boost EE, which may relieve the data scarcity problem (This is referred to as cross-domain data augmentation). The second advantage also opens a door for zero-shot EE: for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation process, which can generate questions that ar"
2020.emnlp-main.128,P18-1191,0,0.212484,"for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation process, which can generate questions that are both relevant and context-dependent. Specifically, in our approach, we assume that each question can be decomposed as two parts, reflecting query topic and context-related inform"
2020.emnlp-main.128,W19-5932,0,0.14612,"defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation process, which can generate questions that are both relevant and context-dependent. Specifically, in our approach, we assume that each question can be decomposed as two parts, reflecting query topic and context-related information respectively. For example, Q1 ca"
2020.emnlp-main.128,P17-1038,1,0.900128,"sets without using any EE training data. 1 Figure 1: Comparison of the event extraction task and machine reading comprehension task. base augmentation (Ji and Grishman, 2011), document summarization, question answering (Berant et al., 2014), and others. In the current study, EE is mostly formulated as a classification problem, aiming to locate and categorize each event trigger/argument (Ahn, 2006; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016). Despite many advances, classification based methods are data-hungry, which require a great deal of training data to ensure good performance (Chen et al., 2017; Li et al., 2013; Liu et al., 2018a). Moreover, such methods generally cannot deal with new event types never encountered during training time (Huang et al., 2018). Introduction Event extraction (EE), a crucial information extraction (IE) task, aims to extract event information in texts. For example, in a sentence S1 (shown in Figure 1 (a)), an EE system should recognize an Attack event1 , expressed by an event trigger stabbed with four event arguments — Sunday (Role=Time), a protester (Role=Attacker), an officer (Role=Target), and a paper cutter (Role=Instrument). EE is shown to benefit a wi"
2020.emnlp-main.128,P11-1113,0,0.191705,"pic-relevant and context-dependent, which can better instruct an MRC model for question-answering. • We report on state-of-the-art performance on the benchmark EE dataset. Our method also demonstrate promising results in addressing data-low and zero-shot scenarios. 2 Related Work Event Extraction. EE is a crucial IE task that aims to extract event information in texts, which has attracted extensive attention among researchers. Traditional EE methods employ manual-designed features, such as the syntactic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha et al., 2018), Graph Convolutional Neural"
2020.emnlp-main.128,P15-1017,1,0.961366,"tic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha et al., 2018), Graph Convolutional Neural Networks (Liu et al., 2018b, 2019b), and other advanced architectures (Yang and Mitchell, 2016; Liu et al., 2018a, 2019a; Nguyen and Nguyen, 2019; Zhang et al., 2019). Despite many advances, as mentioned in Introduction, most previous approaches formulate EE as a classification problem, which usually suffer from the data scarcity problem, and they generally cannot deal with new event types never seen at the training time. MRC for Other Tasks. Our work also relates to works connecting MRC and other t"
2020.emnlp-main.128,P08-1030,0,0.199329,"method can generate questions that are both topic-relevant and context-dependent, which can better instruct an MRC model for question-answering. • We report on state-of-the-art performance on the benchmark EE dataset. Our method also demonstrate promising results in addressing data-low and zero-shot scenarios. 2 Related Work Event Extraction. EE is a crucial IE task that aims to extract event information in texts, which has attracted extensive attention among researchers. Traditional EE methods employ manual-designed features, such as the syntactic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha"
2020.emnlp-main.128,P11-1115,0,0.118647,"Missing"
2020.emnlp-main.128,J82-2005,0,0.680084,"Missing"
2020.emnlp-main.128,K17-1034,0,0.267082,"for zero-shot EE: for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation process, which can generate questions that are both relevant and context-dependent. Specifically, in our approach, we assume that each question can be decomposed as two parts, reflecting query topic a"
2020.emnlp-main.128,P13-1008,0,0.422567,"MRC model for question-answering. • We report on state-of-the-art performance on the benchmark EE dataset. Our method also demonstrate promising results in addressing data-low and zero-shot scenarios. 2 Related Work Event Extraction. EE is a crucial IE task that aims to extract event information in texts, which has attracted extensive attention among researchers. Traditional EE methods employ manual-designed features, such as the syntactic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha et al., 2018), Graph Convolutional Neural Networks (Liu et al., 2018b, 2019b), and other advanced archi"
2020.emnlp-main.128,P19-1129,0,0.0518452,"Missing"
2020.emnlp-main.128,P10-1081,0,0.356045,"ch can better instruct an MRC model for question-answering. • We report on state-of-the-art performance on the benchmark EE dataset. Our method also demonstrate promising results in addressing data-low and zero-shot scenarios. 2 Related Work Event Extraction. EE is a crucial IE task that aims to extract event information in texts, which has attracted extensive attention among researchers. Traditional EE methods employ manual-designed features, such as the syntactic feature (Ahn, 2006), document-level feature (Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha et al., 2018), Graph Convolutional Neural Networks (Liu et al., 2018b, 2019b), and ot"
2020.emnlp-main.128,D19-1068,1,0.906963,"Missing"
2020.emnlp-main.128,D18-1156,0,0.306091,"Missing"
2020.emnlp-main.128,N16-1034,0,0.790654,"(Ji and Grishman, 2008), entity-level feature (Hong et al., 2011) and other features (Liao and Grishman, 2010; Li et al., 2013) 1642 Figure 2: The overview of the proposed model RCEE. Given S1, RCEE first uses a special query [EVENT] to locate event trigger and predict the type. Then RCEE generates questions for each semantic role related to the predicted event type. Finally, RCEE answers each question and synthesizes all of the answers as the EE result. for the task. Modern EE methods employ neural models, such as Convolutional Neural Networks (Chen et al., 2015), Recurrent Neural Networks (Nguyen et al., 2016; Sha et al., 2018), Graph Convolutional Neural Networks (Liu et al., 2018b, 2019b), and other advanced architectures (Yang and Mitchell, 2016; Liu et al., 2018a, 2019a; Nguyen and Nguyen, 2019; Zhang et al., 2019). Despite many advances, as mentioned in Introduction, most previous approaches formulate EE as a classification problem, which usually suffer from the data scarcity problem, and they generally cannot deal with new event types never seen at the training time. MRC for Other Tasks. Our work also relates to works connecting MRC and other tasks, such as relation extraction (Levy et al.,"
2020.emnlp-main.128,P18-1080,0,0.0395064,"and context-dependent. Specifically, in our approach, we assume that each question can be decomposed as two parts, reflecting query topic and context-related information respectively. For example, Q1 can be decomposed as “What instrument” and “did the protester use to stab the officer?”. To generate the query topic expression, we design a template-based generation method, combining role categorization and interrogative words realization. To generate the more challenging contextdependent expression, we formulate it as an unsupervised translation task (Lample et al., 2018b) (or style transfer (Prabhumoye et al., 2018)), which transforms a descriptive statement into a questionstyle expression, based on in-domain de-noising auto-encoding (Vincent et al., 2008) and crossdomain back-translation (Sennrich et al., 2016). 2 Figure 1 (b) gives another example. Note the training process only needs large volume of descriptive statements and unaligned questionstyle statements. Finally, after the questions are generated, we build a BERT based MRC model (Devlin et al., 2019) to answer each of question and synthesize all of the answers as the result of EE. To evaluate our approach, we have conducted extensive experiment"
2020.emnlp-main.128,P18-2124,0,0.0754648,"Missing"
2020.emnlp-main.128,P16-1009,0,0.265758,"an be decomposed as “What instrument” and “did the protester use to stab the officer?”. To generate the query topic expression, we design a template-based generation method, combining role categorization and interrogative words realization. To generate the more challenging contextdependent expression, we formulate it as an unsupervised translation task (Lample et al., 2018b) (or style transfer (Prabhumoye et al., 2018)), which transforms a descriptive statement into a questionstyle expression, based on in-domain de-noising auto-encoding (Vincent et al., 2008) and crossdomain back-translation (Sennrich et al., 2016). 2 Figure 1 (b) gives another example. Note the training process only needs large volume of descriptive statements and unaligned questionstyle statements. Finally, after the questions are generated, we build a BERT based MRC model (Devlin et al., 2019) to answer each of question and synthesize all of the answers as the result of EE. To evaluate our approach, we have conducted extensive experiments on the benchmark EE datasets, and the experimental results have justified the effectiveness of our approach. Specifically, 1) in the standard evolution, our method attains state-ofthe-art performanc"
2020.emnlp-main.128,W17-2603,0,0.0232458,"the abundant MRC datasets to boost EE, which may relieve the data scarcity problem (This is referred to as cross-domain data augmentation). The second advantage also opens a door for zero-shot EE: for unseen event types, we can list questions defining their schema and use an MRC model to retrieve answers as EE results, instead of obtaining training data for them in advance. To bridge MRC and EE, the key challenge lies in generating relevant questions describing an event scheme (e.g., generating Q1 for Instrument). Note we cannot adopt supervised question generation methods (Duan et al., 2017; Yuan et al., 2017; Elsahar et al., 2018), owing to the lack of aligned question-event pairs. Previous works connecting MRC and other tasks usually adopt humandesigned templates (Levy et al., 2017; FitzGerald et al., 2018; Li et al., 2019b,a; Gao et al., 2019; Wu et al., 2019). For example, in QA-SRL (FitzGerald et al., 2018), the question for a predicate publish is always “Who published something?”, regardless of the contexts. Such questions may not expressive enough to instruct an MRC model to find answers. We overcome the above challenge by proposing an unsupervised question generation process, which can gen"
2020.emnlp-main.247,P19-1470,0,0.0288482,"knowledge nodes, such as the effect on PersonX (e.g., Person X’s heart races), the cause of PerosnX (e.g., X wanted to protect himself), the effect on PersonY (e.g., Y gets hurt) and so on. As those knowledge nodes are also events, there are totally 877,108 hevent, relation, eventi triples. Nevertheless, due to the diversity of real-world events, Atomic cannot cover all the events. Meanwhile, even if the coverage is acceptable for everyday events, the accuracy of event linking (link a certain event text to Atomic) also cannot be ensured. Therefore, we employ the pre-training framework, Comet (Bosselut et al., 2019), which is originally proposed for the task of knowledge base completion. Specifically, Comet is obtained by fine-tuning GPT (Radford et al., 2018) on Atomic. The training task is inputting the start event and the relation Scene Graph Building Having annotated the person roles and obtained relevant knowledge for every event, we build a graph, named “scene graph”, to present a structured description for the scene. We believe that compared with the unstructured text, the graph can provide a more intuitive description from the perspective of the events for the scene. (a) inner-event graph (b) cro"
2020.emnlp-main.247,P17-2097,0,0.121442,"or the answer. 3 Experiments and Analysis 3.1 Datasets and Metrics The datasets we choose are ROCStories (Mostafazadeh et al., 2017) and CosmosQA (Huang et al., 2019). The passages of both the above datasets are narrative. ROCStories: a popular dataset of Story Cloze Test (SCT), annotated by Amazon Mechanical Turk (MTurk) workers based on a collection of short stories. In development and test set, each instance contains a four-sentence passage, and two candidate endings, while the train set only provides the original five-sentence story containing the proper ending. Following previous works (Cai et al., 2017; Chaturvedi et al., 2017; Cui et al., 2019), we take the development set for training and evaluate the performance on the test set. CosmosQA: a recently proposed dataset formulated as multiple choice. The narratives are collected from the Spinn3r Blog dataset (Burton et al., 2009) and annotated by MTurk. We train and validate the model on the train set and the development set, respectively. As the label of the test set is not public, we evaluate our model by submitting the predictions to the official website1 . Evaluation Metrics: As the targets of both the above tests are making a choice amo"
2020.emnlp-main.247,P08-1090,0,0.0219451,"Flow (BiDAF) (Seo et al., 2016) and R-Net (Wang et al., 2017b). Recently, there are some new trends in this field, such as multipassage MRC (Campos et al., 2016), knowledgebased MRC (Ostermann et al., 2018) and multi-hop MRC (Yang et al., 2018; Min et al., 2019). Narrative Comprehension: Understanding narrative is a challenging task in natural language understanding, for the passages contain rich cause and effect relations. A large body of previous works focus on scripts learning (Schank and Abelson, 1977). Some previous works addressed script learning by focusing on the narrative cloze test (Chambers and Jurafsky, 2008). Story Cloze Test (Mostafazadeh et al., 2017) is then introduced as a new evaluation framework, and gains wide attention (Chaturvedi et al., 2017; Zhou et al., 2019b). Besides, recent works present other test frameworks for narrative comprehension, such as multiple choice (Huang et al., 2019) and answer generation (Kocisk´y et al., 2018). Compared with the other complex forms of test, e.g., answer generation, the test frameworks we choose (selecting ending or answer) are more focused on narrative comprehension itself. 5 Conclusion In this paper, we focus on Narrative Machine Reading Comprehen"
2020.emnlp-main.247,D17-1168,0,0.218277,"Experiments and Analysis 3.1 Datasets and Metrics The datasets we choose are ROCStories (Mostafazadeh et al., 2017) and CosmosQA (Huang et al., 2019). The passages of both the above datasets are narrative. ROCStories: a popular dataset of Story Cloze Test (SCT), annotated by Amazon Mechanical Turk (MTurk) workers based on a collection of short stories. In development and test set, each instance contains a four-sentence passage, and two candidate endings, while the train set only provides the original five-sentence story containing the proper ending. Following previous works (Cai et al., 2017; Chaturvedi et al., 2017; Cui et al., 2019), we take the development set for training and evaluate the performance on the test set. CosmosQA: a recently proposed dataset formulated as multiple choice. The narratives are collected from the Spinn3r Blog dataset (Burton et al., 2009) and annotated by MTurk. We train and validate the model on the train set and the development set, respectively. As the label of the test set is not public, we evaluate our model by submitting the predictions to the official website1 . Evaluation Metrics: As the targets of both the above tests are making a choice among the candidates, we use"
2020.emnlp-main.247,P16-1223,0,0.0547277,"Missing"
2020.emnlp-main.247,P17-1168,0,0.0609914,"Missing"
2020.emnlp-main.247,D19-1243,0,0.0328037,"Missing"
2020.emnlp-main.247,Q18-1023,0,0.039482,"Missing"
2020.emnlp-main.247,C18-1149,0,0.0380817,"Missing"
2020.emnlp-main.247,2021.ccl-1.108,0,0.112171,"Missing"
2020.emnlp-main.247,P19-1613,0,0.0180302,"hen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This dataset enlightens a lot of classical MRC model, like Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and R-Net (Wang et al., 2017b). Recently, there are some new trends in this field, such as multipassage MRC (Campos et al., 2016), knowledgebased MRC (Ostermann et al., 2018) and multi-hop MRC (Yang et al., 2018; Min et al., 2019). Narrative Comprehension: Understanding narrative is a challenging task in natural language understanding, for the passages contain rich cause and effect relations. A large body of previous works focus on scripts learning (Schank and Abelson, 1977). Some previous works addressed script learning by focusing on the narrative cloze test (Chambers and Jurafsky, 2008). Story Cloze Test (Mostafazadeh et al., 2017) is then introduced as a new evaluation framework, and gains wide attention (Chaturvedi et al., 2017; Zhou et al., 2019b). Besides, recent works present other test frameworks for narrative"
2020.emnlp-main.247,P15-1121,0,0.0197521,"e that the proper step is the balance point where each event retains its unique information, and at the same time, also gets the associated information from the whole passage. 4 Related Work Machine Reading Comprehension: Due to the fast development of deep learning techniques and large-scale datasets, Machine Reading Comprehension(MRC) has gained increasingly wide attention over the past few years. Richardson et al. (2013) build the multiple-choice dataset MCTest, and this dataset encourages the early research of machine reading comprehension, and a strand of MRC models (Sachan et al., 2015; Narasimhan and Barzilay, 2015) are inspired by the dataset. Hermann et al. (2015) propose a cloze test dataset CNN & Daily Mail, which is large-scale and more suitable than MCTest for deep learning methods. Based on this dataset, Hermann et al. (2015) proposes an attention-based LSTM model named Attentive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This dataset enlightens a lot of class"
2020.emnlp-main.247,L18-1564,0,0.0156299,"es an attention-based LSTM model named Attentive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This dataset enlightens a lot of classical MRC model, like Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and R-Net (Wang et al., 2017b). Recently, there are some new trends in this field, such as multipassage MRC (Campos et al., 2016), knowledgebased MRC (Ostermann et al., 2018) and multi-hop MRC (Yang et al., 2018; Min et al., 2019). Narrative Comprehension: Understanding narrative is a challenging task in natural language understanding, for the passages contain rich cause and effect relations. A large body of previous works focus on scripts learning (Schank and Abelson, 1977). Some previous works addressed script learning by focusing on the narrative cloze test (Chambers and Jurafsky, 2008). Story Cloze Test (Mostafazadeh et al., 2017) is then introduced as a new evaluation framework, and gains wide attention (Chaturvedi et al., 2017; Zhou et al., 2019b). Besides,"
2020.emnlp-main.247,D19-1602,1,0.769548,"ryday commonsense reasoning. Each center node of Atomic is an event like “PersonX’s face is covered in blood”, and the nodes associated with it are the cause, the effect, and the attribute of the roles of the events. Therefore, Atomic is beneficial for the machine to know “what happens”. Secondly, we utilize a structured description to restore the scene. Specifically, we build a scene graph based on the original narrative and the knowledge from Atomic. Compared with the unstructured text, graph data can represent the scene more intuitively. In MRC task, previous works (Kipf and Welling, 2016; Qiu et al., 2019) that utilize structured data generally regard the words or noun phrases as the nodes of the graph. Those methods have no specific for Narrative MRC, where the events and the roles are the key factors. Therefore, we build the scene graph by taking the events, the persons, and the external knowledge of the event as the nodes. Meanwhile, we design the connections of the graph from both the perspectives of each event and the whole passage. Instead of the typical plane graph, we build a threedimensional graph, which can not only model the relevance among the events in the passage but also retain t"
2020.emnlp-main.247,D16-1264,0,0.0388517,"et MCTest, and this dataset encourages the early research of machine reading comprehension, and a strand of MRC models (Sachan et al., 2015; Narasimhan and Barzilay, 2015) are inspired by the dataset. Hermann et al. (2015) propose a cloze test dataset CNN & Daily Mail, which is large-scale and more suitable than MCTest for deep learning methods. Based on this dataset, Hermann et al. (2015) proposes an attention-based LSTM model named Attentive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This dataset enlightens a lot of classical MRC model, like Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and R-Net (Wang et al., 2017b). Recently, there are some new trends in this field, such as multipassage MRC (Campos et al., 2016), knowledgebased MRC (Ostermann et al., 2018) and multi-hop MRC (Yang et al., 2018; Min et al., 2019). Narrative Comprehension: Understanding narrative is a challenging task in natural language understanding, for the passages contain rich cause a"
2020.emnlp-main.247,D13-1020,0,0.0365935,", the performance rises up rapidly and then drops down slowly. This phenomenon indicates that in addition to enabling the iteration, it is also important to select a proper iteration step. We deduce that the proper step is the balance point where each event retains its unique information, and at the same time, also gets the associated information from the whole passage. 4 Related Work Machine Reading Comprehension: Due to the fast development of deep learning techniques and large-scale datasets, Machine Reading Comprehension(MRC) has gained increasingly wide attention over the past few years. Richardson et al. (2013) build the multiple-choice dataset MCTest, and this dataset encourages the early research of machine reading comprehension, and a strand of MRC models (Sachan et al., 2015; Narasimhan and Barzilay, 2015) are inspired by the dataset. Hermann et al. (2015) propose a cloze test dataset CNN & Daily Mail, which is large-scale and more suitable than MCTest for deep learning methods. Based on this dataset, Hermann et al. (2015) proposes an attention-based LSTM model named Attentive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match"
2020.emnlp-main.247,P15-1024,0,0.0257487,"ration step. We deduce that the proper step is the balance point where each event retains its unique information, and at the same time, also gets the associated information from the whole passage. 4 Related Work Machine Reading Comprehension: Due to the fast development of deep learning techniques and large-scale datasets, Machine Reading Comprehension(MRC) has gained increasingly wide attention over the past few years. Richardson et al. (2013) build the multiple-choice dataset MCTest, and this dataset encourages the early research of machine reading comprehension, and a strand of MRC models (Sachan et al., 2015; Narasimhan and Barzilay, 2015) are inspired by the dataset. Hermann et al. (2015) propose a cloze test dataset CNN & Daily Mail, which is large-scale and more suitable than MCTest for deep learning methods. Based on this dataset, Hermann et al. (2015) proposes an attention-based LSTM model named Attentive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This d"
2020.emnlp-main.247,K17-1004,0,0.0411627,"Missing"
2020.emnlp-main.247,S18-1120,0,0.0323042,"Missing"
2020.emnlp-main.247,P18-2118,0,0.0283283,"Missing"
2020.emnlp-main.247,P17-1018,0,0.14481,"entation Details In practice, we regard each sentence in the narrative passages as an event. When annotating the person roles in a particular sentence, we employ spaCy2 for dependency parsing. To link two mentions for the same person across the sentences while building a graph, we utilize Neural Coreference3 for 1 https://leaderboard.allenai.org/cosmosqa/submissions/public a Python library for natural language processing https://spacy.io/ 3 a toolkit to annotate and resolve coreference clusters https://github.com/huggingface/neuralcoref 3067 2 Method DSSM (Huang et al., 2013) Conditional GAN (Wang et al., 2017a) End Attn (Cai et al., 2017) LR+RNNLM (Schwartz et al., 2017) HCM (Chaturvedi et al., 2017) SeqMANN (Li et al., 2018) GPT-FT (Radford et al., 2018) Concept (Chen et al., 2019) BERT-FT (Devlin et al., 2018) BERT+Diff-Net (Cui et al., 2019) Our method (BERT+GDIN) Accuracy 58.5 60.9 74.7 75.2 77.6 84.7 86.5 87.6 89.2 90.1 91.9 Method Stanford Attentive (Chen et al., 2016) Co-Matching (Wang et al., 2018b) Gated-Attention (Dhingra et al., 2017) Commonsense (Wang et al., 2018a) GPT-FT (Radford et al., 2018) BERT-FT (Devlin et al., 2018) DMCN (Zhang et al., 2020) RoBERTa-FT (Liu et al., 2019) K-Ada"
2020.emnlp-main.247,D18-1259,0,0.0202224,"ntive Reader, and Chen et al. (2016) simplify this model by directly utilize the query-aware context representations to match the candidate answer. Moreover, Rajpurkar et al. (2016) release the span extraction dataset, SQuAD, which has become the most popular MRC dataset over recent years. This dataset enlightens a lot of classical MRC model, like Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and R-Net (Wang et al., 2017b). Recently, there are some new trends in this field, such as multipassage MRC (Campos et al., 2016), knowledgebased MRC (Ostermann et al., 2018) and multi-hop MRC (Yang et al., 2018; Min et al., 2019). Narrative Comprehension: Understanding narrative is a challenging task in natural language understanding, for the passages contain rich cause and effect relations. A large body of previous works focus on scripts learning (Schank and Abelson, 1977). Some previous works addressed script learning by focusing on the narrative cloze test (Chambers and Jurafsky, 2008). Story Cloze Test (Mostafazadeh et al., 2017) is then introduced as a new evaluation framework, and gains wide attention (Chaturvedi et al., 2017; Zhou et al., 2019b). Besides, recent works present other test frame"
2020.findings-emnlp.229,W06-0901,0,0.0753411,"g paradigm, called contextselective mask generalization, which can effective mine context-specific patterns for ED, shedding lights on building ED systems of decent robustness. 3) We report on extensive experiments demonstrating the advantages of our model in defending against adversarial attack, handling unseen predicates, and tackling ambiguous cases. We also give a deeper analysis exploring the predictive bias of our method. 2 2.1 Related Work Event Detection ED is a crucial subtask of EE that aims to find event triggers in texts. Earlier approaches for ED are feature based. To name a few, Ahn (2006) exploited lexical, syntactic, and external knowledge based features for the task; Ji and Grishman (2008) combined global and local decision features for the task. Liao and Grishman (2010) and Hong et al. (2011) investigated cross-event/cross-entity inference for the task; Li et al. (2013) proposed a joint framework for the task. Modern approaches for ED are neural network based. For example, Chen et al. 2524 (2015) leveraged Convolutional Neural Networks (CNNs) for the task; Nguyen et al. (2016) used Recurrent Neural Networks (RNNs) for the task; Feng et al. (2016) combined CNNs with RNNs and"
2020.findings-emnlp.229,D18-1316,0,0.0968754,"onal dimension of our work regarding the generalization of ED models. 2.2 Robustness Probing in Natural Language Processing Applications Enhancing the robustness of a model is a challenging and long-standing goal of AI research community. In computer vision, Szegedy et al. (2014) first pointed out that a crafted input with small perturbations could easily fool a neural model, referring to it as adversarial example. Papernot et al. (2016) first studied adversarial example in texts, and they proposed to producing adversarial input sequences on Recurrent Neural Network (RNN). Following the work, Alzantot et al. (2018) proposed a populationbased optimization method to generate more semantically similar adversarial examples. Many researchers have investigated robustness modeling in specific NLP problems. To name a few, Jia and Liang (2017) inserted adversarial perturbations into paragraphs for machine reading comprehension (MRC). The work was further extended by Mudrakarta et al. (2018), which cast the generation of adversarial examples as an optimization problem for the task of natural language inference (NLI); Belinkov and Bisk (2017); Ebrahimi et al. (2018) investigated how to tackle adversarial examples"
2020.findings-emnlp.229,C18-1055,0,0.0207553,"urrent Neural Network (RNN). Following the work, Alzantot et al. (2018) proposed a populationbased optimization method to generate more semantically similar adversarial examples. Many researchers have investigated robustness modeling in specific NLP problems. To name a few, Jia and Liang (2017) inserted adversarial perturbations into paragraphs for machine reading comprehension (MRC). The work was further extended by Mudrakarta et al. (2018), which cast the generation of adversarial examples as an optimization problem for the task of natural language inference (NLI); Belinkov and Bisk (2017); Ebrahimi et al. (2018) investigated how to tackle adversarial examples in neural machine translation (NMT). A very recent work of Hsieh et al. (2019) investigated the robustness of self-attentive architectures (Vaswani et al., 2017) in sentiment analysis, entailment and machine translation under adversarial attacks. But to our best knowledge, there is no work systematically studying the robustness of ED. 3 Approach Figure 2 visualizes the overview of our approach, by taking S1 as an example. Let a sentence of N words be S = [w1 , w2 , ..., wN ]. Following previous works (Li et al., 2013; Chen et al., 2015; Nguyen e"
2020.findings-emnlp.229,P16-2011,0,0.0904582,"d context-selective mask generalization for ED, which can effectively mine contextspecific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our model regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use full context for feature learning. 1 Replace With S2: During a war, invaders annihilated the whole town. Event Detector et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Liu et al., 2018b,a, 2019b). However, the vast majority of existing studies focus on improving the overall performance of an ED model (usually on a fixed test set), which rarely consider the robustness (and generalization capability) of an ED model. For example, most of existing methods do not answer questions such as when/why an ED system would fail, how to handle new, previously unseen data, despite these considerations are especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and categorize event triggers"
2020.findings-emnlp.229,W04-1017,0,0.157653,"swer questions such as when/why an ED system would fail, how to handle new, previously unseen data, despite these considerations are especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and categorize event triggers in texts. For example, in a sentence S1: “During a war, invaders destroyed the whole town”, ED requires a system to detect an event trigger destroyed, along with its event type ATTACK1 . Building a robust ED system is shown to benefit a wide range of applications including document summarization (Filatova and Hatzivassiloglou, 2004), knowledge base population (Ji and Grishman, 2011; Mitamura et al., 2017), question answering (Berant et al., 2014), and others. In recent years, great advances have been made in ED (Ji and Grishman, 2008; Li et al., 2013; Chen According to ACE event ontology. NIL Figure 1: Example of adversarial attack in ED. Introduction 1 Attack This paper focuses on the robustness aspect of ED models. We first emphasize the necessity of this research by pinpointing three stark cases demonstrating the vulnerability of existing ED models. These cases are: 1) adversarial attack, which refers to adding small"
2020.findings-emnlp.229,P11-1113,0,0.0322351,"experiments demonstrating the advantages of our model in defending against adversarial attack, handling unseen predicates, and tackling ambiguous cases. We also give a deeper analysis exploring the predictive bias of our method. 2 2.1 Related Work Event Detection ED is a crucial subtask of EE that aims to find event triggers in texts. Earlier approaches for ED are feature based. To name a few, Ahn (2006) exploited lexical, syntactic, and external knowledge based features for the task; Ji and Grishman (2008) combined global and local decision features for the task. Liao and Grishman (2010) and Hong et al. (2011) investigated cross-event/cross-entity inference for the task; Li et al. (2013) proposed a joint framework for the task. Modern approaches for ED are neural network based. For example, Chen et al. 2524 (2015) leveraged Convolutional Neural Networks (CNNs) for the task; Nguyen et al. (2016) used Recurrent Neural Networks (RNNs) for the task; Feng et al. (2016) combined CNNs with RNNs and Liu et al. (2018b) explored Graph Convolutional Networks (GCNs) for the task. More recent works have designed advanced architectures for the task (Liu et al., 2017, 2018a; Lu et al., 2019; Liu et al., 2019a). D"
2020.findings-emnlp.229,P19-1147,0,0.0183299,"more semantically similar adversarial examples. Many researchers have investigated robustness modeling in specific NLP problems. To name a few, Jia and Liang (2017) inserted adversarial perturbations into paragraphs for machine reading comprehension (MRC). The work was further extended by Mudrakarta et al. (2018), which cast the generation of adversarial examples as an optimization problem for the task of natural language inference (NLI); Belinkov and Bisk (2017); Ebrahimi et al. (2018) investigated how to tackle adversarial examples in neural machine translation (NMT). A very recent work of Hsieh et al. (2019) investigated the robustness of self-attentive architectures (Vaswani et al., 2017) in sentiment analysis, entailment and machine translation under adversarial attacks. But to our best knowledge, there is no work systematically studying the robustness of ED. 3 Approach Figure 2 visualizes the overview of our approach, by taking S1 as an example. Let a sentence of N words be S = [w1 , w2 , ..., wN ]. Following previous works (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Lu et al., 2019), we formulate the ED task as a token-level classification problem. That is, for each word in S, w"
2020.findings-emnlp.229,P18-1201,0,0.0113385,"e work has studied the robustness (and generalization capability) of an ED model. The work of Lu et al. (2019) is related to ours, which improved the generalization of an ED model by decoupling lexicalspecific and lexical-free representations via adversarial training. Compared to their work, the introduction of placeholders in our work can naturally decouple lexical-specific and lexical-free representations, which avoids the unstable adversarial learning process. Moreover, our work evaluates three aspects of robustness, rather than only unseen predicates. Our work also relates to the study of Huang et al. (2018), which aims to recognize events of never-seen event types, i.e. zero-shot EE. Their work lies in an orthogonal dimension of our work regarding the generalization of ED models. 2.2 Robustness Probing in Natural Language Processing Applications Enhancing the robustness of a model is a challenging and long-standing goal of AI research community. In computer vision, Szegedy et al. (2014) first pointed out that a crafted input with small perturbations could easily fool a neural model, referring to it as adversarial example. Papernot et al. (2016) first studied adversarial example in texts, and the"
2020.findings-emnlp.229,P08-1030,0,0.709562,"subtask of event extraction (EE), aims to identify and categorize event triggers in texts. For example, in a sentence S1: “During a war, invaders destroyed the whole town”, ED requires a system to detect an event trigger destroyed, along with its event type ATTACK1 . Building a robust ED system is shown to benefit a wide range of applications including document summarization (Filatova and Hatzivassiloglou, 2004), knowledge base population (Ji and Grishman, 2011; Mitamura et al., 2017), question answering (Berant et al., 2014), and others. In recent years, great advances have been made in ED (Ji and Grishman, 2008; Li et al., 2013; Chen According to ACE event ontology. NIL Figure 1: Example of adversarial attack in ED. Introduction 1 Attack This paper focuses on the robustness aspect of ED models. We first emphasize the necessity of this research by pinpointing three stark cases demonstrating the vulnerability of existing ED models. These cases are: 1) adversarial attack, which refers to adding small perturbations in the original sentences (Papernot et al., 2016; Alzantot et al., 2018). As shown in Figure 1, a well-trained event detector can correctly recognize the event trigger destroyed at first. But"
2020.findings-emnlp.229,P11-1115,0,0.0445066,"andle new, previously unseen data, despite these considerations are especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and categorize event triggers in texts. For example, in a sentence S1: “During a war, invaders destroyed the whole town”, ED requires a system to detect an event trigger destroyed, along with its event type ATTACK1 . Building a robust ED system is shown to benefit a wide range of applications including document summarization (Filatova and Hatzivassiloglou, 2004), knowledge base population (Ji and Grishman, 2011; Mitamura et al., 2017), question answering (Berant et al., 2014), and others. In recent years, great advances have been made in ED (Ji and Grishman, 2008; Li et al., 2013; Chen According to ACE event ontology. NIL Figure 1: Example of adversarial attack in ED. Introduction 1 Attack This paper focuses on the robustness aspect of ED models. We first emphasize the necessity of this research by pinpointing three stark cases demonstrating the vulnerability of existing ED models. These cases are: 1) adversarial attack, which refers to adding small perturbations in the original sentences (Papernot"
2020.findings-emnlp.229,D14-1159,0,0.0225971,"especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and categorize event triggers in texts. For example, in a sentence S1: “During a war, invaders destroyed the whole town”, ED requires a system to detect an event trigger destroyed, along with its event type ATTACK1 . Building a robust ED system is shown to benefit a wide range of applications including document summarization (Filatova and Hatzivassiloglou, 2004), knowledge base population (Ji and Grishman, 2011; Mitamura et al., 2017), question answering (Berant et al., 2014), and others. In recent years, great advances have been made in ED (Ji and Grishman, 2008; Li et al., 2013; Chen According to ACE event ontology. NIL Figure 1: Example of adversarial attack in ED. Introduction 1 Attack This paper focuses on the robustness aspect of ED models. We first emphasize the necessity of this research by pinpointing three stark cases demonstrating the vulnerability of existing ED models. These cases are: 1) adversarial attack, which refers to adding small perturbations in the original sentences (Papernot et al., 2016; Alzantot et al., 2018). As shown in Figure 1, a well"
2020.findings-emnlp.229,D17-1215,0,0.0264339,"h community. In computer vision, Szegedy et al. (2014) first pointed out that a crafted input with small perturbations could easily fool a neural model, referring to it as adversarial example. Papernot et al. (2016) first studied adversarial example in texts, and they proposed to producing adversarial input sequences on Recurrent Neural Network (RNN). Following the work, Alzantot et al. (2018) proposed a populationbased optimization method to generate more semantically similar adversarial examples. Many researchers have investigated robustness modeling in specific NLP problems. To name a few, Jia and Liang (2017) inserted adversarial perturbations into paragraphs for machine reading comprehension (MRC). The work was further extended by Mudrakarta et al. (2018), which cast the generation of adversarial examples as an optimization problem for the task of natural language inference (NLI); Belinkov and Bisk (2017); Ebrahimi et al. (2018) investigated how to tackle adversarial examples in neural machine translation (NMT). A very recent work of Hsieh et al. (2019) investigated the robustness of self-attentive architectures (Vaswani et al., 2017) in sentiment analysis, entailment and machine translation unde"
2020.findings-emnlp.229,P15-1017,1,0.947044,"17); Ebrahimi et al. (2018) investigated how to tackle adversarial examples in neural machine translation (NMT). A very recent work of Hsieh et al. (2019) investigated the robustness of self-attentive architectures (Vaswani et al., 2017) in sentiment analysis, entailment and machine translation under adversarial attacks. But to our best knowledge, there is no work systematically studying the robustness of ED. 3 Approach Figure 2 visualizes the overview of our approach, by taking S1 as an example. Let a sentence of N words be S = [w1 , w2 , ..., wN ]. Following previous works (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Lu et al., 2019), we formulate the ED task as a token-level classification problem. That is, for each word in S, we consider it as a candidate trigger, and our goal is to assign a correct event label to it (A type of NIL is used to indicate a non-trigger word). The technical details of our approach are presented in the following, including: trigger delexicalization (§ 3.1), context-selective discriminative learning (§ 3.2), contextualized similarity learning (§ 3.3), attentive representation fusion (§ 3.4), and the training strategy (§ 3.5). 3.1 Trigger Delexicalization"
2020.findings-emnlp.229,N19-1423,0,0.0216916,"l classification problem. That is, for each word in S, we consider it as a candidate trigger, and our goal is to assign a correct event label to it (A type of NIL is used to indicate a non-trigger word). The technical details of our approach are presented in the following, including: trigger delexicalization (§ 3.1), context-selective discriminative learning (§ 3.2), contextualized similarity learning (§ 3.3), attentive representation fusion (§ 3.4), and the training strategy (§ 3.5). 3.1 Trigger Delexicalization Following recent advances in ED (Yang et al., 2019), we adopt BERT architecture (Devlin et al., 2019) to learn the input representations, by first adding special tokens at the both ends of S to construct an extended sequence “[CLS] S [SEP]”. Note we do not allow our model to leverage lexical clues, we explicitly delexicalize the candidate trigger, by replacing it with a placeholder [MASK]. Consider S1 and S2 in Figure 1. If we take destroyed or annihilated as the candidate trigger, the mask-containing sequence is “[CLS] During a war, invaders [MASK] the whole town [SEP]”. Next. we use BERT for sequence encoding and take the final hidden layer2 of BERT as the input representations, denoted as"
2020.findings-emnlp.229,P19-1262,0,0.041953,"Missing"
2020.findings-emnlp.229,D17-1018,0,0.0249684,"a placeholder [MASK]. Consider S1 and S2 in Figure 1. If we take destroyed or annihilated as the candidate trigger, the mask-containing sequence is “[CLS] During a war, invaders [MASK] the whole town [SEP]”. Next. we use BERT for sequence encoding and take the final hidden layer2 of BERT as the input representations, denoted as HS ∈ R(N +2)×d . We use hwi ∈ Rd to denote the representation of a specific token wi . 2 In case a word may be split into many sub-word pieces, we conduct a self-attentive computation over sub-word pieces to compute the representation of original word, as suggested by Lee et al. (2017). 2525 During a war, invaders destroyed the whole town. Context-selective Discrimination Learning Contextualized Similarity Learning [CLS] During a war, invaders [MASK] the whole town [SEP] S1: During [Ă] invaders [MASK] the whole town. Attack S2: An American tank [MASK] the hotel. Attack BERT H[CLS] H1 H2 H3  H[M] 1 0 1 Uncertainty modeling And select the better one  random mask attention  1 high value S1: During [Ă] invaders [MASK] the whole town. Attack S2: Government [MASK] 5000 dollars. Transfer-Money  0 S1: H[M] , S2: H[M] BERT HN H[SEP] selective attention  0  S1: H[M]"
2020.findings-emnlp.229,P13-1008,0,0.618556,"ction (EE), aims to identify and categorize event triggers in texts. For example, in a sentence S1: “During a war, invaders destroyed the whole town”, ED requires a system to detect an event trigger destroyed, along with its event type ATTACK1 . Building a robust ED system is shown to benefit a wide range of applications including document summarization (Filatova and Hatzivassiloglou, 2004), knowledge base population (Ji and Grishman, 2011; Mitamura et al., 2017), question answering (Berant et al., 2014), and others. In recent years, great advances have been made in ED (Ji and Grishman, 2008; Li et al., 2013; Chen According to ACE event ontology. NIL Figure 1: Example of adversarial attack in ED. Introduction 1 Attack This paper focuses on the robustness aspect of ED models. We first emphasize the necessity of this research by pinpointing three stark cases demonstrating the vulnerability of existing ED models. These cases are: 1) adversarial attack, which refers to adding small perturbations in the original sentences (Papernot et al., 2016; Alzantot et al., 2018). As shown in Figure 1, a well-trained event detector can correctly recognize the event trigger destroyed at first. But when we replace"
2020.findings-emnlp.229,N16-1034,0,0.236098,"ning mechanism, called context-selective mask generalization for ED, which can effectively mine contextspecific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our model regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use full context for feature learning. 1 Replace With S2: During a war, invaders annihilated the whole town. Event Detector et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Liu et al., 2018b,a, 2019b). However, the vast majority of existing studies focus on improving the overall performance of an ED model (usually on a fixed test set), which rarely consider the robustness (and generalization capability) of an ED model. For example, most of existing methods do not answer questions such as when/why an ED system would fail, how to handle new, previously unseen data, despite these considerations are especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and catego"
2020.findings-emnlp.229,P10-1081,0,0.213772,"s. 3) We report on extensive experiments demonstrating the advantages of our model in defending against adversarial attack, handling unseen predicates, and tackling ambiguous cases. We also give a deeper analysis exploring the predictive bias of our method. 2 2.1 Related Work Event Detection ED is a crucial subtask of EE that aims to find event triggers in texts. Earlier approaches for ED are feature based. To name a few, Ahn (2006) exploited lexical, syntactic, and external knowledge based features for the task; Ji and Grishman (2008) combined global and local decision features for the task. Liao and Grishman (2010) and Hong et al. (2011) investigated cross-event/cross-entity inference for the task; Li et al. (2013) proposed a joint framework for the task. Modern approaches for ED are neural network based. For example, Chen et al. 2524 (2015) leveraged Convolutional Neural Networks (CNNs) for the task; Nguyen et al. (2016) used Recurrent Neural Networks (RNNs) for the task; Feng et al. (2016) combined CNNs with RNNs and Liu et al. (2018b) explored Graph Convolutional Networks (GCNs) for the task. More recent works have designed advanced architectures for the task (Liu et al., 2017, 2018a; Lu et al., 2019"
2020.findings-emnlp.229,P16-1200,0,0.0673205,"Missing"
2020.findings-emnlp.229,D19-1068,1,0.875764,"Missing"
2020.findings-emnlp.229,P17-1164,1,0.848366,"s for the task. Liao and Grishman (2010) and Hong et al. (2011) investigated cross-event/cross-entity inference for the task; Li et al. (2013) proposed a joint framework for the task. Modern approaches for ED are neural network based. For example, Chen et al. 2524 (2015) leveraged Convolutional Neural Networks (CNNs) for the task; Nguyen et al. (2016) used Recurrent Neural Networks (RNNs) for the task; Feng et al. (2016) combined CNNs with RNNs and Liu et al. (2018b) explored Graph Convolutional Networks (GCNs) for the task. More recent works have designed advanced architectures for the task (Liu et al., 2017, 2018a; Lu et al., 2019; Liu et al., 2019a). Despite many advances in ED, to date rare work has studied the robustness (and generalization capability) of an ED model. The work of Lu et al. (2019) is related to ours, which improved the generalization of an ED model by decoupling lexicalspecific and lexical-free representations via adversarial training. Compared to their work, the introduction of placeholders in our work can naturally decouple lexical-specific and lexical-free representations, which avoids the unstable adversarial learning process. Moreover, our work evaluates three aspects of"
2020.findings-emnlp.229,D14-1162,0,0.0841568,"adversarial attacks, unseen predicates, and tackling ambiguity cases. To maintain tractability, in the following experiments, we take model achieving best performance on the development set for testing, instead of adopting 5-run average as in previous evaluation. Moreover, to simplicity analysis, our experiments are mostly conducted on ACE 2005. 5.2.1 Defending Against Adversarial Attacks In adversarial attacks, we adopt list-based method (Alzantot et al., 2018) to generate adversarial examples. Specifically, for a word, we first find its semantically similar words based on GloVe embeddings (Pennington et al., 2014), and then we replace the original word with each word and evaluate the new sentence with a GPT language model (Radford et al., 2019). We take the new sentence with the largest score as adversarial example. Some cases in 2528 M ODEL O RG ADT ADC ∆F1 M ODEL LA HA ∆F1 DNNED DMCNN JRNN Delta-Adv MBERT 66.7 69.0 69.5 71.8 74.2 18.8 20.1 19.3 20.4 36.1 16.6 19.2 18.9 19.6 33.2 -47.1/50.1 -48.9/49.8 -50.2/50.6 -51.4/52.2 -38.1/41.0 DNNED DMCNN (2015) JRNN (2016) Delta-Adv (2019) MBERT 70.6 72.7 71.0 72.2 73.5 50.4 55.2 49.5 52.1 60.3 -20.2 -17.5 -21.5 -20.1 -13.2 MFULL MMASK 76.0 45.0 47.9 45.0 43.3"
2020.findings-emnlp.229,D19-1002,0,0.0254995,"ry vector (Bahdanau et al., 2014). Wa ∈ Rd×d is an attention matrix. Then we conduct a weighted summation computation over HS using αu as the weight vector and compute a feature vector for the masked candidate trigger, denoted by F[MS] . Finally, F[MS] is used for event label prediction by computing an output vector containing the probability of different event labels: o[MS] = Wm F[MS] + bm (1) where Wm and bm are model parameters. The predicted event label corresponds to the index having the highest value in o[MS] . Considering that unsupervised attention may not always learn a good pattern (Wiegreffe and Pinter, 2019), we devise a “trial-and-error” approach to guide the learning. Specifically, at the training time, we also generate random context mask3 and normalize it as a weight vector αr . Our intuition is, if αr leads to a better result than using αu , it might be a better selective pattern for our model to learn. Note there are cases where the predicted event labels are the same for αr and αu , and here we introduce model uncertainty (Gal and Ghahramani, 2016) to evaluate whether the result is improved. Specifically, we compute the model uncertainty by making predictions many times but with dropout la"
2020.findings-emnlp.229,P19-1522,0,0.116406,"2019), we formulate the ED task as a token-level classification problem. That is, for each word in S, we consider it as a candidate trigger, and our goal is to assign a correct event label to it (A type of NIL is used to indicate a non-trigger word). The technical details of our approach are presented in the following, including: trigger delexicalization (§ 3.1), context-selective discriminative learning (§ 3.2), contextualized similarity learning (§ 3.3), attentive representation fusion (§ 3.4), and the training strategy (§ 3.5). 3.1 Trigger Delexicalization Following recent advances in ED (Yang et al., 2019), we adopt BERT architecture (Devlin et al., 2019) to learn the input representations, by first adding special tokens at the both ends of S to construct an extended sequence “[CLS] S [SEP]”. Note we do not allow our model to leverage lexical clues, we explicitly delexicalize the candidate trigger, by replacing it with a placeholder [MASK]. Consider S1 and S2 in Figure 1. If we take destroyed or annihilated as the candidate trigger, the mask-containing sequence is “[CLS] During a war, invaders [MASK] the whole town [SEP]”. Next. we use BERT for sequence encoding and take the final hidden layer2"
2020.findings-emnlp.229,D18-1156,0,0.0503645,"mask generalization for ED, which can effectively mine contextspecific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our model regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use full context for feature learning. 1 Replace With S2: During a war, invaders annihilated the whole town. Event Detector et al., 2015; Nguyen et al., 2016; Feng et al., 2016; Liu et al., 2018b,a, 2019b). However, the vast majority of existing studies focus on improving the overall performance of an ED model (usually on a fixed test set), which rarely consider the robustness (and generalization capability) of an ED model. For example, most of existing methods do not answer questions such as when/why an ED system would fail, how to handle new, previously unseen data, despite these considerations are especially crucial for designing real-world ED systems. Event detection (ED), a crucial subtask of event extraction (EE), aims to identify and categorize event triggers in texts. For exa"
2020.findings-emnlp.229,P19-1429,0,0.0291083,"Missing"
2021.acl-long.218,C18-1139,0,0.0141834,"tch a lexicon. (2) ZEN (Diao et al., 2020) is a pretrained Chinese text encoder enhanced by an n-gram lexicon. In ZEN, n-gram contexts are extracted, encoded and integrated with the character encoder. For more details about Lattice-LSTM and ZEN, we refer readers to Zhang and Yang (2018) and Diao et al. (2020). 5.1 5.3 5 Baselines Text-Only Model Open-Source NLP Toolkit: Many open-source NLP toolkits, such as spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), support Chinese NER. In spaCy, a multitask CNN is employed. In Stanza, a contextualized string representation based tagger from Akbik et al. (2018) is adopted. In both spaCy and Stanza, the tagger is trained on OntoNote (Weischedel et al., 2011). To map the output of taggers to CNERTA’s label space, expert-designed rules are used, such as PERSON → PER. Since these toolkits are only designed for flat structure, we do not evaluate these toolkits in nested settings. Multimodal Model To leverage the acoustic modality, several multimodal models are introduced. In these models, fusion modules are built on the top of the acoustic encoder and the textual encoder, which are designed for capturing the interaction between the textual hidden represe"
2021.acl-long.218,W15-4319,0,0.0733109,"Missing"
2021.acl-long.218,D18-1017,1,0.851384,"imodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER. Second, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by no word delimiter between words in written. T"
2021.acl-long.218,Q16-1026,0,0.0397644,"on “Speech is a part of thought.” — Oliver Sacks, Seeing Voices As a fundamental subtask of information extraction, named entity recognition (NER) aims to locate and classify named entities mentioned in unstructured texts into predefined semantic categories, such as person names, locations and organizations. NER plays a crucial role in many natural language processing (NLP) tasks, including relation extraction (Zelenko et al., 2003), question answering (Moll´a et al., 2006) and summarization (Aramaki et al., 2009). Most of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted to couple the textual moda"
2021.acl-long.218,W04-1213,0,0.126643,"nese character and sj denotes the j-th waveform frame, the goal of the task is to leverage textual and speech clues to identify and classify all named entities contained in the text. 3.2 4.2 Dataset Comparison We compare CNERTA with several widely used NER datasets in Table 2. Specifically, we first compare our corpus with some Chinese NER datasets, such as MSRA (Levow, 2006), OntoNotes (Weischedel et al., 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018). Then, we compare our corpus with several widely used nested NER datasets, like GENIA (Kim et al., 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al., 2004) and ACE-2005 (Walker et al., 2004). Finally, multimodal NER datasets, including Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al., 2018), are compared with our corpus. From Table 2, we observe that our corpus has unique value compared with the existing datasets. The value is reflected in the following aspects: (1) CNERTA is a large-scale dataset; (2) CNERTA is the first Chinese multimodal dataset; (3) Not only the topmost entities but also nested entities are annotated; (4) Among these datasets, the acoustic modality is only introduced in CNERTA"
2021.acl-long.218,2020.findings-emnlp.58,0,0.103877,"ual encoder and conditional random fields (CRF) (Lafferty et al., 2001) as the decoder, the widely used BiLSTM-CRF (Lample et al., 2016) is adopted as an important baseline. PLM-CRF: Instead of training a model from scratch, we also adopt the framework of fine-tuning a pretrained language model (PLM) on a downstream task (Radford et al., 2018). In this framework, we adopt BERT (Devlin et al., 2019) as the textual encoder and use CRF as the decoder. In addition to initializing the textual encoder with the original pretrained BERT model, a SoTA Chinese pretrained language model, called MacBERT (Cui et al., 2020), is used. Compared with BERT, MacBERT is built upon RoBERTa (Liu et al., 2019b) and the original MLM task in BERT is replaced with the MLM as correction task. For more details, we refer readers to Cui et al. (2020). 5.2 Lexicon-Enhanced Model: Based on the annotated dataset, a family of strong and representative baselines is established, including (1) text-only models presented in Section 5.1, (2) lexicon-enhanced models shown in Section 5.2 and (3) multimodal models introduced in Section 5.3. A drawback of the text-only methods mentioned above is that explicit word and word sequence informat"
2021.acl-long.218,N19-1423,0,0.611401,"hinese NER, since named entity boundaries are usually word boundaries (Zhang and Yang, 2018). Fortunately, cues contained in the fluent acoustic modality, especially pauses between adjacent words, are able to aid the NER model in discovering word boundaries. A classic example shown in Figure 1 can perfectly illustrate this point. In this example, the sentence with ambiguous word segmentation would be disambiguated with the aid of the acoustic modality, which would absolutely assist the model to infer correct NER tags. In this work, we make the following efforts to advance multimodal NER: CRF (Devlin et al., 2019). Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al., 2020), to explore whether the acoustic modality can provide word information beyond the lexicon. Finally, to verify the effectiveness of introducing the acoustic modality, we test some widely used multimodal models, such as CMA (Tsai et al., 2019) and MMI (Yu et al., 2020), on our dataset. Third, upon these strong baselines, we further propose"
2021.acl-long.218,2020.findings-emnlp.425,0,0.32266,"lassic example shown in Figure 1 can perfectly illustrate this point. In this example, the sentence with ambiguous word segmentation would be disambiguated with the aid of the acoustic modality, which would absolutely assist the model to infer correct NER tags. In this work, we make the following efforts to advance multimodal NER: CRF (Devlin et al., 2019). Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al., 2020), to explore whether the acoustic modality can provide word information beyond the lexicon. Finally, to verify the effectiveness of introducing the acoustic modality, we test some widely used multimodal models, such as CMA (Tsai et al., 2019) and MMI (Yu et al., 2020), on our dataset. Third, upon these strong baselines, we further propose a simple Multi-Modal Multi-Task model (short for M3T) to make better use of the pause information in the acoustic modality. Specifically, different from coupling the visual modality with the textual modality, there is a monotonic alignment between the acousti"
2021.acl-long.218,P19-1141,0,0.0306561,"Missing"
2021.acl-long.218,doddington-etal-2004-automatic,0,0.0362605,"j-th waveform frame, the goal of the task is to leverage textual and speech clues to identify and classify all named entities contained in the text. 3.2 4.2 Dataset Comparison We compare CNERTA with several widely used NER datasets in Table 2. Specifically, we first compare our corpus with some Chinese NER datasets, such as MSRA (Levow, 2006), OntoNotes (Weischedel et al., 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018). Then, we compare our corpus with several widely used nested NER datasets, like GENIA (Kim et al., 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al., 2004) and ACE-2005 (Walker et al., 2004). Finally, multimodal NER datasets, including Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al., 2018), are compared with our corpus. From Table 2, we observe that our corpus has unique value compared with the existing datasets. The value is reflected in the following aspects: (1) CNERTA is a large-scale dataset; (2) CNERTA is the first Chinese multimodal dataset; (3) Not only the topmost entities but also nested entities are annotated; (4) Among these datasets, the acoustic modality is only introduced in CNERTA. 4 4.1 Preliminaries Task Descripti"
2021.acl-long.218,D19-1096,0,0.0413513,"Missing"
2021.acl-long.218,W06-0115,0,0.0429611,"three parts: training, development, and test set. Table 1 shows the high level statistics of data splits for CNERTA. Given a text X = x1 , x2 , ..., xn and its corresponding speech S = s1 , s2 , ..., st , where xi denotes the i-th Chinese character and sj denotes the j-th waveform frame, the goal of the task is to leverage textual and speech clues to identify and classify all named entities contained in the text. 3.2 4.2 Dataset Comparison We compare CNERTA with several widely used NER datasets in Table 2. Specifically, we first compare our corpus with some Chinese NER datasets, such as MSRA (Levow, 2006), OntoNotes (Weischedel et al., 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018). Then, we compare our corpus with several widely used nested NER datasets, like GENIA (Kim et al., 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al., 2004) and ACE-2005 (Walker et al., 2004). Finally, multimodal NER datasets, including Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al., 2018), are compared with our corpus. From Table 2, we observe that our corpus has unique value compared with the existing datasets. The value is reflected in the following aspe"
2021.acl-long.218,2020.acl-main.611,0,0.012065,"om two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER. Second, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by no word delimiter between words in written. This language characteristic is one of the major roadblocks in Chinese NER,"
2021.acl-long.218,N19-1247,0,0.0122547,"decoder, the widely used BiLSTM-CRF (Lample et al., 2016) is adopted as an important baseline. PLM-CRF: Instead of training a model from scratch, we also adopt the framework of fine-tuning a pretrained language model (PLM) on a downstream task (Radford et al., 2018). In this framework, we adopt BERT (Devlin et al., 2019) as the textual encoder and use CRF as the decoder. In addition to initializing the textual encoder with the original pretrained BERT model, a SoTA Chinese pretrained language model, called MacBERT (Cui et al., 2020), is used. Compared with BERT, MacBERT is built upon RoBERTa (Liu et al., 2019b) and the original MLM task in BERT is replaced with the MLM as correction task. For more details, we refer readers to Cui et al. (2020). 5.2 Lexicon-Enhanced Model: Based on the annotated dataset, a family of strong and representative baselines is established, including (1) text-only models presented in Section 5.1, (2) lexicon-enhanced models shown in Section 5.2 and (3) multimodal models introduced in Section 5.3. A drawback of the text-only methods mentioned above is that explicit word and word sequence information is not fully exploited, which can be potentially useful. With this conside"
2021.acl-long.218,P18-1185,0,0.0281341,"Missing"
2021.acl-long.218,2020.acl-main.528,0,0.0202317,"otivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER. Second, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by no word delimiter between words in written. This language characteristic is one of the major roadbloc"
2021.acl-long.218,P16-1101,0,0.0333306,"ifferent. Introduction “Speech is a part of thought.” — Oliver Sacks, Seeing Voices As a fundamental subtask of information extraction, named entity recognition (NER) aims to locate and classify named entities mentioned in unstructured texts into predefined semantic categories, such as person names, locations and organizations. NER plays a crucial role in many natural language processing (NLP) tasks, including relation extraction (Zelenko et al., 2003), question answering (Moll´a et al., 2006) and summarization (Aramaki et al., 2009). Most of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted t"
2021.acl-long.218,U06-1009,0,0.178673,"Missing"
2021.acl-long.218,N16-1030,0,0.477357,"ations are radically different. Introduction “Speech is a part of thought.” — Oliver Sacks, Seeing Voices As a fundamental subtask of information extraction, named entity recognition (NER) aims to locate and classify named entities mentioned in unstructured texts into predefined semantic categories, such as person names, locations and organizations. NER plays a crucial role in many natural language processing (NLP) tasks, including relation extraction (Zelenko et al., 2003), question answering (Moll´a et al., 2006) and summarization (Aramaki et al., 2009). Most of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (202"
2021.acl-long.218,N18-1078,0,0.164262,"n (Zelenko et al., 2003), question answering (Moll´a et al., 2006) and summarization (Aramaki et al., 2009). Most of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted to couple the textual modality with the visual modality and witnessed a stable improvement. In this work, we also focus on multimodal NER. But differently from previous studies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Join"
2021.acl-long.218,P16-2025,0,0.345455,"tudies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER. Second, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by n"
2021.acl-long.218,2020.acl-demos.14,0,0.0168824,"(1) Lattice-LSTM (Zhang and Yang, 2018) is a classic method that can encode a sequence of input characters as well as all potential words that match a lexicon. (2) ZEN (Diao et al., 2020) is a pretrained Chinese text encoder enhanced by an n-gram lexicon. In ZEN, n-gram contexts are extracted, encoded and integrated with the character encoder. For more details about Lattice-LSTM and ZEN, we refer readers to Zhang and Yang (2018) and Diao et al. (2020). 5.1 5.3 5 Baselines Text-Only Model Open-Source NLP Toolkit: Many open-source NLP toolkits, such as spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), support Chinese NER. In spaCy, a multitask CNN is employed. In Stanza, a contextualized string representation based tagger from Akbik et al. (2018) is adopted. In both spaCy and Stanza, the tagger is trained on OntoNote (Weischedel et al., 2011). To map the output of taggers to CNERTA’s label space, expert-designed rules are used, such as PERSON → PER. Since these toolkits are only designed for flat structure, we do not evaluate these toolkits in nested settings. Multimodal Model To leverage the acoustic modality, several multimodal models are introduced. In these models, fusion modules are"
2021.acl-long.218,2020.coling-main.168,0,0.0327781,"e et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted to couple the textual modality with the visual modality and witnessed a stable improvement. In this work, we also focus on multimodal NER. But differently from previous studies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much rec"
2021.acl-long.218,P19-1656,0,0.0596754,"r correct NER tags. In this work, we make the following efforts to advance multimodal NER: CRF (Devlin et al., 2019). Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al., 2020), to explore whether the acoustic modality can provide word information beyond the lexicon. Finally, to verify the effectiveness of introducing the acoustic modality, we test some widely used multimodal models, such as CMA (Tsai et al., 2019) and MMI (Yu et al., 2020), on our dataset. Third, upon these strong baselines, we further propose a simple Multi-Modal Multi-Task model (short for M3T) to make better use of the pause information in the acoustic modality. Specifically, different from coupling the visual modality with the textual modality, there is a monotonic alignment between the acoustic modality and the textual modality. Armed with such an alignment, the position of each Chinese character in the continuous speech would be determined, which would make it easy to discover pauses between adjacent words. Therefore, to automati"
2021.acl-long.218,W95-0107,0,0.123723,"e value is reflected in the following aspects: (1) CNERTA is a large-scale dataset; (2) CNERTA is the first Chinese multimodal dataset; (3) Not only the topmost entities but also nested entities are annotated; (4) Among these datasets, the acoustic modality is only introduced in CNERTA. 4 4.1 Preliminaries Task Description Nested Structure Linearization Unlike flat NER, named entities may overlap and also be labeled with more than one label in nested NER. To solve nested NER, we follow Strakov´a et al. (2019) to encode the nested entity structure into a CoNLL-like, per-character BIO encoding (Ramshaw and Marcus, 1995). There are two rules to guide the linearization: (1) entity mentions starting earlier have priority over entities starting later, and (2) for mentions with the same beginning, longer entity mentions have priority over shorter ones. A multilabel for a given Chinese character is a concatenation of all intersecting entity mentions, from the highest priority to the lowest. For more details, we refer readers to Strakov´a et al. (2019). 2810 4.3 Acoustic Encoder The acoustic encoder is used to map raw speech signals into continuous space. There are three parts in the proposed acoustic encoder: a sp"
2021.acl-long.218,E12-2021,0,0.0773052,"Missing"
2021.acl-long.218,P19-1527,0,0.0432651,"Missing"
2021.acl-long.218,2020.coling-main.340,0,0.0476125,"Missing"
2021.acl-long.218,2020.acl-main.306,0,0.445007,"of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted to couple the textual modality with the visual modality and witnessed a stable improvement. In this work, we also focus on multimodal NER. But differently from previous studies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computatio"
2021.acl-long.218,P18-1144,0,0.237485,"tention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects: 2807 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818 August 1–6, 2021. ©2021 Association for Computational Linguistics First, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER. Second, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by no word delimiter between"
2021.acl-long.276,P98-1013,0,0.194369,"e take causal sentence generation via lexical knowledge expanding as an example. erative architecture to generate new well-formed causal/non-causal sentences that contain them. Knowledge Guiding KSG introduces event pairs that are probabilistic causal or non-causal from multiple knowledge bases in two ways. (1) Lexical knowledge expanding: expanding annotated event pairs via external dictionaries, such as WordNet (Miller, 1995) and VerbNet (Schuler, 2005). (2) Connective knowledge introducing: introducing event pairs from external event-annotated documents (KBP corpus) assisted with FrameNet (Baker et al., 1998) and Penn Discourse Treebank (PDTB2) (Group et al., 2008). As shown in Table 1, we illustrate how to extract event pairs from multiple knowledge bases. Then, inspired by Bordes et al. (2013), we filter the extracted event pairs by converting them into triples &lt;ei , causal/noncausal, ej &gt; and calculating the causal-distance by maximizing L in a causal representation space: L= X X [λ + d(e0i , e0j ) − d(ei , ej )]+ , (3) (ei ,ej )∈T (e0i ,e0j )∈T 0 (2) where θG is the parameter of G, c is the input relation, t is one of the generated tokens Ts of the generated sentence s0 , and p(t|c; θG ) is th"
2021.acl-long.276,P19-1007,0,0.0585267,"Missing"
2021.acl-long.276,2020.acl-main.608,0,0.0678662,"Missing"
2021.acl-long.276,W17-2711,0,0.235676,"between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in cause two sentences: (1) attack −→ killed in S1; (2) cause statement −→ protests in S2. Most existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016; Riaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b). In these frameworks, data augmentation and target task are modeled independently. This often leads to a lack of task-related characteristics in the ge"
2021.acl-long.276,P16-1135,0,0.0207477,"generated causal sentences via iteratively learning in the dual interaction. • Experimental results on two benchmarks show that our model achieves the best performance on ECI. Moreover, it also shows definite advantages over previous data augmentation methods. 2 Related Work To date, many researches attempt to identify the causality with linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Cas"
2021.acl-long.276,W17-2708,0,0.0239346,"that our model achieves the best performance on ECI. Moreover, it also shows definite advantages over previous data augmentation methods. 2 Related Work To date, many researches attempt to identify the causality with linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for 3559 Knowledge relation-&gt;sentence-&gt;relation Annotate"
2021.acl-long.276,W17-5540,0,0.136483,"s in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017). This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in cause two sentences: (1) attack −→ killed in S1; (2) cause statement −→ protests in S2. Most existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016; Riaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NLP-related augmentation methods are a task-inde"
2021.acl-long.276,P14-3002,0,0.0281709,"017), a dependency path based 4 https://github.com/google-research/ bert 3563 sequential model that models the context between events to identify causality; 2) Seq (Choubey and Huang, 2017), a sequence model explores complex human designed features for ECI; 3) LR+ and ILP (Gao et al., 2019), document-level models adopt document structures for ECI. For Causal-TB, we prefer 1) RB, a rule-based system; 2) DD, a data driven machine learning based system; 3) VR-C, a verb rule based model with data filtering and gold causal signals enhancement. These models are designed by Mirza and Tonelli (2014); Mirza (2014) for ECI. Owing to our methods are constructed on BERT, we build BERT-based methods: 1) BERT, a BERTbased baseline, our basic proposed event causality identifier. 2) MM (Liu et al., 2020), the BERTbased SOTA method with mention masking generalization. 3) MM+Aug, the further re-trained MM with our dual augmented data. 4) KnowDis (Zuo et al., 2020) improved the performance of ECI with the distantly labeled training data. We compare with it to illustrate the quality of our generated ECI-related training data. 5) MM+ConceptAug, to make a fair comparison, we introduce causalrelated events from Conc"
2021.acl-long.276,W14-0702,0,0.0314991,"th linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for 3559 Knowledge relation-&gt;sentence-&gt;relation Annotated Data Pre-training event pair (ep) R causal/non-causal relation (c) Primal Cycle sentence-&gt;relation-&gt;sentence G Dual Cycle Pre-trained Generator causal/ non-causal relaiton event pair Pre-trained Identifier Learnable"
2021.acl-long.276,C14-1198,0,0.104622,"vious data augmentation methods. 2 Related Work To date, many researches attempt to identify the causality with linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for 3559 Knowledge relation-&gt;sentence-&gt;relation Annotated Data Pre-training event pair (ep) R causal/non-causal relation (c) Primal Cycle sentence-&gt;relation-&gt;sentence G D"
2021.acl-long.276,W14-4322,0,0.148529,"aims to identify causal relations between events in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017). This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in cause two sentences: (1) attack −→ killed in S1; (2) cause statement −→ protests in S2. Most existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016; Riaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NL"
2021.acl-long.276,C16-1007,0,0.131821,"ity identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017). This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in cause two sentences: (1) attack −→ killed in S1; (2) cause statement −→ protests in S2. Most existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016; Riaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. H"
2021.acl-long.276,W14-0707,0,0.126083,"aims to identify causal relations between events in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017). This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in cause two sentences: (1) attack −→ killed in S1; (2) cause statement −→ protests in S2. Most existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016; Riaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NL"
2021.acl-long.276,W16-1007,0,0.0177884,"s (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for 3559 Knowledge relation-&gt;sentence-&gt;relation Annotated Data Pre-training event pair (ep) R causal/non-causal relation (c) Primal Cycle sentence-&gt;relation-&gt;sentence G Dual Cycle Pre-trained Generator causal/ non-causal relaiton event pair Pre-trained Identifier Learnable Dual Augmentation Architecture Causal-Generator Rs NCausal-Generator Rc Dual-trained Identifier Rc Further training Sentence→Relation I Full-trained Identifier ep, c&apos; Primal Cycle Figure 2: Overvi"
2021.acl-long.276,P13-1170,0,0.0385941,"Missing"
2021.acl-long.276,P02-1040,0,0.109347,"impact on ECI; 2) Back translation introduces limited new causal expressions by translation, thus it slightly increases the recall value on ECI; 3) EDA can introduce new expressions via substitution, but the augmented data is not canonical and cannot accurately express the causality, therefore, its impact on ECI is also limited. Quantitative Evaluation of Task-relevance We select five Ph.D. students majoring in NLP to manual score the 100 randomly selected augmented sentences given their corresponding original sentences as reference (Cohen’s kappa = 0.85). Furthermore, we calculate the BLEU (Papineni et al., 2002) value to further evaluate the 3565 Generator Dual reward feedback A was crash by B because C targeted ... A was crash by B as C targeted ... Identifier non-causal relation a) ate task-related sentences for ECI. Moreover, our framework is knowledge guided and learnable. Our method achieves state-of-the-art performance on EventStoryLine and Causal-TimeBank datasets. ... A order when B attack ... &lt;crash, target&gt; causal relation Generator non-causal relation causal relation Dual reward feedback Acknowledgments Identifier &lt;order, attack&gt; ... A ordered B to attack ... b) Figure 6: The modification"
2021.acl-long.276,W13-4004,0,0.0265743,"generated causal sentences. We also employ a constrained generative architecture to gradually generate well-formed causal linguistic expressions of generated causal sentences via iteratively learning in the dual interaction. • Experimental results on two benchmarks show that our model achieves the best performance on ECI. Moreover, it also shows definite advantages over previous data augmentation methods. 2 Related Work To date, many researches attempt to identify the causality with linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017). Recently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the p"
2021.acl-long.276,P19-1178,0,0.0633589,"Missing"
2021.acl-long.276,2020.acl-main.52,0,0.024628,"ard R consists of a causality reward Rc from itself and a semantic alignment reward Rs from G (dual cycle). I and G are optimized interactively with dual reinforcement learning. Specifically, for G, an action is the generation from relation to sentence, a state is denoted by the representation of input event pair and its relation, a policy is defined by the parameters of generator. For I, an action is the identification from sentence to relation, a state is denoted by the representation of input event pair and its 3560 sentence, a policy is defined by the parameters of identifier. Inspired by Shen and Feng (2020), we utilize a probability distribution over actions given states to represent the policys, i.e., the probability distribution of the generation of G and identification of I. As aforementioned, we introduce two rewards, causality (Rc ) and semantic alignment (Rs ) rewards, which encourage G to generate taskrelated sentences with the feedback from identifier, while further optimize I with the feedback from generator. Definitions are as following: Causality Reward (Rc ) If the relation of input event pair can be clearly expressed by the generated sentence, it will be easier to be understood by i"
2021.acl-long.276,P19-1545,0,0.0548547,"Missing"
2021.acl-long.276,2020.acl-main.63,0,0.0488736,"Missing"
2021.acl-long.276,D18-1236,0,0.0620774,"Missing"
2021.acl-long.276,D15-1306,0,0.053515,"Missing"
2021.acl-long.276,D19-1670,0,0.0193009,"problem that urgently needs to be addressed for ECI. Up to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b). In these frameworks, data augmentation and target task are modeled independently. This often leads to a lack of task-related characteristics in the generated data, such as taskrelated linguistic expression and knowledge. For example, easy data augmentation (EDA) (Wei and Zou, 2019) is the most representative method that relies on lexical substitution, deletion, swapping, and insertion to produce new data. However, solely relying on such word operations often generates new data that dissatisfies task-related qualities. As shown in Figure 1, S3 is produced by EDA, it lacks a linguistic expression that expresses the causal semantics between kill and attack. Therefore, how to 3558 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3558–3571 August 1–6, 2021"
2021.acl-long.276,P19-1522,0,0.117495,"e mention masking generalization. Unlike computer vision, the augmentation of text data in NLP is pretty rare (Chaudhary, 2020). Zuo et al. (2020) solved the data lacking problem of ECI with the distantly supervised labeled training data. However, including the distant supervision, most of the existing data augmentation methods for NLP tasks are task-independent frameworks (Related work of data augmentation and dual learning are detailed in Appendix B). Inspired by some generative methods which try to generate additional training data while preserving the class label (AnabyTavor et al., 2019; Yang et al., 2019; Papanikolaou and Pierleoni, 2020), we introduce a new learnable framework for augmenting task-related training data for ECI via dual learning enhanced with external knowledge. 3 ep, s&apos; Rs Dual Augmented Data causal/ non-causal sentence Relation→Sentence Methodology As shown in Figure 2, LearnDA jointly models a knowledge guided sentence generator (input: event pair and its causal/non-causal relation, output: causal/non-causal sentence) and an event causality identifier (input: event pair and its sentence, output: causal/non-causal relation) with dual learning. LearnDA iteratively optimizes i"
2021.acl-long.376,D14-1159,0,0.0805137,"Missing"
2021.acl-long.376,P19-1470,0,0.0232665,"unds e1 to a concept via matching the event mention with the tokens of concepts in C ONCEPT N ET. We enhance the matching approach with some rules, such as soft matching with lemmatization and filtering of stop words. The grounded concept is called zero-hop concept. Then, our method grows zero-hop concept with one-hop concepts. The zero-hop concept, one-hop concepts and all relations between them form the descriptive graph for e1 (denoted as Gd1 ). (2) If the descriptive knowledge cannot be retrieved from the KB, we adopt the generation method. Our method employs the pre-trained model, COMET (Bosselut et al., 2019), which is originally proposed for the knowledge base completion. Specifically, COMET is obtained by finetuning GPT (Radford et al., 2018) on C ONCEPTN ET. The input of COMET is the head event and candidate relation, and the output is the tail event. The relation types are the same as the ones used in Bosselut et al. (2019). By leveraging COMET, we can generate the descriptive graph Gd1 for e1 . In the same way, we can also construct the descriptive graph Gd2 for e2 . 3.2.2 Knowledge Encoding Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et"
2021.acl-long.376,W17-2711,0,0.254172,"l., 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014). Identifying event causal relation is inherently challenging, because event causality is usually expressed in diverse forms that often lack explicit clues indicating its existence. For example in Figure 1, the sentence has no explicit clue indicating the causal relation between “global warming” and “tsunami”. In this scenario, models can resort to a large amount of labeled data to learn diverse causal expressions. However, existing ECI datasets are very small. For example, the largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, which is not sufficient to train neural network models (Liu et al., 2020). Consequently, models cannot thoroughly understand the text and possibly make a wrong prediction. Nonetheless, humans could make a correct judgement, because humans have the background knowledge about the two events. To be more specific, humans not only know what the two events are, but also know the connection between them. Fortunately, existing knowledge bases (KBs) usually contain the Descriptive Knowledge of events and Relational Knowledge between events, which can be regarded as the bac"
2021.acl-long.376,P17-2001,0,0.0542099,"Missing"
2021.acl-long.376,D17-1190,0,0.123547,"evaluate our proposed method on two widely used datasets, including EventStoryLine (Caselli and Vossen, 2017) and Causal-TimeBank (Mirza et al., 2014). For EventStoryLine, the dataset contains 258 documents, 5,334 events in total, and 1,770 of 7,805 event pairs are causally related. For Causal-TimeBank, the dataset contains 184 documents, 6,813 events, and 318 of 7,608 event pairs are causally related. We conduct the 5-fold and 10fold cross-validation on the EventStoryLine dataset and Causal-TimeBank dataset respectively, same as previous methods to ensure fairness. Following previous works (Choubey and Huang, 2017; Gao et al., 2019), we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics. 4.2 Parameter Settings In our implementations, our method uses the HuggingFace’s Transformers library4 to implement the uncased BERT base model, which has 12-layers, 768-hidden, and 12-heads. The learning rate is initialized as 2e-5 with a linear decay. We use the Adam algorithm (Kingma and Ba, 2015) to optimize model parameters. The batch size is set to 20. The number of induction blocks (i.e., N ) is set to 2. The dropout of GCN is set to 0.3. Due to the sparseness of positive examples, we adopt"
2021.acl-long.376,N19-1423,0,0.00505751,"ptive knowledge for each event, and then encodes the graph-structured knowledge; (3) Relational Graph Induction (§3.3), which automatically induces a reasoning structure and performs causality reasoning on the induced structure. We will illustrate each component in detail. 3.1 Context Encoding Given a sentence with a pair of events (denoted as e1 and e2 ), the context encoding module aims to extract context features, which takes the sentence as input and outputs the context representations. Our context encoder is based on the Transformer architecture (Vaswani et al., 2017). We adopt the BERT (Devlin et al., 2019) to encode the input sentence,2 which has achieved the state-of-the-art performance for ECI task (Liu et al., 2020; Zuo et al., 2020). After using BERT encoder to compute the contextual representations of the entire sentence, we concatenate representations of [CLS], e1 and e2 as the context representation regarding to the event pair (e1 , e2 ), namely (e ,e2 ) FC 1 Methodology 2 Following previous works (Ning et al., 2018; Liu et al., 2020), we formulate ECI as a binary clas= h[CLS] ⊕ he1 ⊕ he2 , (1) Note that the encoder is not our focus in this paper. In fact, other models like convolutional"
2021.acl-long.376,D11-1027,0,0.0214556,"ted in the KB; (2) Not all the knowledge on the path is related to causality, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Causes Knowledge Encoding death ocean Descriptive Knowledge Representat"
2021.acl-long.376,N19-1179,0,0.0565021,"in the wikipedia page of “sea-level rising”1 , while it is not annotated in the KB; (2) Not all the knowledge on the path is related to causality, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Caus"
2021.acl-long.376,Q19-1019,0,0.0218713,"∈ Rd . We first calculate the pair-wise unnormalized attention score sij between the i-th node and the j-th node: where δ is the Kronecker delta (Koo et al., 2007) and ·−1 denotes matrix inversion. Ar can be regarded as a weighted adjacency matrix of the graph Gr . Finally, Ar is fed into the iterative refinement for event causality reasoning. 3.3.3 Iterative Refinement After obtaining the relational graph structure, we perform event causality reasoning on the induced structure. To better capture potential reasoning clues, we adopt the densely connected graph convolutional networks (DCGCNs) (Guo et al., 2019), which allows training a deeper reasoning model. The convolution computation of each layer is: (l) vi nr X (l) = ρ( Arij Wv(l) gj + b(l) v ), T (10) j=1 sij = (tanh(Wp mi )) Wb (tanh(Wc mj )), (4) (l) where Wp and Wc are weights matrixes. Wb are the weights for the bilinear transformation. Next, we compute the root score sri which represents the unnormalized probability of the i-th node to be selected as the root node of the structure: sri = Wr mi , (5) where Wr ∈ R1×d is the weight for linear transformation. Suppose the graph Gr has nr nodes, we first assign non-negative weights P ∈ Rnr ×nr"
2021.acl-long.376,D19-1296,0,0.0119862,"ectly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Causes Knowledge Encoding death ocean Descriptive Knowledge Representation Descriptive Graph Relational Knowledge Representation Relational Graph Induction global warming Causes sea-level rising CapableOf tsunam"
2021.acl-long.376,P14-1093,0,0.169075,"ledge base. Introduction Event causality identification (ECI) aims to identify causal relation of events in texts. For example, in the sentence “The earthquake generated a tsunami.”, an ECI model should be able to identify a causal relationship that holds between the two cause mentioned events, i.e., earthquake −−−→ tsunami. ECI is an important task in natural language processing (NLP) area and can support many NLP applications, such as machine reading comprehension (Berant et al., 2014), process extraction (Thalappillil Scaria et al., 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014). Identifying event causal relation is inherently challenging, because event causality is usually expressed in diverse forms that often lack explicit clues indicating its existence. For example in Figure 1, the sentence has no explicit clue indicating the causal relation between “global warming” and “tsunami”. In this scenario, models can resort to a large amount of labeled data to learn diverse causal expressions. However, existing ECI datasets are very small. For example, the largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, which is not sufficient to tra"
2021.acl-long.376,W17-2708,0,0.0207882,") Not all the knowledge on the path is related to causality, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Causes Knowledge Encoding death ocean Descriptive Knowledge Representation Descriptive Gr"
2021.acl-long.376,W17-5540,0,0.0235892,"cean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Causes Knowledge Encoding death ocean Descriptive Knowledge Representation Descriptive Graph Relational Knowledge Representation Relational Graph Induction global warming Causes sea-level risin"
2021.acl-long.376,D19-1590,0,0.0116605,"auses Knowledge Encoding death ocean Descriptive Knowledge Representation Descriptive Graph Relational Knowledge Representation Relational Graph Induction global warming Causes sea-level rising CapableOf tsunami AtLocation glacier melting ocean AtLocation Iterative Refinement Structure Induction Relational Graph Relational Path global Causes tsunami warming Figure 2: The architecture of our proposed latent structure induction network for event causality identification. network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020). Liu et al. (2020) propose a mention masking generalization method and also consider the external structural knowledge. The very recent work (Zuo et al., 2020) propose a data augmentation method to alleviate the data lacking problem for the task. Regarding datasets construction, Mirza (2014) annotates the CausalTimeBank dataset about event causal relations in the TempEval-3 corpus. Caselli and Vossen (2017) construct a dataset called EventStoryLine for event causality identification. Despite many efforts for this task, most existing methods typically train"
2021.acl-long.376,D07-1015,0,0.351579,"andomly select one path for avoiding information redundancy. 3.3.2 Structure Induction To capture potentially useful information and reduce the impact of irrelevant knowledge on the relational path, our model treats the reasoning structure as a latent variable and induces it with the 4865 3 https://networkx.org input of the relational path, which can be shown in Figure 2. We call the induced reasoning structure as Relational Graph (denoted as Gr ). The structure induction module is built based on the structured attention (Kim et al., 2017). We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Nan et al., 2020) to learn the graph structure. Formally, the nodes of relational graph are the concepts on the relational path. The initialized representation of each node is obtained via the pretrained model (i.e., BERT). The representation of the i-th node is denoted as mi ∈ Rd . We first calculate the pair-wise unnormalized attention score sij between the i-th node and the j-th node: where δ is the Kronecker delta (Koo et al., 2007) and ·−1 denotes matrix inversion. Ar can be regarded as a weighted adjacency matrix of the graph Gr . Finally, Ar is fed into the iterative refinement for ev"
2021.acl-long.376,P14-3002,0,0.018477,"uses tsunami warming Figure 2: The architecture of our proposed latent structure induction network for event causality identification. network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020). Liu et al. (2020) propose a mention masking generalization method and also consider the external structural knowledge. The very recent work (Zuo et al., 2020) propose a data augmentation method to alleviate the data lacking problem for the task. Regarding datasets construction, Mirza (2014) annotates the CausalTimeBank dataset about event causal relations in the TempEval-3 corpus. Caselli and Vossen (2017) construct a dataset called EventStoryLine for event causality identification. Despite many efforts for this task, most existing methods typically train the models on manually labeled data solely, rarely considering the external structural knowledge. As a result, these methods cannot handle well the cases where there is no explicit causal clue. Although Liu et al. (2020) leverage the descriptive knowledge to enrich event representations, they directly retrieve the descriptive k"
2021.acl-long.376,W14-0702,0,0.0310344,"inary classification by taking Fe1 ,e2 as input: pe1 ,e2 = softmax(Ws Fe1 ,e2 + bs ). (13) For training, we adopt cross entropy as the loss function: X X J(Θ) = − yei ,ej log(pei ,ej ), (14) s∈D ei ,ej ∈Es ei 6=ej 4866 where Θ denotes the model parameters. s denotes a sentence in the training set D. Es is the set of events in sentence s. yei ,ej is a one-hot vector representing the gold label between ei and ej . 4 4.1 Experiments Datasets and Evaluation Metrics We evaluate our proposed method on two widely used datasets, including EventStoryLine (Caselli and Vossen, 2017) and Causal-TimeBank (Mirza et al., 2014). For EventStoryLine, the dataset contains 258 documents, 5,334 events in total, and 1,770 of 7,805 event pairs are causally related. For Causal-TimeBank, the dataset contains 184 documents, 6,813 events, and 318 of 7,608 event pairs are causally related. We conduct the 5-fold and 10fold cross-validation on the EventStoryLine dataset and Causal-TimeBank dataset respectively, same as previous methods to ensure fairness. Following previous works (Choubey and Huang, 2017; Gao et al., 2019), we adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics. 4.2 Parameter Settings In our i"
2021.acl-long.376,C14-1198,0,0.0547001,"Missing"
2021.acl-long.376,2020.acl-main.141,0,0.0775993,"path for avoiding information redundancy. 3.3.2 Structure Induction To capture potentially useful information and reduce the impact of irrelevant knowledge on the relational path, our model treats the reasoning structure as a latent variable and induces it with the 4865 3 https://networkx.org input of the relational path, which can be shown in Figure 2. We call the induced reasoning structure as Relational Graph (denoted as Gr ). The structure induction module is built based on the structured attention (Kim et al., 2017). We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Nan et al., 2020) to learn the graph structure. Formally, the nodes of relational graph are the concepts on the relational path. The initialized representation of each node is obtained via the pretrained model (i.e., BERT). The representation of the i-th node is denoted as mi ∈ Rd . We first calculate the pair-wise unnormalized attention score sij between the i-th node and the j-th node: where δ is the Kronecker delta (Koo et al., 2007) and ·−1 denotes matrix inversion. Ar can be regarded as a weighted adjacency matrix of the graph Gr . Finally, Ar is fed into the iterative refinement for event causality reaso"
2021.acl-long.376,P18-1212,0,0.0495353,"Missing"
2021.acl-long.376,D19-1282,0,0.102953,"T (Bosselut et al., 2019), which is originally proposed for the knowledge base completion. Specifically, COMET is obtained by finetuning GPT (Radford et al., 2018) on C ONCEPTN ET. The input of COMET is the head event and candidate relation, and the output is the tail event. The relation types are the same as the ones used in Bosselut et al. (2019). By leveraging COMET, we can generate the descriptive graph Gd1 for e1 . In the same way, we can also construct the descriptive graph Gd2 for e2 . 3.2.2 Knowledge Encoding Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. In addition, many works show that relational graph convolutional networks (RGCNs) (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019). We thus apply GCNs (Kipf and Welling, 2017) to encode the related descriptive knowledge of e1 and e2 . Formally, given a descriptive graph Gd (i.e., Gd1 or Gd2 ) with nd nodes (i.e., concepts), which can be represented with an"
2021.acl-long.376,W13-4004,0,0.0293358,"tsunami) is described in the wikipedia page of “sea-level rising”1 , while it is not annotated in the KB; (2) Not all the knowledge on the path is related to causality, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house Cap"
2021.acl-long.376,W14-4322,0,0.0261063,"lity, such as (sea-level rising, AtLocation, ocean). Therefore, directly reasoning along the multi-hop path struc1 https://en.wikipedia.org/wiki/Sea_ level_rise 2 Related Work Event causality identification (ECI) is a very important task in natural language processing area, which has attracted extensive attention in the past few years. Early studies for the task are feature-based methods which utilize lexical and syntactic features (Riaz and Girju, 2013; Gao et al., 2019), explicit causal patterns (Beamer and Girju, 2009; Do et al., 2011; Hu et al., 2017), and statistical causal associations (Riaz and Girju, 2014; Hashimoto et al., 2014; Hu and Walker, 2017; Hashimoto, 2019) for the task. With the development of deep learning, neural 4863 Context Encoding Deep Transformer Global warming worsened, and tsunami strengthened. Context Representation greenhouse gas Descriptive Graph Induction glacier melting CreatedBy Knowledge Obtaining global warming IsA heating ConceptNet tidal wave Causes IsA AtLocation IsA temperature change destroy house CapableOf tsunami Causes Knowledge Encoding death ocean Descriptive Knowledge Representation Descriptive Graph Relational Knowledge Representation Relational Graph In"
2021.acl-long.376,N19-1173,0,0.0241258,"ights P ∈ Rnr ×nr to the edges of the induced relational graph: ( 0, if i = j Pij = (6) exp(sij ), otherwise, where Pij is the weight of the edge between the i-th and the j-th node. Then, following Koo et al. (2007), we define the Laplacian matrix ˆ ∈ Rnr ×nr , L ∈ Rnr ×nr of Gr , and its variant L respectively: (P nr k=1 Pkj , if i = j Lij = (7) −Pij , otherwise, where gj is the concatenation of the initial node representation and the node representations pro(l) duced in layers 1, . . . , l − 1, namely gj = mj ⊕ (1) (l−1) . vj ⊕ · · · ⊕ vj The induced structure at once is relatively shallow (Liu et al., 2019; Nan et al., 2020) and may not be optimal for causality reasoning. Therefore, we iteratively refine the induced structure to learn a more informative structure. We stack N blocks (each block is structure induction and DCGCNs reasoning) of this module to induce the structure N times. Intuitively, as the structure gets more refined, the structure is more reasonable. After the iterative refinement, the representations of e1 and e2 are denoted as ve1 and ve2 , respectively. We concatenate them as the relational knowledge representation: (e ,e2 ) FR 1 3.4 ˆ ij = L exp(sri ), Lij , if i = 1 otherwi"
2021.acl-long.376,D13-1177,0,0.0252817,"ledge for ECI task. The dashed arrow indicates a missing link in the knowledge base. Introduction Event causality identification (ECI) aims to identify causal relation of events in texts. For example, in the sentence “The earthquake generated a tsunami.”, an ECI model should be able to identify a causal relationship that holds between the two cause mentioned events, i.e., earthquake −−−→ tsunami. ECI is an important task in natural language processing (NLP) area and can support many NLP applications, such as machine reading comprehension (Berant et al., 2014), process extraction (Thalappillil Scaria et al., 2013) and future event prediction (Radinsky et al., 2012; Hashimoto et al., 2014). Identifying event causal relation is inherently challenging, because event causality is usually expressed in diverse forms that often lack explicit clues indicating its existence. For example in Figure 1, the sentence has no explicit clue indicating the causal relation between “global warming” and “tsunami”. In this scenario, models can resort to a large amount of labeled data to learn diverse causal expressions. However, existing ECI datasets are very small. For example, the largest dataset EventStoryLine (Caselli a"
2021.acl-long.376,2020.findings-emnlp.369,0,0.0241244,"A, temperature change), (global warming, CreatedBy, greenhouse gas) and so on. If the model can make use of such knowledge, it is obvious that the model can better understand the meaning of the event itself than using only the given text. Therefore, incorporating the descriptive knowledge is very helpful for this task. However, when leveraging this kind of knowledge, we find two critical challenges: (1) As shown in Figure 1, the descriptive knowledge forms a sub-graph. How to effectively encode the graph-structured knowledge is a very challenging problem; (2) The knowledge base is incomplete (Wang et al., 2020), which will inevitably cause the descriptive knowledge of some events cannot be obtained from the KB. Thus, the model should have the ability to obtain and encode such knowledge, even if it does not exist in the KB. Relational Knowledge: The external knowledge base contains connections between events, which can be referred as the relational knowledge between events. It is usually defined by the multi-hop path between two events. This kind of knowledge can provide useful information for event causality reasoning, especially when the text lacks causal clues. For example in Figure 1, the relatio"
2021.acl-long.376,D19-1451,0,0.0115509,", 2019), which is originally proposed for the knowledge base completion. Specifically, COMET is obtained by finetuning GPT (Radford et al., 2018) on C ONCEPTN ET. The input of COMET is the head event and candidate relation, and the output is the tail event. The relation types are the same as the ones used in Bosselut et al. (2019). By leveraging COMET, we can generate the descriptive graph Gd1 for e1 . In the same way, we can also construct the descriptive graph Gd2 for e2 . 3.2.2 Knowledge Encoding Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. In addition, many works show that relational graph convolutional networks (RGCNs) (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019). We thus apply GCNs (Kipf and Welling, 2017) to encode the related descriptive knowledge of e1 and e2 . Formally, given a descriptive graph Gd (i.e., Gd1 or Gd2 ) with nd nodes (i.e., concepts), which can be represented with an nd × nd adjacency m"
2021.acl-long.376,D18-1244,0,0.0258851,"By leveraging COMET, we can generate the descriptive graph Gd1 for e1 . In the same way, we can also construct the descriptive graph Gd2 for e2 . 3.2.2 Knowledge Encoding Graph neural networks have been widely used to encode graph-structured data (Lin et al., 2019; Yang et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. In addition, many works show that relational graph convolutional networks (RGCNs) (Schlichtkrull et al., 2018) usually overparameterize the model and cannot effectively utilize multi-hop relational information (Zhang et al., 2018; Lin et al., 2019). We thus apply GCNs (Kipf and Welling, 2017) to encode the related descriptive knowledge of e1 and e2 . Formally, given a descriptive graph Gd (i.e., Gd1 or Gd2 ) with nd nodes (i.e., concepts), which can be represented with an nd × nd adjacency matrix Ad . If there is a connection between node i and node j, the Adij is set to 1. For the node i at the l-th layer, the convolution computation can be defined as follows: (l) ui nd X (l−1) = ρ( Adij Wu(l) uj + b(l) u ), (2) j=1 (l) (l) where Wu and bu are the weight matrix and bias vector for the l-th layer, respectively. ρ is a"
2021.acl-long.376,2020.coling-main.135,1,0.882817,"riptive Knowledge Representation Descriptive Graph Relational Knowledge Representation Relational Graph Induction global warming Causes sea-level rising CapableOf tsunami AtLocation glacier melting ocean AtLocation Iterative Refinement Structure Induction Relational Graph Relational Path global Causes tsunami warming Figure 2: The architecture of our proposed latent structure induction network for event causality identification. network-based methods have been proposed for the task and achieved the state-of-the-art performance (Kruengkrai et al., 2017; Kadowaki et al., 2019; Liu et al., 2020; Zuo et al., 2020). Liu et al. (2020) propose a mention masking generalization method and also consider the external structural knowledge. The very recent work (Zuo et al., 2020) propose a data augmentation method to alleviate the data lacking problem for the task. Regarding datasets construction, Mirza (2014) annotates the CausalTimeBank dataset about event causal relations in the TempEval-3 corpus. Caselli and Vossen (2017) construct a dataset called EventStoryLine for event causality identification. Despite many efforts for this task, most existing methods typically train the models on manually labeled data"
2021.acl-long.417,D18-1407,0,0.0631541,"Missing"
2021.acl-long.417,P18-2103,0,0.0994309,"ation. It shows that the model reaches the right prediction reasonably: it identifies People – Passengers, walk through– car driving and store – street to make up the alignment rationale. A REC is flexible to apply on any co-attention architectures, allowing us for deep investigations of well-trained models. With the proposed A REC, we study three typical co-attention based models Decomposable Attention (DA) (Parikh et al., 2016), Enhanced LSTM (ESIM) (Chen et al., 2017) and BERT (Devlin et al., 2019) on four benchmarks including SNLI (Bowman et al., 2015), ESNLI (Camburu et al., 2018), BNLI (Glockner et al., 2018) and HANS (McCoy et al., 2019). Experimental results show that our method could generate more faithful and readable explanations. Moreover, we employ our proposed A REC to analyze these models deeply from the aspect of alignments. Based on our explanations, we further present a simple improvement strategy that greatly increases robustness of different models without modifying their architectures or retraining. This proves that our method could factually reflect how models work. Our contributions are summarized as follows: 1) We come up with A REC, a post-hoc local explanation method to extract"
2021.acl-long.417,N19-1357,0,0.119102,"pages 5372–5387 August 1–6, 2021. ©2021 Association for Computational Linguistics alignments instead of isolated words/phrases. For the example in Figure 1, the contradicted phrase pair street – store is one of the key alignments responsible for the correct prediction. To explain NLI models over alignments, the literature usually looks at co-attention weights (Parikh et al., 2016; Pang et al., 2016; Chen et al., 2017), which is a dominant way to implicitly align word pairs (Wang et al., 2017; Gong et al., 2018; Devlin et al., 2019). However, attention is argued not as explainable as expected (Jain and Wallace, 2019; Serrano and Smith, 2019; Bastings and Filippova, 2020). Moreover, co-attention assigns scores among words thus forbids us to observe phraselevel alignments, which is a flaw that generally exists for attribution explanations as shown in Figure 1 (c). Other works build hard alignments resorting sparse attention (Yu et al., 2019; Bastings et al., 2019; Swanson et al., 2020). But their selfexplanatory architectures pay for the interpretability at a cost of performance dropping on accuracy (Molnar, 2020). Meanwhile, these techniques are unable to analyze well-trained models. To resolve above prob"
2021.acl-long.417,2020.acl-main.409,0,0.0178683,"2 respectively. We obtain the following findings: 7 We don’t use Area Over Perturbation Curve (AOPC) (DeYoung et al., 2020) because our method is to reserve features (i.e., alignments) that keep the prediction, it is fitter to utilize reservation curve. 8 Both annotators are well-educated postgraduates major in computer science. We conduct human evaluation on randomly sampled 300 examples in SNLI testing set. 9 The threshold is set to L0 + 0.1 of A REC to obtain alignment rationales with similar fidelity for fair comparison. We don’t use fix size constraint to construct rationales as done in Jain et al. (2020) because we think the size of a rationale depends on the instance. 1) A REC is quite faithful with the lowest AORC and fidelity value in most cases. Perturbation-based methods are equally matched with moderate performances, while gradient-based ones have the least faithfulness. Surprisingly, co-attention is a very strong baseline to indicate important alignments for NLI, surpassing most other baselines on AORC, extremely for ESIM. This result is of accordance with Vashishth et al. (2019) that attention is more faithful in cross-sentence tasks compared with singlesentence tasks. 2) A REC is qui"
2021.acl-long.417,N06-1006,0,0.234961,"Missing"
2021.acl-long.417,N18-1101,0,0.0250462,"People walk through a store . (c) Alignment Attribution (Gradient) (d) Alignment Rationale (AREC) Passengers in a rusty yellow car driving down the street . a store walk through People . a store walk through People . Figure 1: Different post-hoc explanations. For attribution explanations, features with deeper colors are considered more important. Introduction Natural Language Inference (NLI) is a fundamental task in Natural Language Processing (NLP) which is to determine if a hypothesis entails a premise. Recently, with the introduction of large-scale annotated datasets (Bowman et al., 2015; Williams et al., 2018), deep learning models are adopted to solve the task in a supervised manner (Conneau et al., 2017; Chen et al., 2017; Devlin et al., 2019) and achieve great success, while inner mechanisms of these methods are still opaque due to high computational complexities. Towards interpretability, explaining the model behavior has gained increasing attention. Lots of approaches are based on feature attribution which 1 Our code is available at https://github.com/ changmenseng/arec assigns saliency scores for input features (Bahdanau et al., 2015; Lundberg and Lee, 2017; Thorne et al., 2019; Kim et al., 2"
2021.acl-long.463,2020.acl-main.282,1,0.889912,"e code set and tedious clinical notes. As statistics, the cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007). Automatic ICD coding methods (Stanfill et al., 2010) have been proposed to resolve the deficiency of manual annotation, regarding it as a multi-label text classification task. As shown in Figure 1, given a plain clinical text, the model tries to predict all the standardized codes from ICD-9. Recently, neural networks were introduced (Mullenbach et al., 2018) (Falis et al., 2019) (Cao et al., 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al., 2014) in ICD coding task, and great progresses have been made. Although effective, those methods either ignore the 5948 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5948–5957 August 1–6, 2021. ©2021 Association for Computational Linguistics long-tail distribution of the code frequency or not target the noisy text in clinical"
2021.acl-long.463,D19-6220,0,0.0506867,"Missing"
2021.acl-long.463,N18-1100,0,0.399657,"professional coders, due to the large candidate code set and tedious clinical notes. As statistics, the cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007). Automatic ICD coding methods (Stanfill et al., 2010) have been proposed to resolve the deficiency of manual annotation, regarding it as a multi-label text classification task. As shown in Figure 1, given a plain clinical text, the model tries to predict all the standardized codes from ICD-9. Recently, neural networks were introduced (Mullenbach et al., 2018) (Falis et al., 2019) (Cao et al., 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al., 2014) in ICD coding task, and great progresses have been made. Although effective, those methods either ignore the 5948 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5948–5957 August 1–6, 2021. ©2021 Association for Computational Linguistics long-tail distribution of the code frequency o"
2021.acl-long.463,P18-1098,0,0.0191667,"based methods firstly brought to solve this task. (Larkey and Croft, 1996) explored traditional machine learning algorithms, including KNN, relevance feedback, and Bayesian applying to ICD coding. (Perotte et al., 2014) utilized SVM for classification in consideration of the hierarchy of ICD codes. With the popularity of neural networks, researchers have proven the effectiveness of CNN and LSTM in ICD coding task. (Mullenbach et al., 2018) propose a convolutional neural network with an attention mechanism to capture each code’s desire information in source text also exhibit interpretability. (Xie and Xing, 2018) develop tree LSTM to utilize code descriptions. To further improve the performance, customized structures were introduced to utilize the code cooccurrence and code hierarchy of ICD taxonomies. (Cao et al., 2020) embedded the ICD codes into hyperbolic space to explore their hierarchical nature and constructed a co-graph to import code co-occurrence prior. We argue that they capture code co-occurrence in a static manner rather than dynamic multi-hop relations. (Vu et al., 2020) consider learning attention distribution for each code and introduce hierarchical joint learning architecture to handl"
2021.acl-long.463,2020.coling-main.311,0,0.0255211,"shared multi-attention for multi-label image labeling. Our work further constructs a label interaction module for label relevant shared representation to utilize dynamic label co-occurrence. Lots of effects tried to normalize noisy texts before inputting to downstream tasks. (Vateekul and Koomsubha, 2016) (Joshi and Deshpande, 2018) apply pre-processing techniques on twitter data for sentiment classification. (Lourentzou et al., 2019) utilized seq2seq model for text normalization. Others targeted at noisy input in an end2end manner by designing customized architecture. (Sergio and Lee, 2020) (Sergio et al., 2020). Different from previous works on noisy text, our method neither need extra text processing nor bring in specific parameters. 3 Method This section describes our interactive shared representation learning mechanism and self-distillation learning paradigm for ICD coding. Figure 2 shows the architecture of interactive shared representation networks and manifest the inference workflow of our method. We first encode the source clinical note to the hidden state with a multi-scale convolution neural network. Then a shared attention module further extracts code relevant information shared among all"
2021.acl-long.492,W06-0901,0,0.127515,"0000 shares of the company. The 20000 shares were frozen by the Shenzhen Inter mediate People's Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed w"
2021.acl-long.492,W18-2311,0,0.0617749,"Missing"
2021.acl-long.492,P19-3006,0,0.0180635,"ing the relationship between event roles and more inter-attention modules allow for integrating information of candidate arguments into roles. 2 61.6 71.3 65.4 62.9 50.8 42.1 35.7 62.1 59.6 53.2 38.3 26.4 3 4 Number of Annotated Events >=5 Figure 4: F1-score for performance differences of generated events. have been proposed to improve performance on this task. These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020). A few methods make extraction decisions beyond individual sentences. Ji and Grishman (2008) and Liao and Grishman (2010) used event type co-occurrence patterns for event detection. Yang and Mitchell (2016) introduced event structure to jointly extract events and entities within a document. Although these approaches make decisions beyond sentence boundary, their extractions are still done at the sentence level. 4.2 Document-level Event Extraction Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-"
2021.acl-long.492,2020.aacl-main.81,1,0.801741,"decisions beyond sentence boundary, their extractions are still done at the sentence level. 4.2 Document-level Event Extraction Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-4 (1992) proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event argument extraction model by conditional text generation. However, the"
2021.acl-long.492,P15-1017,1,0.914082,"Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specifi"
2021.acl-long.492,doddington-etal-2004-automatic,0,0.163991,"d Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE: arguments-scattering and multievents. Specifically, arguments-scattering indicates that arguments of an event may scatter across multiple sentences. For example, As shown in Figure 1, the arguments of Event-1 are distributed"
2021.acl-long.492,2020.acl-main.714,0,0.0114849,"their extractions are still done at the sentence level. 4.2 Document-level Event Extraction Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-4 (1992) proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event argument extraction model by conditional text generation. However, these works focused on the sub-task of DEE"
2021.acl-long.492,2020.acl-main.718,0,0.098399,"Missing"
2021.acl-long.492,P11-1113,0,0.0317212,"the Shenzhen Inter mediate People's Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In co"
2021.acl-long.492,P11-2000,0,0.241582,"Missing"
2021.acl-long.492,P18-1201,0,0.0136029,"People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE: arguments-scattering and multievents. Specifically, arguments-sc"
2021.acl-long.492,P11-1114,0,0.0334207,"act events and entities within a document. Although these approaches make decisions beyond sentence boundary, their extractions are still done at the sentence level. 4.2 Document-level Event Extraction Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-4 (1992) proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event ar"
2021.acl-long.492,E12-1029,0,0.047866,"Missing"
2021.acl-long.492,P08-1030,0,0.107392,"of the company. The 20000 shares were frozen by the Shenzhen Inter mediate People's Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event informati"
2021.acl-long.492,W15-4502,0,0.0187149,"e decoder layers, the better performance on the results. We conjecture that more layers of the decoder with the more self-attention modules allow for better modeling the relationship between event roles and more inter-attention modules allow for integrating information of candidate arguments into roles. 2 61.6 71.3 65.4 62.9 50.8 42.1 35.7 62.1 59.6 53.2 38.3 26.4 3 4 Number of Annotated Events >=5 Figure 4: F1-score for performance differences of generated events. have been proposed to improve performance on this task. These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Bj¨orne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020). A few methods make extraction decisions beyond individual sentences. Ji and Grishman (2008) and Liao and Grishman (2010) used event type co-occurrence patterns for event detection. Yang and Mitchell (2016) introduced event structure to jointly extract events and entities within a document. Although these approaches make decisions beyond sentence boundary, their extractions are still done at the senten"
2021.acl-long.492,P13-1008,0,0.075044,"mediate People's Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, th"
2021.acl-long.492,2021.naacl-main.69,0,0.0398292,"le types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event argument extraction model by conditional text generation. However, these works focused on the sub-task of DEE (i.e., role filler extraction or argument extraction) and ignored the challenge of multi-events. To simultaneously address both challenges for DEE (i.e., arguments-scattering and multi-events), previous works focus on the ChFinAnn (Zheng et al., 2019) dataset and model DEE as an event table filling task, i"
2021.acl-long.492,P10-1081,0,0.124062,"000 shares were frozen by the Shenzhen Inter mediate People's Court on November 1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the wh"
2021.acl-long.492,2020.emnlp-main.128,1,0.899655,"r 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE: arguments-scattering and multievents. Specifically, arguments-scattering indicates that arguments of a"
2021.acl-long.492,N16-1034,0,0.0991911,"1, 2018. Jing Yan 10000 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE:"
2021.acl-long.492,D09-1016,0,0.0464206,"ent structure to jointly extract events and entities within a document. Although these approaches make decisions beyond sentence boundary, their extractions are still done at the sentence level. 4.2 Document-level Event Extraction Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-4 (1992) proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an en"
2021.acl-long.492,N16-1033,0,0.0718802,"00 shares 20000 shares Shenzhen Intermediate People's Court October 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE: arguments-scattering and"
2021.acl-long.492,P18-4009,1,0.902203,"ent with the same event type and there is no obvious textual boundary between the two events. The multi-events problem requires the DEE method to recognize how many events are contained in a document and achieve accurate arguments assembling (i.e., assign arguments to the corresponding event). As a result of these two complications, SEE methods are ill-suited for the DEE task, which calls for a model that can integrate document-level information, assemble relevant arguments across multiple sentences and capture multiple events simultaneously. To handle these challenges in DEE, previous works (Yang et al., 2018; Zheng et al., 2019) formulate DEE as an event table filling task, i.e., filling candidate arguments into a predefined event table. Specifically, they model the DEE as a serial prediction paradigm, in which arguments are predicted in a predefined role order and multiple events are also extracted in predefined event order. Such a manner is restricted to the extraction of individual arguments, and the former extraction will not consider the latter extraction results. As a result, errors will be propagated and the extraction performance is under satisfaction. In this paper, to avoid the shortage"
2021.acl-long.492,P19-1522,0,0.0713168,"ber 30, 2018 October 30, 2019 November 1, 2018  Figure 1: An example of a document contains two Equity Freeze type events: Event-1 and Event-2. Words in bold-faced are arguments that scatter across multiple sentences. Introduction The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace Shanghai Fukong Co., Ltd a sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document. In contrast to SEE, there are two specific challenges in DEE: arguments-scattering and multievents. Specifically, arguments-scattering indicates"
2021.acl-long.492,2020.acl-main.667,0,0.0276894,") proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset 6305 WIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event argument extraction model by conditional text generation. However, these works focused on the sub-task of DEE (i.e., role filler extraction or argument extraction) and ignored the challenge of multi-events. To simultaneously address both challenges for DEE (i.e., arguments-scattering and multi-events), previous works"
2021.acl-long.492,D19-1032,0,0.0457033,"Missing"
2021.eacl-main.175,grivaz-2010-human,0,0.0853443,"Missing"
2021.eacl-main.175,W17-5540,0,0.065173,"Missing"
2021.eacl-main.175,W14-2903,0,0.0714461,"Missing"
2021.eacl-main.175,W10-0206,0,0.110895,"Missing"
2021.eacl-main.175,P16-1101,0,0.0386419,"future research. The selected baselines are as followed (see more details in Appendix C): Regular Expressions (RegExp): In this setting, we regard the FinReason task as a causal sentence detection problem and employ some ad-hoc regular expressions to solve it. Specifically, we use five modifiers (因为(because),由于(since),原 因(cause),为(in order to),目的(aims to)) as causal clues to detect the sentence as the reasons for an event. BiLSTM-CRF (BiLSTM): We can take the reasons as one part of the event description and regard the task as an EE task. Similar to Yang et al. (2018), we employ a BiLSTM-CRF (Ma and Hovy, 2016) to predict the start and end positions of each reason. Specifically, We simply get the event participants in the documents via string matching between the documents and the given structural events. Such information is used as features in a BIO tagging format. BERT-QA: We can take this task as an MRC problem if the structural event is regarded as a query and the target reason as the answer. In particular, we use templates to turn each structural event into a why-question and employ BERT-QA (Devlin et al., 2019) model to find the corresponding reasons. Type RegExp BiLSTM BERT-QA Human Pledge O/"
2021.eacl-main.175,C14-1198,0,0.170116,"n 2 for Event 1 and Event 2 respectively. Introduction Why does the event happen? People are always eager to find the reasons for an event. Automatically extracting the causal explanations of the given events from texts is useful and important for common users and downstream applications. For example, in the financial domain, returning the reasons of a concerned financial event in an Information Retrieval system can free analysts from reading the enormous company announcements and help investors make financial decisions. Previous work on event causality (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014; Caselli and Vossen, 2017) mainly focus on ∗ Most of the work was done when the first author was a research engineer in the Institute of Automation, CAS. the identification of causal relations between two given events that are usually presented as event trigger words. However, in reality, users may only know a particular event happened but without knowing its mention or trigger in the documents, and they just wonder the reasons for it. Therefore, we propose a new task aiming at extracting the causal explanations of the given structurally presented events from document-level texts. Specificall"
2021.eacl-main.175,P18-2124,0,0.077376,"Missing"
2021.eacl-main.175,W13-4004,0,0.0221398,"act Reason 1 and Reason 2 for Event 1 and Event 2 respectively. Introduction Why does the event happen? People are always eager to find the reasons for an event. Automatically extracting the causal explanations of the given events from texts is useful and important for common users and downstream applications. For example, in the financial domain, returning the reasons of a concerned financial event in an Information Retrieval system can free analysts from reading the enormous company announcements and help investors make financial decisions. Previous work on event causality (Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014; Caselli and Vossen, 2017) mainly focus on ∗ Most of the work was done when the first author was a research engineer in the Institute of Automation, CAS. the identification of causal relations between two given events that are usually presented as event trigger words. However, in reality, users may only know a particular event happened but without knowing its mention or trigger in the documents, and they just wonder the reasons for it. Therefore, we propose a new task aiming at extracting the causal explanations of the given structurally presented events from document"
2021.eacl-main.175,P18-4009,1,0.929031,"different aspects. However, they have found it is difficult to agree on if a causal relationship exists in reality due to the ambiguity of causality definition. Our dataset mitigates this problem by only identifying contextual causality and do not check with reality. In addition, plenty of work also only identify context-level causal relationships, such as general causality detection tasks PDTB (Prasad et al., 1 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/finreason1.html ˜ To construct this dataset, we first collect a corpus of structural events with their corresponding documents following Yang et al. (2018). The collected documents are constrained to company financial announcements, which are relatively formal documents. Such a setting could improve annotation IAA because of the logical consistency and clarity. In specific, we crawl the public company financial announcements as documents from sohu.com3 and the structural events from eastmoney.com4 . Since the documents are not in line with their corresponding structural events, we leverage key event items (see more details in Appendix B) matching to align them. Same as Yang et al. (2018), we assume that if the key event items of a structural eve"
2021.emnlp-demo.32,W16-5808,0,0.0427636,"Missing"
2021.emnlp-demo.32,E12-2021,0,0.126546,"Missing"
2021.emnlp-demo.32,P18-4006,0,0.125221,"se disagreement. ∗ co-first authors, they contributed equally to this work. We call this phenomenon instance-level label incon275 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 275–282 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: The screenshot of Annotator Interface. Figure 3: The screenshot of Disagreement Adjudicator. sistency, because this inconsistency occurs in the same instance. Instance-level label inconsistency is easy to locate but difficult to display and correct. as follows: YEDDA (Yang et al., 2018) employs a comparison report to show these inconsistencies to annotation • We propose a crowd annotation platform, experts. But it failed to display detailed informawhich can promote label consistency of tion of inconsistent entities, and annotation experts the Chinese NER dataset. The site can cannot directly correct these entities through the be accessed by http://116.62.20.198:3000, comparison report. To solve these display and corand instruction video is provided at rection issues, CroAno uses a multi-dimensional https://www.youtube.com/watch?v=wt2ma9F display mode to show inconsistent ins"
2021.emnlp-demo.32,P18-1144,0,0.0550584,"Missing"
2021.emnlp-main.176,N19-1423,0,0.00905722,"omains arrive. 3 Method In this work, we propose Knowledge Preservation Networks (KPN) to handle the DLL-DST task. KPN consists of two core components, i.e., multi-prototype enhanced retrospection and multistrategy knowledge distillation, for dealing with the two main challenges, i.e., expression diversity and combinatorial explosion. The framework of KPN is shown in Figure 3. In each dialogue turn, SOM-DST simplifies the dialogue history to the last system response and the last dialogue state, and then combines them with the current user utterance into an input sequence for the BERT encoder (Devlin et al., 2019). BERT is a multi-layer Transformer (Vaswani et al., 2017), pre-trained on large-scale unlabeled corpora. To fit the input form of BERT, the tokens [CLS] and [SEP] are placed in the input sequence. In addition, a special token [SLOT] is placed at the beginning of each slot in the last dialogue state. The BERT encoder obtains the contextual representation for the input sequence. The encoded hidden state of [SLOT] is used as the feature vector of each slot. For each slot, SOM-DST first classifies it into four categories, including “dontcare”, “carryover”, “update”, and “delete”. “dontcare” means"
2021.emnlp-main.176,2020.acl-main.60,0,0.0532759,"Missing"
2021.emnlp-main.176,2020.acl-main.573,0,0.488258,"the parameters important to old data (Kirkpatrick et al., 2017; Aljundi et al., 2018); (2) replay-based methods, which reserve some representative old samples and combine them with new data to re-train the model (Rebuffi et al., 2017; Wang et al., 2019; Hou • To the best of our knowledge, we are the first et al., 2019). Recently, replay-based methods have to formally introduce domain-lifelong learnshown promising results in alleviating catastrophic ing into dialogue state tracking and we conforgetting of class-lifelong learning tasks in NLP struct two benchmarks through two widely scenarios (Han et al., 2020; Cao et al., 2020). used DST datasets, MultiWOZ 2.1 and SGD. However, when deploying previous replay-based methods on the DLL-DST task, we find two main • We propose Knowledge Preservation Netproblems: expression diversity and combinatorial works, which handle expression diversity and explosion. Expression diversity: In the DST task, combinatorial explosion in the DLL-DST task dialogue texts usually contain a variety of expresvia multi-prototype enhanced retrospection sions for each dialogue state, as shown in Figure and multi-strategy knowledge distillation. 2. The expression diversity makes"
2021.emnlp-main.176,W14-4337,0,0.0928393,"Missing"
2021.emnlp-main.176,2020.emnlp-main.52,1,0.893191,"portant to old data (Kirkpatrick et al., 2017; Aljundi et al., 2018); (2) replay-based methods, which reserve some representative old samples and combine them with new data to re-train the model (Rebuffi et al., 2017; Wang et al., 2019; Hou • To the best of our knowledge, we are the first et al., 2019). Recently, replay-based methods have to formally introduce domain-lifelong learnshown promising results in alleviating catastrophic ing into dialogue state tracking and we conforgetting of class-lifelong learning tasks in NLP struct two benchmarks through two widely scenarios (Han et al., 2020; Cao et al., 2020). used DST datasets, MultiWOZ 2.1 and SGD. However, when deploying previous replay-based methods on the DLL-DST task, we find two main • We propose Knowledge Preservation Netproblems: expression diversity and combinatorial works, which handle expression diversity and explosion. Expression diversity: In the DST task, combinatorial explosion in the DLL-DST task dialogue texts usually contain a variety of expresvia multi-prototype enhanced retrospection sions for each dialogue state, as shown in Figure and multi-strategy knowledge distillation. 2. The expression diversity makes it difficult for •"
2021.emnlp-main.176,W14-4340,0,0.217189,"h various tasks, such as reserving domains, and the updated model should be able restaurants, booking flights, and checking weather (Young et al., 2013; Lei et al., 2018; Gao et al., to make accurate predictions for all the domains 2020). Dialogue state tracking (DST) is an essen- observed so far. tial component of task-oriented dialogue systems, A plain approach to domain-lifelong learning is which estimates user goals for downstream mod- to simply fine-tune a pre-trained model on new data. ules (Bohus and Rudnicky, 2006; Williams et al., However, this approach suffers from the problem 2013; Henderson et al., 2014b; Mrkši´c et al., 2017; of catastrophic forgetting (McCloskey and Cohen, Shan et al., 2020). Given a user utterance and its 1989; French, 1999). To be more specific, fine2301 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2301–2311 c November 7–11, 2021. 2021 Association for Computational Linguistics State: restaurant-price=expensive, restaurant-area=north User 1: I want an upscale restaurant in the northern part. User 2: Hello, I want an expensive restaurant in the north. User 3: Is there a fine dining restaurant in the north? Figure 2: An examp"
2021.emnlp-main.176,P84-1044,0,0.730448,"Missing"
2021.emnlp-main.176,2020.acl-main.567,0,0.345175,"combination of training data for the Flight domain and the old data stored in memory. KPN adopts multi-strategy knowledge distillation to retain previous knowledge. The numbers 1 to 4 represent four different knowledge distillation strategies. Then, the method selects new representative samples to reserve via multi-prototype enhanced retrospection. across the same multiple domains as data of a special domain, since these cross-domain dialogues usually contain specific expressions that distinguish them from other dialogues, such as domain transformation and slot reference (Ouyang et al., 2020; Hu et al., 2020). Each new data has its own training/validation/test set (Ditrain , Didev , Ditest ). When new data (Di ) arrives, the DST model is optimized using the new training data (Ditrain ). The updated model should still perform well on all previous domains. Therefore, in the testing stage of the ith step, we evaluate the updated model Si ontestthe test data of all observed domains (i.e., k=1 Dk ). The evaluation protocol indicates that it will be more and more difficult to achieve high performance for DST models as more and more domains arrive. 3 Method In this work, we propose Knowledge Preservation"
2021.emnlp-main.176,2020.acl-main.53,0,0.459014,"its category. The cross-entropy loss is used to train the classifier: 1 XX s Lc = − y log(ps ) (1) |N | x∈N s∈C 3.1 Background where y s is the one-hot label for the slot s and ps is Our method, KPN, is a lifelong learning framework, the predicted probability. N is the training samples which is model-agnostic. The DST model is only and C is the slots of all observed domains. a basic component, not our research focus. DST For each slot belonging to the “update” category, models, such as TRADE (Wu et al., 2019), SAS SOM-DST generates a value for this slot via the (Hu et al., 2020), and SOM-DST (Kim et al., 2020), GRU decoder (Cho et al., 2014). The decoder is can all be used as this basic component. We adopt equipped with the ability to copy words from the inthe previous best model, SOM-DST, in this work. put sequence (Kim et al., 2020). The cross-entropy 2303 loss is used to train the generation probability: Lg = − 1 X XX v yi log(pv (vi |v&lt;i )) (2) |N | x∈N s∈U i∈d where pv (vi |v&lt;i ) is the predicted probability of the i-th word of the value v. yiv is the one-hot label. d is the length of the value. U is the slots that are predicted to be the “update” category. 3.2 Multi-Prototype Enhanced Retrosp"
2021.emnlp-main.176,2020.emnlp-main.237,0,0.0346954,"t al., 2019; Han mainly divided into two categories: discriminative DST methods and generative DST methods. Dis- et al., 2020). In addition, generative replay-based methods generate old data to alleviate catastrophic criminative DST methods use predefined values as categories to simplify DST as a multi-class classifi- forgetting (Shin et al., 2017; Kemker and Kanan, 2018; Ostapenko et al., 2019; Zhai et al., 2019). cation task (Bohus and Rudnicky, 2006; Williams et al., 2013; Henderson et al., 2014a). These meth- Although lifelong learning has been widely investigated in NLP and CV scenarios (Kou et al., 2020; ods mainly focus on modeling the relation between Kundu et al., 2020), its exploration in DST is relaslots and dialogue history, such as NBT (Mrkši´c tively rare. et al., 2017), GLAD (Zhong et al., 2018), SST (Chen et al., 2020), and CHAN (Shan et al., 2020). In other dialogue tasks, Lee (2017) fine-tunes a Generative DST methods treat dialogue state track- dialogue model trained on open-domain dialogues ing as a generation task (Rastogi et al., 2017; Xu to learn task-oriented dialogues. However, their setand Hu, 2018; Wu et al., 2019). By generating val- ting is only a one-step incremental"
2021.emnlp-main.176,P18-1133,0,0.0182383,"l costs, storage budgets, and data privacy (McMahan et al., 2017). To tackle this realistic issue, we explore Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST). As shown in Figure 1, the DLL-DST task aims to 1 Introduction continually train a DST model on new data to learn incessantly emerging new domains. At each step, Task-oriented dialogue systems aim at helping new data generally contains one or multiple new users to accomplish various tasks, such as reserving domains, and the updated model should be able restaurants, booking flights, and checking weather (Young et al., 2013; Lei et al., 2018; Gao et al., to make accurate predictions for all the domains 2020). Dialogue state tracking (DST) is an essen- observed so far. tial component of task-oriented dialogue systems, A plain approach to domain-lifelong learning is which estimates user goals for downstream mod- to simply fine-tune a pre-trained model on new data. ules (Bohus and Rudnicky, 2006; Williams et al., However, this approach suffers from the problem 2013; Henderson et al., 2014b; Mrkši´c et al., 2017; of catastrophic forgetting (McCloskey and Cohen, Shan et al., 2020). Given a user utterance and its 1989; French, 1999). T"
2021.emnlp-main.176,P17-1163,0,0.0329378,"Missing"
2021.emnlp-main.176,2020.acl-main.5,0,0.0172524,"is updated with the combination of training data for the Flight domain and the old data stored in memory. KPN adopts multi-strategy knowledge distillation to retain previous knowledge. The numbers 1 to 4 represent four different knowledge distillation strategies. Then, the method selects new representative samples to reserve via multi-prototype enhanced retrospection. across the same multiple domains as data of a special domain, since these cross-domain dialogues usually contain specific expressions that distinguish them from other dialogues, such as domain transformation and slot reference (Ouyang et al., 2020; Hu et al., 2020). Each new data has its own training/validation/test set (Ditrain , Didev , Ditest ). When new data (Di ) arrives, the DST model is optimized using the new training data (Ditrain ). The updated model should still perform well on all previous domains. Therefore, in the testing stage of the ith step, we evaluate the updated model Si ontestthe test data of all observed domains (i.e., k=1 Dk ). The evaluation protocol indicates that it will be more and more difficult to achieve high performance for DST models as more and more domains arrive. 3 Method In this work, we propose Know"
2021.emnlp-main.176,D19-1196,0,0.0301055,"Missing"
2021.emnlp-main.176,2020.acl-main.563,0,0.0889719,"oking flights, and checking weather (Young et al., 2013; Lei et al., 2018; Gao et al., to make accurate predictions for all the domains 2020). Dialogue state tracking (DST) is an essen- observed so far. tial component of task-oriented dialogue systems, A plain approach to domain-lifelong learning is which estimates user goals for downstream mod- to simply fine-tune a pre-trained model on new data. ules (Bohus and Rudnicky, 2006; Williams et al., However, this approach suffers from the problem 2013; Henderson et al., 2014b; Mrkši´c et al., 2017; of catastrophic forgetting (McCloskey and Cohen, Shan et al., 2020). Given a user utterance and its 1989; French, 1999). To be more specific, fine2301 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2301–2311 c November 7–11, 2021. 2021 Association for Computational Linguistics State: restaurant-price=expensive, restaurant-area=north User 1: I want an upscale restaurant in the northern part. User 2: Hello, I want an expensive restaurant in the north. User 3: Is there a fine dining restaurant in the north? Figure 2: An example of expression diversity. Different users have different expressions for a dialogue state."
2021.emnlp-main.176,D19-1126,0,0.028236,"makes it impractical in real-world applications. Our method handles the domain-lifelong learning problem, where data of new domains continually arrives, whether it is new single-domain or new multi-domain data. 5.2 4.6 Discussion: Memory Capacity Lifelong Learning Lifelong learning, also called continual learning, is a long-standing research topic in machine learning, which enables models to perform online learning on new data (Cauwenberghs and Poggio, 2000; Kuzborskij et al., 2013). Architecture-based methods dynamically extend the model architecture to learn new data (Fernando et al., 2017; Shen et al., 2019). However, the model size grows rapidly with the increase of new data, which limits the application of architecture-based methods. Existing lifelong learning methods can be divided into two main categories: regularization-based methods (Zenke et al., 2017; Aljundi et al., 2018) and replay-based methods (Rebuffi et al., 2017; Hou et al., 2019). Regularization-based methods design reasonable 5 Related Work metrics to identify the parameters important to old 5.1 Dialogue State Tracking data and slow down the update of them (Kirkpatrick et al., 2017; Li and Hoiem, 2017). Replay-based Dialogue stat"
2021.emnlp-main.176,N19-1086,0,0.29526,"accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively. The contributions of this paper are listed as follows: tuning the model on new data usually results in a significant performance drop on old data. To address this problem, there are two mainstream lifelong learning methods: (1) regularization-based methods, which try to identify and preserve the parameters important to old data (Kirkpatrick et al., 2017; Aljundi et al., 2018); (2) replay-based methods, which reserve some representative old samples and combine them with new data to re-train the model (Rebuffi et al., 2017; Wang et al., 2019; Hou • To the best of our knowledge, we are the first et al., 2019). Recently, replay-based methods have to formally introduce domain-lifelong learnshown promising results in alleviating catastrophic ing into dialogue state tracking and we conforgetting of class-lifelong learning tasks in NLP struct two benchmarks through two widely scenarios (Han et al., 2020; Cao et al., 2020). used DST datasets, MultiWOZ 2.1 and SGD. However, when deploying previous replay-based methods on the DLL-DST task, we find two main • We propose Knowledge Preservation Netproblems: expression diversity and combinato"
2021.emnlp-main.176,W13-4065,0,0.0273415,"ve research methods retain the previous knowledge by storing area recently, where typical DST models can be a small amount of old data (Wang et al., 2019; Han mainly divided into two categories: discriminative DST methods and generative DST methods. Dis- et al., 2020). In addition, generative replay-based methods generate old data to alleviate catastrophic criminative DST methods use predefined values as categories to simplify DST as a multi-class classifi- forgetting (Shin et al., 2017; Kemker and Kanan, 2018; Ostapenko et al., 2019; Zhai et al., 2019). cation task (Bohus and Rudnicky, 2006; Williams et al., 2013; Henderson et al., 2014a). These meth- Although lifelong learning has been widely investigated in NLP and CV scenarios (Kou et al., 2020; ods mainly focus on modeling the relation between Kundu et al., 2020), its exploration in DST is relaslots and dialogue history, such as NBT (Mrkši´c tively rare. et al., 2017), GLAD (Zhong et al., 2018), SST (Chen et al., 2020), and CHAN (Shan et al., 2020). In other dialogue tasks, Lee (2017) fine-tunes a Generative DST methods treat dialogue state track- dialogue model trained on open-domain dialogues ing as a generation task (Rastogi et al., 2017; Xu to"
2021.emnlp-main.176,P19-1078,0,0.0750791,"es not contain any value. A softmax classifier is added to the feature vector of each slot to predict its category. The cross-entropy loss is used to train the classifier: 1 XX s Lc = − y log(ps ) (1) |N | x∈N s∈C 3.1 Background where y s is the one-hot label for the slot s and ps is Our method, KPN, is a lifelong learning framework, the predicted probability. N is the training samples which is model-agnostic. The DST model is only and C is the slots of all observed domains. a basic component, not our research focus. DST For each slot belonging to the “update” category, models, such as TRADE (Wu et al., 2019), SAS SOM-DST generates a value for this slot via the (Hu et al., 2020), and SOM-DST (Kim et al., 2020), GRU decoder (Cho et al., 2014). The decoder is can all be used as this basic component. We adopt equipped with the ability to copy words from the inthe previous best model, SOM-DST, in this work. put sequence (Kim et al., 2020). The cross-entropy 2303 loss is used to train the generation probability: Lg = − 1 X XX v yi log(pv (vi |v&lt;i )) (2) |N | x∈N s∈U i∈d where pv (vi |v&lt;i ) is the predicted probability of the i-th word of the value v. yiv is the one-hot label. d is the length of the val"
2021.emnlp-main.176,P18-1134,0,0.0137614,"on task (Rastogi et al., 2017; Xu to learn task-oriented dialogues. However, their setand Hu, 2018; Wu et al., 2019). By generating val- ting is only a one-step incremental process. Shen ues from the dialogue history and the vocabulary, et al. (2019) continually train a slot-filling model generative DST methods handle unknown values on new data from the same domain. Madotto et al. that are not predefined in the ontology. Therefore, (2020) introduce continual learning into multiple generative DST methods dominate this research, dialogue tasks. However, they ignore cross-domain such as SpanPtr (Xu and Hu, 2018), COMER (Ren dialogues that exist widely in the real world. In et al., 2019), BERT-DST (Chao and Lane, 2019), addition, they only adopt a plain architecture-based TRADE (Wu et al., 2019), SAS (Hu et al., 2020), method, which does not address the main chaland SOM-DST (Kim et al., 2020). lenges of the dialogue tasks. 2308 As shown in Table 5, we test the models that reserve different numbers of samples. Both EMAR and our method KPN achieve performance improvements as the number of reserved samples increases. In each case, our method significantly outperforms EMAR. Our method using only 30 sample"
2021.emnlp-main.176,P18-1135,0,0.0954356,"19) and SGD (Rastogi et al., 2019), we propose two instantiations of the above construction method. MultiWOZ benchmark: We use the data splitting of the official MultiWOZ 2.1 dataset. Since the domains in MultiWOZ 2.1 has a long-tail frequency distribution, we use the data of the top 10 most frequent domains (including the combined domains). SGD benchmark: Same as the MultiWOZ benchmark, we use the data of the top 15 most frequent domains. Table 1 shows the statistics of the two benchmarks. 4.2 Experimental Settings For the DST task, joint goal accuracy (JGA) is used as the evaluation metric (Zhong et al., 2018). For where α and β are two adjustment coefficients. The the DLL-DST task, every time the model finishes coefficients are used to balance the performance of training on new domains, we report JGA on the the old domains and the new domains. If α and β test data of all observed domains. For example, are very small, the model will pay more attention after the i-th step, the result is denoted as JGAi . to the new domains, thus hurting the performance In addition, after the last step, we report Aver2305 L = Lc +Lg +α(Lef +Ldf )+β(Lcp +Lgp ) (8) 1 UpperBound (105:06:40) KPN (Ours) (26:38:32) EMAR (3"
2021.emnlp-main.284,W19-1912,0,0.0333504,"Missing"
2021.emnlp-main.284,P15-2049,0,0.0662467,"Missing"
2021.emnlp-main.284,N18-1202,0,0.0302036,"Figure 1 is training process is beneficial for encoders, since an example of BCN from NCBI dataset (Do˘gan et al., 2014), the mention B-cell non-Hodgkins lym- currently used encoders like BioBERT only enphomas should be linked to D016393 Lymphoma, codes the context semantics in biomedical corpora B-Cell in the MEDIC (Davis et al., 2012) dictio- instead of the biomedical concept structural information. nary. Recent works on BCN usually adopt encoders To this end, we propose Biomedical Concept like CNN (Li et al., 2017), LSTM (Phan et al., Normalizer with Hypernyms (BCNH), a novel 2019), ELMo (Peters et al., 2018; Schumacher framework combining the list-wise cross entropy et al., 2020) or BioBERT (Lee et al., 2020; Fakhraei loss with norm constraint on hypernym-hyponym et al., 2019; Ji et al., 2020) to embed both the men- entity pairs. Concretely, we reformulate the cantion and the concept’s name entities, and then feed didate target list as a three-level relevance list to the representations to the following classifier or consider both synonyms and hypernyms, and apply 3512 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3512–3517 c November 7–11, 2021. 2"
2021.emnlp-main.284,P19-1317,0,0.0843077,"le. Owing to numerous sur- synonyms (acceptable name variants, synonyms), and related concepts (mainly hypernym concepts). face variants of biomedical concepts, BCN still remains challenging and unsolved. In this pa- Therefore, effectively using the limited information in the biomedical dictionary where the candidate per, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Con- entities came from is paramount for the BCN task. cept Normalizer with Hypernyms (BCNH), a novel For concept’s synonym entities, recent BNE framework that adopts list-wise training to make (Phan et al., 2019) and BIOSYN (Sung et al., 2020) use of both hypernyms and synonyms, and also tries to make full use of them by synonym marginalemploys norm constraint on the representation of ization to enhance biomedical entity representation hypernym-hyponym entity pairs. The experimental and achieved consistent performance improvement. results show that BCNH outperform the previous Unfortunately, previous works generally ignore state-of-the-art model on the NCBI dataset. Code concept hypernym hierarchy structure, which is will be available at https://github.com/ exactly the initial motivation of biomedical"
2021.emnlp-main.284,2020.acl-main.760,0,0.0604844,"Missing"
2021.emnlp-main.284,2020.acl-main.335,0,0.25566,"nyms (acceptable name variants, synonyms), and related concepts (mainly hypernym concepts). face variants of biomedical concepts, BCN still remains challenging and unsolved. In this pa- Therefore, effectively using the limited information in the biomedical dictionary where the candidate per, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Con- entities came from is paramount for the BCN task. cept Normalizer with Hypernyms (BCNH), a novel For concept’s synonym entities, recent BNE framework that adopts list-wise training to make (Phan et al., 2019) and BIOSYN (Sung et al., 2020) use of both hypernyms and synonyms, and also tries to make full use of them by synonym marginalemploys norm constraint on the representation of ization to enhance biomedical entity representation hypernym-hyponym entity pairs. The experimental and achieved consistent performance improvement. results show that BCNH outperform the previous Unfortunately, previous works generally ignore state-of-the-art model on the NCBI dataset. Code concept hypernym hierarchy structure, which is will be available at https://github.com/ exactly the initial motivation of biomedical dictioyan-cheng/BCNH. nary: or"
2021.emnlp-main.284,N18-1103,0,0.0617895,"Missing"
2021.emnlp-main.760,D17-1277,0,0.013745,"en output embedding hd ∈ Rd , we first compute the predicted logit values of the entities: logith = Ve relu(W1 hd ) (6) logitt = Ve relu(W2 hd ) where W1 , W2 ∈ Rd×d are learnable parameters, and Ve is entity embedding matrix mentioned in Section 2.1. Then, we conduct masked softmax1 to compute the distribution of the entities: ph = masked_softmax(logith , C(X)) pt = masked_softmax(logitt , C(X)) (7) where C(X) is the entity candidates of the given sentence X and is obtained through the process mention in the following paragraph. Candidate Selection. Inspired by the studies in entity linking (Ganea and Hofmann, 2017; Kolitsas et al., 2018), we conduct the candidate selection to avoid involving an extremely large number of entities. For each span s in the given sentence X, we select up to 10 entity candidates that might be referred by this span. These top entities are based on an empirical probabilistic entity-map p(e|s) built from hyperlinks and disambiguation pages in Wikipedia. We denote this candidate set as C(X) and use it at both training and test time. For more details about the candidate selection, we refer readers to Kolitsas et al. (2018). 2.4 Bipartite Matching Loss The main difficulty of train"
2021.emnlp-main.760,N13-1092,0,0.0281207,"Missing"
2021.emnlp-main.760,D17-1278,0,0.0346279,"Missing"
2021.emnlp-main.760,L18-1245,0,0.0282996,"previous step. We define the loss as: ˆ Y) = L(Y, m X {− log prπ? (i) (ri ) + 1{ri 6=∅} [ i=1 • RQ1: How well do our proposed set generation networks (SGN) perform, in comparison with the competitive baselines? • RQ2: How efficient is the training and inference of the model? • RQ3: How does each design of the proposed networks matter? • RQ4: What is the performance of the proposed networks in sentences that mention different numbers of facts? In the remainder of this section, we describe the datasets, experimental settings (in the Appendix), and all baselines. 3.1 The Cold Start track in TAC (Getman et al., 2018) provides a testbed for KBP systems. However, the dataset is not publicly available and manual evaluation is used to examine a system’s “justification” (Mesquita et al., 2019), which make it difficult to reproduce TAC’s evaluation for new systems. Instead, we validate the proposed method on two publicly available datasets: WIKI and GEO4 (Trisedya et al., 2019). The statistics of these datasets are shown in Table 2. The training set, validation set and WIKI are constructed from Wikipedia articles. To evaluate methods on a different style of text than the training data, GEO is used as a testbed,"
2021.emnlp-main.760,D17-1109,0,0.0116875,"avoiding while the fact <h, r, t> does not exist in the KB, considering the order of multiple facts. To since KBs typically have much better coverage on solve the set generation problem, we propose entities than on relationships. networks featured by transformers with nonautoregressive parallel decoding. Unlike previConventionally, KBP is solved by several inous approaches that use an autoregressive dedividual components in a pipeline manner (Shin coder to generate facts one by one, the proet al., 2015; Angeli et al., 2015; Zhang et al., 2017; posed networks can directly output the final set Chaganty et al., 2017; Mesquita et al., 2019), typof facts in one shot. Furthermore, to train the ically including (1) entity discovery or named ennetworks, we also design a set-based loss that tity recognition (Tjong Kim Sang and De Meulder, forces unique predictions via bipartite matching. Compared with cross-entropy loss that 2003), (2) entity linking (Milne and Witten, 2008) highly penalizes small shifts in fact order, the and (3) relation extraction (Zelenko et al., 2003). proposed bipartite matching loss is invariant Entity discovery seeks to locate and classify named to any permutation of predictions. Benef"
2021.emnlp-main.760,D16-1236,0,0.0321919,"Missing"
2021.emnlp-main.760,D11-1072,0,0.0138019,"ram combination of attention weight to capture the verbal or noun phrase context. Note that all of these end-to-end models are based on the encoder-decoder framework and are required to sort the ground truth facts. Following previous work (Trisedya et al., 2019), we build the ground truth sequence according to the inherent order in these datasets. We compare the proposed model with the following systems that report SoTA results on these datasets. Firstly, we compare our proposed model with pipeline models. In these pipeline models, we use two entity discovery and entity linking systems, AIDA (Hoffart et al., 2011; Yosef et al., 2011) and NeuralEL (Kolitsas et al., 2018). In AIDA, entity mentions are automatically detected by using the Stanford NER Tagger (Manning et al., 2014), and then are mapped to entities by using a probabilistic 3.4 Main Results graphical model. In NeuralEL, all possible spans To start, we address the research question RQ1. that have at least one possible entity candidate are Table 3 shows the results of our proposed model generated, and are linked to entities by using a against baselines on two benchmark datasets. context-aware compatibility score. To label the relationship betw"
2021.emnlp-main.760,P11-1115,0,0.0410348,". However, high-quality KBs still rely almost all based on the sequence-to-sequence (seq2seq) exclusively on human-curated structured or semi- framework (Sutskever et al., 2014; Cho et al., 2014). structured data (Mesquita et al., 2019). Such a Under this framework, end-to-end KBP is treated reliance on human curation is a major barrier to as a translation of a sentence into a sequence of fact creating always-up-to-date KBs. elements (entity or predicate). Considering the runTo overcome this barrier, knowledge base popu- ning example in Table 1, a seq2seq model would lation (KBP) is proposed (Ji and Grishman, 2011; translate the sentence “President Obama 9650 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9650–9660 c November 7–11, 2021. 2021 Association for Computational Linguistics Input Sentence: President Obama welcomed President Xi Jinping of China to visit the United States. Output Facts: <Q76, P39, Q11696>; <Q15031, P39, Q655407> Table 1: An example of KBP. In this example, “Obama”, “President of United States”, “Xi Jinping”, and “President of People’s Republic of China” are mapped to their unique Wikidata identifiers “Q76”, “Q11696”, “Q15031” and “"
2021.emnlp-main.760,K18-1050,0,0.0412472,"Rd , we first compute the predicted logit values of the entities: logith = Ve relu(W1 hd ) (6) logitt = Ve relu(W2 hd ) where W1 , W2 ∈ Rd×d are learnable parameters, and Ve is entity embedding matrix mentioned in Section 2.1. Then, we conduct masked softmax1 to compute the distribution of the entities: ph = masked_softmax(logith , C(X)) pt = masked_softmax(logitt , C(X)) (7) where C(X) is the entity candidates of the given sentence X and is obtained through the process mention in the following paragraph. Candidate Selection. Inspired by the studies in entity linking (Ganea and Hofmann, 2017; Kolitsas et al., 2018), we conduct the candidate selection to avoid involving an extremely large number of entities. For each span s in the given sentence X, we select up to 10 entity candidates that might be referred by this span. These top entities are based on an empirical probabilistic entity-map p(e|s) built from hyperlinks and disambiguation pages in Wikipedia. We denote this candidate set as C(X) and use it at both training and test time. For more details about the candidate selection, we refer readers to Kolitsas et al. (2018). 2.4 Bipartite Matching Loss The main difficulty of training is to score the pred"
2021.emnlp-main.760,2020.emnlp-main.79,0,0.0360792,"when the number of facts increases, the performance of models decreases significantly. the advancement of this line of research is that an inexistent order of facts must be introduced to train the seq2seq model. In this paper, we introduce set generation networks to overcome this roadblock. Non-Autoregressive Model for Generation. Gu et al. (2018) began to explore non-autoregressive model, the aim of which is to generate sequences in a parallel manner. Since then, there is rich literature devoted to this topic, such as Lee et al. (2018); Ma et al. (2019); Ren et al. (2020); Ran et al. (2020); Kong et al. (2020). Nowadays, non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation (Lee et al., 2018; Ma et al., 2019) and automatic speech recognition (Chen et al., 2019; Tian et al., 2020; Bai et al., 2020). To the best of our knowledge, this is the first work to apply the non-autoregressive model to knowledge base population. In this work, we resort to the nonautoregressive model to generate the set of relational facts in one shot. Set Prediction. The problem with predicting sets is that the output order of the elements is arbitrary,"
2021.emnlp-main.760,D18-1149,0,0.022059,"1 fact or 2 facts, most models can achieve the best performance. However, when the number of facts increases, the performance of models decreases significantly. the advancement of this line of research is that an inexistent order of facts must be introduced to train the seq2seq model. In this paper, we introduce set generation networks to overcome this roadblock. Non-Autoregressive Model for Generation. Gu et al. (2018) began to explore non-autoregressive model, the aim of which is to generate sequences in a parallel manner. Since then, there is rich literature devoted to this topic, such as Lee et al. (2018); Ma et al. (2019); Ren et al. (2020); Ran et al. (2020); Kong et al. (2020). Nowadays, non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation (Lee et al., 2018; Ma et al., 2019) and automatic speech recognition (Chen et al., 2019; Tian et al., 2020; Bai et al., 2020). To the best of our knowledge, this is the first work to apply the non-autoregressive model to knowledge base population. In this work, we resort to the nonautoregressive model to generate the set of relational facts in one shot. Set Prediction. The problem"
2021.emnlp-main.760,P16-1200,0,0.0172154,"Missing"
2021.emnlp-main.760,D19-1437,0,0.0129576,"most models can achieve the best performance. However, when the number of facts increases, the performance of models decreases significantly. the advancement of this line of research is that an inexistent order of facts must be introduced to train the seq2seq model. In this paper, we introduce set generation networks to overcome this roadblock. Non-Autoregressive Model for Generation. Gu et al. (2018) began to explore non-autoregressive model, the aim of which is to generate sequences in a parallel manner. Since then, there is rich literature devoted to this topic, such as Lee et al. (2018); Ma et al. (2019); Ren et al. (2020); Ran et al. (2020); Kong et al. (2020). Nowadays, non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation (Lee et al., 2018; Ma et al., 2019) and automatic speech recognition (Chen et al., 2019; Tian et al., 2020; Bai et al., 2020). To the best of our knowledge, this is the first work to apply the non-autoregressive model to knowledge base population. In this work, we resort to the nonautoregressive model to generate the set of relational facts in one shot. Set Prediction. The problem with predicting s"
2021.emnlp-main.760,P18-1136,0,0.0488494,"Missing"
2021.emnlp-main.760,P14-5010,0,0.00251492,"and are required to sort the ground truth facts. Following previous work (Trisedya et al., 2019), we build the ground truth sequence according to the inherent order in these datasets. We compare the proposed model with the following systems that report SoTA results on these datasets. Firstly, we compare our proposed model with pipeline models. In these pipeline models, we use two entity discovery and entity linking systems, AIDA (Hoffart et al., 2011; Yosef et al., 2011) and NeuralEL (Kolitsas et al., 2018). In AIDA, entity mentions are automatically detected by using the Stanford NER Tagger (Manning et al., 2014), and then are mapped to entities by using a probabilistic 3.4 Main Results graphical model. In NeuralEL, all possible spans To start, we address the research question RQ1. that have at least one possible entity candidate are Table 3 shows the results of our proposed model generated, and are linked to entities by using a against baselines on two benchmark datasets. context-aware compatibility score. To label the relationship between two entities, we adopt superTaken overall, our proposed model substantially vised approaches like CNN (Lin et al., 2016) and outperforms baselines on these dataset"
2021.emnlp-main.760,D19-1069,0,0.147557,"t <h, r, t> does not exist in the KB, considering the order of multiple facts. To since KBs typically have much better coverage on solve the set generation problem, we propose entities than on relationships. networks featured by transformers with nonautoregressive parallel decoding. Unlike previConventionally, KBP is solved by several inous approaches that use an autoregressive dedividual components in a pipeline manner (Shin coder to generate facts one by one, the proet al., 2015; Angeli et al., 2015; Zhang et al., 2017; posed networks can directly output the final set Chaganty et al., 2017; Mesquita et al., 2019), typof facts in one shot. Furthermore, to train the ically including (1) entity discovery or named ennetworks, we also design a set-based loss that tity recognition (Tjong Kim Sang and De Meulder, forces unique predictions via bipartite matching. Compared with cross-entropy loss that 2003), (2) entity linking (Milne and Witten, 2008) highly penalizes small shifts in fact order, the and (3) relation extraction (Zelenko et al., 2003). proposed bipartite matching loss is invariant Entity discovery seeks to locate and classify named to any permutation of predictions. Benefiting entities mentioned"
2021.emnlp-main.760,2020.acl-main.277,0,0.0113114,"formance. However, when the number of facts increases, the performance of models decreases significantly. the advancement of this line of research is that an inexistent order of facts must be introduced to train the seq2seq model. In this paper, we introduce set generation networks to overcome this roadblock. Non-Autoregressive Model for Generation. Gu et al. (2018) began to explore non-autoregressive model, the aim of which is to generate sequences in a parallel manner. Since then, there is rich literature devoted to this topic, such as Lee et al. (2018); Ma et al. (2019); Ren et al. (2020); Ran et al. (2020); Kong et al. (2020). Nowadays, non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation (Lee et al., 2018; Ma et al., 2019) and automatic speech recognition (Chen et al., 2019; Tian et al., 2020; Bai et al., 2020). To the best of our knowledge, this is the first work to apply the non-autoregressive model to knowledge base population. In this work, we resort to the nonautoregressive model to generate the set of relational facts in one shot. Set Prediction. The problem with predicting sets is that the output order of the el"
2021.emnlp-main.760,2020.acl-main.15,0,0.0140361,"chieve the best performance. However, when the number of facts increases, the performance of models decreases significantly. the advancement of this line of research is that an inexistent order of facts must be introduced to train the seq2seq model. In this paper, we introduce set generation networks to overcome this roadblock. Non-Autoregressive Model for Generation. Gu et al. (2018) began to explore non-autoregressive model, the aim of which is to generate sequences in a parallel manner. Since then, there is rich literature devoted to this topic, such as Lee et al. (2018); Ma et al. (2019); Ren et al. (2020); Ran et al. (2020); Kong et al. (2020). Nowadays, non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation (Lee et al., 2018; Ma et al., 2019) and automatic speech recognition (Chen et al., 2019; Tian et al., 2020; Bai et al., 2020). To the best of our knowledge, this is the first work to apply the non-autoregressive model to knowledge base population. In this work, we resort to the nonautoregressive model to generate the set of relational facts in one shot. Set Prediction. The problem with predicting sets is that the out"
2021.emnlp-main.760,P19-1023,0,0.0759135,"s to discover facts about entities from texts As shown in Table 1, a KBP system is required to and expand a knowledge base with these facts. take a given sentence as input and transform it into Previous studies shape end-to-end KBP as a machine translation task, which is required to a set of facts. A fact is in the form of <h, r, t>, convert unordered fact into a sequence accordwhere h is a head entity, t is a tail entity, and r is a ing to a pre-specified order. However, the facts predicate that falls in a predefined set of predicates. stated in a sentence are unordered in essence. Following Trisedya et al. (2019), we also assume In this paper, we formulate end-to-end KBP that h and t are existing entities in the given KB as a direct set generation problem, avoiding while the fact <h, r, t> does not exist in the KB, considering the order of multiple facts. To since KBs typically have much better coverage on solve the set generation problem, we propose entities than on relationships. networks featured by transformers with nonautoregressive parallel decoding. Unlike previConventionally, KBP is solved by several inous approaches that use an autoregressive dedividual components in a pipeline manner (Shin c"
2021.emnlp-main.760,K16-1025,0,0.0214373,"chitecture of set generation networks. The set generation networks predict the final set of facts in parallel by combining a transformer-based encoder with a non-autoregressive decoder. In the training phrase, bipartite matching uniquely assigns predictions with ground truths to provide accurate training signals. 2.1 Joint Learning of Word and Entity Embeddings set are not included in the KB). The loss function of the TransE model is defined as : In the first step, we jointly embed words, entities and predicates into the same vector space. To achieve this, we combine the anchor context model (Yamada et al., 2016) to compute the word embeddings with TransE (Bordes et al., 2013) to compute the entity and predicate embeddings. Specifically, we first utilize the anchor context model to establish the interaction between the entity and word embeddings. In this model, a modified Wikipedia corpus is generated by replacing the hyperlinks with the related entity identifiers, and a skip-gram model (Mikolov et al., 2013) is trained on this corpus to compute the word and entity embeddings. Formally, given a sequence [w1 , w2 , ..., wT ], the loss function of the anchor context model is: JW = − T X X log P (wt+j |w"
2021.findings-acl.190,W17-2711,0,0.171512,"clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality inference, where those KBs store inherent causal relations between some given events. For those unseen events and unlabeled causalities in KBs, Liu et al. (2020) proposed a mention-mask based reasoner to enhance the caus"
2021.findings-acl.190,P17-2001,0,0.171009,"ng: EventStoryLine v0.9 (ESC) (Caselli and Vossen, 2017) described above; and (2) Causal-TimeBank (CTB) (Mirza and Tonelli, 2014) which contains 184 documents, 6813 events, and 318 causal event pairs. Same as previous methods, we use the last two topics of ESC as the development set for two datasets. For evaluation, we adopt Precision (P), Recall (R), and F1-score (F1) as evaluation metrics. We conduct 5-fold and 10fold cross-validation on ESC and CTB respectively, same as previous methods. All the results are the average of three independent experiments. 2166 Methods P EventStoryLine S-Path (Cheng and Miyao, 2017) 34.0 S-Fea (Choubey and Huang, 2017) 32.7 LR+ (Gao et al., 2019) 37.0 ILP (Gao et al., 2019) 37.4 BERT 36.0 KnowDis (Zuo et al., 2020) 39.7 MasG (Liu et al., 2020) 41.9 KnowDis+CauSeRL (Ours) 40.1 MasG+CauSeRL (Ours) 40.8 CauSeRLDIST AN T (Ours) 39.9 CauSeRLAT OM IC (Ours) 41.0 CauSeRLGLU -GEN (Ours) 41.4 CauSeRLGLU -SP E (Ours) 41.9 Causal-TimeBank Rule-B (Mirza and Tonelli, 2014) 36.8 Data-D (Mirza and Tonelli, 2014) 67.3 VerR-C (Mirza, 2014) 69.0 BERT 39.5 MasG (Liu et al., 2020) 36.6 KnowDis (Zuo et al., 2020) 42.3 MasG+CauSeRL (Ours) 42.6 KnowDis+CauSeRL (Ours) 42.5 CauSeRLDIST AN T (Our"
2021.findings-acl.190,D17-1190,0,0.116152,"lli and Vossen, 2017) described above; and (2) Causal-TimeBank (CTB) (Mirza and Tonelli, 2014) which contains 184 documents, 6813 events, and 318 causal event pairs. Same as previous methods, we use the last two topics of ESC as the development set for two datasets. For evaluation, we adopt Precision (P), Recall (R), and F1-score (F1) as evaluation metrics. We conduct 5-fold and 10fold cross-validation on ESC and CTB respectively, same as previous methods. All the results are the average of three independent experiments. 2166 Methods P EventStoryLine S-Path (Cheng and Miyao, 2017) 34.0 S-Fea (Choubey and Huang, 2017) 32.7 LR+ (Gao et al., 2019) 37.0 ILP (Gao et al., 2019) 37.4 BERT 36.0 KnowDis (Zuo et al., 2020) 39.7 MasG (Liu et al., 2020) 41.9 KnowDis+CauSeRL (Ours) 40.1 MasG+CauSeRL (Ours) 40.8 CauSeRLDIST AN T (Ours) 39.9 CauSeRLAT OM IC (Ours) 41.0 CauSeRLGLU -GEN (Ours) 41.4 CauSeRLGLU -SP E (Ours) 41.9 Causal-TimeBank Rule-B (Mirza and Tonelli, 2014) 36.8 Data-D (Mirza and Tonelli, 2014) 67.3 VerR-C (Mirza, 2014) 69.0 BERT 39.5 MasG (Liu et al., 2020) 36.6 KnowDis (Zuo et al., 2020) 42.3 MasG+CauSeRL (Ours) 42.6 KnowDis+CauSeRL (Ours) 42.5 CauSeRLDIST AN T (Ours) 41.6 CauSeRLAT OM IC (Ours) 42.8 C"
2021.findings-acl.190,N19-1423,0,0.152696,"sion targets to train the online network which makes it learn the commonalities among two input causal statements, that is, the causal representations reflecting different context-specific causal patterns. Structurally, the online network is defined as a set of weights θ which is comprised of three submodules: an encoder Encθ , a projector P rojθ and a predictor P redθ . And the target network has the same architecture as the online network, but no predictor and uses a different set of weights δ. In specific, we iteratively sample two external causal statements, initially encode them by BERT (Devlin et al., 2019), and input them into two net2164 works respectively. After encoding and projection, the online network and target network respectively output a projection zθ and zδ0 . Then the online network outputs a prediction yθ , and takes the following mean square error between `2 -normalized y ¯θ and z¯δ0 as the training objective to learn the commonalities of two causal statements, that are regarded as the context-specific causal patterns. hyθ , zδ0 i , kyθ k2 · zδ0 2 (1) y ¯θ , yθ / kyθ k2 , z¯δ0 , zδ0 / zδ0 2 . (2) 2 Lθ,δ , y ¯θ − z¯δ0 2 =2 − 2 · To reduce the bias, we symmetrize the Lθ,δ by swappin"
2021.findings-acl.190,D11-1027,0,0.0320078,"all, we design a self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into target ECI model for identification. • Experimental results on two benchmarks show that our model achieves the best performance. 2 Related Work Event Causality Identification Up to now, identifying the causality implied in the text has attracted more and more attention (Hu and Walker, 2017a; Riaz and Girju, 2014b; Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh"
2021.findings-acl.190,W15-1622,0,0.018033,"Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE (Dunietz et al., 2015) of causal relation and other seven relations. Based on the above benchmarks, Gao et al. (2019) modeled document-level structures to identify the causalities of events. Liu et al. (2020) identified event causalities with the mention masking generalization and external KBs. Zuo et al. (2020) improved the performance of ECI with the distantly automatically labeled training data. However, these methods only rely on a small scale of labeled data. In this paper, we introduce external causal statements to help identify event causalities. Self-Supervised Representation Learning Self-supervised repres"
2021.findings-acl.190,W17-0812,0,0.0155953,"eown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE (Dunietz et al., 2015) of causal relation and other seven relations. Based on the above benchmarks, Gao et al. (2019) modeled document-level structures to identify the causalities of events. Liu et al. (2020) identified event causalities with the mention masking generalization and external KBs. Zuo et al. (2020) improved the performance of ECI with the distantly automatically labeled training data. However, these methods only rely on a small scale of labeled data. In this paper, we introduce external causal statements to help identify event"
2021.findings-acl.190,N19-1179,0,0.207505,"n S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality inference, where those KBs store inherent causal relations between some"
2021.findings-acl.190,W03-1210,0,0.28597,"een Case [Entity] find/notice/feel/... [Entity] >Causes/Enables> [Entity] call/give/alert/... [Entity] noticedE1 Context-specific Causal Pattern alertedE3 Prediction Figure 1: S1 is a labeled data that contains unseen causal events and their statement when training; S2 is an external causal statement; The bottom illustrates the context-specific causal pattern in S2 could help identify the causality of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 docume"
2021.findings-acl.190,P14-1093,0,0.086047,"the context-specific causal pattern in S2 could help identify the causality of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enha"
2021.findings-acl.190,P16-1135,0,0.0211599,"self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into target ECI model for identification. • Experimental results on two benchmarks show that our model achieves the best performance. 2 Related Work Event Causality Identification Up to now, identifying the causality implied in the text has attracted more and more attention (Hu and Walker, 2017a; Riaz and Girju, 2014b; Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et"
2021.findings-acl.190,W17-2708,0,0.0237781,"fic causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into target ECI model for identification. • Experimental results on two benchmarks show that our model achieves the best performance. 2 Related Work Event Causality Identification Up to now, identifying the causality implied in the text has attracted more and more attention (Hu and Walker, 2017a; Riaz and Girju, 2014b; Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new"
2021.findings-acl.190,W17-5540,0,0.391188,"ity of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality inference, where those KBs store inherent causal re"
2021.findings-acl.190,P14-3002,0,0.0680568,"Missing"
2021.findings-acl.190,W14-0702,0,0.0303958,"orate the learned context-specific causal patterns into target ECI model for identification. • Experimental results on two benchmarks show that our model achieves the best performance. 2 Related Work Event Causality Identification Up to now, identifying the causality implied in the text has attracted more and more attention (Hu and Walker, 2017a; Riaz and Girju, 2014b; Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE (Dunietz et al., 2015) of causal relation and other seven relations. Based on the above be"
2021.findings-acl.190,C14-1198,0,0.38248,"e best performance. 2 Related Work Event Causality Identification Up to now, identifying the causality implied in the text has attracted more and more attention (Hu and Walker, 2017a; Riaz and Girju, 2014b; Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE (Dunietz et al., 2015) of causal relation and other seven relations. Based on the above benchmarks, Gao et al. (2019) modeled document-level structures to identify the causalities of events. Liu et al. (2020) identified event causalities with the mention maski"
2021.findings-acl.190,C16-1007,0,0.124547,"help identify the causality of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality inference, where those KBs sto"
2021.findings-acl.190,W16-1007,0,0.0254408,"et al., 2011; Hidey and McKeown, 2016; Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017b). Recently, some benchmarks on the event causality have been released. Mirza et al. (2014), Mirza and Tonelli (2016) extracted causal relation of events with a rule-based multi-sieve approach incorporating with event temporal relation. Mirza and Tonelli (2014) annotated the Causal-TimeBank of event causal relations. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for event causality identification in 320 short stories based on the temporal and causal relations annotated dataset (Mostafazadeh et al., 2016). Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE (Dunietz et al., 2015) of causal relation and other seven relations. Based on the above benchmarks, Gao et al. (2019) modeled document-level structures to identify the causalities of events. Liu et al. (2020) identified event causalities with the mention masking generalization and external KBs. Zuo et al. (2020) improved the performance of ECI with the distantly automatically labeled training data. However, these methods only rely on a small scale of labeled data. In this paper, we introduce external causal statements"
2021.findings-acl.190,2020.emnlp-main.370,0,0.245465,"ed reasoner to enhance the causal statement representation. However, such mention-mask based reasoner is still trained on the human-annotated examples solely. It will still suffer from data limitations and have no capacity to handling unseen contexts. Moreover, Zuo et al. (2020) improved the performance of ECI with the distantly supervised labeled training data. However, their models are still limited to the unsatisfied qualities of the automatically generated data. To address the insufficient annotated example problem, we employ a large number of external causal statements (Sap et al., 2018; Mostafazadeh et al., 2020) that can support adequate evidence of context-specific causal patterns (Liu et al., 2020) 2162 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2162–2172 August 1–6, 2021. ©2021 Association for Computational Linguistics for understanding event causalities. For example in Figure 1, the context-specific causal pattern support by an external causal statement S2 is helpful for identifying the causality of event noticedE1 and event alertedE3 in S1 , which is unseen when only training with labeled data. However, different from annotated examples for the ECI task, th"
2021.findings-acl.190,P13-1170,0,0.0638961,"Missing"
2021.findings-acl.190,W14-4322,0,0.0938932,"sal pattern in S2 could help identify the causality of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality infe"
2021.findings-acl.190,W14-0707,0,0.161159,"sal pattern in S2 could help identify the causality of unseen events in S1 . Introduction Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for deep textual understanding (Girju, 2003; Oh et al., 2013, 2017). For example in Figure 1, an ECI system should identify two causal relations in S1 with mentioned events: noticedE1 cause cause −→ alertedE3 and alertedE3 −→ ranE2 . To date, most existing methods regard this task as a classification problem and usually train ECI models on annotated data (Hashimoto et al., 2014; Riaz and Girju, 2014b; Mirza and Tonelli, 2016; Hu and Walker, 2017b; Gao et al., 2019). However, the scale of current annotated datasets are relatively limited, where the so far largest dataset EventStoryLine (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. As a result, on the limited annotated examples, existing ECI models could not easily capture useful indicators from causal statements, especially for handing those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality infe"
2021.findings-acl.190,2020.coling-main.135,1,0.562556,"g those new, unseen cases. To address this problem, Liu et al. (2020) employed external event-related knowledge bases (KBs) to enhance the causality inference, where those KBs store inherent causal relations between some given events. For those unseen events and unlabeled causalities in KBs, Liu et al. (2020) proposed a mention-mask based reasoner to enhance the causal statement representation. However, such mention-mask based reasoner is still trained on the human-annotated examples solely. It will still suffer from data limitations and have no capacity to handling unseen contexts. Moreover, Zuo et al. (2020) improved the performance of ECI with the distantly supervised labeled training data. However, their models are still limited to the unsatisfied qualities of the automatically generated data. To address the insufficient annotated example problem, we employ a large number of external causal statements (Sap et al., 2018; Mostafazadeh et al., 2020) that can support adequate evidence of context-specific causal patterns (Liu et al., 2020) 2162 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2162–2172 August 1–6, 2021. ©2021 Association for Computational Linguistics"
2021.findings-emnlp.52,P17-1040,0,0.0188525,"proposed distant supervision by using a knowledge base to ated settings. Due to the lack of comparison with annotate a large-scale dataset automatically. HowS1 , previous denoising methods would mistakenly regard S2 as a true positive instance. As a result, ever, automatic labeling inevitably accompanies with label noise. To deal with label noise, most disS2 is retained and then poisons the local model in tantly supervised approaches (Riedel et al., 2010; platform 2, which would affect the global model in Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng turn. et al., 2015; Lin et al., 2016; Luo et al., 2017; Ye To suppress label noise in federated settings, we and Ling, 2019; Yuan et al., 2019; Yu et al., 2020a) propose a federated denoising framework in this focus on reducing label noise at bag 2 level prepaper. The core of this framework is a multiple instance learning (MIL) (Dietterich et al., 1997; diction. These studies fall under multiple instance learning framework, which assumes that at least Maron and Lozano-Pérez, 1998) based denoising algorithm, called Lazy MIL, which is only ex- one sentence expresses the relation in a bag. Another line of work aims to reduce label noise at senecuted"
2021.findings-emnlp.52,P09-1113,0,0.0728067,"r- ies in federated learning and federated learning in rier between Platform 1 and Platform 2; therefore, natural language processing (NLP). Distant Supervision. Relation extraction is a simultaneously considering S1 and S2 can easily task of mining factual knowledge from free text filter out noise via only selecting S1 (Zeng et al., 2015) or placing a small weight on S2 (Lin et al., by labeling relations between entity mentions. To 2016; Ye and Ling, 2019). However, raw data ex- alleviate the dependence of supervised methods change between platforms is prohibited in feder- on annotated data, Mintz et al. (2009) proposed distant supervision by using a knowledge base to ated settings. Due to the lack of comparison with annotate a large-scale dataset automatically. HowS1 , previous denoising methods would mistakenly regard S2 as a true positive instance. As a result, ever, automatic labeling inevitably accompanies with label noise. To deal with label noise, most disS2 is retained and then poisons the local model in tantly supervised approaches (Riedel et al., 2010; platform 2, which would affect the global model in Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng turn. et al., 2015; Lin et al., 2016;"
2021.findings-emnlp.52,P18-1046,0,0.0214348,"reducing label noise at bag 2 level prepaper. The core of this framework is a multiple instance learning (MIL) (Dietterich et al., 1997; diction. These studies fall under multiple instance learning framework, which assumes that at least Maron and Lozano-Pérez, 1998) based denoising algorithm, called Lazy MIL, which is only ex- one sentence expresses the relation in a bag. Another line of work aims to reduce label noise at senecuted at the beginning of each communication round and then would rest until the next round. tence level prediction. These studies (Zeng et al., 2018; Feng et al., 2018; Qin et al., 2018a,b) use Since the sentences containing the same entity pair scatter around different platforms, Lazy MIL al- reinforcement learning or adversarial training to 2 gorithm coordinates multiple platforms to jointly A set of sentences containing the same entity pair is called select reliable sentences. Once sentences have been a “bag&quot; 570 select trustable relation labels by matching the predicted labels with distantly supervised labels. In this paper, we follows the line of bag level prediction. Different from previous studies, our work extends distant supervision to federated settings. Federated"
2021.findings-emnlp.52,P18-1199,0,0.0210876,"reducing label noise at bag 2 level prepaper. The core of this framework is a multiple instance learning (MIL) (Dietterich et al., 1997; diction. These studies fall under multiple instance learning framework, which assumes that at least Maron and Lozano-Pérez, 1998) based denoising algorithm, called Lazy MIL, which is only ex- one sentence expresses the relation in a bag. Another line of work aims to reduce label noise at senecuted at the beginning of each communication round and then would rest until the next round. tence level prediction. These studies (Zeng et al., 2018; Feng et al., 2018; Qin et al., 2018a,b) use Since the sentences containing the same entity pair scatter around different platforms, Lazy MIL al- reinforcement learning or adversarial training to 2 gorithm coordinates multiple platforms to jointly A set of sentences containing the same entity pair is called select reliable sentences. Once sentences have been a “bag&quot; 570 select trustable relation labels by matching the predicted labels with distantly supervised labels. In this paper, we follows the line of bag level prediction. Different from previous studies, our work extends distant supervision to federated settings. Federated"
2021.findings-emnlp.52,P16-1162,0,0.0481373,"Missing"
2021.findings-emnlp.52,2020.emnlp-main.300,0,0.0337082,", then all sentences a great deal of centralized unstructured text. However, in practice, texts are usually disthat mention these two entities will express this tributed on different platforms and cannot be relation. Since then, there has been a rich litercentralized due to privacy restrictions. Thereature devoted to this topic, such as Riedel et al. fore, it is worthwhile to investigate distant su(2010); Hoffmann et al. (2011); Zeng et al. (2015); pervision in the federated learning paradigm, Lin et al. (2016); Ye and Ling (2019); Yuan et al. which decouples the training of the model (2019); Xiao et al. (2020). from the need for direct access to raw texts. However, overcoming label noise of distant suThough the progress is exciting, distant superpervision becomes more difficult in federated vision approaches have so far been limited to the settings, because texts containing the same encentralized learning paradigm, which assumes that tity pair scatter around different platforms. In a great deal of text is easily accessible. However, this paper, we propose a federated denoising in practice, texts are usually distributed on differframework to suppress label noise in federated ent platforms and are ma"
2021.findings-emnlp.52,N19-1288,0,0.136527,"ening studies in this field have assumed there is tities have a relation in the KB, then all sentences a great deal of centralized unstructured text. However, in practice, texts are usually disthat mention these two entities will express this tributed on different platforms and cannot be relation. Since then, there has been a rich litercentralized due to privacy restrictions. Thereature devoted to this topic, such as Riedel et al. fore, it is worthwhile to investigate distant su(2010); Hoffmann et al. (2011); Zeng et al. (2015); pervision in the federated learning paradigm, Lin et al. (2016); Ye and Ling (2019); Yuan et al. which decouples the training of the model (2019); Xiao et al. (2020). from the need for direct access to raw texts. However, overcoming label noise of distant suThough the progress is exciting, distant superpervision becomes more difficult in federated vision approaches have so far been limited to the settings, because texts containing the same encentralized learning paradigm, which assumes that tity pair scatter around different platforms. In a great deal of text is easily accessible. However, this paper, we propose a federated denoising in practice, texts are usually distribute"
2021.findings-emnlp.52,2020.coling-main.146,0,0.0674843,"h annotate a large-scale dataset automatically. HowS1 , previous denoising methods would mistakenly regard S2 as a true positive instance. As a result, ever, automatic labeling inevitably accompanies with label noise. To deal with label noise, most disS2 is retained and then poisons the local model in tantly supervised approaches (Riedel et al., 2010; platform 2, which would affect the global model in Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng turn. et al., 2015; Lin et al., 2016; Luo et al., 2017; Ye To suppress label noise in federated settings, we and Ling, 2019; Yuan et al., 2019; Yu et al., 2020a) propose a federated denoising framework in this focus on reducing label noise at bag 2 level prepaper. The core of this framework is a multiple instance learning (MIL) (Dietterich et al., 1997; diction. These studies fall under multiple instance learning framework, which assumes that at least Maron and Lozano-Pérez, 1998) based denoising algorithm, called Lazy MIL, which is only ex- one sentence expresses the relation in a bag. Another line of work aims to reduce label noise at senecuted at the beginning of each communication round and then would rest until the next round. tence level predi"
2021.naacl-main.261,P07-1056,0,0.082676,"re constructed by corrupting either subjects or objects. Knowledge Retrieval Inspired by the previous studies (Yang and Mitchell, 2017; Yang et al., 2019), exact string matching (Charras and Lecroq, 2004) is used to recognize entity mentions from a given passage and link recognized entity mentions to subjects in KB. Then, we collect the corresponding objects (concepts) as candidates. After this retrieval process, we obtain a set of potentially relevant KB concepts, where each KB concept is associated with a KB embedding. 4 4.1 Experiment Dataset Our model is evaluated on the widely used ARSC (Blitzer et al., 2007) dataset, which comprises reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets form 69 tasks in total. Following Yu et al. (2018), we select 12 tasks from four domains (Books, DVDs, Electronics, and Kitchen) as testing set, with only (8) 5 examples as support set for each class. 3268 4.2 Implementation Details In our experiments, we use hugginface’s implementation1 of BERT (base version) and initialize parameters of the BERT encoding layer with pre-trained models officially released by Google2 . To represent k"
2021.naacl-main.261,N19-1423,0,0.0321339,"hown in Equation 2) and the knowledge of the support set as input, and produces a scalar in range of 0 to 1 representing the similarity between the query sentence and the class representation, which is called relation score. Compared with the original relation network (Sung et al., 2018), we decompose the relation network into two parts, task-agnostic relation network and task-relevant relation network, in order to serve two purposes. Task agnostic relation network models a basic metric function, while taskrelevant relation network adapts to diverse tasks. In this network, a pre-trained BERT (Devlin et al., 2019) encoder is used to model sentences. Given Task-Agnostic Relation Network The taskan input text xi = ([CLS], w1 , w2 , ..., wT , [SEP]) agnostic relation network uses a learned unified as input, the output of BERT encoder is denoted metric for all tasks, which is the same with the orig(T +2)×d 1 as H(xi ) ∈ R , where d1 is the output inal relation network (Sung et al., 2018). With this agn dimension of the BERT encoder. We use the first unified metric, C task-agnostic relation scores rz,j 3267 are generated for modeling the relation between one query input xj and the class representation cz ,"
2021.naacl-main.261,2020.acl-main.102,0,0.0329205,"Missing"
2021.naacl-main.261,D19-1403,0,0.0709613,"e different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models. 1 Introduction To adapt metric learning to significantly diverse tasks, we propose a knowledge guided metric learning method. This method is inspired by the fact that human beings approach diverse tasks armed with knowledge obtained from relevant tasks (Lake et al., 2017). We use external knowledge from the knowledge base (KB) to imitate human knowledge, whereas the role of external knowledge has been ignored in previous methods (Yu et al., 2018; Bao et al., 2019; Geng et al., 2019, 2020). In detail, we resort to distributed representations of the KB instead of symbolic facts, since symbolic facts face the issues of poor generalization and data sparsity. Based on such KB embeddings, we investigate a novel parameter generator network (Ha et al., 2016; Jia et al., 2016) to generate task-relevant relation network parameters. With these generated parameters, the task-relevant relation network is able to apply diverse metrics to diverse tasks and ensure that similar tasks use similar metrics while different tasks use different metrics. In summary, the major contributions of"
2021.naacl-main.261,D18-2024,0,0.0294656,"assification tasks. These buckets form 69 tasks in total. Following Yu et al. (2018), we select 12 tasks from four domains (Books, DVDs, Electronics, and Kitchen) as testing set, with only (8) 5 examples as support set for each class. 3268 4.2 Implementation Details In our experiments, we use hugginface’s implementation1 of BERT (base version) and initialize parameters of the BERT encoding layer with pre-trained models officially released by Google2 . To represent knowledge in NELL (Carlson et al., 2010), BILINEAR model (Yang et al., 2015) is implemented with the open-source framework OpenKE (Han et al., 2018) to obtain the embedding of entities and relations. The size of embeddings of entities and relations is set to 100. To train our model, We use Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.00001. All experiments are run with an NVIDIA GeForce RTX 2080 Ti. 4.3 Experiment Results Baseline. We compare our method to the following baselines: (1) Match Network is a metricbased attention method for few-shot learning; (2) Prototypical Network is a metric-based method that uses sample averages as class prototypes; (3) MAML is an optimization-based method through learning to learn with"
2021.naacl-main.261,P19-1226,0,0.0175714,"ed in the real number space. We adopt the BILINEAR model (Yang et al., 2015) to measure the validity of triples: f (s, r, o) = sT diag(r)o ∈ R (9) where s, r, o ∈ Rd2 are the embeddings associated with s, r, o, respectively, and diag(r) is a diagonal matrix with the main diagonal given by the relation embedding r. To learn these vector embeddings, a margin-based ranking loss is designed, where triples in the KB are adopted to be positive and negative triples are constructed by corrupting either subjects or objects. Knowledge Retrieval Inspired by the previous studies (Yang and Mitchell, 2017; Yang et al., 2019), exact string matching (Charras and Lecroq, 2004) is used to recognize entity mentions from a given passage and link recognized entity mentions to subjects in KB. Then, we collect the corresponding objects (concepts) as candidates. After this retrieval process, we obtain a set of potentially relevant KB concepts, where each KB concept is associated with a KB embedding. 4 4.1 Experiment Dataset Our model is evaluated on the widely used ARSC (Blitzer et al., 2007) dataset, which comprises reviews for 23 types of products on Amazon. For each product domain, there are three different binary class"
2021.naacl-main.261,P17-1132,0,0.0266367,"the triple can be measured in the real number space. We adopt the BILINEAR model (Yang et al., 2015) to measure the validity of triples: f (s, r, o) = sT diag(r)o ∈ R (9) where s, r, o ∈ Rd2 are the embeddings associated with s, r, o, respectively, and diag(r) is a diagonal matrix with the main diagonal given by the relation embedding r. To learn these vector embeddings, a margin-based ranking loss is designed, where triples in the KB are adopted to be positive and negative triples are constructed by corrupting either subjects or objects. Knowledge Retrieval Inspired by the previous studies (Yang and Mitchell, 2017; Yang et al., 2019), exact string matching (Charras and Lecroq, 2004) is used to recognize entity mentions from a given passage and link recognized entity mentions to subjects in KB. Then, we collect the corresponding objects (concepts) as candidates. After this retrieval process, we obtain a set of potentially relevant KB concepts, where each KB concept is associated with a KB embedding. 4 4.1 Experiment Dataset Our model is evaluated on the widely used ARSC (Blitzer et al., 2007) dataset, which comprises reviews for 23 types of products on Amazon. For each product domain, there are three di"
2021.naacl-main.261,N18-1109,0,0.318992,"ar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models. 1 Introduction To adapt metric learning to significantly diverse tasks, we propose a knowledge guided metric learning method. This method is inspired by the fact that human beings approach diverse tasks armed with knowledge obtained from relevant tasks (Lake et al., 2017). We use external knowledge from the knowledge base (KB) to imitate human knowledge, whereas the role of external knowledge has been ignored in previous methods (Yu et al., 2018; Bao et al., 2019; Geng et al., 2019, 2020). In detail, we resort to distributed representations of the KB instead of symbolic facts, since symbolic facts face the issues of poor generalization and data sparsity. Based on such KB embeddings, we investigate a novel parameter generator network (Ha et al., 2016; Jia et al., 2016) to generate task-relevant relation network parameters. With these generated parameters, the task-relevant relation network is able to apply diverse metrics to diverse tasks and ensure that similar tasks use similar metrics while different tasks use different metrics. In"
2021.smm4h-1.13,2021.ccl-1.108,0,0.0491427,"Missing"
2021.smm4h-1.13,P16-1101,0,0.0906702,"Missing"
2021.smm4h-1.13,2021.smm4h-1.3,0,0.0283476,"tional Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences 2 School of Artificial Intelligence University of Chinese Academy of Sciences 3 Beijing University of Posts and Telecommunications 4 Beijing University of Chemical Technology 5 Beijing Unisound Information Technology Co., Ltd {tongzhou21, niukun}@bupt.edu.cn {zhucong.li,baoli.zhang,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn {ganzhen, wanj}@mail.buct.edu.cn {shiyafei,chongweifeng,liushengping}@unisound.com that mentions ADE in the tweet; (3) normalization ADE mentions to standard terms. Subtask b of task 7 (Miranda-Escalada et al., 2021) is designed to identify professions and occupations (ProfNER) in Spanish tweets during the COVID-19 outbreak. Task 8 is targeting the classification of self-reported breast cancer posts on Twitter. The ubiquitous two challenges of all the SMM4H shared tasks are (1) how to properly model the colloquial text in tweets; (2) avoid prediction bias caused by learning from unbalanced annotated data. The tweet’s text, mixing with informal spelling, various emojis, usernames mentioned, and hyperlinks, will hinder the real semantic comprehension by a common pre-trained language model. Meanwhile, medica"
2021.smm4h-1.13,2020.emnlp-demos.2,0,0.0332969,"Missing"
C12-1193,P09-1082,0,0.63434,"由人们提供，它们对回答真实问题起到了很好的作用 (Wang et al., 2009)。 为了更好地利用大规模的问答对，具备帮助用户检索先前答案的功能非常必 要 (Duan et al., 2008)。因此， 检索与查询问题语义等价或相关的问题是一件非常有意 义的任务。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇鸿 沟。词汇歧义通常会引发问题检索模型检索出许多与用户查询意图不匹配的历史问题。 这也是由问题和用户的高度多样化造成的。例如，依据不同的用户，词&quot;interest&quot;既可以 指&quot;curiosity&quot;也可以指&quot;a charge for borrowing money&quot;。另外一个挑战是查询问题与历史问 题的词汇鸿沟。查询问题中的词不同于历史问题中的词但是它们之间是相关的词。 词汇鸿 沟问题对社区问答的问题检索而言更加严重，主要是问答对通常很短，查找相同的内容表 达往往使用不同的词(Xue et al., 2008)。 为了解决词汇鸿沟问题，大多数学者将问题检索任务看作是一个统计机器翻译的问题， 并 利 用IBM模 型1(Brown et al., 1993)来 学 习 词 与 词 之 间 的 翻 译 概 率(Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009)。 实验 结果一致表明基于词的翻译模型取得了比传统检索方法更好的性能。 最近，Riezler et al. (2007)和Zhou et al. (2011)提出了基于统计短语翻译的问题和答案检索方法。基于短语的 翻译模型可以刻画上下文信息，在翻译的过程中对整个短语建模， 从而在某种程度上降低 了词汇歧义的问题。然而，目前公开发表的工作都是基于单语的方法，仅仅利用了原始语 言的信息， 而没有利用来自其它语言潜在的丰富的语义信息。通过其它语言，可以利用各 种方法增加原始问题的语义信息，从而提高仅仅利用原始语言方法的性能。 通过利用外国语言，我们提出利用翻译表示通过外国语言词汇来替换原始语言中的词， 其中外国语言是指不同于原始语言的。 利用双语信息进行问题检索的基本思想如下： （1）从一种语言翻译成另一种语言的过程中可以利用上下文信息，如表1所示，英文 单词&quot;interest&quot;和 &quot;bank&quot;在不同的上下文中有多种意思，在利用Google Translate (GoogleTrans)翻译的过程中正确的意思可以得到纠正。因此，问题中词的歧义在翻译的过程中可 以根据上下文信息得到解决。 （2）多个语言相关的词在某种语言中可以被翻成另外一种 语言的唯一表示。如表1所示，英文单词例如&quot;company&quot;和&quot;firm&quot;可以被翻译成中文单词&quot;公 司 (gōngsī)&quot;，&quot;rheum&quot;和&quot;catar"
C12-1193,J93-2003,0,0.0481394,"ve QnA，在这些在线社区上，人们可以回答他人提出的问 题。这种在线社区称为基于社区的问答服务。 在这些社区中，任何人都可以提问和回答关 于任何主题的问题，寻找信息的人与那些知道答案的人就联系起来了。 由于社区问答上的 答案通常以显式的形式由人们提供，它们对回答真实问题起到了很好的作用 (Wang et al., 2009)。 为了更好地利用大规模的问答对，具备帮助用户检索先前答案的功能非常必 要 (Duan et al., 2008)。因此， 检索与查询问题语义等价或相关的问题是一件非常有意 义的任务。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇鸿 沟。词汇歧义通常会引发问题检索模型检索出许多与用户查询意图不匹配的历史问题。 这也是由问题和用户的高度多样化造成的。例如，依据不同的用户，词&quot;interest&quot;既可以 指&quot;curiosity&quot;也可以指&quot;a charge for borrowing money&quot;。另外一个挑战是查询问题与历史问 题的词汇鸿沟。查询问题中的词不同于历史问题中的词但是它们之间是相关的词。 词汇鸿 沟问题对社区问答的问题检索而言更加严重，主要是问答对通常很短，查找相同的内容表 达往往使用不同的词(Xue et al., 2008)。 为了解决词汇鸿沟问题，大多数学者将问题检索任务看作是一个统计机器翻译的问题， 并 利 用IBM模 型1(Brown et al., 1993)来 学 习 词 与 词 之 间 的 翻 译 概 率(Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009)。 实验 结果一致表明基于词的翻译模型取得了比传统检索方法更好的性能。 最近，Riezler et al. (2007)和Zhou et al. (2011)提出了基于统计短语翻译的问题和答案检索方法。基于短语的 翻译模型可以刻画上下文信息，在翻译的过程中对整个短语建模， 从而在某种程度上降低 了词汇歧义的问题。然而，目前公开发表的工作都是基于单语的方法，仅仅利用了原始语 言的信息， 而没有利用来自其它语言潜在的丰富的语义信息。通过其它语言，可以利用各 种方法增加原始问题的语义信息，从而提高仅仅利用原始语言方法的性能。 通过利用外国语言，我们提出利用翻译表示通过外国语言词汇来替换原始语言中的词， 其中外国语言是指不同于原始语言的。 利用双语信息进行问题检索的基本思想如下： （1）从一种语言翻译成另一种语言的过程中可以利用上下文信息，如表1所示，英文 单词&quot;interest&quot;和 &quot;bank&quot;在不同的上下文中有多种意思，在利用Google Translate (GoogleTrans)翻译的过程中正确的意思可以得到纠正。因此，问题"
C12-1193,J03-3003,0,0.0413095,"e expected frequency of a word computed from all possible translated representations, while we use the state-of-the-art commercial machine translation service (e.g., Google Tranlate), which is much simpler than their translation strategies. 6.3 Machine Translation for Cross-Lingual Information Retrieval Cross-lingual retrieval information retrieval (CLIR) addresses the problem of retrieving documents written in a language different from the query language. The common approach in CLIR is to perform query translation or document translation using a machine translation system(Chen and Gey, 2004; Kraaij et al., 2003). However, the major difference is that our goal is to improve monolingual question retrieval and not CLIR. Moreover, these studies performed translation without taking into account the context information of an original word(Chen and Gey, 2004; Kraaij et al., 2003). On the contrary, our approach is contextdependent and thus produces different translated words depending on the context of a word in original language. Conclusion and Future Work In this paper, we intend to address two fundamental issues in question retrieval: word ambiguity and lexical gap. To solve these problems, we propose the"
C12-1193,D08-1043,0,0.780681,"于社区问答上的 答案通常以显式的形式由人们提供，它们对回答真实问题起到了很好的作用 (Wang et al., 2009)。 为了更好地利用大规模的问答对，具备帮助用户检索先前答案的功能非常必 要 (Duan et al., 2008)。因此， 检索与查询问题语义等价或相关的问题是一件非常有意 义的任务。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇鸿 沟。词汇歧义通常会引发问题检索模型检索出许多与用户查询意图不匹配的历史问题。 这也是由问题和用户的高度多样化造成的。例如，依据不同的用户，词&quot;interest&quot;既可以 指&quot;curiosity&quot;也可以指&quot;a charge for borrowing money&quot;。另外一个挑战是查询问题与历史问 题的词汇鸿沟。查询问题中的词不同于历史问题中的词但是它们之间是相关的词。 词汇鸿 沟问题对社区问答的问题检索而言更加严重，主要是问答对通常很短，查找相同的内容表 达往往使用不同的词(Xue et al., 2008)。 为了解决词汇鸿沟问题，大多数学者将问题检索任务看作是一个统计机器翻译的问题， 并 利 用IBM模 型1(Brown et al., 1993)来 学 习 词 与 词 之 间 的 翻 译 概 率(Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009)。 实验 结果一致表明基于词的翻译模型取得了比传统检索方法更好的性能。 最近，Riezler et al. (2007)和Zhou et al. (2011)提出了基于统计短语翻译的问题和答案检索方法。基于短语的 翻译模型可以刻画上下文信息，在翻译的过程中对整个短语建模， 从而在某种程度上降低 了词汇歧义的问题。然而，目前公开发表的工作都是基于单语的方法，仅仅利用了原始语 言的信息， 而没有利用来自其它语言潜在的丰富的语义信息。通过其它语言，可以利用各 种方法增加原始问题的语义信息，从而提高仅仅利用原始语言方法的性能。 通过利用外国语言，我们提出利用翻译表示通过外国语言词汇来替换原始语言中的词， 其中外国语言是指不同于原始语言的。 利用双语信息进行问题检索的基本思想如下： （1）从一种语言翻译成另一种语言的过程中可以利用上下文信息，如表1所示，英文 单词&quot;interest&quot;和 &quot;bank&quot;在不同的上下文中有多种意思，在利用Google Translate (GoogleTrans)翻译的过程中正确的意思可以得到纠正。因此，问题中词的歧义在翻译的过程中可 以根据上下文信息得到解决。 （2）多个语言相关的词在某种语言中可以被翻成另外一种 语言的唯一表示。如表1所示，英文单词例如&quot;company&quot;和&quot;firm&quot;可以被翻译成中文单"
C12-1193,P07-1059,0,0.690468,"索先前答案的功能非常必 要 (Duan et al., 2008)。因此， 检索与查询问题语义等价或相关的问题是一件非常有意 义的任务。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇鸿 沟。词汇歧义通常会引发问题检索模型检索出许多与用户查询意图不匹配的历史问题。 这也是由问题和用户的高度多样化造成的。例如，依据不同的用户，词&quot;interest&quot;既可以 指&quot;curiosity&quot;也可以指&quot;a charge for borrowing money&quot;。另外一个挑战是查询问题与历史问 题的词汇鸿沟。查询问题中的词不同于历史问题中的词但是它们之间是相关的词。 词汇鸿 沟问题对社区问答的问题检索而言更加严重，主要是问答对通常很短，查找相同的内容表 达往往使用不同的词(Xue et al., 2008)。 为了解决词汇鸿沟问题，大多数学者将问题检索任务看作是一个统计机器翻译的问题， 并 利 用IBM模 型1(Brown et al., 1993)来 学 习 词 与 词 之 间 的 翻 译 概 率(Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009)。 实验 结果一致表明基于词的翻译模型取得了比传统检索方法更好的性能。 最近，Riezler et al. (2007)和Zhou et al. (2011)提出了基于统计短语翻译的问题和答案检索方法。基于短语的 翻译模型可以刻画上下文信息，在翻译的过程中对整个短语建模， 从而在某种程度上降低 了词汇歧义的问题。然而，目前公开发表的工作都是基于单语的方法，仅仅利用了原始语 言的信息， 而没有利用来自其它语言潜在的丰富的语义信息。通过其它语言，可以利用各 种方法增加原始问题的语义信息，从而提高仅仅利用原始语言方法的性能。 通过利用外国语言，我们提出利用翻译表示通过外国语言词汇来替换原始语言中的词， 其中外国语言是指不同于原始语言的。 利用双语信息进行问题检索的基本思想如下： （1）从一种语言翻译成另一种语言的过程中可以利用上下文信息，如表1所示，英文 单词&quot;interest&quot;和 &quot;bank&quot;在不同的上下文中有多种意思，在利用Google Translate (GoogleTrans)翻译的过程中正确的意思可以得到纠正。因此，问题中词的歧义在翻译的过程中可 以根据上下文信息得到解决。 （2）多个语言相关的词在某种语言中可以被翻成另外一种 语言的唯一表示。如表1所示，英文单词例如&quot;company&quot;和&quot;firm&quot;可以被翻译成中文单词&quot;公 司 (gōngsī)&quot;，&quot;rheum&quot;和&quot;catarrh&quot;可以被翻译成中文单词&quot;感冒(gǎnmào)&quot;。 在本文中，通过机器翻译，每个原始语言（例如：英语）的问题都被自动翻"
C12-1193,D12-1116,0,0.539535,"rformance. Zhou et al. (2011) proposed a monolingual phrase-based translation model for question retrieval. This method can capture some contextual information in modeling the translation of phrases as a whole. To implement the word-based translation models, we use the GIZA++ alignment toolkit8 trained on one million question-answer pairs from another data set9 to learn the word-to-word translation probabilities. For phrase-based translation model described in (Zhou et al., 2011), we employ Moses toolkit10 to extract the phrase translation and set the maximum length of phrases to 5. Recently, Singh (2012) extended the word-based translation model and explored strategies to learn the translation probabilities between words and the concepts using the CQA archives and a popular entity catalog. However, these existing studies in the literature are basically monolingual translation, which are restricted to the use of the original language of the CQA archives, without taking advantage of potentially rich semantic information drawn from other languages. In this paper, we propose the use of translated words to enrich question representation, going beyond the words in original language to represent the"
C12-1193,P12-1029,0,0.0276312,"uestion retrieval: word ambiguity and lexical gap. To solve these problems, we enrich the question representation via bilingual translation. Compared to the traditional monolingual approaches, our proposed bilingual translation is much more effective due to the recent advance in statistical machine translation. To the best of our knowledge, it is the first work to improve question retrieval in CQA via bilingual translation. 3167 6.2 WSD and Query Expansion for Monolingual Information Retrieval Besides in CQA, word ambiguity and lexical gap have been investigated in information retrieval (IR). Zhong and Ng (2012) proposed a novel approach to incorporate word senses into the language modeling approach to IR. Experimental results showed that word sense disambiguation (WSD) can significantly improve a state-of-the-art IR system. Query expansion has been one of the most effective approaches to resolve the lexical gap problem, which enrich the original query by adding some additional words (Lv and Zhai, 2010; Xu et al., 2009). Recently, Trieschnigg et al. (2010) enriched the original word-based representation with a concept-based representation, thereby proposing the translation of the original word langua"
C12-1193,P11-1066,1,0.898597,"al., 2008)。因此， 检索与查询问题语义等价或相关的问题是一件非常有意 义的任务。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇鸿 沟。词汇歧义通常会引发问题检索模型检索出许多与用户查询意图不匹配的历史问题。 这也是由问题和用户的高度多样化造成的。例如，依据不同的用户，词&quot;interest&quot;既可以 指&quot;curiosity&quot;也可以指&quot;a charge for borrowing money&quot;。另外一个挑战是查询问题与历史问 题的词汇鸿沟。查询问题中的词不同于历史问题中的词但是它们之间是相关的词。 词汇鸿 沟问题对社区问答的问题检索而言更加严重，主要是问答对通常很短，查找相同的内容表 达往往使用不同的词(Xue et al., 2008)。 为了解决词汇鸿沟问题，大多数学者将问题检索任务看作是一个统计机器翻译的问题， 并 利 用IBM模 型1(Brown et al., 1993)来 学 习 词 与 词 之 间 的 翻 译 概 率(Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009)。 实验 结果一致表明基于词的翻译模型取得了比传统检索方法更好的性能。 最近，Riezler et al. (2007)和Zhou et al. (2011)提出了基于统计短语翻译的问题和答案检索方法。基于短语的 翻译模型可以刻画上下文信息，在翻译的过程中对整个短语建模， 从而在某种程度上降低 了词汇歧义的问题。然而，目前公开发表的工作都是基于单语的方法，仅仅利用了原始语 言的信息， 而没有利用来自其它语言潜在的丰富的语义信息。通过其它语言，可以利用各 种方法增加原始问题的语义信息，从而提高仅仅利用原始语言方法的性能。 通过利用外国语言，我们提出利用翻译表示通过外国语言词汇来替换原始语言中的词， 其中外国语言是指不同于原始语言的。 利用双语信息进行问题检索的基本思想如下： （1）从一种语言翻译成另一种语言的过程中可以利用上下文信息，如表1所示，英文 单词&quot;interest&quot;和 &quot;bank&quot;在不同的上下文中有多种意思，在利用Google Translate (GoogleTrans)翻译的过程中正确的意思可以得到纠正。因此，问题中词的歧义在翻译的过程中可 以根据上下文信息得到解决。 （2）多个语言相关的词在某种语言中可以被翻成另外一种 语言的唯一表示。如表1所示，英文单词例如&quot;company&quot;和&quot;firm&quot;可以被翻译成中文单词&quot;公 司 (gōngsī)&quot;，&quot;rheum&quot;和&quot;catarrh&quot;可以被翻译成中文单词&quot;感冒(gǎnmào)&quot;。 在本文中，通过机器翻译，每个原始语言（例如：英语）的问题都被自动翻译成另一种外 国语言（例如：汉语），"
C14-1064,baccianella-etal-2010-sentiwordnet,0,0.0912821,"oi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shifted to another domain. Our method does not require annotated data. 3 The Proposed Method To detect opinion relations, previous methods often leverage some seed terms, such as opinion word seeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Hai et al., 2012). These seeds can be used as positive labeled examples to train a classifier. However, it is hard to get negative labeled examples for this task. Because opinion words or targets are often domain 678 dependent and words that do not bear any sentiment polarity in one domain may be used to express opinion in another domain. It is also very hard to specify in what case there is no linking relation between two words. To deal with this problem, we employ one-class classification, and develop a One-Class Deep Neural Network (OCDNN) fo"
C14-1064,W06-1651,0,0.0302305,"hich introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shifted to another domain. Our method does not require annotated data. 3 The Proposed Method To detect opinion relations, previous methods often leverage some seed terms, such as opinion word seeds (Hu and Liu, 2004; Baccianella et"
C14-1064,J81-4005,0,0.800789,"Missing"
C14-1064,de-marneffe-etal-2006-generating,0,0.0530095,"Missing"
C14-1064,J93-1003,0,0.160852,"f-the-art weakly supervised methods which are based on Assumption 1. 2 Related Work In opinion relation detection task, previous works often used co-occurrence statistics or syntax information to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a pioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) defined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features. Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests (LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the a"
C14-1064,D10-1101,0,0.0284811,"shi et al. (2007) identified opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shifted to another domain. Our method does not require annot"
C14-1064,P10-1060,0,0.0432684,"Missing"
C14-1064,D07-1114,0,0.0272531,"tion based on association rules. Popescu and Etzioni (2005) defined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features. Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests (LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevyc"
C14-1064,C10-1074,0,0.0150685,"ed opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shifted to another domain. Our method does not require annotated data. 3 The P"
C14-1064,P13-1172,1,0.821098,"65 0.68 Avg. F 0.54 0.59 0.63 0.66 0.68 0.48 0.55 0.50 0.63 0.61 0.59 0.68 0.59 0.54 0.57 0.58 0.61 0.56 0.60 0.60 0.65 Table 3: Results of opinion terms extraction on the four domains. From Table 2, we can see that our method outperforms co-occurrence-based methods AdjRule and LRTBOOT, but achieves comparable or a little worse results than syntax-based methods DP and DPHITS. This is because CRD is quite small, which only contains several hundred sentences for each product review set. In this case, methods based on careful-designed syntax rules have superiority over those based on statistics (Liu et al., 2013). For results on larger datasets shown in Table 3, our method outperforms all of the competitors. Comparing OCDNN with DP-HITS, the two approaches use similar term ranking metrics (Eq. 7 and Eq. 8), but OCDNN significantly outperforms DP-HITS. Therefore, the positive proportion score estimated by OCDNN is more effective than the importance score in DP-HITS. Comparing OCDNN with LRTBOOT, we find that LRTBOOT achieves better recall but lower precision. This is because LRTBOOT follows Assumption 1 during bootstrapping, which suffers a lot from error propagation, while our joint classification app"
C14-1064,H05-1043,0,0.620349,"1, s={clear}, t={sceen}, and there is a linking relation between the two words because clear is used to modify screen. Example 1. This mp3 has a clear screen. For a valid opinion relation, there are three requirements corresponding to the three factors: (i) the opinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) the opinion word modifies the opinion target. Previous weakly supervised methods often expand a seed set and identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) or syntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below. Assumption 1. Terms that are likely to have linking relation with the seed terms are believed to be opinion words or opinion targets. For example, if one has an opinion word seed clear (which satisfies requirement i), and one finds that it modifies the word screen in Example 1 (which satisfies requirement iii). Then one infers that screen is an opinion target according to Assumption 1 (whether screen is correct is not checked). However, in Example 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related to"
C14-1064,W03-1014,0,0.0468117,"opinion relation detection task, previous works often used co-occurrence statistics or syntax information to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a pioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) defined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features. Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests (LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relatio"
C14-1064,D11-1014,0,0.0178336,"embedding vector by xi = LT bi . The training criterion for word embeddings is, θˆ = argmin θ X X max{0, 1 − sθ (c) + sθ (v)} (2) c∈C v∈Vw where θ is the parameters of neural network used for training. See Collobert et al. (2011) for the detailed implementation. 3.4 Linking Relation Representation by Using Recursive Autoencoder The goal of this section is to represent the linking relation between an opinion word and an opinion target by a n-element vector as we do during word representation. Specifically, we combine embedding vectors of words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntactic dependency structure. In this way, linking relations are no longer limited to the initial seeds during classification, because linking relations that are similar to the seed relations will have similar vector representations. Figure 2 shows a linking relation representation process by an example: too loud to listen to the player. First, we get its dependency path between the opinion word cs :loud and the opinion target ct :player. Then cs and ct are replaced by wildcards [SC] and [TC] because they are not concerned in the linking relation. The dash line box in Figure 2"
C14-1064,P13-1045,0,0.0194304,"yer. It takes two n-element vectors as input and compresses semantics of the two vectors into one n-element vector in hidden layer by, 1 y = f (W (dep) [x1 ; x2 ] + b), W (dep) = [I1 ; I2 ; Ib ] +  (3) 2 where [x1 ; x2 ] is the concatenation of the two input vectors and f is the sigmoid function; W (dep) is a parameter matrix that is chosen according to the dependency relation between x1 and x2 (In the case of y1 , W (dep) = W (xcomp) ), which is initialized by Ii , where Ii is a n × n unit matrix, Ib is a n-element null vector, and  is sampled from a uniform distribution U [−0.001, 0.001] (Socher et al., 2013). Then W (dep) are updated during training. The training criterion of autoencoder is to minimize Euclidean distance between the original input and its output, Erae = ||[x1 ; x2 ] − [x01 ; x02 ]||2 T (4) where [x01 ; x02 ] = W (out) y and W (out) is initialized by W (dep) . We always start the combination process from [SC] and it is repeated along the dependency path. For example, the result vector y1 of the first combination is used as the input vector when computing y2 . Finally, the linking relation is represented by a n-element vector (the green vector in Figure 2). 680 3.5 One-Class Classi"
C14-1064,P10-1040,0,0.00569373,"relation. To get cr , we first get dependency tree of a sentence using Stanford Parser (de 679 Marneffe et al., 2006). Then, the shortest dependency path between a cs and a ct is taken as a cr . To avoid introducing too many noise candidates, we constrain that there are at most four terms in a cr . 3.3 Word Representation by Word Embedding Learning Word embedding, a.k.a word representation, is a mathematical object associated with each word, which is often used in a vector form, where each dimension’s value corresponds to a feature and might even have a semantic or grammatical interpretation (Turian et al., 2010). By word embedding learning, words are embedded into a hyperspace, where two words that are more semantically similar to each other are located closer. This characteristic is precisely what we want, because the key to one-class classification is semantic similarity measuring (illustrated in Section 3.5). For word representation, we use a matrix LT ∈ Rn×|Vw |, where i-th column represents the embedding vector for term ti , n is the size of embedding vector and Vw is the vocabulary of LT . Therefore, we can denote ti by a binary vector bi ∈ R|Vw |and get its embedding vector by xi = LT bi . The"
C14-1064,D09-1159,0,0.0183625,"to supervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shift"
C14-1064,P13-1173,1,0.837785,"tes term frequency; L(p, k, n) = pk (1 − p)n−k , n1 = k1 + k3 , n2 = k2 + k4 , p1 = k1 /n1 , p2 = k2 /n2 and p = (k1 + k2 )/(n1 + n2 ). We measure LRT between a domain name (e.g. mp3, hotel, etc.) and all opinion target candidates. Then N terms with highest LRT scores are added into the opinion target seed set T S. Linking Relation Seeds. Linking relation can be naturally captured by syntactic dependency, because it directly models the modification relation between opinion word and opinion target. We employ an automatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013) and get 12 opinion patterns with highest confidence as the linking relation seed set RS. After seed generation, every opinion relation so = (ss , st , sr ) in review corpus that satisfies ss ∈ SS, st ∈ T S and sr ∈ RS is taken as a positive labeled training instance. 3.2 Opinion Relation Candidate Generation The opinion term candidate set is denoted by C = {SC, T C}, where SC/T C represents opinion word/target candidate. Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu et al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phras"
C14-1064,P13-1161,0,0.017447,"ht heuristic syntactic rules to detect opinion relations. However, none of the above methods could verify opinion words/targets/relations simultaneously during opinion relation detection. To perform joint extraction, various models had been proposed, most of which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, optimal models such as Integer Linear Programming (ILP) were also employed to perform joint inference for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013). Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless, most existing joint models rely on full supervision, which have the difficulty of obtaining annotated training data in practical applications. Also, supervised models that are trained on one domain often fail to give satisfactory results when shifted to another domain. Our method does not require annotated data. 3 The Proposed Method To detect opinion relations, previous methods often leverage some seed terms, such as opinion word seeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion t"
C14-1064,C10-2167,0,0.0449761,"Missing"
C14-1064,H05-2017,0,\N,Missing
C14-1199,P05-1045,0,0.102615,"or each entity mention. The top list types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ∈ 1, 2, 3} type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the substitution method explained above. 4 Experiments 4.1 Settings We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences in the years 2005-2006 are used as training corpus Σtrain for distant supervision and sentences in 2007 are used as testing corpus Σpredict . The data was first tagged with an NER system (Finkel et al., 2005) and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions Etrain in training corpus are aligned to facts ∆ in Freebase as training examples to train the models. We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multilabel extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints. • Aggregated extraction: Aggregated extraction is corpus-level extraction. When given"
C14-1199,P11-1055,0,0.557259,"es mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the p"
C14-1199,N13-1095,0,0.0324603,"without the mentioned assumptions. Their work predicted negative patterns using a generative model and remove labeled data containing negative patterns to reducing noise in labeled data. 2114 Besides the problem of false positive training examples caused by distant supervision. There were a bunch of researches trying to solve the problem of false negative training examples caused by incomplete knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min (Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would be often mentioned in the text. They proposed a latent-variable approach to model it and showed its improvement over aggregate and sentential extraction. 6 Conclusion In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised relation extraction. We leverage"
C14-1199,P09-1113,0,0.516899,"om sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those c"
C14-1199,P02-1006,0,0.0380121,"we propose a novel method to explore fine-grained entity type constraints, and we study a series of methods to integrate the constraints with the relation extracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to tex"
C14-1199,Q13-1030,0,0.129219,"in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training instances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni, 2013) used hidden variables to model the missing data in databases based on a graphical model. The training data generation process for all"
C14-1199,D12-1042,0,0.372137,"ently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negativ"
C14-1199,P12-1076,0,0.211384,"lly generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative training instances. To overcome the false positive problem caused by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training in"
C14-1199,P10-1013,0,0.0263185,"n which aims to automatically generate labeled data by aligning with data in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum (Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUNNER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic labeled data from Penn Treebank and Wikipedia infoboxes respectively. Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a method to generate training data automatically, however it also bring the problem of noisy labeling. After their work, a variety of methods focused to solve this proble"
C14-1199,P13-2117,0,0.063904,"Missing"
C14-1199,N07-4013,0,0.0132693,"hods in information extraction which aims to automatically generate labeled data by aligning with data in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum (Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUNNER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic labeled data from Penn Treebank and Wikipedia infoboxes respectively. Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a method to generate training data automatically, however it also bring the problem of noisy labeling. After their work, a variety of methods"
C14-1199,P13-2141,0,0.064606,"sumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Surdeanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al. (Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail when there was only one sentence containing both entities. They proposed a method to learn and filter noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudorelevance feedback method trying to find out the false negative instances and add them into positive training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training instances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni, 2013) used hidden variables to model the missing data in databases based on a graphical model. The training data"
C14-1199,P05-1053,0,0.175111,"with the relation extracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instan"
C14-1199,D07-1076,0,0.0304933,"xtracting model. Experimental results show that our methods achieve better precision/recall curves in sentential extraction with smoother curves in aggregated extraction which mean more stable models. 1 Introduction Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences containing them. It can potentially benefit many applications, such as knowledge base construction, question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Traditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to manually label training data, which is expensive and limits the ability to scale up. Due to the shortcoming of supervised approaches mentioned above, recently, a more promising approach named distantly supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has become popular. Instead of manual labeling, it automatically generates training data by aligning facts in existing knowledge bases to text. However, the paradigm of distant supervision also causes new problems of noisy training data both in positive training instances and negative tr"
C14-1199,W04-3206,0,\N,Missing
C14-1220,H05-1091,0,0.225571,"on classification is to predict semantic relations between pairs of nominals and can be defined as follows: given a sentence S with the annotated pairs of nominals e1 and e2 , we aim to identify the relations between e1 and e2 (Hendrickx et al., 2010). There is considerable interest in automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP applications. The most representative methods for relation classification use supervised paradigm; such methods have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided into feature-based methods and kernel-based methods. Feature-based methods use a set of features that are selected after performing textual analysis. They convert these features into symbolic IDs, which are then transformed into a vector using a paradigm that is similar to the bag-of-words model2 . Conversely, kernel-based methods require pre-processed input data in the form of parse trees (such as dependency parse trees). These approaches are effective because they leverage a large body of linguistic knowledge"
C14-1220,I05-2045,0,0.270956,"rious features to identify the relations between nominals using different methods. In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply selected the most frequent words in the contexts to represent the relation between the nominals. Chen et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative label identification to address this problem. In the supervised paradigm, relation classification is considered a multi-classification problem, and researchers concentrate on extracting more complex features. Generally, these methods can be categorized into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies have been exploited to convert the classification clues (such as sequences and parse trees) into feature vectors (Kambhatla, 2004; Suchanek et al., 2006). Fe"
C14-1220,J81-4005,0,0.676377,"Missing"
C14-1220,P04-1053,0,0.00799516,"s one of the most important topics in NLP. Many approaches have been explored for relation classification, including unsupervised relation discovery and supervised classification. Researchers have proposed various features to identify the relations between nominals using different methods. In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply selected the most frequent words in the contexts to represent the relation between the nominals. Chen et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative label identification to address this problem. In the supervised paradigm, relation classification is considered a multi-classification problem, and researchers concentrate on extracting more complex features. Generally, these methods can be categorized into two types: feature-based and kernel-based. In feat"
C14-1220,D13-1137,0,0.055406,"formance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al. (2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical and sentence level features for relation classification; our method effectively alleviates the shortcomings of traditional features. 3 3.1 Methodology The Neural Network Architecture Figure 1 describes the architecture of the neural network that we use for relation classification. The network t"
C14-1220,S10-1006,0,0.531175,"Missing"
C14-1220,P11-1055,0,0.771669,"08), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and Mooney, 2005), have been proposed to solve the relation classification problem. However, the methods mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed distant supervision (DS) to address this problem. The DS method selects sentences that match the facts in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Takamatsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative model to model the heuristic labeling process in order to reduce the wrong labels. The supervised method has been demonstrated to be effective for relation detection and yields relatively high performance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features"
C14-1220,P09-1113,0,0.984637,"tions between pairs of nominals and can be defined as follows: given a sentence S with the annotated pairs of nominals e1 and e2 , we aim to identify the relations between e1 and e2 (Hendrickx et al., 2010). There is considerable interest in automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP applications. The most representative methods for relation classification use supervised paradigm; such methods have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided into feature-based methods and kernel-based methods. Feature-based methods use a set of features that are selected after performing textual analysis. They convert these features into symbolic IDs, which are then transformed into a vector using a paradigm that is similar to the bag-of-words model2 . Conversely, kernel-based methods require pre-processed input data in the form of parse trees (such as dependency parse trees). These approaches are effective because they leverage a large body of linguistic knowledge. However, the extracted features or ela"
C14-1220,C08-1088,0,0.515261,"Missing"
C14-1220,D12-1110,0,0.0871516,"ovel generative model to model the heuristic labeling process in order to reduce the wrong labels. The supervised method has been demonstrated to be effective for relation detection and yields relatively high performance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al. (2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical and sentence level features for relation classification; our method effectively"
C14-1220,P12-1076,0,0.0306476,"ey, 2005), have been proposed to solve the relation classification problem. However, the methods mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed distant supervision (DS) to address this problem. The DS method selects sentences that match the facts in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Takamatsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative model to model the heuristic labeling process in order to reduce the wrong labels. The supervised method has been demonstrated to be effective for relation detection and yields relatively high performance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed representation for each wor"
C14-1220,P10-1040,0,0.0302636,"fail and proposed a novel generative model to model the heuristic labeling process in order to reduce the wrong labels. The supervised method has been demonstrated to be effective for relation detection and yields relatively high performance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al. (2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical and sentence level features for relation classification; o"
C14-1220,P05-1053,0,0.770749,"edict semantic relations between pairs of nominals and can be defined as follows: given a sentence S with the annotated pairs of nominals e1 and e2 , we aim to identify the relations between e1 and e2 (Hendrickx et al., 2010). There is considerable interest in automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP applications. The most representative methods for relation classification use supervised paradigm; such methods have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided into feature-based methods and kernel-based methods. Feature-based methods use a set of features that are selected after performing textual analysis. They convert these features into symbolic IDs, which are then transformed into a vector using a paradigm that is similar to the bag-of-words model2 . Conversely, kernel-based methods require pre-processed input data in the form of parse trees (such as dependency parse trees). These approaches are effective because they leverage a large body of linguistic knowledge. However, the extr"
C18-1277,P15-1127,0,0.0128944,"me name “desperado”, contains the gold predicate “music/release track/recording” in common. 3273 2 Related Work The research of KB-QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang et al., 2001) to open-domain QA based on large-scale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the similarity between a qu"
C18-1277,P16-1076,0,0.343451,"r challenge is the vast amount of facts in large scale KB. KB is quite large to some extent. KB has a huge number of fact triples and entities. A common and effective way to remain only a small subset of facts and entities is to conduct entity linking of a question over KB firstly. There are some work conducting entity linking by searching n-gram words of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). While some other work propose a special-purpose sequence labeling network to focus on more probable candidates in question utterance, then linking them to entities (Dai et al., 2016; Yin et al., 2016b). Previous approaches assume that their preceding steps are correct to produce a good result for next procedure. While it should be pointed out that both the labeling and the entity linking process may lead to error propagation problems. For example, if the previous steps provide a wrong labeling result or don’t recall the gold subject entity in the generated candidates, the following procedure can’t make a good choice to retrieve the gold subject and predicate pair. Faced with these problems and following previous work, we propose to do pattern extraction and entity linkin"
C18-1277,P15-1026,0,0.092775,"A based on large-scale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the similarity between a question and an evidence triple with a single model, Yih et al. (2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question utterance to produce a partial similarity score by a dedicated mode"
C18-1277,P17-1021,1,0.763292,"earch directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the similarity between a question and an evidence triple with a single model, Yih et al. (2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question utterance to produce a partial similarity score by a dedicated model. Then these partial scores are combined to generate"
C18-1277,D16-1166,0,0.530039,"der” relation in KB. The word “Apple” in the questions refers to the “Apple Corporation” instead of the fruit “apple”. Natural language is complex while KB is much more universal. Another challenge is the vast amount of facts in large scale KB. KB is quite large to some extent. KB has a huge number of fact triples and entities. A common and effective way to remain only a small subset of facts and entities is to conduct entity linking of a question over KB firstly. There are some work conducting entity linking by searching n-gram words of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). While some other work propose a special-purpose sequence labeling network to focus on more probable candidates in question utterance, then linking them to entities (Dai et al., 2016; Yin et al., 2016b). Previous approaches assume that their preceding steps are correct to produce a good result for next procedure. While it should be pointed out that both the labeling and the entity linking process may lead to error propagation problems. For example, if the previous steps provide a wrong labeling result or don’t recall the gold subject entity in the generated candidates, the following procedure"
C18-1277,D13-1161,0,0.0162378,"sets linked to these entities which has the same name “desperado”, contains the gold predicate “music/release track/recording” in common. 3273 2 Related Work The research of KB-QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang et al., 2001) to open-domain QA based on large-scale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead"
C18-1277,Q14-1030,0,0.0247011,"ies which has the same name “desperado”, contains the gold predicate “music/release track/recording” in common. 3273 2 Related Work The research of KB-QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang et al., 2001) to open-domain QA based on large-scale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the sim"
C18-1277,Q16-1010,0,0.0393818,"Missing"
C18-1277,C16-1226,0,0.0794983,"ale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the similarity between a question and an evidence triple with a single model, Yih et al. (2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question utterance to produce a partial similarity score by a dedicated model. Then these par"
C18-1277,P16-1220,0,0.156165,"ale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the background KB, such that the correct supporting evidence will be the nearest neighbor of the question utterance in the learned vector space. Instead of measuring the similarity between a question and an evidence triple with a single model, Yih et al. (2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question utterance to produce a partial similarity score by a dedicated model. Then these par"
C18-1277,P14-1090,0,0.253559,"Missing"
C18-1277,P14-2105,0,0.0735624,"Missing"
C18-1277,P15-1128,0,0.637477,"ction, which identifies the KB predicate that a question utterance refers to, to reorder the entity linking results. Enhanced entity linking provides a strong support to generate highquality candidate subjects. In SimpleQuestions benchmark, a question, such as “which release was desperado the release track off of?”, asks a direct relation of an entity called “desperado”. While there are dozens of entities named “desperado” in Freebase which linked to different types and predicates5 . Previous work either conduct relation inference firstly (Dai et al., 2016), or conduct entity linking firstly (Yih et al., 2015) may lead to no recall problem. In our framework, we propose to do joint fact selection to alleviate the problem. We leverage entities’ name information and type information to represent entities’ different aspects. As for predicates, we use the unique relation name and dispersive words information. In order to represent the sentence utterance properly, char-level and word-level encodings both are incorporated. The experimental results demonstrate the effectiveness of the proposed approach. To sum up, our main contributions are: (1) We propose to conduct pattern extraction and entity linking,"
C18-1277,P16-2033,0,0.0931074,", the correct entity mention is “carlos gomez”, and the pattern should be “what position does #head entity# play”. 5 Some predicate sets linked to these entities which has the same name “desperado”, contains the gold predicate “music/release track/recording” in common. 3273 2 Related Work The research of KB-QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang et al., 2001) to open-domain QA based on large-scale KB. There are two mainstream research directions for the KB-QA task. One of the promising approaches is semantic parsing (Cai and Yates, 2013; Yih et al., 2015; Yih et al., 2016; Reddy et al., 2016), which uses logic language CCG (zettlemoyer and Collins, 2009; zettlemoyer and Collins, 2012; Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015) or DCS (Berant et al., 2013) to map a question to its formal logical form to query on a KB. The other category exploit vector space embedding approaches (Bordes et al., 2014a; Bordes et al., 2014b; Bordes et al., 2015; Dong et al., 2015; Xu et al., 2016a; Xu et al., 2016b; Hao et al., 2017; Lukovnikov et al., 2017) to measure the semantic similarity between question utterance and candidate resources in the backgroun"
C18-1277,W16-0103,0,0.0455415,"Missing"
C18-1277,C16-1164,0,0.426127,"Missing"
C18-1277,P09-1110,0,0.0274386,"Missing"
D08-1013,P08-1034,0,0.0107598,"Missing"
D08-1013,P04-1035,0,0.165308,"Missing"
D08-1013,P05-1015,0,0.034582,"004; Bo et al. 2005, Hu et al. 2004; Alina et al 2008; Alistair et al 2006). But fewer researchers deal with this problem using CRFs model. For identifying the subjective sentences, there are several research, like (Wiebe et al, 2005). For polarity classification on sentence level, (Kim and Hovy, 2004) judged the sentiment by classifying a pseudo document composed of synonyms of indicators in one sentence. (Pang and Lee, 04) proposed a semi-supervised machine learning method based on subjectivity detection and minimum-cut in graph. Cascaded models for sentiment classification were studied by (Pang and Lee, 2005). Their work mainly used the cascaded frame for determining the orientation of a document and the sentences. In that work, an initial model is used to determine the orientation of each sentence firstly, then the top subjective sentences are input into a document level model to determine the document’s orientation. The CRFs has previously been used for sentiment classification. Those methods based on CRFs are related to our work. (Mao et al, 2007) used a sequential CRFs regression model to measure the polarity of a sentence in order to determine the sentiment flow of the authors in reviews. How"
D08-1013,P08-1036,0,0.0875683,"Missing"
D08-1013,C04-1200,0,0.139975,"ch introduces redundant features into training, can increase the accuracies of all tasks in the different layers at the same time compared with other baselines. It proves that considering label redundancy are effective for promoting the performance of a sentimental classifier. 6 Related Works Recently, many researchers have devoted into the problem of the sentiment classification. Most of researchers focus on how to extract useful textual features (lexical, syntactic, punctuation, etc.) for determining the semantic orientation of the sentences using machine learning algorithm (Bo et al. 2002; Kim and Hovy, 2004; Bo et al. 2005, Hu et al. 2004; Alina et al 2008; Alistair et al 2006). But fewer researchers deal with this problem using CRFs model. For identifying the subjective sentences, there are several research, like (Wiebe et al, 2005). For polarity classification on sentence level, (Kim and Hovy, 2004) judged the sentiment by classifying a pseudo document composed of synonyms of indicators in one sentence. (Pang and Lee, 04) proposed a semi-supervised machine learning method based on subjectivity detection and minimum-cut in graph. Cascaded models for sentiment classification were studied by (Pan"
D08-1013,P07-1055,0,0.0517309,"Missing"
D08-1013,N03-1028,0,\N,Missing
D08-1013,W02-1011,0,\N,Missing
D12-1123,J10-3002,0,0.0277322,"Missing"
D12-1123,D09-1051,0,0.34329,"orresponding modifier through monolingual word alignment. For example in Figure 1, the opinion words “colorful” and “amazing” are aligned with the target “screen” through word alignment. To this end, we use WTM to perform monolingual word alignment for mining associations between opinion targets and opinion words. In this process, several factors, such as word co-occurrence frequencies, word positions 1347 etc., can be considered globally. Compared with adjacent methods, WTM doesn’t identify opinion relations between words in a given window, so long-span relations can be effectively captured (Liu et al., 2009). Compared with syntax-based methods, without using parsing, WTM can effectively avoid errors from parsing informal texts. So it will be more robust. In addition, by using WTM, our method can capture the “one-to-many” or “many-to-one” relations (“one-to-many” means that, in a sentence one opinion word modifies several opinion targets, and “many-to-one” means several opinion words modify one opinion target). Thus, it’s reasonable to expect that WTM is likely to yield better performance than traditional methods for mining associations between opinion targets and opinion words. Based on the mined"
D12-1123,J93-2003,0,0.0628302,"Missing"
D12-1123,C10-1031,0,0.0141296,"Missing"
D12-1123,C10-2090,0,0.407432,"Missing"
D12-1123,D09-1159,0,0.505065,"nown to be opinion words, “screen” is likely to be an opinion target in this domain. In addition, the extracted opinion targets can be used to expand more opinion words according to their associations. It’s a mutual reinforcement procedure. Therefore, mining associations between opinion targets and opinion words is a key for opinion 1346 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1346–1356, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics target extraction (Wu et al., 2009). To this end, most previous methods (Hu et al., 2004; Ding et al., 2004; Wang et al., 2008), named as adjacent methods, employed the adjacent rule, where an opinion target was regarded to have opinion relations with the surrounding opinion words in a given window. However, because of the limitation of window size, opinion relations cannot be captured precisely, especially for long-span relations, which would hurt estimating associations between opinion targets and opinion words. To resolve this problem, several studies exploited syntactic information such as dependency trees (Popescu et al.,"
D12-1123,P11-1066,1,0.694592,", which have been widely adopted in previous work (Hu et al., 2004; Ding et al., 2008; Wang et al., 2008; Qiu et al., 2011). Thus, our aim is to find potential opinion relations between nouns/noun phrases and adjectives in sentences, and calculate the associations between them. As mentioned in the first section, we formulate opinion relation identification as a word alignment task. We employ the word-based translation model (Brown et al. 1993) to perform monolingual word alignment, which has been widely used in many tasks, such as collocation extraction (Liu et al., 2009), question retrieval (Zhou et al., 2011) and so on. In our method, every sentence is replicated to generate a parallel corpus, and we apply the bilingual word alignment algorithm to the monolingual scenario to align a noun/noun phase with its modifier. n words Given a sentence with S  {w1 , w2 ,..., wn } , the word alignment A  {(i, ai ) |i [1, n]} can be obtained by maximizing the word alignment probability of the sentence as follows. Aˆ = arg max P( A |S ) (1) A where (i, ai ) means that a noun/noun phrase at position i is aligned with an adjective at position ai . If we directly use this alignment model to our task, a noun/nou"
D12-1123,C10-1074,0,\N,Missing
D12-1123,H05-2017,0,\N,Missing
D12-1123,H05-1043,0,\N,Missing
D12-1123,C10-2167,0,\N,Missing
D12-1123,J11-1002,0,\N,Missing
D12-1123,I08-1038,0,\N,Missing
D14-1116,P14-1091,0,0.0335715,"sing structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question Question Answering over Linked Data (QALD) aims to evaluate a question answering system over structured data, the key objective of which is to translate questions posed using natural language into structured queries. This technique can help common users to directly access open-structured knowledge on the Web and, accordingly, has attracted much attention. To this end, we propose a novel method using first-order logic. We formulate the knowledge for resolving the ambiguities in the main three steps of QALD (phrase detection, phrase-"
D14-1116,D13-1160,0,0.190921,"the word2vec tool8 . The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detection and in mapping phrases to semantic items. This step focuses on addressing these ambiguities and deter8 mining the argument match relations among the mapped semantic items. This is the core component of our system, and it per"
D14-1116,P13-1042,0,0.0252567,"containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a wor"
D14-1116,de-marneffe-etal-2006-generating,0,0.030571,"Missing"
D14-1116,J13-2005,0,0.0472527,"eterogeneous datasets. For questions containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sente"
D14-1116,D11-1142,0,0.00898206,"sePosTag hasPhrase hasResource hasMeanWord hasRelation ... Figure 2: Framework of our system. from film, movie and show, we compute the similarity between the phrase and the class in the KB with the word2vec tool8 . The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detection and in mapping phrases to se"
D14-1116,P13-1158,0,0.0682519,"Missing"
D14-1116,N09-1018,0,0.0141322,"idely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on the lexical level, but our analysis focuses on the knowledge-base level and aims to obtain an executable query and to support natural language inference. 7 Conclusions and Future Work For the task of QALD, we present a joint learning framework for phrase detection, phrase mapping and semantic item grouping. The novelty of our method lies in the"
D14-1116,C12-1050,0,0.0168705,"methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identificat"
D14-1116,D12-1104,0,0.022193,"hasRelatedness isTypeCompatible phrasePosTag hasPhrase hasResource hasMeanWord hasRelation ... Figure 2: Framework of our system. from film, movie and show, we compute the similarity between the phrase and the class in the KB with the word2vec tool8 . The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detect"
D14-1116,D12-1114,0,0.0275366,", the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on"
D14-1116,D12-1035,0,0.0831561,"there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question Question Answering over Linked Data (QALD) aims to evaluate a question answering system over structured data, the key objective of which is to translate questions posed using natural language into structured queries. This technique can help common users to directly access open-structured knowledge on the Web and, accordingly, has attracted much attention. To this end, we propose a novel method using first-order logic. We formulate the knowledge for r"
D14-1116,P14-1090,0,0.0385705,"Missing"
D14-1116,D13-1097,0,0.0172487,"method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on the lexical level, but our analysis focuses on the knowledge-base level and aims to obtain an executable query and to support natural language inf"
D14-1116,P11-1060,0,\N,Missing
D15-1203,H05-1091,0,0.0182546,"Suchanek et al., 2006). Feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors. Kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse trees. Kernelbased methods enable the use of a large set of features without needing to extract them explicitly. Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005). Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools. With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et al., 2012; Zeng et al., 2014). Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction. Dietterich et al. (1997) suggested that the design of multi-instance modifications for neural networks is a part"
D15-1203,P11-1055,0,0.847103,"the distant supervision assumption is too strong and causes the wrong label problem. A sentence that mentions two entities does not necessarily express their relation in a knowledge base. It is possible that these two entities may simply share the same topic. For instance, the upper sentence indeed expresses the “company/founders” relation in Figure 1. The lower sentence, however, does not express this relation but is still selected as a training instance. This will hinder the performance of a model trained on such noisy data. Second, previous methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision. These features are often derived from preexisting Natural Language Processing (NLP) tools. Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation. Distant supervised relation extraction generally ad1 http://www.freebase.com/ 1753 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Com"
D15-1203,J14-1004,0,0.00734325,"larity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for 1755 ... hired , the son of ma x(c 1 ... ma x(c 1 2 , in 1 ) ) ... max(c13) word position Vector representation Convolution Piecewise max pooling Softmax classifier Figure 3: The architecture of PCNNs (better viewed in color) used for distant supervised relation extraction, illustrating the procedure for handling one instance of a bag and predicting the relation between Kojo Annan and Kofi Annan. enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014). A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm. Recent research (Erhan et al., 2010) has shown that neural networks can converge to better local minima when they are initialized with word embeddings. Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text. Researchers have proposed several methods of training word embeddings (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). In this paper, we use"
D15-1203,P09-1113,0,0.971786,"reated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods. 1 Introduction In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples. One common technique for coping with this difficulty is distant supervision (Mintz et al., 2009) which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way. Figure 1 shows an example of the automatic labeling of data through distant supervision. In this example, Apple and Steve Jobs are two related entities in Freebase1 . All sentences that contain these two entities are selected as training instances. The distant supervision strategy is an effective method of automatically labeling training data. However, it has two major shortcomings when used for relation extraction. Fi"
D15-1203,P14-1100,0,0.00732672,"in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for 1755 ... hired , the son of ma x(c 1 ... ma x(c 1 2 , in 1 ) ) ... max(c13) word position Vector representation Convolution Piecewise max pooling Softmax classifier Figure 3: The architecture of PCNNs (better viewed in color) used for distant supervised relation extraction, illustrating the procedure for handling one instance of a bag and predicting the relation between Kojo Annan and Kofi Annan. enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014). A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm. Recent research (Erhan et al., 2010) has shown that neural networks can converge to better local minima when they are initialized with word embeddings. Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text. Researchers have proposed several methods of training word embeddings (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). I"
D15-1203,D14-1162,0,0.125053,"onal vectors. In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings. Moreover, we use position features (PFs) to specify entity pairs, which are also transformed into vectors by looking up position embeddings. 3.1.1 Word Embeddings Word embeddings are distributed representations of words that map each word in a text to a ‘k’dimensional real-valued vector. They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for 1755 ... hired , the son of ma x(c 1 ... ma x(c 1 2 , in 1 ) ) ... max(c13) word position Vector representation Convolution Piecewise max pooling Softmax classifier Figure 3: The architecture of PCNNs (better viewed in color) used for distant supervised relation extraction, illustrating the procedure for handling one instance of a bag and predicting the relation between Kojo Annan and Kofi Annan. enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014). A common method of training a neural ne"
D15-1203,C08-1088,0,0.0992936,"to convert classification clues (e.g., sequences, parse trees) into feature vectors (Kambhatla, 2004; Suchanek et al., 2006). Feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors. Kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse trees. Kernelbased methods enable the use of a large set of features without needing to extract them explicitly. Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005). Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools. With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et al., 2012; Zeng et al., 2014). Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction. Die"
D15-1203,D12-1110,0,0.0128094,"arse trees. Kernelbased methods enable the use of a large set of features without needing to extract them explicitly. Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005). Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools. With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et al., 2012; Zeng et al., 2014). Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction. Dietterich et al. (1997) suggested that the design of multi-instance modifications for neural networks is a particularly interesting topic. Zhang and Zhou (2006) successfully incorporated multiinstance learning into traditional Backpropagation (BP) and Radial Basis Function (RBF) networks and optimized these networks by minimizing a sum-of-squares error function. In contrast to their method, we define the obje"
D15-1203,D12-1042,0,0.923468,"2007) showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length. Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious. In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address the two problems described above. To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to previous studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In multi-instance problem, the training set consists of many bags, and each contains many instances. The labels of the bags are known; however, the labels of the instances in the bags are unknown. We design an objective function at the bag level. In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem. To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014). Our proposal is an extension of Zeng et al. ("
D15-1203,D14-1181,0,0.0100498,"ive function at the bag level. In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem. To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014). Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features. Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014). For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them. Several approaches have employed manually crafted features that attempt to model such structural information. These approaches usually consider both internal and external contexts. A sentence is inherently divided into three segments according to the two given entities. The internal context includes"
D15-1203,C14-1220,1,0.777227,"iedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). In multi-instance problem, the training set consists of many bags, and each contains many instances. The labels of the bags are known; however, the labels of the instances in the bags are unknown. We design an objective function at the bag level. In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem. To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014). Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features. Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014). For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them. Several approaches have e"
D15-1203,D07-1013,0,0.00778306,"m/ 1753 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 8000 Number of sentence 6000 4000 2000 0 0 20 40 60 Sentence Length 80 100 Figure 2: The sentence length distribution of Riedel’s dataset. dresses corpora from the Web, including many informal texts. Figure 2 shows the sentence length distribution of a benchmark distant supervision dataset that was developed by Riedel et al. (2010). Approximately half of the sentences are longer than 40 words. McDonald and Nivre (2007) showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length. Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious. In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address the two problems described above. To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to previous studies (Riedel et al., 2010; Hoffmann et al., 2011; Surde"
D15-1203,P06-1104,0,0.0256735,"tities (Graham, 2014). For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them. Several approaches have employed manually crafted features that attempt to model such structural information. These approaches usually consider both internal and external contexts. A sentence is inherently divided into three segments according to the two given entities. The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities (Zhang et al., 2006). Clearly, single max pooling is not sufficient to capture such structural information. To capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer. The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence. Thus, it is expected to exhibit superior performance compared with traditional methods. The contributions of this paper can be summarized"
D15-1203,P05-1053,0,0.547616,"earning into the PCNNS for distant supervised relation extraction. • In the proposed network, we devise a piecewise max pooling layer, which aims to capture structural information between two entities. 2 Related Work Relation extraction is one of the most important topics in NLP. Many approaches to relation extraction have been developed, such as bootstrapping, unsupervised relation discovery and supervised classification. Supervised approaches are the most commonly used methods for relation 1754 extraction and yield relatively high performance (Bunescu and Mooney, 2006; Zelenko et al., 2003; Zhou et al., 2005). In the supervised paradigm, relation extraction is considered to be a multi-class classification problem and may suffer from a lack of labeled data for training. To address this problem, Mintz et al. (2009) adopted Freebase to perform distant supervision. As described in Section 1, the algorithm for training data generation is sometimes faced with the wrong label problem. To address this shortcoming, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) developed the relaxed distant supervision assumption for multi-instance learning. The term ‘multiinstance learning was coined"
D16-1083,P12-2034,0,0.464028,"eviews (also called untruthful opinions), reviews on the brand only, and nonreviews.Stepping studies focus on studying fake reviews because of its difficulty to be detected. Most efforts are devoted to represent fake and non-fake reviews with effective features. Linguistic Features Ott et al. (2011) apply psychological and linguistic clues to identify review spam. They produce the first dataset of goldstandard deceptive review spam, employing crowdsourcing through the Amazon Mechanical Turk. Harris (2012) explores several human- and machinebased assessment methods with writing style features. Feng et al. (2012a) investigate syntactic stylometry for review spam detection. Li et al. (2013) propose a generative LDA-based topic modeling approach for fake review detection. They (Li et al., 2014b) further investigate the general difference of 873 language usage between deceptive and truthful reviews. Li et al. (2014a) propose a positive-unlabeled learning method base on unigrams and bigrams. Kim et al. (2015) carry out a frame-based deep semantic analysis on deceptive opinions. Behavioral Features Lim et al. (2010) investigate reviewers’ rating behavioral features. Jindal et al. (2010) identify unusual r"
D16-1083,P13-2039,0,0.502261,"ews.Stepping studies focus on studying fake reviews because of its difficulty to be detected. Most efforts are devoted to represent fake and non-fake reviews with effective features. Linguistic Features Ott et al. (2011) apply psychological and linguistic clues to identify review spam. They produce the first dataset of goldstandard deceptive review spam, employing crowdsourcing through the Amazon Mechanical Turk. Harris (2012) explores several human- and machinebased assessment methods with writing style features. Feng et al. (2012a) investigate syntactic stylometry for review spam detection. Li et al. (2013) propose a generative LDA-based topic modeling approach for fake review detection. They (Li et al., 2014b) further investigate the general difference of 873 language usage between deceptive and truthful reviews. Li et al. (2014a) propose a positive-unlabeled learning method base on unigrams and bigrams. Kim et al. (2015) carry out a frame-based deep semantic analysis on deceptive opinions. Behavioral Features Lim et al. (2010) investigate reviewers’ rating behavioral features. Jindal et al. (2010) identify unusual review patterns which can represent suspicious behaviors of reviews. Li et al. ("
D16-1083,P14-1147,0,0.840895,"s are devoted to represent fake and non-fake reviews with effective features. Linguistic Features Ott et al. (2011) apply psychological and linguistic clues to identify review spam. They produce the first dataset of goldstandard deceptive review spam, employing crowdsourcing through the Amazon Mechanical Turk. Harris (2012) explores several human- and machinebased assessment methods with writing style features. Feng et al. (2012a) investigate syntactic stylometry for review spam detection. Li et al. (2013) propose a generative LDA-based topic modeling approach for fake review detection. They (Li et al., 2014b) further investigate the general difference of 873 language usage between deceptive and truthful reviews. Li et al. (2014a) propose a positive-unlabeled learning method base on unigrams and bigrams. Kim et al. (2015) carry out a frame-based deep semantic analysis on deceptive opinions. Behavioral Features Lim et al. (2010) investigate reviewers’ rating behavioral features. Jindal et al. (2010) identify unusual review patterns which can represent suspicious behaviors of reviews. Li et al. (2011) provide a two-view semi-supervised method, co-training method base on behavioral features. Feng et"
D16-1083,P11-1032,0,0.475013,"Missing"
D16-1145,D11-1049,0,0.840291,"e. This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there are a lot of sparse structures in KBs which will lead to inaccurate confidence. According to our statistics, more than 90 percent of high-confidence formulas produced by random walk are noise. 2) Employing heuristic rules to direct random walks. This method directs random walks to find 1380 useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas. For example, PRA (Lao and Cohen, 2010; Lao et al., 2011) assumes the more narrow distributions of elements in a formula are, the higher score the formula will obtain. However, formulas with high scores in PRA are not always true. For example, the formula in Figure 1.c has a high score in PRA, but it is not true. Oppositely, formulas with low scores in PRA are not always useless. For example, the formula, F ather(x, y) ∧ F ather(y, z) ⇒ Grandf ather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandf ather. According to our investigations, the situations are common in KBs. In this paper,"
D16-1145,D15-1082,0,0.113545,"Missing"
D16-1145,D15-1166,0,0.0192572,"ts. In practice, we set Er(i,j) = [Er , Ej ] and ER(H,T ) = [ER , ET ] because Ei is the same for all triplets r(i, ∗), where [] is a concatenation operator. In the view of the neural network, our goaldirected mechanism is analogous to the attention mechanism. At each step, the algorithm estimates attentions for each neighboring edges by Ψ. Therefore, there are several existing expressions of Ψ, e.g., the dot product (Sukhbaatar et al., 2015) and the single-layer perceptron (Bahdanau et al., 2015). We will not compare different forms of Ψ, the detail comparison has been presented in the work (Luong et al., 2015). We directly employ the simplest dot product for Ψ as follows, Ψ(Er(i,j) , ER(H,T ) ) = σ(Er(i,j) · ER(H,T ) ) (7) where σ is a nonlinear function and we set it as an exponential function. Ψ has no parameters beside KB embeddings which are updated during the training period. 3.3 Integrated Inference Model To handle the dependence between goal-directed random walk and subsequent inference, we combine them into an integrated model and optimize them together. To predict ρ = R(H, T ), the integrated model first collects formulas for R(H, T ), and then 1383 Algorithm 1: Train Integrated Inference"
D16-1145,D10-1106,0,0.0409943,"risk factors(y,x) ⇒ disease risk factors(x,y) Table 3: Example Formulas Obtained by Goal-directed Random Walk cause the body of the formula is same as the head. Such useless formula can be removed by a superrule, which is that the head of a formula cannot occur in its body. 5 Related Work Our work has two aspects, which are related to mining formula automatically and inference on KBs, respectively. Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Association Rule Mining (ARM) (Agrawal et al., 1993) are both early works on mining formulas. FOIT (Quinlan, 1990) and SHERLOCK (Schoenmackers et al., 2010) are typical ILP systems, but the former one usually need a lot of negative facts and the latter one focuses on mining formulas from text. AMIE (Gal´arraga et al., 2013) is based on ARM and proposes a new measure for formulas instead of the confidence. Several structure learning algorithms (Kok and Domingos, 2005; Kok and Domingos, 2009; Kok and Domingos, 2010) based on Markov Logic Network (MLN) (Richardson and Domingos, 2006) can also learn first order logic formulas automatically, but they are too slow to run on large KBs. ProPPR (Wang et al., 2013; Wang et al., 2014a) performs structure le"
D18-1017,A97-1029,0,0.532514,"for some words such as “ • (Hilton)” and “» (leaves)”, while Chinese NER has more coarse-grained boundaries Introduction The task of named entity recognition (NER) is to recognize the named entities in given text. NER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yao and Van Durme, 2014). In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Currently, with the development 182 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 182–192 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics than CWS task for certain word such as “ ¯• :: (Houston Airport)” in the example of Figure 1, which we call task-specific information. In order to incorporate word boundary information from CWS task into NER task, Peng and Dredze (2016) prop"
D18-1017,W06-0130,0,0.756932,"different domains. The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain. The improvement of performance indicates that our proposed adversarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem. 4.3.2 Evaluation on SighanNER Table 4 lists the comparisons on SighanNER dataset. We observe that our proposed model achieves new state-of-the-art performance. In the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset. Chen et al. (2006) propose a character-based CRF model for Chinese NER task. Zhou et al. (2006) introduce a pipeline model, which first segments the text with characterlevel CRF model and then applies word-level CRF to tag. Luo and Yang (2016) first train a word segmenter and then use word segmentation as addiWe also conduct an experiment on the updated WeiboNER dataset. Table 3 lists the performance of the latest models and our proposed model on the updated dataset. In the first block of Table 3, 188 Models BiLSTM+CRF BiLSTM+CRF+transfer BiLSTM+CRF+adversarial BiLSTM+CRF+self-attention BiLSTM+CRF+adversarial+s"
D18-1017,P17-1110,0,0.0288707,"and task discriminator. introduce self-attention mechanism to Chinese NER task. NER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese NER task. Adversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015). In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018). Bousmalis et al. (2016) propose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task. Self-Attention Self-attention has been introduced to machine translation by Vaswani et al. (2017) for capturing global dependencies between input and output and a"
D18-1017,P15-1017,1,0.833514,"ER task and CWS task, which we call task-shared information. As shown in Figure 1, given a sentence “ •» ¯•: : (Hilton leaves Houston Airport)”, the two tasks have the same boundaries for some words such as “ • (Hilton)” and “» (leaves)”, while Chinese NER has more coarse-grained boundaries Introduction The task of named entity recognition (NER) is to recognize the named entities in given text. NER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yao and Van Durme, 2014). In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Currently, with the development 182 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 182–192 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics than CWS task for certain word such as “ ¯• :: (Houston"
D18-1017,D17-1302,0,0.0478753,"tractor (Shared BiLSTM), self-attention and task discriminator. introduce self-attention mechanism to Chinese NER task. NER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese NER task. Adversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015). In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018). Bousmalis et al. (2016) propose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task. Self-Attention Self-attention has been introduced to machine translation by Vaswani et al. (2017) for capturing global dep"
D18-1017,N16-1030,0,0.117738,"methods have been proposed for NER task. Early studies on NER often exploit SVMs (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRFs (Lafferty et al., 2001), heavily relying on feature engineering. Zhou et al. (2013) formulate Chinese NER as a joint identification and categorization task. In recent years, neural network models have been introduced to NER task (Collobert et al., 2011; Huang et al., 2015; Peng and Dredze, 2016). Huang et al. (2015) exploit BiLSTM to extract features and feed them into CRF decoder. After that, the BiLSTM-CRF model is usually exploited as the baseline. Lample et al. (2016) use a character LSTM to represent spelling characteristics. In addition, Wang et al. (2017) propose a gated convolutional neural network (GCNN) model for Chinese NER. Peng and Dredze (2016) propose a joint model for Chinese To address the above problems, we propose an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER task in this paper. The adversarial transfer learning is incorporating adversarial training into transfer learning. To better capture long range dependencies and synthesize the information of the sentence, we extend se"
D18-1017,W06-0115,0,0.849277,"Missing"
D18-1017,P17-1001,0,0.073611,"Missing"
D18-1017,N16-1028,0,0.261227,"Missing"
D18-1017,D17-1256,0,0.0414657,"and CRF layer. The middle part is shared space consisting of feature extractor (Shared BiLSTM), self-attention and task discriminator. introduce self-attention mechanism to Chinese NER task. NER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese NER task. Adversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015). In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018). Bousmalis et al. (2016) propose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task. Self-Attention Self-attention has been introduced t"
D18-1017,E17-2113,0,0.25823,"Missing"
D18-1017,D15-1064,0,0.0916977,"tasks. Besides the task loss LT ask , we introduce an adversarial loss LAdv to prevent specific features of CWS task from creeping into shared space. The adversarial loss trains the shared model to produce shared features such that the task discriminator cannot reliably recognize which task the sentence comes from. The adversarial loss can be computed as follows: LAdv = min(max θs θd Tk K X X Training 4 Experiments (i) logD(Es (xk ))) 4.1 k=1 i=1 Datasets To evaluate our proposed model on Chinese NER, we experiment on two different widely used datasets, including Weibo NER dataset (WeiboNER) (Peng and Dredze, 2015; He and Sun, (17) where θs denotes the trainable parameters of shared BiLSTM. Es denotes the shared feature extractor. Tk is the number of training examples of 186 Dataset WeiboNER SighanNER MSR Task Chinese NER Chinese NER CWS # Train sent 1350 41728 86924 # Dev sent 270 4636 — # Test sent 270 4365 3985 Table 1: Statistics of the datasets. Models CRF (Peng and Dredze, 2015) CRF+word (Peng and Dredze, 2015) CRF+character (Peng and Dredze, 2015) CRF+character+position (Peng and Dredze, 2015) Joint(cp) (main) (Peng and Dredze, 2015) Pipeline Seg.Repr.+NER (Peng and Dredze, 2016) Jointly Train C"
D18-1017,P16-2025,0,0.262044,"ake full use of task-shared boundaries information and prevent the taskspecific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit selfattention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-ofthe-art methods. 1 Figure 1: An example of illustrating the similarities and specificities between Chinese NER and CWS. of deep learning, neural networks (Lample et al., 2016; Peng and Dredze, 2016; Luo and Yang, 2016) have been introduced to NER task. All these methods need to determine entities boundaries and classify them into pre-defined categories. Although great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed. One significant drawback is that there is only a very small amount of annotated data available. Weibo NER dataset (Peng and Dredze, 2015; He and Sun, 2017a) and Sighan2006 NER dataset (Levow, 2006) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively. On"
D18-1017,C02-1054,0,0.887775,", while Chinese NER has more coarse-grained boundaries Introduction The task of named entity recognition (NER) is to recognize the named entities in given text. NER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yao and Van Durme, 2014). In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Currently, with the development 182 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 182–192 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics than CWS task for certain word such as “ ¯• :: (Houston Airport)” in the example of Figure 1, which we call task-specific information. In order to incorporate word boundary information from CWS task into NER task, Peng and Dredze (2016) propose a joint model that performs Chinese NER with CWS task."
D18-1017,P14-1090,0,0.0207541,"Missing"
D18-1017,Q17-1036,0,0.0277527,"TM), self-attention and CRF layer. The middle part is shared space consisting of feature extractor (Shared BiLSTM), self-attention and task discriminator. introduce self-attention mechanism to Chinese NER task. NER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese NER task. Adversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015). In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018). Bousmalis et al. (2016) propose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task. Self-Attention Self-attention ha"
D18-1017,W06-0140,0,0.328467,"Missing"
D18-1158,P11-1113,0,0.865199,"specifically. S2: The project leader was fired for the bankruptcy of the subsidiary company. Event detection (ED) is a crucial subtask of event extraction, which aims to identify event triggers and classify them into specific types from texts. According to the task defined in Automatic Context Extraction1 (ACE), given the following sentence S1, a robust ED system should be able to recognize two events: a Die event triggered by died and an Attack event triggered by fired. S1: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. To this end, most methods (Ahn, 2006; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2016; Liu et al., 2017) model ED as a multiclassification task and predict every word in the http://projects.ldc.upenn.edu/ace/ Transport Figure 1: Top 5 event types that co-occur with Attack event in the same sentence in ACE 2005. Introduction 1 Die Event interdependency: In S1, fired triggers an Attack event, while it triggers an End-Position event in S2. Because of the ambiguity, a traditional approach may mislabel fired in S1 as a trigger of End-Position event. However, if we know died triggers a Die event in S1, which is easier to disambiguate, we"
D18-1158,W06-0901,0,0.9252,"wo problems specifically. S2: The project leader was fired for the bankruptcy of the subsidiary company. Event detection (ED) is a crucial subtask of event extraction, which aims to identify event triggers and classify them into specific types from texts. According to the task defined in Automatic Context Extraction1 (ACE), given the following sentence S1, a robust ED system should be able to recognize two events: a Die event triggered by died and an Attack event triggered by fired. S1: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. To this end, most methods (Ahn, 2006; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2016; Liu et al., 2017) model ED as a multiclassification task and predict every word in the http://projects.ldc.upenn.edu/ace/ Transport Figure 1: Top 5 event types that co-occur with Attack event in the same sentence in ACE 2005. Introduction 1 Die Event interdependency: In S1, fired triggers an Attack event, while it triggers an End-Position event in S2. Because of the ambiguity, a traditional approach may mislabel fired in S1 as a trigger of End-Position event. However, if we know died triggers a Die event in S1, which is easier"
D18-1158,P17-1038,1,0.923322,"hods for comparison, which can be classified as two types: separate and collective methods: Separate methods: 1) Li’s MaxEnt: the method that detects events in one sentence separately by using human-designed features (Li et al., 2013). 2) Liao’s CrossEvent : the method that uses cross event information (Liao and Grishman, 2010). 3) Hong’s CrossEntity: the method that uses cross entity information (Hong et al., 2011). 4) Chen’s DMCNN: the dynamic multipooling convolutional neural networks method (Chen et al., 2015). 5) Chen’s DMCNN+: the DMCNN method argumented with automatically labeled data (Chen et al., 2017). 6) Liu’s FrameNet : the method that leverages FrameNet as extended training data to improve ED (Liu et al., 2016a). 7) Liu’s ANN-Aug: the method that use the annotated argument information via a supervised attention to improve ED (Liu et al., 2017). Collective methods: 1) Li’s Structure: the method that collectively detects events by using human-designed features (Li et al., 2013). 2) Yang’s JointEE: the method that detects events and entities in one sentence jointly based on human-designed features (Yang and Mitchell, 2016). 3) Nguyen’s JRNN: the method that exploits a RNN model to collecti"
D18-1158,P15-1017,1,0.818833,"The project leader was fired for the bankruptcy of the subsidiary company. Event detection (ED) is a crucial subtask of event extraction, which aims to identify event triggers and classify them into specific types from texts. According to the task defined in Automatic Context Extraction1 (ACE), given the following sentence S1, a robust ED system should be able to recognize two events: a Die event triggered by died and an Attack event triggered by fired. S1: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. To this end, most methods (Ahn, 2006; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2016; Liu et al., 2017) model ED as a multiclassification task and predict every word in the http://projects.ldc.upenn.edu/ace/ Transport Figure 1: Top 5 event types that co-occur with Attack event in the same sentence in ACE 2005. Introduction 1 Die Event interdependency: In S1, fired triggers an Attack event, while it triggers an End-Position event in S2. Because of the ambiguity, a traditional approach may mislabel fired in S1 as a trigger of End-Position event. However, if we know died triggers a Die event in S1, which is easier to disambiguate, we tend to predict th"
D18-1158,I17-1036,0,0.227844,"and collective methods. Separate methods: These methods regard multiple events in one sentence as independent ones and recognize them separately. These methods include feature-based methods which exploit a diverse set of strategies to convert classification clues into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012), and neural-based methods which use neural networks to automatically capture clues from plain texts (Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Chen et al., 2017; Duan et al., 2017; Liu et al., 2017). Though effective these methods, they neglect event interdependency by separately predicting each event. Collective methods: These methods try to model the event interdependency and detect multiple events in one sentence collectively. However, nearly all of these methods are feature-based methods (McClosky et al., 2011; Li et al., 2013; Yang and Mitchell, 2016; Liu et al., 2016b), which rely on elaborately designed features and suffer error propagation from existing NLP tools. Nguyen et al. (2016) exploits a neural-based method to detect multiple events collectively. Howeve"
D18-1158,P08-1030,0,0.876093,"eft triggers a Transport event, while a clue like “They held a party for his retirement.” would indicate the aforementioned event is an End-Position event. We call such clues as document-level information. Moreover, the confidence of sentence-level and document-level information should be taken into consideration when using them together to construct a broader range of contextual information. For example in S3, document-level information will give us more evidence, while in S1 sentence-level information is enough to disambiguate the types of events. There have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED. However, they suffer from two problems: (1) The features they used often need to be manually designed and may involve error propagation from existing NLP tools; (2) Sentence-level and document-level information are integrated by a large number of fixed rules, which is complicated to construct and it will be far from complete. Thus, how to use a neural-based model to automatically extract sentence-level and document-level information and dynamically inte"
D18-1158,N16-1030,0,0.0106837,"7 S-Attention S-Attention S-Attention S-Attention S-Attention S-Attention S-Attention ℎ1 ℎ2 ℎ3 ℎ4 ℎ5 ℎ6 ℎ7 BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM ?1 ?2 ?3 ?4 ?5 ?6 died when an Document ?ℎ? D-Attention … (Sentence i) BiLSTM Layer Embedding Layer Input … cameraman American ?7 tank fired Figure 2: The architecture of our proposed hierarchical and bias tagging networks with gated multi-level attention. 3.2 BiLSTM Layer semantic information sht is calculated as follows: In sequence labelling problems, the BiLSTM has been proven effective to capture the semantic information of each word (Lample et al., 2016). In this paper, we use the LSTM unit as described in (Zaremba and Sutskever, 2014). For each word wt , the forward LSTM encodes wt by considering the contextual information from word w1 to ! wt , which is marked as h t . Similarly, the backward LSTM encodes wt based on the contextual information from wNw to wt , which is marked as ! h t . Finally, we concatenate h t and h t to represent the information of the word wt , denoted as ! ! ht = [ h t , h t ], and we concatenate h Nw and h 1 to represent the encoding information of the whole ! sentence si , denoted as hsi = [ h Nw , h 1 ]. 3.3 Gated"
D18-1158,P13-1008,0,0.794389,"es the occurrence of an event. Event arguments: the mentions that are involved in an event (viz., participants). Event mention: a phrase or sentence within which an event is described, including a trigger and arguments. Given an English text document, an ED system should identify event triggers and categorize their event types for each sentence. For instance, in the sentence “He died in the hospital”, an ED system is expected to detect a Die event along with the trigger word “died”. The ACE 2005 evaluation defines 8 event types and 33 subtypes, such as Attack or Die. Following previous works (Li et al., 2013; Chen et al., 2015; Liu et al., 2017; Nguyen and Grishman, 2016), we categorize triggers into these 33 subtypes. 3 Methodology In this paper, we formulate event detection as a sequence labelling task. As shown in Figure 2, 3 Our source code, including all hyper-parameter settings and pre-trained word embeddings, is openly available at https://github.com/yubochen/NBTNGMA4ED we label all words in one sentence collectively via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA). We assign a tag for each word to indicate whether it triggers a specific ty"
D18-1158,P10-1081,0,0.911701,"t event, while a clue like “They held a party for his retirement.” would indicate the aforementioned event is an End-Position event. We call such clues as document-level information. Moreover, the confidence of sentence-level and document-level information should be taken into consideration when using them together to construct a broader range of contextual information. For example in S3, document-level information will give us more evidence, while in S1 sentence-level information is enough to disambiguate the types of events. There have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED. However, they suffer from two problems: (1) The features they used often need to be manually designed and may involve error propagation from existing NLP tools; (2) Sentence-level and document-level information are integrated by a large number of fixed rules, which is complicated to construct and it will be far from complete. Thus, how to use a neural-based model to automatically extract sentence-level and document-level information and dynamically integrate them is another cha"
D18-1158,P16-1201,1,0.943202,"s MaxEnt: the method that detects events in one sentence separately by using human-designed features (Li et al., 2013). 2) Liao’s CrossEvent : the method that uses cross event information (Liao and Grishman, 2010). 3) Hong’s CrossEntity: the method that uses cross entity information (Hong et al., 2011). 4) Chen’s DMCNN: the dynamic multipooling convolutional neural networks method (Chen et al., 2015). 5) Chen’s DMCNN+: the DMCNN method argumented with automatically labeled data (Chen et al., 2017). 6) Liu’s FrameNet : the method that leverages FrameNet as extended training data to improve ED (Liu et al., 2016a). 7) Liu’s ANN-Aug: the method that use the annotated argument information via a supervised attention to improve ED (Liu et al., 2017). Collective methods: 1) Li’s Structure: the method that collectively detects events by using human-designed features (Li et al., 2013). 2) Yang’s JointEE: the method that detects events and entities in one sentence jointly based on human-designed features (Yang and Mitchell, 2016). 3) Nguyen’s JRNN: the method that exploits a RNN model to collectively detects events by only using sentence-level information (Nguyen et al., 2016). 4) Liu’s PSL : the method that"
D18-1158,P17-1164,1,0.892173,"Missing"
D18-1158,P11-1163,0,0.0427987,"n, 2010; Hong et al., 2011; Huang and Riloff, 2012), and neural-based methods which use neural networks to automatically capture clues from plain texts (Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Chen et al., 2017; Duan et al., 2017; Liu et al., 2017). Though effective these methods, they neglect event interdependency by separately predicting each event. Collective methods: These methods try to model the event interdependency and detect multiple events in one sentence collectively. However, nearly all of these methods are feature-based methods (McClosky et al., 2011; Li et al., 2013; Yang and Mitchell, 2016; Liu et al., 2016b), which rely on elaborately designed features and suffer error propagation from existing NLP tools. Nguyen et al. (2016) exploits a neural-based method to detect multiple events collectively. However, they only use the sentence-level information and ne1274 glect document-level clues, and can only capture the interdependencies between the current event candidate and its former predicted events. Moreover, there method can not handle the multiple words trigger problem. Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji, Bing Qin, and Ting"
D18-1158,P15-2060,0,0.367125,"c in NLP. Generally, existing approaches could roughly be divided into two groups: separate and collective methods. Separate methods: These methods regard multiple events in one sentence as independent ones and recognize them separately. These methods include feature-based methods which exploit a diverse set of strategies to convert classification clues into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2012), and neural-based methods which use neural networks to automatically capture clues from plain texts (Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Chen et al., 2017; Duan et al., 2017; Liu et al., 2017). Though effective these methods, they neglect event interdependency by separately predicting each event. Collective methods: These methods try to model the event interdependency and detect multiple events in one sentence collectively. However, nearly all of these methods are feature-based methods (McClosky et al., 2011; Li et al., 2013; Yang and Mitchell, 2016; Liu et al., 2016b), which rely on elaborately designed features and suffer error propagation from existing NLP tools. Nguyen et al."
D18-1158,N16-1034,0,0.791988,"re 1. We call such clues as event 1267 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1267–1276 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics interdependency. Some works (Li et al., 2013; Yang and Mitchell, 2016; Liu et al., 2016b) rely on a set of elaborately designed features and complicated natural language processing (NLP) tools to capture event interdependency. However, these methods lack generalization, take a large amount of human effort and are prone to error propagation problem. Though Nguyen et al. (2016) use a Recurrent Neural Networks (RNN) based classification model to capture the event interdependency between current event candidate and the former (left) predicted events, they miss the event interdependency between current event candidate and the later (right) predicted events, and the later events can not change the type of current event. The reason is that they classify the words of the sentence from left to right one by one and only use the former events to predict the later event types. We claim that both of the former and later predicted events are important to predict the event type"
D18-1158,D16-1085,0,0.792479,"was fired for the bankruptcy of the subsidiary company. Event detection (ED) is a crucial subtask of event extraction, which aims to identify event triggers and classify them into specific types from texts. According to the task defined in Automatic Context Extraction1 (ACE), given the following sentence S1, a robust ED system should be able to recognize two events: a Die event triggered by died and an Attack event triggered by fired. S1: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. To this end, most methods (Ahn, 2006; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2016; Liu et al., 2017) model ED as a multiclassification task and predict every word in the http://projects.ldc.upenn.edu/ace/ Transport Figure 1: Top 5 event types that co-occur with Attack event in the same sentence in ACE 2005. Introduction 1 Die Event interdependency: In S1, fired triggers an Attack event, while it triggers an End-Position event in S2. Because of the ambiguity, a traditional approach may mislabel fired in S1 as a trigger of End-Position event. However, if we know died triggers a Die event in S1, which is easier to disambiguate, we tend to predict that fired triggers an Attack"
D18-1158,N16-1033,0,0.300227,"en’s DMCNN+: the DMCNN method argumented with automatically labeled data (Chen et al., 2017). 6) Liu’s FrameNet : the method that leverages FrameNet as extended training data to improve ED (Liu et al., 2016a). 7) Liu’s ANN-Aug: the method that use the annotated argument information via a supervised attention to improve ED (Liu et al., 2017). Collective methods: 1) Li’s Structure: the method that collectively detects events by using human-designed features (Li et al., 2013). 2) Yang’s JointEE: the method that detects events and entities in one sentence jointly based on human-designed features (Yang and Mitchell, 2016). 3) Nguyen’s JRNN: the method that exploits a RNN model to collectively detects events by only using sentence-level information (Nguyen et al., 2016). 4) Liu’s PSL : the method that uses a probabilistic soft logic to detect events by using human-designed features (Liu et al., 2016b). Experimental results are shown in Table 1. From the table, we have the following observations: (1) Among all the methods, our HBTNGMA achieves the best performance. It can improve the best collective method’s F1 by 1272 4 5 https://code.google.com/p/word2vec/ https://catalog.ldc.upenn.edu/LDC2008T19 Methods Li’s"
D19-1035,D16-1127,0,0.0144309,"a RL process and optimize the model with REINFORCE (Williams, 1992) algorithm. Therefore, we don’t have to determine the triplets order of each sentence beforehand, we let the model generate triplets freely. We show the RL process in Figure 2. RL has attracted lot of attention recently. It has been successfully applied in many games (Mnih et al., 2015; Silver et al., 2016). Narasimhan et al. (2015); He et al. (2016) applied RL on text based games. Narasimhan et al. (2016) employed deep Q-network to optimize a reward function that reflects the extraction accuracy while penalizing extra effort. Li et al. (2016) applied policy gradient method to model future reward in chatbot dialogue. They designed a reward to promote three conversational properties: informativity, coherence and ease of answering. Su et al. (2016) using on-line activate reward learning for policy optimization in spoken dialogue systems because the user feedback is often unreliable and costly to collect. Yu et al. (2017) applied RL method to overcome the limitations that the Generative Adversarial Net (GAN) in generating sequences of discrete tokens. Our work is related to Li et al. (2016); Yu et al. (2017) since we also apply RL to"
D19-1035,D18-1307,0,0.0612225,"a, 410114, China {xiangrong.zeng, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn zengdj@csust.edu.cn Abstract According to the predicted tags, they can recognize the entities and the relation between each entity pair. 3) NovelTagging genre, including Zheng et al. (2017). This method can be seen as a development of TableFilling method. They assigned a pre-defined semantic tag to each word of the sentence and collected triplets based on the tags. Their tags include both entity and relation information. Therefore, they don’t need to maintain a entity-relation table. 4) MultiHeadSelection genre, including Bekoulis et al. (2018a) and Bekoulis et al. (2018b). They first recognized the entities, then they formulated the relation extraction task as a multi-head selection problem. For each entity, they calculated the score between it and every other entities for a given relation. The combination of the entity pair and relation with the score exceeding a threshold will be kept as a triplet. 5) Generative genre, including Zeng et al. (2018b). They directly generate triplets one by one by a sequence-to-sequence model with copy mechanism (Gu et al., 2016; Vinyals et al., 2015). To generate a triplet, they first generated th"
D19-1035,P14-1038,0,0.28487,"r, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entity pair is given beforehand and a sentence only contains two entities. To extract both entities and relation from sentence, early works like Zelenko et al. (2003); Chan and Roth (2011) adopted pipeline methods. However, such pipeline methods neglect the relevance between entities and relation. Latter works focused on joint models that extract entities and relation jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Bansal (2016) relied on NLP tools to do feature engineering, which suffered from the error propagation problem. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) applied neural networks to jointly extract entities and relations. They converted the relation extraction task into a table filling task. Zheng et al. (2017) took a step further and converted this task into a tagging task. They assigned a semantic tag to each word in the sentence and collected triplets according to the tag information. Bekoulis et al. (2018b,a) model the relation extraction task as a multi-he"
D19-1035,P16-1105,0,0.55872,"g (NLP). RE can be used in information extraction (Wu and Weld, 2010), question answering (Yih et al., 2015; Dai et al., 2016) and other NLP tasks. Most existing works assumed that a sentence only contains one relational facts (a relational fact, or a triplet, contains a relation and two entities). But in fact, a sentence often contains multiple relational facts (Zeng et al., 2018b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then extracted features for each entity pair and predicted their relation. They trained the entity recognition model and relation prediction model together instead of separately. Therefore, we call them PseudoPipeline methods. 2) TableFilling genre, including Miwa and Sasaki (2014); Gupta et al. (2016) and Zhang et al. (2017). They maintained a entity-relation table and predicted a semantic tag (either entity tags or relation tags) for each cell in the table. 367 Proceedings of the 2019 Conference on Empirical Methods"
D19-1035,P11-1056,0,0.436471,"tion automatically. In the following, dos Santos et al. (2015); Xu et al. (2015a) also applied CNN to extract relation. Xu et al. (2015b) utilized shortest dependency path between two entities with a LSTM (Hochreiter and Schmidhuber, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entity pair is given beforehand and a sentence only contains two entities. To extract both entities and relation from sentence, early works like Zelenko et al. (2003); Chan and Roth (2011) adopted pipeline methods. However, such pipeline methods neglect the relevance between entities and relation. Latter works focused on joint models that extract entities and relation jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Bansal (2016) relied on NLP tools to do feature engineering, which suffered from the error propagation problem. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) applied neural networks to jointly extract entities and relations. They converted the relation extraction task into a table filling task. Zheng et al. (2017) took a step further and con"
D19-1035,D14-1200,0,0.666682,"facts (Zeng et al., 2018b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then extracted features for each entity pair and predicted their relation. They trained the entity recognition model and relation prediction model together instead of separately. Therefore, we call them PseudoPipeline methods. 2) TableFilling genre, including Miwa and Sasaki (2014); Gupta et al. (2016) and Zhang et al. (2017). They maintained a entity-relation table and predicted a semantic tag (either entity tags or relation tags) for each cell in the table. 367 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 367–377, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Sentence Relational Facts lem in the multiple relation extraction task. In our knowledge, this problem has never been addressed before. Cubanelle is i"
D19-1035,P16-1076,0,0.0173815,"xtraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the reinforcement learning into a sequence-to-sequence model. The proposed model could generate relational facts freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed method. 1 Introduction Relation extraction (RE) is a core task in natural language processing (NLP). RE can be used in information extraction (Wu and Weld, 2010), question answering (Yih et al., 2015; Dai et al., 2016) and other NLP tasks. Most existing works assumed that a sentence only contains one relational facts (a relational fact, or a triplet, contains a relation and two entities). But in fact, a sentence often contains multiple relational facts (Zeng et al., 2018b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then extracted features for"
D19-1035,D15-1001,0,0.0134825,"neural model is also a sequence-tosequence model with copy mechanism. It reads in a raw sentence and generates triplets one by one. Instead of training the model with NLL loss, we regard the triplets generation process as a RL process and optimize the model with REINFORCE (Williams, 1992) algorithm. Therefore, we don’t have to determine the triplets order of each sentence beforehand, we let the model generate triplets freely. We show the RL process in Figure 2. RL has attracted lot of attention recently. It has been successfully applied in many games (Mnih et al., 2015; Silver et al., 2016). Narasimhan et al. (2015); He et al. (2016) applied RL on text based games. Narasimhan et al. (2016) employed deep Q-network to optimize a reward function that reflects the extraction accuracy while penalizing extra effort. Li et al. (2016) applied policy gradient method to model future reward in chatbot dialogue. They designed a reward to promote three conversational properties: informativity, coherence and ease of answering. Su et al. (2016) using on-line activate reward learning for policy optimization in spoken dialogue systems because the user feedback is often unreliable and costly to collect. Yu et al. (2017) a"
D19-1035,D16-1261,0,0.0378246,"eads in a raw sentence and generates triplets one by one. Instead of training the model with NLL loss, we regard the triplets generation process as a RL process and optimize the model with REINFORCE (Williams, 1992) algorithm. Therefore, we don’t have to determine the triplets order of each sentence beforehand, we let the model generate triplets freely. We show the RL process in Figure 2. RL has attracted lot of attention recently. It has been successfully applied in many games (Mnih et al., 2015; Silver et al., 2016). Narasimhan et al. (2015); He et al. (2016) applied RL on text based games. Narasimhan et al. (2016) employed deep Q-network to optimize a reward function that reflects the extraction accuracy while penalizing extra effort. Li et al. (2016) applied policy gradient method to model future reward in chatbot dialogue. They designed a reward to promote three conversational properties: informativity, coherence and ease of answering. Su et al. (2016) using on-line activate reward learning for policy optimization in spoken dialogue systems because the user feedback is often unreliable and costly to collect. Yu et al. (2017) applied RL method to overcome the limitations that the Generative Adversaria"
D19-1035,P17-1017,0,0.138749,"base with New York Times news articles. Like Zheng et al. (2017); Zeng et al. (2018b) do, we ignore the noise in this dataset and use it as a supervised dataset. We use the pre-processed dataset used in Zeng et al. (2018b), which contains 5000 sentences in the test set and 5000 sentences in the validation set and 56195 sentences in the train set. In the train set, there are 36868 sentences that contain one triplet, 19327 sentences that contain multiple triplets. In the test set, the sentence number are 3244 and 1756, respectively. There are 24 relations in total. WebNLG dataset is proposed by Gardent et al. (2017). This data set is originally created for Natural Language Generation (NLG) task. Given a Training The model can be trained with either supervised learning loss or reinforcement learning loss. However, the supervised learning forces the model to 2 How to determine the reward in RL is difficult. We tried several different reward assignments but only this one works. 371 both NYT and WebNLG dataset. We show the results of different extraction order of different models with LSTM cell in Table 1. The results of models with GRU cell are shown in Appendix B. We box the best results of a model and the"
D19-1035,P18-1199,0,0.0436741,"Missing"
D19-1035,P16-1154,0,0.233482,"a entity-relation table. 4) MultiHeadSelection genre, including Bekoulis et al. (2018a) and Bekoulis et al. (2018b). They first recognized the entities, then they formulated the relation extraction task as a multi-head selection problem. For each entity, they calculated the score between it and every other entities for a given relation. The combination of the entity pair and relation with the score exceeding a threshold will be kept as a triplet. 5) Generative genre, including Zeng et al. (2018b). They directly generate triplets one by one by a sequence-to-sequence model with copy mechanism (Gu et al., 2016; Vinyals et al., 2015). To generate a triplet, they first generated the relation, then they copy the first entity and the second entity from the source sentence. However, none of them have considered the extraction order of multiple triplets in a sentence. Given a sentence, the PseudoPipeline methods extract relations of different entity pairs separately. Although they jointly training the entity model and relation model, they ignore the influence between triplets actually. The TableFilling, NovelTagging and MultiHeadSelection methods extract the triplets in the word order of this sentence. T"
D19-1035,C16-1239,0,0.66407,"b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then extracted features for each entity pair and predicted their relation. They trained the entity recognition model and relation prediction model together instead of separately. Therefore, we call them PseudoPipeline methods. 2) TableFilling genre, including Miwa and Sasaki (2014); Gupta et al. (2016) and Zhang et al. (2017). They maintained a entity-relation table and predicted a semantic tag (either entity tags or relation tags) for each cell in the table. 367 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 367–377, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Sentence Relational Facts lem in the multiple relation extraction task. In our knowledge, this problem has never been addressed before. Cubanelle is in Arros negre, a dish"
D19-1035,P15-1061,0,0.012294,"the model to generate triplets in the order of the ground truth, reinforcement learning allows the model generate triplets freely to achieve higher reward. The main contributions of this work are: 2 Related Work Given a sentence with two annotated entities (an entity pair), the relation classification task aims to identify the predefined relation between these two entities. Zeng et al. (2014) was among the first to apply neural networks in relation classification task. They adopted the Convolutional Neural Network (CNN) to learn the sentence representation automatically. In the following, dos Santos et al. (2015); Xu et al. (2015a) also applied CNN to extract relation. Xu et al. (2015b) utilized shortest dependency path between two entities with a LSTM (Hochreiter and Schmidhuber, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entity pair is given beforehand and a sentence only contains two entities. To extract both entities and relation from sentence, early works like Zelenko et al. (2003); Chan and Roth (2011) adopted pipeline methods. However, such p"
D19-1035,P16-1153,0,0.0158077,"quence-tosequence model with copy mechanism. It reads in a raw sentence and generates triplets one by one. Instead of training the model with NLL loss, we regard the triplets generation process as a RL process and optimize the model with REINFORCE (Williams, 1992) algorithm. Therefore, we don’t have to determine the triplets order of each sentence beforehand, we let the model generate triplets freely. We show the RL process in Figure 2. RL has attracted lot of attention recently. It has been successfully applied in many games (Mnih et al., 2015; Silver et al., 2016). Narasimhan et al. (2015); He et al. (2016) applied RL on text based games. Narasimhan et al. (2016) employed deep Q-network to optimize a reward function that reflects the extraction accuracy while penalizing extra effort. Li et al. (2016) applied policy gradient method to model future reward in chatbot dialogue. They designed a reward to promote three conversational properties: informativity, coherence and ease of answering. Su et al. (2016) using on-line activate reward learning for policy optimization in spoken dialogue systems because the user feedback is often unreliable and costly to collect. Yu et al. (2017) applied RL method t"
D19-1035,P18-1047,1,0.476504,"ts based on the tags. Their tags include both entity and relation information. Therefore, they don’t need to maintain a entity-relation table. 4) MultiHeadSelection genre, including Bekoulis et al. (2018a) and Bekoulis et al. (2018b). They first recognized the entities, then they formulated the relation extraction task as a multi-head selection problem. For each entity, they calculated the score between it and every other entities for a given relation. The combination of the entity pair and relation with the score exceeding a threshold will be kept as a triplet. 5) Generative genre, including Zeng et al. (2018b). They directly generate triplets one by one by a sequence-to-sequence model with copy mechanism (Gu et al., 2016; Vinyals et al., 2015). To generate a triplet, they first generated the relation, then they copy the first entity and the second entity from the source sentence. However, none of them have considered the extraction order of multiple triplets in a sentence. Given a sentence, the PseudoPipeline methods extract relations of different entity pairs separately. Although they jointly training the entity model and relation model, they ignore the influence between triplets actually. The T"
D19-1035,P16-1230,0,0.0462455,"We show the RL process in Figure 2. RL has attracted lot of attention recently. It has been successfully applied in many games (Mnih et al., 2015; Silver et al., 2016). Narasimhan et al. (2015); He et al. (2016) applied RL on text based games. Narasimhan et al. (2016) employed deep Q-network to optimize a reward function that reflects the extraction accuracy while penalizing extra effort. Li et al. (2016) applied policy gradient method to model future reward in chatbot dialogue. They designed a reward to promote three conversational properties: informativity, coherence and ease of answering. Su et al. (2016) using on-line activate reward learning for policy optimization in spoken dialogue systems because the user feedback is often unreliable and costly to collect. Yu et al. (2017) applied RL method to overcome the limitations that the Generative Adversarial Net (GAN) in generating sequences of discrete tokens. Our work is related to Li et al. (2016); Yu et al. (2017) since we also apply RL to generate better sequences. 3.1 Sequence-to-Sequence Model with Copy Mechanism The sequence-to-sequence model with copy mechanism is a kind of CopyNet (Gu et al., 2016) or PointerNetwork (Vinyals et al., 2015"
D19-1035,D17-1182,0,0.145444,"Missing"
D19-1035,D18-1249,0,0.421209,"in information extraction (Wu and Weld, 2010), question answering (Yih et al., 2015; Dai et al., 2016) and other NLP tasks. Most existing works assumed that a sentence only contains one relational facts (a relational fact, or a triplet, contains a relation and two entities). But in fact, a sentence often contains multiple relational facts (Zeng et al., 2018b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then extracted features for each entity pair and predicted their relation. They trained the entity recognition model and relation prediction model together instead of separately. Therefore, we call them PseudoPipeline methods. 2) TableFilling genre, including Miwa and Sasaki (2014); Gupta et al. (2016) and Zhang et al. (2017). They maintained a entity-relation table and predicted a semantic tag (either entity tags or relation tags) for each cell in the table. 367 Proceedings of the 2019 Conference on Empirical Methods in Natural Languag"
D19-1035,P17-1113,0,0.673281,"ement Learning 1 Xiangrong Zeng1,2,3 , Shizhu He1,2 , Daojian Zeng4 , Kang Liu1,2 , Jun Zhao1,2 NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 2 University of Chinese Academy of Sciences, Beijing, 100049, China 3 Unisound AI Technology Co.,Ltd, Beijing, 100000, China 4 Changsha University of Science & Technology, Changsha, 410114, China {xiangrong.zeng, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn zengdj@csust.edu.cn Abstract According to the predicted tags, they can recognize the entities and the relation between each entity pair. 3) NovelTagging genre, including Zheng et al. (2017). This method can be seen as a development of TableFilling method. They assigned a pre-defined semantic tag to each word of the sentence and collected triplets based on the tags. Their tags include both entity and relation information. Therefore, they don’t need to maintain a entity-relation table. 4) MultiHeadSelection genre, including Bekoulis et al. (2018a) and Bekoulis et al. (2018b). They first recognized the entities, then they formulated the relation extraction task as a multi-head selection problem. For each entity, they calculated the score between it and every other entities for a gi"
D19-1035,P16-2034,0,0.0298607,"ith two annotated entities (an entity pair), the relation classification task aims to identify the predefined relation between these two entities. Zeng et al. (2014) was among the first to apply neural networks in relation classification task. They adopted the Convolutional Neural Network (CNN) to learn the sentence representation automatically. In the following, dos Santos et al. (2015); Xu et al. (2015a) also applied CNN to extract relation. Xu et al. (2015b) utilized shortest dependency path between two entities with a LSTM (Hochreiter and Schmidhuber, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entity pair is given beforehand and a sentence only contains two entities. To extract both entities and relation from sentence, early works like Zelenko et al. (2003); Chan and Roth (2011) adopted pipeline methods. However, such pipeline methods neglect the relevance between entities and relation. Latter works focused on joint models that extract entities and relation jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Bansal (2016) relied on NLP tool"
D19-1035,P15-1128,0,0.0178025,"n’t consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the reinforcement learning into a sequence-to-sequence model. The proposed model could generate relational facts freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed method. 1 Introduction Relation extraction (RE) is a core task in natural language processing (NLP). RE can be used in information extraction (Wu and Weld, 2010), question answering (Yih et al., 2015; Dai et al., 2016) and other NLP tasks. Most existing works assumed that a sentence only contains one relational facts (a relational fact, or a triplet, contains a relation and two entities). But in fact, a sentence often contains multiple relational facts (Zeng et al., 2018b). The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works on multiple relation extraction task can be divided into five genres. 1) PseudoPipeline genre, including Miwa and Bansal (2016); Sun et al. (2018). They first recognized all the entities of the sentence, then ext"
D19-1035,C10-2160,0,0.51498,"iter and Schmidhuber, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entity pair is given beforehand and a sentence only contains two entities. To extract both entities and relation from sentence, early works like Zelenko et al. (2003); Chan and Roth (2011) adopted pipeline methods. However, such pipeline methods neglect the relevance between entities and relation. Latter works focused on joint models that extract entities and relation jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Bansal (2016) relied on NLP tools to do feature engineering, which suffered from the error propagation problem. Miwa and Sasaki (2014); Gupta et al. (2016); Zhang et al. (2017) applied neural networks to jointly extract entities and relations. They converted the relation extraction task into a table filling task. Zheng et al. (2017) took a step further and converted this task into a tagging task. They assigned a semantic tag to each word in the sentence and collected triplets according to the tag information. Bekoulis et al. (2018b,a) model the relation extraction"
D19-1035,C14-1220,1,0.91215,"d triplets. The RL reward is related to the generated triplets. In general, the more triplets are correctly generated, the higher the reward. Unlike supervised learning with negative log likelihood (NLL) loss, which forces the model to generate triplets in the order of the ground truth, reinforcement learning allows the model generate triplets freely to achieve higher reward. The main contributions of this work are: 2 Related Work Given a sentence with two annotated entities (an entity pair), the relation classification task aims to identify the predefined relation between these two entities. Zeng et al. (2014) was among the first to apply neural networks in relation classification task. They adopted the Convolutional Neural Network (CNN) to learn the sentence representation automatically. In the following, dos Santos et al. (2015); Xu et al. (2015a) also applied CNN to extract relation. Xu et al. (2015b) utilized shortest dependency path between two entities with a LSTM (Hochreiter and Schmidhuber, 1997) based recurrent neural network. Zhou et al. (2016) applied attention mechanism to learn different weights for each word and used LSTM to represent sentence. These methods all assumed that the entit"
D19-1068,W06-0901,0,0.822084,"nt languages, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual cotraining. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario. 1 Introduction Event detection (ED) is a crucial natural language processing problem that aims to identify event triggers in texts (Ahn, 2006; Nguyen and Grishman, 2015). For example, in the sentence: “A man died when a tank fired on the hotel”, ED requires a system to identify two event triggers, died and fired, along with their types Die and Attack. Generally, training an ED system requires to obtain a considerably large amount of labeled data. However, owing to the complexity and high costs of annotation, existing event resources are scarce and unbalanced across languages (Hsi et al., 2016), which prevents us from building an ED system in languages with insufficient training data. Cross-lingual ED (Ji, 2009; Chen and ∗ Equal con"
D19-1068,P15-1017,1,0.904732,"dent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-proje"
D19-1068,W09-2209,0,0.0339393,"ure-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-projection method with multilingual feature extraction for bilingual ED. Nevertheless, the heavily dependency on parallel resources often limits the applicability of these methods. Our study also relates to cross-lingual studies in other applications (Guo et al., 2015; Ni et al., 2017; Mayhew et al., 2017; Xie et al., 2018; Lample"
D19-1068,P07-2045,0,0.0062674,"Missing"
D19-1068,P16-2011,0,0.0380258,"GCNs with a self-attention network (by comparing CL Trans GCN with CL Trans GCN Self). tent. 2) Retrieving more translation candidates could consistently improve Recall. But when too many candidates (e.g., 5) are added, the Precision drops, which harms the overall F1 measure. Exploring the Syntactic Order Event Detector. We compare our syntactic order event detector (CL Trans GCN) with several event detectors, including 1) CL Trans MLP, which employs a feed-forward network as event detector; 2) CL Trans CNN, which uses CNNs as the event detector; 3) CL Trans Hbrid, which use a hybrid network (Feng et al., 2016) for event detection. We also compared our model with several variants including 4) CL Trans Self., which replaces the GCNs with a self-attention network, and 5) CL Trans GCN Self, which combines GCNs with a self-attention network. We train these models on the same translated English data. Table 3 shows the results. From the results, 1) CL Trans MLP, CL Trans CNN, and CL Trans Hbrid behave poorly, as expected. The reason might be that these models usually employ order-sensitive structures (e.g., CNNs) for ED, which would suffer the word order inconsistency problem when trained on the translate"
D19-1068,P15-1119,0,0.0335088,"we propose a new simple but effective method for cross-lingual ED, which can overcome the data scarcity problem in annotationpoor languages by jointly training with resources in other languages. Compared with previous methods, our approach demonstrates a minimal dependency on parallel resources, which may fit with language pairs that do not have large bitexts. To achieve cross-lingual transfer, two challenges exist: 1) how to build lexical mappings between different languages, and 2) how to handle the word order difference problem (Xie et al., 2018). For the first challenge, previous studies (Guo et al., 2015; Ni et al., 2017; Mayhew et al., 2017; Xie et al., 2018; Lample et al., 2018) have investigated embedding projection-based method in cross-lingual applications and achieved promising results. For example, (Xie et al., 2018) proposed a novel “cheap translation” based method which has greatly advanced the performance in zero-shot named entity recognition (NER). However, these methods may not directly fit with crosslingual ED, as the lexical mapping in ED is usually context-dependent but not deterministic as in other tasks. Consider the following English-to-Chinese lexical mapping examples. To p"
D19-1068,P14-1038,0,0.0385122,"es, it might not fit with languages which lack syntactic parsers. In the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2"
D19-1068,P11-1113,0,0.230944,"y of syntax trees of training examples, it might not fit with languages which lack syntactic parsers. In the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese E"
D19-1068,P13-1008,0,0.236192,"f training examples, it might not fit with languages which lack syntactic parsers. In the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2"
D19-1068,P10-1081,0,0.222557,"icated on the availability of syntax trees of training examples, it might not fit with languages which lack syntactic parsers. In the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for"
D19-1068,C16-1114,0,0.0894339,"n-poor scenario. 1 Introduction Event detection (ED) is a crucial natural language processing problem that aims to identify event triggers in texts (Ahn, 2006; Nguyen and Grishman, 2015). For example, in the sentence: “A man died when a tank fired on the hotel”, ED requires a system to identify two event triggers, died and fired, along with their types Die and Attack. Generally, training an ED system requires to obtain a considerably large amount of labeled data. However, owing to the complexity and high costs of annotation, existing event resources are scarce and unbalanced across languages (Hsi et al., 2016), which prevents us from building an ED system in languages with insufficient training data. Cross-lingual ED (Ji, 2009; Chen and ∗ Equal contribution. 738 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 738–748, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics E1: A man died when a tank fired on the hotel event), and “the house caught fire” (which evokes an NA event) should be translated as different Chinese words “开 火(open fire)” and “着"
D19-1068,P18-1145,0,0.0219583,"Missing"
D19-1068,W09-1704,0,0.170583,"t triggers in texts (Ahn, 2006; Nguyen and Grishman, 2015). For example, in the sentence: “A man died when a tank fired on the hotel”, ED requires a system to identify two event triggers, died and fired, along with their types Die and Attack. Generally, training an ED system requires to obtain a considerably large amount of labeled data. However, owing to the complexity and high costs of annotation, existing event resources are scarce and unbalanced across languages (Hsi et al., 2016), which prevents us from building an ED system in languages with insufficient training data. Cross-lingual ED (Ji, 2009; Chen and ∗ Equal contribution. 738 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 738–748, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics E1: A man died when a tank fired on the hotel event), and “the house caught fire” (which evokes an NA event) should be translated as different Chinese words “开 火(open fire)” and “着 火(be on fire)” respectively. But in previous lexical mapping methods, the “fired” is always having the same transferre"
D19-1068,L16-1147,0,0.0587121,"Missing"
D19-1068,P08-1030,0,0.397709,"as our approach is predicated on the availability of syntax trees of training examples, it might not fit with languages which lack syntactic parsers. In the future, we plan to investigate more language-independent patterns in crosslingual transfer to circumvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to o"
D19-1068,D18-1127,0,0.114004,"y. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-projection method with multilingual feature extraction for bilingual ED"
D19-1068,P17-4012,0,0.0303764,"Missing"
D19-1068,P14-5010,0,0.0057334,"Missing"
D19-1068,D17-1269,0,0.0497465,"Missing"
D19-1068,D18-1034,0,0.191686,"tly limits the applicability of these methods. In this paper, we propose a new simple but effective method for cross-lingual ED, which can overcome the data scarcity problem in annotationpoor languages by jointly training with resources in other languages. Compared with previous methods, our approach demonstrates a minimal dependency on parallel resources, which may fit with language pairs that do not have large bitexts. To achieve cross-lingual transfer, two challenges exist: 1) how to build lexical mappings between different languages, and 2) how to handle the word order difference problem (Xie et al., 2018). For the first challenge, previous studies (Guo et al., 2015; Ni et al., 2017; Mayhew et al., 2017; Xie et al., 2018; Lample et al., 2018) have investigated embedding projection-based method in cross-lingual applications and achieved promising results. For example, (Xie et al., 2018) proposed a novel “cheap translation” based method which has greatly advanced the performance in zero-shot named entity recognition (NER). However, these methods may not directly fit with crosslingual ED, as the lexical mapping in ED is usually context-dependent but not deterministic as in other tasks. Consider th"
D19-1068,N15-1104,0,0.0617684,"Missing"
D19-1068,C00-2137,0,0.0627521,"Missing"
D19-1068,P14-2136,0,0.0173877,"et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-projection method with multilingual feature extraction for bilingual ED. Nevertheless, the heavily dependency on parallel resources often limits the applicability of these methods. Our study also relates to cross-lingual studies in other applications (Guo et al., 2015; Ni et al., 2017; Mayhew et al., 2017; Xie et al., 2018; Lample et al., 2018). These approaches adopted embedding projection based method to achieve cross-lingual transfer and achieved prosing re"
D19-1068,N16-1034,0,0.209121,"umvent this dependency. Related Work Event detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-projection method with multilingual feature extractio"
D19-1068,P15-2060,0,0.494094,"s, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual cotraining. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario. 1 Introduction Event detection (ED) is a crucial natural language processing problem that aims to identify event triggers in texts (Ahn, 2006; Nguyen and Grishman, 2015). For example, in the sentence: “A man died when a tank fired on the hotel”, ED requires a system to identify two event triggers, died and fired, along with their types Die and Attack. Generally, training an ED system requires to obtain a considerably large amount of labeled data. However, owing to the complexity and high costs of annotation, existing event resources are scarce and unbalanced across languages (Hsi et al., 2016), which prevents us from building an ED system in languages with insufficient training data. Cross-lingual ED (Ji, 2009; Chen and ∗ Equal contribution. 738 Proceedings o"
D19-1068,P17-1135,0,0.0415959,"Missing"
D19-1068,D18-1122,0,0.0240603,"nt detection (ED) is a hot topic in natural language processing, which has attracted extensive attention in the past few years. Traditionally, the study of ED has focused on monolingual training. The proposed models can be divided into feature-based methods which employ fine-grained features (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Li and Ji, 2014), and deep learning-based methods which employ neural networks to automatically learn features for the task (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018b; Orr et al., 2018; Liu et al., 2019). Usually, their performance is limited by the amount of labeled data in a specific language. Cross-lingual ED attempts transfer knowledge between different languages to boost performance. To name a few, (Chen and Ji, 2009) used an English detector to label events on parallel documents to obtain additional data for boosting Chinese ED; (Zhu et al., 2014; Liu et al., 2018a) used machine translation to obtain additional labeled data for training; (Hsi et al., 2016) combined the embedding-projection method with multilingual feature extraction for bilingual ED. Nevertheless, the"
D19-1396,N19-1078,0,0.0251829,"f-the-art results in various popular Chinese NER datasets, and our model achieves a 6-15x speedup over the existing SOTA model. 2 3 Related Work NER. There is rich literature on NER. This includes statistic methods, such as SVM (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng and Dredze, 2015; He and Sun, 2017a; Zhu and Wang, 2019). Peng and Dredze (2016), Cao et al. (2018) and Wu et al. (2019) propose joint models to train NER together with CWS. Our work is inspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph co"
D19-1396,C18-1139,0,0.029162,"al NLP tools. • We achieve the state-of-the-art results in various popular Chinese NER datasets, and our model achieves a 6-15x speedup over the existing SOTA model. 2 3 Related Work NER. There is rich literature on NER. This includes statistic methods, such as SVM (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng and Dredze, 2015; He and Sun, 2017a; Zhu and Wang, 2019). Peng and Dredze (2016), Cao et al. (2018) and Wu et al. (2019) propose joint models to train NER together with CWS. Our work is inspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks"
D19-1396,D17-1209,0,0.0238957,"nspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of t"
D19-1396,A97-1029,0,0.0963717,"edge directly and efficiently for Chinese NER. • To solve the challenges of integrating selfmatched lexical words and the nearest contextual lexical words, we propose three wordcharacter interactive graphs. These interactive graphs can capture different lexical knowledge and are built without external NLP tools. • We achieve the state-of-the-art results in various popular Chinese NER datasets, and our model achieves a 6-15x speedup over the existing SOTA model. 2 3 Related Work NER. There is rich literature on NER. This includes statistic methods, such as SVM (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng"
D19-1396,H05-1091,0,0.0950913,"Beijing airport.) Matched Lexical Words: 希尔(Hill), 希尔顿(Hilton), 离开(leave), 北京(Beijing), 北京机场(Beijing Airport) Figure 1: An example sentence integrating the nearest contextual lexical words (red line) and self-matched lexical words (green line) Introduction Named entity recognition (NER) aims to locate and classify certain occurrences of words or expressions in unstructured text into predefined semantic categories such as the person names, locations, organizations, etc. NER is an essential pre-processing step for many natural language processing (NLP) applications, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and"
D19-1396,D18-1017,1,0.929315,"me as the boundaries of the named entity “ ¬::” (Beijing airport). Therefore, making full use of word information would help to improve the Chinese NER performance. There are three main ways to incorporate word information in NER. The first one is the pipeline method. The way of pipeline method is to apply Chinese Word Segmentation (CWS) first, and then to use a word-based NER model. However, the pipeline method suffers from error propagation, since the error of CWS may affect the performance of NER. The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016; Cao et al., 2018; Wu et al., 2019). However, the joint models must rely on CWS annotation datasets, which are costly and are annotated under many diverse segmentation criteria (Chen 3830 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3830–3840, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). The third one is to leverage an automatically constructed lexicon, which is pre-trained on large automatically segmented texts. Lexical knowledge in"
D19-1396,N19-1032,0,0.0310958,"r of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed graphs, which can avoid error propagation problem naturally. To our best knowledge, we are the"
D19-1396,W17-7607,0,0.0685853,"Missing"
D19-1396,N13-1006,0,0.429616,"Missing"
D19-1396,W06-0130,0,0.18067,"Missing"
D19-1396,P17-1110,0,0.0610368,"Missing"
D19-1396,P15-1017,1,0.867339,"ill), 希尔顿(Hilton), 离开(leave), 北京(Beijing), 北京机场(Beijing Airport) Figure 1: An example sentence integrating the nearest contextual lexical words (red line) and self-matched lexical words (green line) Introduction Named entity recognition (NER) aims to locate and classify certain occurrences of words or expressions in unstructured text into predefined semantic categories such as the person names, locations, organizations, etc. NER is an essential pre-processing step for many natural language processing (NLP) applications, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et a"
D19-1396,Q16-1026,0,0.528087,"ords (red line) and self-matched lexical words (green line) Introduction Named entity recognition (NER) aims to locate and classify certain occurrences of words or expressions in unstructured text into predefined semantic categories such as the person names, locations, organizations, etc. NER is an essential pre-processing step for many natural language processing (NLP) applications, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014), although such methods could result in the disregard of word information. However, word information is very useful in Chinese NER, be"
D19-1396,E17-2113,0,0.170397,"et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng and Dredze, 2015; He and Sun, 2017a; Zhu and Wang, 2019). Peng and Dredze (2016), Cao et al. (2018) and Wu et al. (2019) propose joint models to train NER together with CWS. Our work is inspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attent"
D19-1396,I08-4022,0,0.272635,"2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014), although such methods could result in the disregard of word information. However, word information is very useful in Chinese NER, because word boundaries are usually the same as named entity boundaries. For example, as shown in Figure 1, the boundaries of the word “ ¬::” (Beijing airport) are the same as the boundaries of the named entity “ ¬::” (Beijing airport). Therefore, making full use of word information would help to improve the Chinese NER performance. There are three main ways to incorporate word information in NER. The first one is the pipeline m"
D19-1396,C02-1054,0,0.102519,"etwork to integrate lexical knowledge directly and efficiently for Chinese NER. • To solve the challenges of integrating selfmatched lexical words and the nearest contextual lexical words, we propose three wordcharacter interactive graphs. These interactive graphs can capture different lexical knowledge and are built without external NLP tools. • We achieve the state-of-the-art results in various popular Chinese NER datasets, and our model achieves a 6-15x speedup over the existing SOTA model. 2 3 Related Work NER. There is rich literature on NER. This includes statistic methods, such as SVM (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for"
D19-1396,N19-1079,0,0.0360056,"chieve the state-of-the-art results in various popular Chinese NER datasets, and our model achieves a 6-15x speedup over the existing SOTA model. 2 3 Related Work NER. There is rich literature on NER. This includes statistic methods, such as SVM (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng and Dredze, 2015; He and Sun, 2017a; Zhu and Wang, 2019). Peng and Dredze (2016), Cao et al. (2018) and Wu et al. (2019) propose joint models to train NER together with CWS. Our work is inspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a numb"
D19-1396,W06-0115,0,0.782574,"Missing"
D19-1396,li-etal-2014-comparison,0,0.426311,"Missing"
D19-1396,P18-2023,0,0.0763397,"Missing"
D19-1396,L16-1138,0,0.253291,"Missing"
D19-1396,P16-1101,0,0.501674,"ontextual lexical words (red line) and self-matched lexical words (green line) Introduction Named entity recognition (NER) aims to locate and classify certain occurrences of words or expressions in unstructured text into predefined semantic categories such as the person names, locations, organizations, etc. NER is an essential pre-processing step for many natural language processing (NLP) applications, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014), although such methods could result in the disregard of word information. However, word information is very u"
D19-1396,D17-1159,0,0.0247942,"(Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed"
D19-1396,N16-1030,0,0.443467,"grating the nearest contextual lexical words (red line) and self-matched lexical words (green line) Introduction Named entity recognition (NER) aims to locate and classify certain occurrences of words or expressions in unstructured text into predefined semantic categories such as the person names, locations, organizations, etc. NER is an essential pre-processing step for many natural language processing (NLP) applications, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015), question answering (Moll´a et al., 2006) etc. In English NER, LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) leveraging word-level representations and character-level representations achieve the stateof-the-art results. In this paper, we focus on Chinese NER. Compared with English, Chinese has no obvious word boundaries. Since without word boundaries information, it is intuitive to use character information 1 The code is available at https://github.com/ DianboWork/Graph4CNER only for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014), although such methods could result in the disregard of word information. However, word in"
D19-1396,N19-1221,0,0.0315652,"ks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed graphs, which can avoid error propagation problem naturally. To our best know"
D19-1396,U06-1009,0,0.112953,"Missing"
D19-1396,D15-1064,0,0.325118,"1997) and CRF (Lafferty et al., 2001), suffering from feature engineering. There are also a number of recent neural network approaches applied to NER, such as (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018; Akbik et al., 2018; Jie et al., 2019; Akbik et al., 2019). Compared with English, Chinese is not featured with obvious word boundaries, but it is important to leverage word boundaries and semantic information in Chinese NER. Many works use word segmentation information as extra features for Chinese NER, such as (Peng and Dredze, 2015; He and Sun, 2017a; Zhu and Wang, 2019). Peng and Dredze (2016), Cao et al. (2018) and Wu et al. (2019) propose joint models to train NER together with CWS. Our work is inspired by lattice LSTM (Zhang and Yang, 2018), which can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging"
D19-1396,P16-2025,0,0.4347,"ing airport) are the same as the boundaries of the named entity “ ¬::” (Beijing airport). Therefore, making full use of word information would help to improve the Chinese NER performance. There are three main ways to incorporate word information in NER. The first one is the pipeline method. The way of pipeline method is to apply Chinese Word Segmentation (CWS) first, and then to use a word-based NER model. However, the pipeline method suffers from error propagation, since the error of CWS may affect the performance of NER. The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016; Cao et al., 2018; Wu et al., 2019). However, the joint models must rely on CWS annotation datasets, which are costly and are annotated under many diverse segmentation criteria (Chen 3830 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3830–3840, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). The third one is to leverage an automatically constructed lexicon, which is pre-trained on large automatically segmented texts. Le"
D19-1396,D18-1032,0,0.0307567,"onvolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed graphs, which can avoid error propagation problem natura"
D19-1396,N19-1306,0,0.0171484,"convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed graphs, which can avoid error propagation problem naturally. To our best knowledge, we are the first to introduce GA"
D19-1396,W06-0126,0,0.583093,"Missing"
D19-1396,P18-1144,0,0.402665,"3840, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). The third one is to leverage an automatically constructed lexicon, which is pre-trained on large automatically segmented texts. Lexical knowledge includes boundaries and semantic information. Boundaries information is provided by the lexicon word itself, and semantic information is provided by pre-trained word embeddings (Bengio et al., 2003; Mikolov et al., 2013). Compared with joint methods, a lexicon is easy to obtain and additional annotation CWS datasets are not required. Recently, Zhang and Yang (2018) propose a lattice LSTM to integrate lexical knowledge in NER. However, integrating lexical knowledge into sentences still faces two challenges. The first challenge is to integrate self-matched lexical words. A self-matched lexical word of a character is the lexical word that contains this character. For instance, “ ¬: ::” (Beijing Airport) and “: : :” (Airport) are the self-matched words of the character “: :” (airplane). “» ” (leave) is not the self-matched word of the character “:” (airplane), since “ :” (airplane) is not contained in the word “ » ” (leave). The lexical knowledge of self-ma"
D19-1396,D18-1244,0,0.0217361,"can integrate lexicon in NER. Graph convolutional networks. There are a number of recent graph convolutional network (GCN) architectures (Kipf and Welling, 2017; Hamilton et al., 2017; Veliˇckovi´c et al., 2018; Qu et al., 2019) for learning over graphs. Our work is closely related to the graph attention networks (GAT), introduced by Veliˇckovi´c et al. (2018), leveraging masked self-attention layers to assign different importance to neighbouring nodes. In recent years, there is more and more literature about the application of GCN in NLP (Bastings et al., 2017; Marcheggiani and Titov, 2017; Zhang et al., 2018; Yao et al., 2019; Wang et al., 2018; Mishra et al., 2019; Cao et al., 2019; Zhang et al., 2019). Cetoli et al. (2017) use GCN to investigate the role of the dependency tree in English named entity recognition. However, most of the works (Bastings et al., 2017; Marcheggiani and Titov, 2017; Cetoli et al., 2017; Zhang et al., 2018) heavily rely on the dependency tree to construct a single graph, which suffer from error propagation. To capture different semantic and boundaries information, we propose a Collaborative Graph Network consisting of three automatically constructed graphs, which can a"
D19-1396,N19-1342,0,0.805485,"Missing"
D19-1602,D18-1454,0,0.335379,"on the ReCoRD dataset. 1 Introduction Machine reading comprehension (MRC) is an important subtask in natural language processing, which requires a system to read a given passage and answer questions about it. The ability of utilizing external knowledge is of great significance in an MRC system (Rajpurkar et al., 2016; Trischler et al., 2017). Latest large-scale datasets, e.g. ReCoRD (Zhang et al., 2018) specify that external knowledge is required to answer questions. Many previous studies have introduced external knowledge in machine comprehension (Weissenborn, 2017; Mihaylov and Frank, 2018; Bauer et al., 2018). They often acquire external knowledge from structural knowledge graphs, such as ConceptNet (Speer et al., 2017) and Freebase (Tanon et al., 2016), in which knowledge is organized by triples like “(shortage, related to, lack)” and “(need, related to, lack)”. However, most of them fail to make full use of the structural information in the knowledge graph, and use sequence modeling methods like recurrent neural networks (RNNs) to generate the representation of knowledge, rather than based on graph structure. In this paper, we present a Structural Knowledge Graph-aware Network (SKG) model to lev"
D19-1602,N19-1240,0,0.0316409,"Missing"
D19-1602,N19-1032,0,0.0177585,"ense paths to help multi-hop reasoning. They treat retrieved knowledge triples as sequences and use sequence modeling methods to compress the representation of knowledge, which are not based on graph structure. On the contrary, we organize knowledge as sub-graphs, then update the representation of nodes on sub-graphs with graph neural network. Graph Neural Netwoks Graph neural netwoks (Kipf and Welling, 2016; Schlichtkrull et al., 2018; Veliˇckovi´c et al., 2017) have been shown successful on many Natural Language Processing (NLP) tasks (Cao et al., 2018; Zhou et al., 2018; Song et al., 2018; Cao et al., 2019). They are good at dealing with graphstructured data. In (Cao et al., 2018; Song et al., 2018; Cao et al., 2019), Graph Convolutional Networks (GCNs) have been applied in multidocumnent machine comprehension for multi-hop reasoning question answering, but they consider only the internal structure information in the MRC context without incorporating external knowledge. To the best of our knowledge, our work is the first to study graph attention networks in machine comprehension with external knowledge. 3 SKG Model The architecture of SKG is shown in Figure 1. It contains four modules: (1) Quest"
D19-1602,P18-1078,0,0.0141037,"k3 for BERT model. We employ the open-source framework OpenKE (Han et al., 2018) to obtain the embedding of entities and relations with the BILINEAR model (Yang et al., 2015). The size of embedding of entities and relations is 100. The update times L of graph attention network is set to 5. We use Adam optimizer. The learning rate uses the linear schedule to decrease from 0.00003 to 0. 1 https://sheng-z.github.io/ReCoRD-explorer/ https://github.com/pytorch/pytorch 3 https://github.com/huggingface/pytorch-pretrainedBERT 2 5898 Model QANet (Yu et al., 2018) SAN (Liu et al., 2018) DocQA w/o ELMo (Clark and Gardner, 2018) DocQA w/ ELMo (Clark and Gardner, 2018) SKG+BERT-Large(ours) EM Dev Test 35.38 36.51 38.14 39.77 36.59 38.52 44.13 45.44 70.94 72.24 F1 Dev Test 36.75 37.79 39.09 40.72 37.89 39.76 45.39 46.65 71.55 72.78 Table 1: The performance of different models on ReCoRD dataset. Model BERT-Base( 2018) SKG+BERT-Base(ours) BERT-Large( 2018) SKG+BERT-Large(ours) EM 54.03 60.26 64.28 70.94 F1 55.99 60.79 66.60 71.55 Table 2: The effectiveness of introducing external knowledge. Model MHPGM+NOIC( 2018) KG+LSTM+Bert-Base SKG+BERT-Base(ours) EM 28.36 58.01 60.26 F1 29.29 58.49 60.79 Table 3: The results of diff"
D19-1602,D18-2024,0,0.0326347,"model to the organization1 to get the results. External Knowledge We consider two knowledge sources as our external knowledge: WordNet and ConceptNet. For WordNet, we use the preprocessed data provided by Bordes et at. (2013), which contains 151,442 triples with 40,943 synsets and 18 relations. For ConceptNet, we use the preprocessed data provided by Bauer et al. (2018), which contains 2,808,998 triples with 978,672 entities and 46 relations. 4.2 Implementation Details Our model is implemented with pytorch2 , and uses the framework3 for BERT model. We employ the open-source framework OpenKE (Han et al., 2018) to obtain the embedding of entities and relations with the BILINEAR model (Yang et al., 2015). The size of embedding of entities and relations is 100. The update times L of graph attention network is set to 5. We use Adam optimizer. The learning rate uses the linear schedule to decrease from 0.00003 to 0. 1 https://sheng-z.github.io/ReCoRD-explorer/ https://github.com/pytorch/pytorch 3 https://github.com/huggingface/pytorch-pretrainedBERT 2 5898 Model QANet (Yu et al., 2018) SAN (Liu et al., 2018) DocQA w/o ELMo (Clark and Gardner, 2018) DocQA w/ ELMo (Clark and Gardner, 2018) SKG+BERT-Large("
D19-1602,P18-1157,0,0.0182249,"h pytorch2 , and uses the framework3 for BERT model. We employ the open-source framework OpenKE (Han et al., 2018) to obtain the embedding of entities and relations with the BILINEAR model (Yang et al., 2015). The size of embedding of entities and relations is 100. The update times L of graph attention network is set to 5. We use Adam optimizer. The learning rate uses the linear schedule to decrease from 0.00003 to 0. 1 https://sheng-z.github.io/ReCoRD-explorer/ https://github.com/pytorch/pytorch 3 https://github.com/huggingface/pytorch-pretrainedBERT 2 5898 Model QANet (Yu et al., 2018) SAN (Liu et al., 2018) DocQA w/o ELMo (Clark and Gardner, 2018) DocQA w/ ELMo (Clark and Gardner, 2018) SKG+BERT-Large(ours) EM Dev Test 35.38 36.51 38.14 39.77 36.59 38.52 44.13 45.44 70.94 72.24 F1 Dev Test 36.75 37.79 39.09 40.72 37.89 39.76 45.39 46.65 71.55 72.78 Table 1: The performance of different models on ReCoRD dataset. Model BERT-Base( 2018) SKG+BERT-Base(ours) BERT-Large( 2018) SKG+BERT-Large(ours) EM 54.03 60.26 64.28 70.94 F1 55.99 60.79 66.60 71.55 Table 2: The effectiveness of introducing external knowledge. Model MHPGM+NOIC( 2018) KG+LSTM+Bert-Base SKG+BERT-Base(ours) EM 28.36 58.01 60.26 F1 29.29"
D19-1602,P18-1076,0,0.152185,"ate-ofthe-art performance on the ReCoRD dataset. 1 Introduction Machine reading comprehension (MRC) is an important subtask in natural language processing, which requires a system to read a given passage and answer questions about it. The ability of utilizing external knowledge is of great significance in an MRC system (Rajpurkar et al., 2016; Trischler et al., 2017). Latest large-scale datasets, e.g. ReCoRD (Zhang et al., 2018) specify that external knowledge is required to answer questions. Many previous studies have introduced external knowledge in machine comprehension (Weissenborn, 2017; Mihaylov and Frank, 2018; Bauer et al., 2018). They often acquire external knowledge from structural knowledge graphs, such as ConceptNet (Speer et al., 2017) and Freebase (Tanon et al., 2016), in which knowledge is organized by triples like “(shortage, related to, lack)” and “(need, related to, lack)”. However, most of them fail to make full use of the structural information in the knowledge graph, and use sequence modeling methods like recurrent neural networks (RNNs) to generate the representation of knowledge, rather than based on graph structure. In this paper, we present a Structural Knowledge Graph-aware Netwo"
D19-1602,D19-5804,0,0.0169556,"ectively leverage external knowledge in MRC task, and achieves state-of-theart performance on the ReCoRD dataset. 5896 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5896–5901, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related work External Knowledge Enhanced MRC Models There are several models that use knowledge for machine comprehension (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Pan et al., 2019). Mihaylov and Frank (2018) relies on the ability of the attention mechanism to retrieve relevant pieces of knowledge, and Bauer et al. (2018) employs multihop commonsense paths to help multi-hop reasoning. They treat retrieved knowledge triples as sequences and use sequence modeling methods to compress the representation of knowledge, which are not based on graph structure. On the contrary, we organize knowledge as sub-graphs, then update the representation of nodes on sub-graphs with graph neural network. Graph Neural Netwoks Graph neural netwoks (Kipf and Welling, 2016; Schlichtkrull et al."
D19-1602,D16-1264,0,0.0609208,"h-aware Network (SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our method dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-ofthe-art performance on the ReCoRD dataset. 1 Introduction Machine reading comprehension (MRC) is an important subtask in natural language processing, which requires a system to read a given passage and answer questions about it. The ability of utilizing external knowledge is of great significance in an MRC system (Rajpurkar et al., 2016; Trischler et al., 2017). Latest large-scale datasets, e.g. ReCoRD (Zhang et al., 2018) specify that external knowledge is required to answer questions. Many previous studies have introduced external knowledge in machine comprehension (Weissenborn, 2017; Mihaylov and Frank, 2018; Bauer et al., 2018). They often acquire external knowledge from structural knowledge graphs, such as ConceptNet (Speer et al., 2017) and Freebase (Tanon et al., 2016), in which knowledge is organized by triples like “(shortage, related to, lack)” and “(need, related to, lack)”. However, most of them fail to make full"
D19-1602,W17-2623,0,0.0265809,"del, constructing sub-graphs for entities in the machine comprehension context. Our method dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-ofthe-art performance on the ReCoRD dataset. 1 Introduction Machine reading comprehension (MRC) is an important subtask in natural language processing, which requires a system to read a given passage and answer questions about it. The ability of utilizing external knowledge is of great significance in an MRC system (Rajpurkar et al., 2016; Trischler et al., 2017). Latest large-scale datasets, e.g. ReCoRD (Zhang et al., 2018) specify that external knowledge is required to answer questions. Many previous studies have introduced external knowledge in machine comprehension (Weissenborn, 2017; Mihaylov and Frank, 2018; Bauer et al., 2018). They often acquire external knowledge from structural knowledge graphs, such as ConceptNet (Speer et al., 2017) and Freebase (Tanon et al., 2016), in which knowledge is organized by triples like “(shortage, related to, lack)” and “(need, related to, lack)”. However, most of them fail to make full use of the structural in"
D19-1602,P17-1132,0,0.057064,"information of external knowledge; • Experiments demonstrate that SKG model is able to effectively leverage external knowledge in MRC task, and achieves state-of-theart performance on the ReCoRD dataset. 5896 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5896–5901, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related work External Knowledge Enhanced MRC Models There are several models that use knowledge for machine comprehension (Yang and Mitchell, 2017; Mihaylov and Frank, 2018; Weissenborn, 2017; Bauer et al., 2018; Pan et al., 2019). Mihaylov and Frank (2018) relies on the ability of the attention mechanism to retrieve relevant pieces of knowledge, and Bauer et al. (2018) employs multihop commonsense paths to help multi-hop reasoning. They treat retrieved knowledge triples as sequences and use sequence modeling methods to compress the representation of knowledge, which are not based on graph structure. On the contrary, we organize knowledge as sub-graphs, then update the representation of nodes on sub-graphs with graph neural network. Gra"
E17-1011,D14-1058,0,0.021703,"t Halo (Friedland et al., 2004) was proposed to create a ”digital” Aristotle that can encompass most of the worlds’s scientific knowledge and be capable of addressing complex problems with novel answers. In this project, (Angele et al., 2003) employed handcrafted rule to answer chemistry questions, (Gunning et al., 2010) took the physics and biology into account. Another important trial is solving the mathematical questions. (Mukherjee and Garain, 2008) attempted to answer them via transforming the natural language description into formal queries with hand-crafted rules, whereas recent works (Hosseini et al., 2014) started to employing learning techniques. However, none of these methods are suitable for history questions which requires large background knowledge, the same to the Aristo Challenge(Clark, 2015) focused on Elementary Grade Tests which is for 6-11 year olds. The Todai Robot Project(Fujita et al., 2014)aims to build a system that can pass the University of Tokyo’s entrance examination. As parts of this project, (Kanayama et al., 2012) mainly focus on addressing the yes-no questions via determining the correctness of the original proposition, and (Miyao et al., 2012) mainly focus on recognizin"
E17-1011,C12-1084,0,0.0292256,"and Garain, 2008) attempted to answer them via transforming the natural language description into formal queries with hand-crafted rules, whereas recent works (Hosseini et al., 2014) started to employing learning techniques. However, none of these methods are suitable for history questions which requires large background knowledge, the same to the Aristo Challenge(Clark, 2015) focused on Elementary Grade Tests which is for 6-11 year olds. The Todai Robot Project(Fujita et al., 2014)aims to build a system that can pass the University of Tokyo’s entrance examination. As parts of this project, (Kanayama et al., 2012) mainly focus on addressing the yes-no questions via determining the correctness of the original proposition, and (Miyao et al., 2012) mainly focus on recognizing textual entailment between a description in Wikipedia and each option of question. But, these two methods are separated for different kinds of questions and none of them introduced neural network approach. It’s inevitable to compare the GKHMC with the factoid questions. (Berant and Liang, 2014) takes 6 Conclusion and Future Work In this work, we detailed the multiple choice questions in subject History of Gaokao, present two differen"
E17-1011,P14-1133,0,0.0651482,"GKHMC. But, the GKHMC questions have their own characteristics. A multiple-choice question in GKHMC such as the examples shown in Figure 1 is composed of a question stem and four candidates. Our goal is to figure out the only one correct candidate. But, there are certain obstacles to achieve it. First, several background sentencess and a lead-in sentence conjointly constitutes the question stem, which makes these questions more complicated than former one-sentence-long factoid questions that can be handled by the existing approaches, like (Kolomiyet and Moens, 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014; Yih et al., 2015). Secondly, the background sentences generally contain various clues to figure out the historical events or personages which may be the perdue key to answer the question. These clues may include Tang poem and Song iambic verse, domainspecific expressions, even some mixture of modIntroduction Gaokao, namely the National College Entrance Examination, is the most important examination for Chinese senior high school students. Every college in China, no matter it is Top10 or Top100, would only accept the exam-takers whose Gaokao score is higher than its threshold score. As there"
E17-1011,W14-4012,0,0.0514781,"Missing"
E17-1011,D13-1161,0,0.0289565,"ension task are similar to GKHMC. But, the GKHMC questions have their own characteristics. A multiple-choice question in GKHMC such as the examples shown in Figure 1 is composed of a question stem and four candidates. Our goal is to figure out the only one correct candidate. But, there are certain obstacles to achieve it. First, several background sentencess and a lead-in sentence conjointly constitutes the question stem, which makes these questions more complicated than former one-sentence-long factoid questions that can be handled by the existing approaches, like (Kolomiyet and Moens, 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014; Yih et al., 2015). Secondly, the background sentences generally contain various clues to figure out the historical events or personages which may be the perdue key to answer the question. These clues may include Tang poem and Song iambic verse, domainspecific expressions, even some mixture of modIntroduction Gaokao, namely the National College Entrance Examination, is the most important examination for Chinese senior high school students. Every college in China, no matter it is Top10 or Top100, would only accept the exam-takers whose Gaokao score is higher than its th"
E17-1011,D16-1147,0,0.0383637,"Missing"
E17-1011,D13-1020,0,0.0634484,"papers from 2011 to 2015 all over the country, and they are released. From the result, we find that the performance of two approaches are significantly discrepant at each kind of questions. That is, IR approach shows noticeable advantages on EQs, while NN approach performs much better on SQs. This will be further discussed in Section 4.4. In this paper, our contributions are as follows: Table 1: The GKHMC dataset. ern Chinese and excerpt from ancient books and etc. The dependence of background knowledge makes the models that are designed for reading comprehension such as (Pe˜nas et al., 2013; Richardson et al., 2013) fail. Thirdly, the diversity of candidates’ granularity, i.e. candidates can either be entities or sentences, makes it harder to match the candidate and stem. So, the answer selection is disparate from the former approaches whose candidates are usually just entities. Lastly, as the candidates are already given, the answer generation step in former neural network approaches based question answering system is no longer necessary. • We gave a detailed description of the Gaokao History Multiple Choice Questions task and showed its importance and difficulty. As mentioned above and shown in Figure"
E17-1011,P15-1128,0,0.0186481,"uestions have their own characteristics. A multiple-choice question in GKHMC such as the examples shown in Figure 1 is composed of a question stem and four candidates. Our goal is to figure out the only one correct candidate. But, there are certain obstacles to achieve it. First, several background sentencess and a lead-in sentence conjointly constitutes the question stem, which makes these questions more complicated than former one-sentence-long factoid questions that can be handled by the existing approaches, like (Kolomiyet and Moens, 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014; Yih et al., 2015). Secondly, the background sentences generally contain various clues to figure out the historical events or personages which may be the perdue key to answer the question. These clues may include Tang poem and Song iambic verse, domainspecific expressions, even some mixture of modIntroduction Gaokao, namely the National College Entrance Examination, is the most important examination for Chinese senior high school students. Every college in China, no matter it is Top10 or Top100, would only accept the exam-takers whose Gaokao score is higher than its threshold score. As there are almost 10 milli"
E17-1011,fujita-etal-2014-overview,0,\N,Missing
I11-1026,P08-1037,0,0.0260622,"which is much simpler than the classification-based method. Koo et al. (2008) introduced lexical intermediaries at a coarser level than words themselves via a cluster method. Our approach is similar to theirs in that we used the fine-grained feature generation scheme based on HowNet hierarchical semantic knowledge, and the fine-grained features can be viewed as being a kind of “back-off” version of the baseline features. However, we focus on the problem of POS representation instead of lexical representation. Recently, there are some studies focusing on parsing task using semantic knowledge. Agirre et al. (2008) used word sense information to improve English parsing and PP attachment. Xiong et al. (2005) and Lin et al. (2009) extracted hypernym features from HowNet semantic knowledge and integrated the features into a generative model for Chinese constituent parsing. As with their work, we also use semantic knowledge for parsing. However, our gold is to employ HowNet hierarchical semantic knowledge to generate fine-grained features to dependency parsing, rather than to PCFGs, requiring a substantially different model formulation. Besides, Bansal and Klein (2011) and Zhou et al. (2011) exploited web-s"
I11-1026,P11-1070,0,0.0313593,"Missing"
I11-1026,D07-1101,0,0.691795,"wledge. Figure 2 shows the number of the most frequent errors relative to POS types on the development set for the first-order parsing. From the figure, it is seen that the main errors are nominal and verbal categories. Therefore, we may suspect that whether the complex and frequent categories like Dependency Parsing In dependency parsing, we attempt to build headmodifier (or head-dependent) relations between words in a sentence. The discriminative parser we used in this paper is based on the part-factored model and features of the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007). The parsing model can be defined as a conditional distribution p(y|x; w) over each projective parse tree y for a particular sentence x, parameterized by a vector w. The probability of a parse tree is {∑ } 1 exp w·Φ(x, ρ) (1) p(y|x; w) = Z(x; w) ρ∈y where Z(x; w) is the partition function and Φ are part-factored feature functions that include headmodifier parts, sibling parts and grandchild parts. Given the training set {(xi , yi )}N i=1 , parameter estimation for log-linear models generally resolve around optimization of a regularized conditional log-likelihood objective w∗ = arg minw L(w) w"
I11-1026,D09-1060,0,0.16198,"by adding fine-grained features. For the second-order parser, we get an absolute improvement of 0.61 points (UAS) by including fine-grained features. The improvements of parsing with fine-grained features are mildly significant using the Z-test of Collins et al. (2005). 233 Models Ord1 Ord1f improvement significant level Ord2 Ord2f improvement significant level UAS 86.57 87.09 +0.52 p &lt; 0.08 88.27 88.88 +0.61 p &lt; 0.05 CM 42.24 43.39 46.84 48.85 - Table 5: Dependency parsing results on the test set after using the merging operator. Systems Wang et al. (2007) Yu at al. (2008) Zhao et al. (2009) Chen et al. (2009) Ours ≤40 words (UAS) 86.6 88.9 92.34 90.86 Full (UAS) 87.26 87.0 89.91 88.88 Table 6: Dependency parsing results on this data set for our second-order model and the previous work. 6.3 Comparison with Previous Work To put our results in perspective, we also compare our second-order system with other best systems: Wang et al. (2007), Yu at al. (2008), Zhao et al. (2009) and Chen et al. (2009), respectively. The results are shown in Table 6, our approach outperforms the first three systems. Chen et al. (2009) reports a very high performance using subtree features from auto-parsed data. In our sy"
I11-1026,P05-1066,0,0.0198571,"orresponding to the best parameter values are evaluated on the test set of CTB 5.0. Table 5 shows the results. The performances can be further increased after using the merging operator. Such a fact validates the effectiveness of merging operator. Overall, for the first-order parser, we find that there is an absolute improvement of 0.52 points (UAS) by adding fine-grained features. For the second-order parser, we get an absolute improvement of 0.61 points (UAS) by including fine-grained features. The improvements of parsing with fine-grained features are mildly significant using the Z-test of Collins et al. (2005). 233 Models Ord1 Ord1f improvement significant level Ord2 Ord2f improvement significant level UAS 86.57 87.09 +0.52 p &lt; 0.08 88.27 88.88 +0.61 p &lt; 0.05 CM 42.24 43.39 46.84 48.85 - Table 5: Dependency parsing results on the test set after using the merging operator. Systems Wang et al. (2007) Yu at al. (2008) Zhao et al. (2009) Chen et al. (2009) Ours ≤40 words (UAS) 86.6 88.9 92.34 90.86 Full (UAS) 87.26 87.0 89.91 88.88 Table 6: Dependency parsing results on this data set for our second-order model and the previous work. 6.3 Comparison with Previous Work To put our results in perspective, w"
I11-1026,D08-1017,0,0.113308,"uite effective in Chinese dependency parsing task (see Table 2 in Section 4 for empirical results). 200 180 160 Error N um ber 140 120 100 80 3 60 40 3.1 20 0 Background NN VV NR M CC AD VA NT P VC DEG VE JJ DEC LC Error Number Relative to POS on Development Set Figure 2: The number of the most frequent errors relative to POS types on development set for firstorder parsing. been developed for dependency parsing, such as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in practice, given limited data, the relationships between the specific words and their context dependencies may be best modeled at a level finer than the POS tags but"
I11-1026,E06-1011,0,0.732817,"2011 AFNLP NN and VV should be split heavily while barely split rare or simple ones. Our experiments demonstrate that this strategy can be quite effective in Chinese dependency parsing task (see Table 2 in Section 4 for empirical results). 200 180 160 Error N um ber 140 120 100 80 3 60 40 3.1 20 0 Background NN VV NR M CC AD VA NT P VC DEG VE JJ DEC LC Error Number Relative to POS on Development Set Figure 2: The number of the most frequent errors relative to POS types on development set for firstorder parsing. been developed for dependency parsing, such as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in practice, given limited data, th"
I11-1026,P05-1012,0,0.778286,"November 8 – 13, 2011. 2011 AFNLP NN and VV should be split heavily while barely split rare or simple ones. Our experiments demonstrate that this strategy can be quite effective in Chinese dependency parsing task (see Table 2 in Section 4 for empirical results). 200 180 160 Error N um ber 140 120 100 80 3 60 40 3.1 20 0 Background NN VV NR M CC AD VA NT P VC DEG VE JJ DEC LC Error Number Relative to POS on Development Set Figure 2: The number of the most frequent errors relative to POS types on development set for firstorder parsing. been developed for dependency parsing, such as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in pra"
I11-1026,P06-1055,0,0.603342,"ch as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in practice, given limited data, the relationships between the specific words and their context dependencies may be best modeled at a level finer than the POS tags but coarser than the words themselves. Take the sentence in Figure 1 for example, although the words 外资(foreign capital) and 增长点(growth) have the same POS tag NN, they should have different context dependencies in dependency parsing tree. In HowNet, the two words are defined with different hypernyms. The word 外资(foreign capital) is defined as a kind of objective things, while the word 增长点(growth) is defined as an event ro"
I11-1026,J98-1004,0,0.0394135,"D DEC AS 2 1 1 4 3 3 5 1 1 1 1 MSP OD DEV BA LJ LB DER SP IJ ETC PU 1 1 1 1 1 1 1 1 1 1 1 Table 2: The number of subcategories generated by our hierarchical semantic knowledge based split-merge procedure. ample, common noun (NN) category is divided into the maximum number of subcategories (24). One subcategory consists primarily of objective things, whose typical semantic knowledge is an entity. Another subcategory is defined as an attribute, and so on. These kinds of semantic-related subcategories are typical, and give a division similar to the distributional clustering results like those of Schuetze (1998). The proper noun (NR) category is split into the 5 subcategories, including entity, institute-Places, attribute, aValue, and so on, which are defined in HowNet. The temporal noun (NT) category is also split into 3 subcategories. Verbal categories are also heavily split. Verbal subcategories sometimes reflect syntactic selectional preferences, and sometimes reflect other aspects of verbal syntax (Petrov and Klein, 2006). For example, the common verb (VV) category is divided into the number of 17 subcategories based on hierarchical split-merge procedure. The predictive adjective (VA) category i"
I11-1026,I05-1007,0,0.165567,"hypernym entity|实 体 in a hierarchical way. HowNet contains very limited words, so there are many words which cannot be found in HowNet. In this paper, we extend HowNet with Chinese Knowledge base “TongYiCiLin” (abbreviation: CiLin) (Mei et al., 1983), which represents 77,343 words in a dendrogram (or tree). CiLin is organized as a hierarchical tree structure, each node represents a semantic category. To balance the words coverage, we extract semantic categories at level 3, which covers 1,400 subcategories. HowNet and CiLin have different ontologies and representations of semantic categories (Xiong et al., 2005), we combine the two dictionaries: given a word w, if we cannot find in HowNet, but found in CiLin, we try to replace w with a synonym s in the synset defined by CiLin. If the synonym s can be found in HowNet, the corresponding semantic-related tag in HowNet will be assigned to w. 4 4.1 Fine-Grained Feature Generation Splitting the POS Tags In this subsection, we split the original POS tags to different degrees based on HowNet hierarchical semantic knowledge. The challenge is how to deal with the problem of polysemous words. Since each word may have multiple senses, and therefore different def"
I11-1026,W00-1213,0,0.0408364,"||w||2 (2) 2 The parameter C &gt; 0 is a constant dictating the level of regularization in the model. Since objective function L(w) is smooth and convex, which is convenient for standard gradient-based optimization techniques. In this paper we use the dual exponentiated gradient (EG)1 descent, which is a particularly effective optimization algorithm for log-linear models (Collins et al., 2008). 3.2 HowNet Semantic Knowledge HowNet is a bilingual general knowledge-base describing relations between concepts and relations between the attributes of concepts in Chinese and their English equivalents (Gan and Wong, 2000). HowNet constructs a hierarchical structure of its knowledge base from hypernym-hyponymy 1 229 http://groups.csail.mit.edu/nlp/egstra/ relations. The unit of meaning is called sememe that can not be further decomposed, which can be represented in Chinese and their English equivalents, such as the sememe fund|资 资 金. The explicated relations of HowNet include hypernym-hyponymy, synonymy, metonymy, antonymy, part-whole, attribute-host, materialproduct, dynamic role and concept co-occurrence, and so on. In this paper, we only consider the hypernym-hyponymy relations at different levels of granula"
I11-1026,W03-3023,0,0.148463,"ile barely split rare or simple ones. Our experiments demonstrate that this strategy can be quite effective in Chinese dependency parsing task (see Table 2 in Section 4 for empirical results). 200 180 160 Error N um ber 140 120 100 80 3 60 40 3.1 20 0 Background NN VV NR M CC AD VA NT P VC DEG VE JJ DEC LC Error Number Relative to POS on Development Set Figure 2: The number of the most frequent errors relative to POS types on development set for firstorder parsing. been developed for dependency parsing, such as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in practice, given limited data, the relationships between the specific words and"
I11-1026,P10-1110,0,0.120623,"Missing"
I11-1026,C08-1132,0,0.134403,"Missing"
I11-1026,P03-1054,0,0.00312574,"lves with the fine-grained semantic-related tags. Unlike the previous work, such as word cluster technique (Koo et al., 2008), data-driven split manner (Matsuzaki et al., 2005; Petrov and Klein, 2006), our approach does not exploit unlabeled data, and the splitting is based on hierarchical semantic knowledge instead of maximizing posterior probability, which is much simpler than their methods. 4.2 Merging Based on Threshold Constraint Intuitively, creating more subcategories can increase parsing accuracy. On the other hand, oversplitting can be a serious problem, the details were presented in Klein and Manning (2003). To prevent oversplitting, we merge the subcategories based on the threshold constraint. After the splitting, each subcategory contains a group of words which share the same semantic-related tag. Then we measure the size of each subcategory to determine whether the subcategory should be further merged. For easy explanation, we show an example in Figure 4, where each node Ci denotes a subcategory, Cj is the nearest hypernym subcategory of Ci , Ck is the nearest hypernym subcategory of Cj , and so on. Assuming that f (Ci ) , f (Cj ) and f (Cj ) denote the number of the words contained in the su"
I11-1026,P08-1068,0,0.638144,"aving Hyponyms… Continuing Splitting… Speciality Figure 3: Fine-grained feature generation for words with POS tag NN. The left part is word subcategory and the right part is HowNet hierarchy from generality to speciality. … Hypernym … … … … Hyponym Figure 4: The merging procedure based on hypernym-hyponymy relations from bottom-up. different levels of the fine-grained subcategories. By this observation, the fine-grained feature generation is just a hierarchical clustering of words themselves with the fine-grained semantic-related tags. Unlike the previous work, such as word cluster technique (Koo et al., 2008), data-driven split manner (Matsuzaki et al., 2005; Petrov and Klein, 2006), our approach does not exploit unlabeled data, and the splitting is based on hierarchical semantic knowledge instead of maximizing posterior probability, which is much simpler than their methods. 4.2 Merging Based on Threshold Constraint Intuitively, creating more subcategories can increase parsing accuracy. On the other hand, oversplitting can be a serious problem, the details were presented in Klein and Manning (2003). To prevent oversplitting, we merge the subcategories based on the threshold constraint. After the s"
I11-1026,D09-1135,0,0.0752963,"we cannot find in HowNet, but found in CiLin, we try to replace w with a synonym s in the synset defined by CiLin. If the synonym s can be found in HowNet, the corresponding semantic-related tag in HowNet will be assigned to w. 4 4.1 Fine-Grained Feature Generation Splitting the POS Tags In this subsection, we split the original POS tags to different degrees based on HowNet hierarchical semantic knowledge. The challenge is how to deal with the problem of polysemous words. Since each word may have multiple senses, and therefore different definitions in HowNet. Following Xiong et al. (2005) and Lin et al. (2009), we just use the first sense to determine the sense of each token instance of a target word (e.g., all token instances of a given word are tagged with the sense that occurs most frequently in HowNet). As mentioned in Section 3.2, the semantic information of each word can be represented as hierarchical hypernym-hyponymy relations. In this paper, we attempt to establish the mapping from top to down and split the words into different subcategories based on hypernym-hyponymy relations defined in HowNet. For easy explanation of the splitting process, we take the words with POS tag NN for example;"
I11-1026,D08-1059,0,0.546709,"ese dependency parsing task (see Table 2 in Section 4 for empirical results). 200 180 160 Error N um ber 140 120 100 80 3 60 40 3.1 20 0 Background NN VV NR M CC AD VA NT P VC DEG VE JJ DEC LC Error Number Relative to POS on Development Set Figure 2: The number of the most frequent errors relative to POS types on development set for firstorder parsing. been developed for dependency parsing, such as graph-based (McDonald et al., 2005; McDonald and Pereira, 2006), transition-based (Yamada and Matsumoto, 2003; Hall et al., 2006), or hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008; Zhang and Clark, 2008). These methods mainly rely on the POS information as important features, but the POS tags are usually too general to encapsulate a word’s syntactic behavior, especially for Chinese dependency parsing on CTB (e.g., it assumes that all the words with the POS tag NN share the same syntactic behavior). In the limit, each word may well have its own unique syntactic behavior (Petrov and Klein, 2006). However, in practice, given limited data, the relationships between the specific words and their context dependencies may be best modeled at a level finer than the POS tags but coarser than the words t"
I11-1026,P09-1007,0,0.219909,"0.52 points (UAS) by adding fine-grained features. For the second-order parser, we get an absolute improvement of 0.61 points (UAS) by including fine-grained features. The improvements of parsing with fine-grained features are mildly significant using the Z-test of Collins et al. (2005). 233 Models Ord1 Ord1f improvement significant level Ord2 Ord2f improvement significant level UAS 86.57 87.09 +0.52 p &lt; 0.08 88.27 88.88 +0.61 p &lt; 0.05 CM 42.24 43.39 46.84 48.85 - Table 5: Dependency parsing results on the test set after using the merging operator. Systems Wang et al. (2007) Yu at al. (2008) Zhao et al. (2009) Chen et al. (2009) Ours ≤40 words (UAS) 86.6 88.9 92.34 90.86 Full (UAS) 87.26 87.0 89.91 88.88 Table 6: Dependency parsing results on this data set for our second-order model and the previous work. 6.3 Comparison with Previous Work To put our results in perspective, we also compare our second-order system with other best systems: Wang et al. (2007), Yu at al. (2008), Zhao et al. (2009) and Chen et al. (2009), respectively. The results are shown in Table 6, our approach outperforms the first three systems. Chen et al. (2009) reports a very high performance using subtree features from auto-par"
I11-1026,P11-1156,1,0.877612,"Missing"
I11-1026,P05-1010,0,\N,Missing
I11-1026,P06-2041,0,\N,Missing
I11-1031,P09-1082,0,0.0504398,"ao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Beijing 100190, China {lcai,gyzhou,kliu,jzhao}@nlpr.ia.ac.cn Abstract questions and the question-answer pairs in the archives (Jeon et al., 2005; Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the wordto-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010). Although the translation-based language model(TRLM) has yielded the state-of-theart performance for question retrieval, they model the word translation probabilities without taking into account the distribution of words in the whole content. In this paper, we argue that it is beneficial to exploit the latent topic information for question retrieval. The basic idea is as follows: first we employ the topic model (e.g., LDA) to discover the latent topics in the content of questions, and calculate the semantic similarity between questions based on the latent topic information."
I11-1031,J93-2003,0,0.0540746,"Missing"
I11-1031,D10-1010,0,0.0354928,"onclusions and Future Work In this paper, we present a new approach to discover the latent topic of questions for improving the performance of translation-based language model for question retrieval. Experiments conducted on real cQA data demonstrate that our proposed approach significantly outperforms the state-of-the-art methods (TRLM and TRLM+CE). There are some ways in which this research could be continued. First, question structure should be considered, so it is necessary to combine the proposed approach with other question retrieval methods (e.g., (Duan et al., 2008; Wang et al., 2009; Bunescu and Huang, 2010)) to further improve the performance. Second, we will try to investigate the use of the proposed approach for other kinds of data set, such as categorized questions from forum sites and FAQ sites. Acknowledgments 5.5 The Effectiveness of Category Information This work was supported by the National Natural Science Foundation of China (No. 60875041 and No. 61070106). We thank the anonymous reviewers for their insightful comments. Like the previous approaches, we treat the questions as a multinomial distribution over latent topics, and each topic is a multinomial distribution over words too. Diff"
I11-1031,W10-1201,0,0.0317696,"a and the high computational burden. Therefore, we employ an alternative approach − Gibbs sampling (Griffiths, 2002), which is gaining popularity in recent work on latent topic analysis (Griffiths and Steyvers, 2004; Zhou et al., 2008; Wang and McCallum, 2006; Guo et al., 2008; Jo and Oh, 2011). After training the model, we can get the following parameter estimations as: representation of our proposed TMC model is presented in Figure 3. Inspired by the related work on topic analysis (Blei et al., 2003; Griffiths and Steyvers, 2004; Zhou et al., 2008; Wang and McCallum, 2006; Guo et al., 2008; Celikyilmaz et al., 2010; Jo and Oh, 2011), we make the following assumptions about the probabilistic structure of TMC model. First, each question is modeled as a multinomial distribution over latent topics, and each topic is modeled as a multinomial distribution over words and a multinomial distribution over categories. Second, the prior distributions for topics, words and categories follow different parameterized Dirichlet distribution, which is conjugate prior for multinomial distribution. In Figure 3, for each word w in question q, a topic z is first drawn from the multinomial distribution θq , and then a word is"
I11-1031,P11-1066,1,0.583483,"rt performance. Subsequent work on translation models focused on providing suitable parallel data to learn the translation probabilities. Lee et al. (2008) tried to further improve the translation probabilities based on question-answer pairs by selecting the most important terms to build compact translation models. Bernhard and Gurevych (2009) proposed to use as a parallel training data set the definitions and glosses provided for the same term by different lexical semantic resources. Cao et al. (2010) explored adding the category information into the translation model for question retrieval. Zhou et al. (2011) proposed a phrase-based translation model for question retrieval and obtained the stateof-the-art performance. However, all the existing methods ignore the latent topics information in calculating the semantic similarity between questions. In this paper, we present a new approach to discover the latent topic of questions for improving the performance of translation-based language models for question retrieval. Moreover, we introduce the category information into the process of discovering the latent topics. To the best of our knowledge, none of the existing studies addressed question retrieva"
I11-1031,D08-1043,0,0.16875,"ng Liu, and Jun Zhao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Beijing 100190, China {lcai,gyzhou,kliu,jzhao}@nlpr.ia.ac.cn Abstract questions and the question-answer pairs in the archives (Jeon et al., 2005; Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the wordto-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009; Cao et al., 2010). Although the translation-based language model(TRLM) has yielded the state-of-theart performance for question retrieval, they model the word translation probabilities without taking into account the distribution of words in the whole content. In this paper, we argue that it is beneficial to exploit the latent topic information for question retrieval. The basic idea is as follows: first we employ the topic model (e.g., LDA) to discover the latent topics in the content of questions, and calculate the semantic similarity between questions based on"
I11-1031,P08-1082,0,\N,Missing
I13-1013,P12-1036,0,0.0238833,"that is a function of observed meta data of the document. Labeled LDA (Ramage et al., 2009) defines a one-toone correspondence between LDA’s latent topics and observed document labels and utilize a transformation matrix to modify Dirichlet priors. Partially Labeled LDA (PLDA) extends Labeled LDA to incorporate per-label latent topics (Ramage et al., 2011). The DF-LDA model (Andrzejewski et al., 2009) employs must-link and cannot-link constraints as Dirichlet Forest priors for LDA learning, but it suffers the scalability issue. Most recently, the aspect extraction model for sentiment analysis (Mukherjee and Liu, 2012) assumes that a seed set is given which consists of words together with their respective aspect category. Then depending on whether a word is a seed or non-seed word, a different route of multinomial distribution will be taken to emit the word. Our work was partially inspired by the previously proposed joint sentiment-topic model (JST) (Lin and He, 2009; Lin et al., 2012), which extracts topics grouped under different sentiments, relying only on domainindependent polarity word prior information. While the afore-mentioned approaches assume the existence of either document label information or w"
I13-1013,N10-1012,0,0.00462675,"xes the Moscow bombing event with the Egyptian protesters Museum attack event. When checking the topics produced by PLDA we can see that it fails to correctly characterise violen and no-violent topics, since PLDA T2 should have been clearly classified as non-violent and the non-violent PLDA T1 as violent. Moreover in the violent PLDA T1 topic which presents violent related words, we can empirically identify more than one event involved. In order to measure the semantic topical coherence of VDM and the proposed baselines, we made use of the Pointwise Mutual Information(PMI) metric proposed in (Newman et al., 2010). PMI is an automatic topic coherence evaluation which has been found to correspond well with human judgements on topic coherence. In particular, a coherent topic should only contain semantically related words and hence any pair of the top words from the same topic should have a large PMI value. For each topic, we compute its PMI by averaging over the PMI of all the word pairs extracted from the top 10 topic words. Figure 3 shows the PMI values of topics extracted under the violence and non-violence classes with the topic numbers varying between 5 and 30. It can be observed that JST and PLDA g"
I13-1013,D09-1026,0,0.0346638,"orporates word prior knowledge into model learning. Here, we also review existing approaches for the incorporation of supervised information into LDA model learning. The supervised LDA (sLDA) (Blei and McAuliffe, 2008) uses empirical topic frequencies as a covariant for http://dbpedia.org http://www.opencalais.com 3 110 http://wikipedia.org a regression on document labels such as movie ratings. The Dirichlet-multinomial regression (DMR) model (Mimno and McCallum, 2008) uses a loglinear prior on document-topic distributions that is a function of observed meta data of the document. Labeled LDA (Ramage et al., 2009) defines a one-toone correspondence between LDA’s latent topics and observed document labels and utilize a transformation matrix to modify Dirichlet priors. Partially Labeled LDA (PLDA) extends Labeled LDA to incorporate per-label latent topics (Ramage et al., 2011). The DF-LDA model (Andrzejewski et al., 2009) employs must-link and cannot-link constraints as Dirichlet Forest priors for LDA learning, but it suffers the scalability issue. Most recently, the aspect extraction model for sentiment analysis (Mukherjee and Liu, 2012) assumes that a seed set is given which consists of words together"
I13-1154,D09-1156,0,0.0266905,"e attributes in each block. Intuitively, more frequent a template is found in a weak semi-block, more likely a string extracted by that template is an attribute. Based on this idea, templates with higher frequencies will have higher priority than those with the lower frequencies when extracting attributes. After we run through all the pages of the site, we get a collection of templates and attributes. Then we rank them to obtain site-level knowledge. 2.1.3 Ranker To rank obtained templates and attributes to get site-level knowledge, we use the graph walk based technique (Wang and Cohen, 2007)(Wang and Cohen, 2009). In the graph (Figure 2), attributes in initial attribute list are used as seeds. And these seeds are used to match the attributes in weak semi-block of a document (or a page) to learn templates. Then these templates are used to extract new attributes from the weak semi-block of a document (or a page). Intuitively, we consider that seeds appearing frequently are with high quality, templates derived by these seeds are tend to have good quality, and documents containing these seeds and templates are also deemed as high quality. Inversely, high quality documents also produce high quality attribu"
I13-1154,P09-1070,0,0.0749009,"Missing"
I17-4005,I17-4034,0,0.0294819,"the candidate with highest similarity score as correct answer. JU NITM, Complex Decision Tree To handle the questions in MCQA, JU NITM (Sarkar et al., 2017) built a complex decision tree classifier using word embedding features to predict the right answer. The overview of the whole system is demonstrated in Figure 6. In distributed semantic similarity module, they trained a word embedding dictionary containing 3 million words in 300-dimensional space on GoogleNews. Then, a complex decision tree is used to select the right answer in step2, classification. Figure 7: Fist adaptation of MappSent(Hazem, 2017). Figure 6: System Framework proposed by JU NITM(Sarkar et al., 2017). 6.6 TALN, MappSent Mappsent is proposed in a previous work of TALN, (Hazem et al., 2017). To adapt to the characteristics of MCQA, they retrofitted MappSent model in two different ways(Hazem, 2017). The first approach illustrated in Figure 7 is to follow the same procedure as the question-to-question Figure 8: Second adaptation of MappSent(Hazem, 2017). 39 7 Conclusions Hang Yuan, You Zhang, Jin Wang, and Xuejie Zhang. 2017. Using an attention-based lstm for multichoice question answering in exams. In IJCNLP2017, Shared Tas"
I17-4005,I17-4032,0,0.0317658,"TM Model with Attention Mechanism Figure 4: Convolutional architecture used in CASIA-NLP’s system (Li and Kong, 2017). 6.3 Cone, Wikipedia and Logistic Regression The system of Cone (Dzendzik et al., 2017), a team from ADAPT Centre, based on a logistic regression over the string similarities between question, answer, and additional text. Their model is constructed as a four-step pipeline as follow. 1. Preprocessing cleaning of the input data; 2. Data selection relative sentences are extracted from Wikipedia based on key words from question; Figure 5: Architecture of the model proposed by G623(Min et al., 2017). 3. Feature Vector Concatenation for every question, a feature vector is built as a concatenation of similarities between the answer candidates and sentences obtained in the previous step; The system of G623 (Min et al., 2017) combined CNN with LSTM network and took into account the attention mechanism. Fistly , question 38 and answer pairs are fed into a CNN network and produce joint representations of these pairs which are then fed into a LSTM network. The two separate vector representations of question and answer are then calculated to generate the weight vector by dot multiplication. Fina"
I17-4005,I17-4036,0,0.0634787,"Missing"
K19-1067,N16-1012,0,0.031135,"as a significant impact on response generation performance. It performs the best when the responding speaker and the target addressee are predicted correctly. “False / False” (both are mispredicted) obtains the worst performance on the referenced metrics. These results demonstrate that both responding speaker and target addressee contribute to generating better responses. 5 Related Work Our work is inspired by a large number of applications utilizing recurrent encoder-decoder frameworks (Cho et al., 2014) on NLP tasks such as machine translation (Bahdanau et al., 2015) and text summarization (Chopra et al., 2016). Recently, many researches extend the encoder-decoder framework on response generation. HRED (Serban et al., 2016) utilizes hierarchical encoder to capture the context. VHRED (Serban et al., 2017) extends HRED by adding a high-dimensional latent variable for utterances. These researches demonstrate the importance of contexts on response generation. Our work is also inspired by researches on multi-party chatbots. Dielmann and Renals (2008) automatically recognize dialogue acts in multiparty speech conversations. Recently, some studies focus on the three elements (speaker, addressee, response)"
K19-1067,P17-1162,0,0.104674,"M 21.0M 2.62M Avg. Tok/Ctx 51.4 51.5 51.4 Avg. Tok/Res 10.6 10.6 10.7 Test 42.3K 15.6K 10.8K 82.0K 2.62M 51.3 10.6 Following (Liu et al., 2018), we leverage two referenced measurements (BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)5 ) for automatic evaluations. Considering that current data-driven approaches tend to generate short and generic (meaningless) responses, two unreferenced (“intrinsic”) metrics are also leveraged to the evaluation. The first one is the average length of responses, which is an objective and surfaced metric reflected the substance of responses (Mou et al., 2016; He et al., 2017a). The other one is the number of nouns6 per response (Liu et al., 2018), which shows the richness of responses since nouns are usually content words. Note that the unreferenced metrics could enrich the evaluations, though they are weak metrics. The detailed results and analyses are shown as follows. Table 2: Data statistics. “#” means number, and “Avg. Tok/Ctx (or Res)” is the number of tokens per context (or response). et al., 2018). The original data comes from the Ubuntu IRC chat log, where each line consists of (Time, Speaker, Utterance). If the addressee is explicitly mentioned in the u"
K19-1067,P17-1019,1,0.930205,"M 21.0M 2.62M Avg. Tok/Ctx 51.4 51.5 51.4 Avg. Tok/Res 10.6 10.6 10.7 Test 42.3K 15.6K 10.8K 82.0K 2.62M 51.3 10.6 Following (Liu et al., 2018), we leverage two referenced measurements (BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)5 ) for automatic evaluations. Considering that current data-driven approaches tend to generate short and generic (meaningless) responses, two unreferenced (“intrinsic”) metrics are also leveraged to the evaluation. The first one is the average length of responses, which is an objective and surfaced metric reflected the substance of responses (Mou et al., 2016; He et al., 2017a). The other one is the number of nouns6 per response (Liu et al., 2018), which shows the richness of responses since nouns are usually content words. Note that the unreferenced metrics could enrich the evaluations, though they are weak metrics. The detailed results and analyses are shown as follows. Table 2: Data statistics. “#” means number, and “Avg. Tok/Ctx (or Res)” is the number of tokens per context (or response). et al., 2018). The original data comes from the Ubuntu IRC chat log, where each line consists of (Time, Speaker, Utterance). If the addressee is explicitly mentioned in the u"
K19-1067,N16-1014,0,0.461386,"he addressee. For example, in the same context of Figure 1, what a1 says to a2 is different from what a1 says to a3 because different addressees (a2 and a3 ) have different information demands. Similarly, as for the same addressee, utIntroduction Human computer conversation has been an important and challenging task in NLP and AI since the Turing Test was proposed in 1950 (Turing, 1950). Recently, with the rapid growth of social conversation data available on the Internet, data-driven chatbots are able to learn to generate responses directly and have attracted much more attention than before (Li et al., 2016a; Tian et al., 2017). Researches in this area mostly focus on the dialog with two interlocutors (Ma´ıra Gatti de Bayser et al., 2017). However, the real-life interaction involves a substantial part of Multi-Party Chatbots (MPC, such as internet forum and chat group), which is a form of conversation with multiple in718 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 718–727 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics based on an open dataset1 . Experimental results show that the proposed model is fairly competitive"
K19-1067,P16-1094,0,0.364932,"he addressee. For example, in the same context of Figure 1, what a1 says to a2 is different from what a1 says to a3 because different addressees (a2 and a3 ) have different information demands. Similarly, as for the same addressee, utIntroduction Human computer conversation has been an important and challenging task in NLP and AI since the Turing Test was proposed in 1950 (Turing, 1950). Recently, with the rapid growth of social conversation data available on the Internet, data-driven chatbots are able to learn to generate responses directly and have attracted much more attention than before (Li et al., 2016a; Tian et al., 2017). Researches in this area mostly focus on the dialog with two interlocutors (Ma´ıra Gatti de Bayser et al., 2017). However, the real-life interaction involves a substantial part of Multi-Party Chatbots (MPC, such as internet forum and chat group), which is a form of conversation with multiple in718 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 718–727 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics based on an open dataset1 . Experimental results show that the proposed model is fairly competitive"
K19-1067,W04-1013,0,0.024716,"onstructed based on the Ubuntu multi-party chatbot corpus2 , which has been widely used as the evaluation dataset for the response selection task (Ouchi and Tsuboi, 2016; Zhang 2 721 https://github.com/hiroki13/response-ranking Total Train Dev # Contexts 423.5K 338.9K 42.3K # Speaker 35.3K 33.5K 15.6K # Addressee 23.4K 22.4K 10.8K # Vocab 276.1K 254.8K 82.2K # Tokens 26.3M 21.0M 2.62M Avg. Tok/Ctx 51.4 51.5 51.4 Avg. Tok/Res 10.6 10.6 10.7 Test 42.3K 15.6K 10.8K 82.0K 2.62M 51.3 10.6 Following (Liu et al., 2018), we leverage two referenced measurements (BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)5 ) for automatic evaluations. Considering that current data-driven approaches tend to generate short and generic (meaningless) responses, two unreferenced (“intrinsic”) metrics are also leveraged to the evaluation. The first one is the average length of responses, which is an objective and surfaced metric reflected the substance of responses (Mou et al., 2016; He et al., 2017a). The other one is the number of nouns6 per response (Liu et al., 2018), which shows the richness of responses since nouns are usually content words. Note that the unreferenced metrics could enrich the evaluations, thou"
K19-1067,C16-1316,0,0.0246881,"2.2K # Tokens 26.3M 21.0M 2.62M Avg. Tok/Ctx 51.4 51.5 51.4 Avg. Tok/Res 10.6 10.6 10.7 Test 42.3K 15.6K 10.8K 82.0K 2.62M 51.3 10.6 Following (Liu et al., 2018), we leverage two referenced measurements (BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)5 ) for automatic evaluations. Considering that current data-driven approaches tend to generate short and generic (meaningless) responses, two unreferenced (“intrinsic”) metrics are also leveraged to the evaluation. The first one is the average length of responses, which is an objective and surfaced metric reflected the substance of responses (Mou et al., 2016; He et al., 2017a). The other one is the number of nouns6 per response (Liu et al., 2018), which shows the richness of responses since nouns are usually content words. Note that the unreferenced metrics could enrich the evaluations, though they are weak metrics. The detailed results and analyses are shown as follows. Table 2: Data statistics. “#” means number, and “Avg. Tok/Ctx (or Res)” is the number of tokens per context (or response). et al., 2018). The original data comes from the Ubuntu IRC chat log, where each line consists of (Time, Speaker, Utterance). If the addressee is explicitly m"
K19-1067,D17-1238,0,0.0429877,"Missing"
K19-1067,P02-1040,0,0.104684,"iment 4.1 Dataset Our dataset is constructed based on the Ubuntu multi-party chatbot corpus2 , which has been widely used as the evaluation dataset for the response selection task (Ouchi and Tsuboi, 2016; Zhang 2 721 https://github.com/hiroki13/response-ranking Total Train Dev # Contexts 423.5K 338.9K 42.3K # Speaker 35.3K 33.5K 15.6K # Addressee 23.4K 22.4K 10.8K # Vocab 276.1K 254.8K 82.2K # Tokens 26.3M 21.0M 2.62M Avg. Tok/Ctx 51.4 51.5 51.4 Avg. Tok/Res 10.6 10.6 10.7 Test 42.3K 15.6K 10.8K 82.0K 2.62M 51.3 10.6 Following (Liu et al., 2018), we leverage two referenced measurements (BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)5 ) for automatic evaluations. Considering that current data-driven approaches tend to generate short and generic (meaningless) responses, two unreferenced (“intrinsic”) metrics are also leveraged to the evaluation. The first one is the average length of responses, which is an objective and surfaced metric reflected the substance of responses (Mou et al., 2016; He et al., 2017a). The other one is the number of nouns6 per response (Liu et al., 2018), which shows the richness of responses since nouns are usually content words. Note that the unreferenced metrics could enrich"
K19-1067,P17-2036,0,0.0224313,"example, in the same context of Figure 1, what a1 says to a2 is different from what a1 says to a3 because different addressees (a2 and a3 ) have different information demands. Similarly, as for the same addressee, utIntroduction Human computer conversation has been an important and challenging task in NLP and AI since the Turing Test was proposed in 1950 (Turing, 1950). Recently, with the rapid growth of social conversation data available on the Internet, data-driven chatbots are able to learn to generate responses directly and have attracted much more attention than before (Li et al., 2016a; Tian et al., 2017). Researches in this area mostly focus on the dialog with two interlocutors (Ma´ıra Gatti de Bayser et al., 2017). However, the real-life interaction involves a substantial part of Multi-Party Chatbots (MPC, such as internet forum and chat group), which is a form of conversation with multiple in718 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 718–727 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics based on an open dataset1 . Experimental results show that the proposed model is fairly competitive on both automatic an"
K19-1067,D16-1231,0,\N,Missing
P08-1062,P02-1051,0,0.168055,"nt causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target language. For example, when “Campbell” is transliterated into “坎/kan贝/bei尔/er”, the “p” is missing. In order to make up the disadvantages of statistical approach, some researchers have been seeking for the assistance of web resource. [Wang et al., 2004; Cheng et al., 2004; Nagata et al., 2001; Zhang et al, 2005] used bilingual web pages to extract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al, 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can’t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 541 Proceedings of ACL-08: HLT, pages 541–549, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics assistance approaches using web-mining to assist transliteration are not suitable for Chinese to English backward transli"
P08-1062,W02-2017,0,0.0303249,"ns: forward transliteration which transforms an original name into target language, and backward transliteration which recovers a name back to its original expression. For instance, the original English per* Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn. son name “Clinton” can be forward transliterated to its Chinese expression “克/ke 林/lin顿/dun” and the backward transliteration is the inverse processing. In this paper, we focus on backward transliteration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward transliteration: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, backward transliteration is more challenging than forward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most i"
P08-1062,P04-1068,0,0.0304025,"forward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target language. For example, when “Campbell” is transliterated into “坎/kan贝/bei尔/er”, the “p” is missing. In order to make up the disadvantages of statistical approach, some researchers have been seeking for the assistance of web resource. [Wang et al., 2004; Cheng et al., 2004; Nagata et al., 2001; Zhang et al, 2005] used bilingual web pages to extract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al, 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can’t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 541 Proceedings of ACL-08: HLT,"
P08-1062,W01-1413,0,0.0742755,"Missing"
P08-1062,W03-1508,0,0.397408,"eration which transforms an original name into target language, and backward transliteration which recovers a name back to its original expression. For instance, the original English per* Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn. son name “Clinton” can be forward transliterated to its Chinese expression “克/ke 林/lin顿/dun” and the backward transliteration is the inverse processing. In this paper, we focus on backward transliteration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward transliteration: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, backward transliteration is more challenging than forward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introduc"
P08-1062,voorhees-tice-2000-trec,0,0.0341698,".65 21.07 26.83 38.59 48.72 57.36 0.5 exp2 10.93 17.25 21.81 33.04 42.79 53.46 ! = exp1 10.83 22.05 27.26 36.52 45.48 55.19 0.6 exp2 11.25 16.84 20.39 31.72 40.49 51.83 ! = exp1 9.62 17.90 24.38 35.25 41.57 55.63 0.7 exp2 10.63 16.26 21.20 29.75 39.94 49.52 ! = exp1 8.73 17.38 25.42 34.65 42.81 53.41 0.8 exp2 10.18 15.34 18.20 27.62 38.07 47.15 Table 4. Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average reciprocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as 1 1 (8) ARR = promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similarity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. M M ! R (i ) i =1 where R (i ) is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Average rank ARR"
P08-1062,W04-3248,0,\N,Missing
P08-1062,J98-4003,0,\N,Missing
P09-1044,P02-1051,0,0.135467,"Missing"
P09-1044,P05-1033,0,0.0289209,"Missing"
P09-1044,P06-2011,0,0.0331589,"Missing"
P09-1044,I05-1053,0,0.332633,"the details of the ON chunking in Section 4. In Section 5, we introduce the approach of heuristic query construction. In section 6, we will analyze the asymmetric alignment method. The experiments are reported in Section 7. The last section gives the conclusion and future work. 2 Related Work In the past few years, researchers have proposed many approaches for organization translation. There are three main types of methods. The first type of methods translates ONs by building a statistical translation model. The model can be built on the granularity of word [Stalls et al., 1998], phrase [Min Zhang et al., 2005] or structure [Yufeng Chen et al., 2007]. The second type of methods finds the translation equivalent based on the results of alignment from the source ON to the target ON [Huang et al., 2003; Feng et al., 2004; Lee et al., 2006]. The ONs are extracted from two corpora. The corpora can be parallel corpora [Moore et al., 2003] or contentaligned corpora [Kumano et al., 2004]. The third type of methods introduces the web resources into ON translation. [Al-Onaizan et al., 2002] uses the web knowledge to assist NE translation and [Huang et al., 2004; Zhang et al., 2005; Chen et al., 2006] extracts"
P09-1044,W98-1005,0,0.0995619,"Missing"
P09-1044,E03-1035,0,0.237785,"Missing"
P09-1044,W04-3248,0,\N,Missing
P09-1044,W03-1502,0,\N,Missing
P09-1044,W01-1413,0,\N,Missing
P11-1066,P09-1082,0,0.0828019,"3–662, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics lem is more serious for Q&A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 wi"
P11-1066,J93-2003,0,0.117642,"ons, but they have very few words in common. This prob653 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653–662, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics lem is more serious for Q&A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/"
P11-1066,D10-1010,0,0.109628,"Missing"
P11-1066,P05-1033,0,0.0335539,"of term t in D, |D |and |C| denote the length of D and C respectively. E: F: M: q: … for good cold home remedies … document [for, good, cold, home remedies] segmentation [for1, best2, stuffy nose3, home remedy4] translation (1Ƥ3⧎2Ƥ1⧎3Ƥ4⧎4Ƥ2) permutation best home remedy for stuffy nose queried question Figure 1: Example describing the generative procedure of the phrase-based translation model. 3 Our Approach: Phrase-Based Translation Model for Question Retrieval 3.1 Phrase-Based Translation Model 2.2 Word-Based Translation Model Phrase-based machine translation models (Koehn et al., 2003; D. Chiang, 2005; Och and Ney, 2004) have shown superior performance compared to word-based translation models. In this paper, the goal of phrase-based translation model is to translate a document4 D into a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, ∏ we might learn that the phrase “stuffy nose” can be Score(q, D) = (1−λ)Ptr (w|D)+λPml (w|C) (3) translated from “cold” with relative high probabilw∈q ity, even though neither of the individual"
P11-1066,W04-3252,0,0.0058224,"he ¯ )1 , . . . , (¯ ¯ )m } to learn P (¯ collection {(¯ a, q a, q q|¯ a), ¯)1 , . . . , (¯ ¯)m , (¯ ¯ )1 , . . . , (¯ ¯ )m } is then {(¯ q, a q, a a, q a, q used here to learn the combination translation probability Ppool (wi |tj ). 3.3.2 Parallel Corpus Preprocessing Unlike the bilingual parallel corpus used in SMT, our parallel corpus is collected from Q&A archives, which is more noisy. Directly using the IBM model 1 can be problematic, it is possible for translation model to contain “unnecessary” translations (Lee et 657 al., 2008). In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004) to identify and eliminate unimportant words from parallel corpus, assuming that a word in a question or answer is unimportant if it holds a relatively low significance in the parallel corpus. Following (Lee et al., 2008), the ranking algorithm proceeds as follows. First, all the words in a given document are added as vertices in a graph G. Then edges are added between words if the words co-occur in a fixed-sized window. The number of co-occurrences becomes the weight of an edge. When the graph is constructed, the score of each vertex is initialized as 1, and the PageRankbased ranking algorith"
P11-1066,N03-1017,0,0.329892,"dent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 with a relative high translation probability. In this paper, we argue that it is beneficial to capture contextual information for question retrieval. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrasebased translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level. This model learns the probability of translating one sequence of words (e.g., phrase) into another sequence of words, e.g., translating a phrase in a historical question into another phrase in a queried question. Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some conte"
P11-1066,D08-1043,0,0.850411,"guistics, pages 653–662, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics lem is more serious for Q&A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008). To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009). Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM. Okapi and LM). However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from"
P11-1066,J04-4002,0,0.0901159,"not take into account any contextual information in modeling word translation probabilities. For example in Table 1, although neither of the individual word pair (e.g., “stuffy”/“cold” and “nose”/“cold”) might have a high translation probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 with a relative high translation probability. In this paper, we argue that it is beneficial to capture contextual information for question retrieval. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrasebased translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level. This model learns the probability of translating one sequence of words (e.g., phrase) into another sequence of words, e.g., translating a phrase in a historical question into another phrase in a queried question. Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some contextual information in"
P11-1066,P10-1028,0,0.0249581,"o a queried question q. Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. For example, ∏ we might learn that the phrase “stuffy nose” can be Score(q, D) = (1−λ)Ptr (w|D)+λPml (w|C) (3) translated from “cold” with relative high probabilw∈q ity, even though neither of the individual word pairs (e.g., “stuffy”/“cold” and “nose”/“cold”) might have ∑ #(t, D) Ptr (w|D) = P (w|t)Pml (t|D), Pml (t|D) = a high word translation probability. Inspired by the |D| t∈D work of (Sun et al., 2010; Gao et al., 2010), we (4) assume the following generative process: first the where P (w|t) denotes the translation probability document D is broken into K non-empty word sefrom word t to word w. quences t1 , . . . , tK , then each t is translated into a new non-empty word sequence w1 , . . . , wK , and fi2.3 Word-Based Translation Language Model nally these phrases are permutated and concatenated Xue et al. (2008) proposed to linearly mix two dif- to form the queried questions q, where t and w deferent estimations by combining language model note the phrases or consecutive sequence of words."
P11-1156,P06-1105,0,0.0238027,"o dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our knowledge, web-derived selectional preference has not been successfully applied to dependency parsing. 6 Conclusion In this paper, we present a novel method which incorporates the web-derived selectional preferences to improve statistical dependency parsing. The results show"
P11-1156,D08-1007,0,0.0314964,"re approximated by Google hits. Another we use is Google V1 (Brants and Franz, 2006). This N-gram corpus records how often each unique sequence of words occurs. N-grams appearing 40 2 http://groups.csail.mit.edu/nlp/egstra/ This kind of feature sets are similar to other feature sets in the literature (McDonald et al., 2005; Carreras, 2007), so we will not attempt to give a exhaustive description. 4 Selectional preference tells us which arguments are plausible for a particular predicate, one way to determine the selectional preference is from co-occurrences of predicates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meaning with N-grams, which model the word-to-word relationships, rather than only considering the predicates and arguments relationships. 3 1558 times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics be"
P11-1156,P10-1089,0,0.0633308,"smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13 Google indexes about more than 8 billion pages and each contains about 1,000 words on average. # of tokens 3.2B 1,024.9B 8,000B θ 1 40 100 1 # of types 3.7B 3.4B - MST2 MST2+N-gram 0.95 Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data θ, and total number of unique N-gram (types) remaining in the data. 92.7 F1"
P11-1156,D07-1101,0,0.537191,"s follows. Section 2 gives a brief introduction of dependency parsing. Section 3 describes the web-derived selectional preference features. Experimental evaluation and results are reported in Section 4. Finally, we discuss related work and draw conclusion in Section 5 and Section 6, respectively. 2 Dependency Parsing In dependency parsing, we attempt to build headmodifier (or head-dependent) relations between words in a sentence. The discriminative parser we used in this paper is based on the part-factored model and features of the MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007). The parsing model can be defined as a conditional distribution p(y|x; w) over each projective parse tree y for a particular sentence x, parameterized by a vector w. The probability of a parse tree is {∑ } 1 p(y|x; w) = exp w · Φ(x, ρ) (1) Z(x; w) ρ∈y where Z(x; w) is the partition function and Φ are part-factored feature functions that include headmodifier parts, sibling parts and grandchild parts. Given the training set {(xi , yi )}N i=1 , parameter estimation for log-linear models generally resolve around optimization of a regularized conditional log-likelihood objective w∗ = arg minw L(w)"
P11-1156,W08-2102,0,0.00570621,"ed parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the"
P11-1156,D09-1060,0,0.532741,"Missing"
P11-1156,P89-1010,0,0.420167,"han only considering the predicates and arguments relationships. 3 1558 times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics between pairs of words. In this paper we generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) p(“x”)p(“y”) (3) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks.6 The value of these features is the PMI, if it is defined. If the PMI is undefined, following the work of (Pitler et al., 2010), we include one of two binary features: p(“x y”) = 0 or p(“x”) ∨"
P11-1156,P05-1066,0,0.0110815,"ser with the baseline features; +hits=N-gram features derived from the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on th"
P11-1156,N06-1021,0,0.0219057,"Missing"
P11-1156,P07-1033,0,0.00987663,"Missing"
P11-1156,W00-1204,0,0.0441918,"from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-grams). To the best of our"
P11-1156,N10-1115,0,0.0123287,"Missing"
P11-1156,P08-1068,0,0.775692,"), while the dependency length increases, the accuracies degrade sharply. These longer dependencies are therefore a major opportunity to improve the overall performance of dependency parsing. Usually, these longer dependencies can be parsed dependent on the specific words involved due to the limited range of features (e.g., a verb and its modifiers). Lexical statistics are therefore needed for resolving ambiguous relationships, yet the lexicalized statistics are sparse and difficult to estimate directly. To solve this problem, some information with different granularity has been investigated. Koo et al. (2008) proposed a semi-supervised dependency parsing by introducing lexical intermediaries at a coarser level than words themselves via a cluster method. This approach, however, ignores the selectional preference for word-to-word interactions, such as head-modifier relationship. Extra resources 1 Precision represents the percentage of predicted arcs of length d that are correct, and recall measures the percentage of gold-standard arcs of length d that are correctly predicted. F1 = 2 × precision × recall/(precision + recall) 1556 Proceedings of the 49th Annual Meeting of the Association for Computati"
P11-1156,J03-3005,0,0.0823483,"to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. 1563 Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb par"
P11-1156,P95-1007,0,0.0611535,"ates and arguments in text (Bergsma et al., 2008). In this paper, the selectional preferences have the same meaning with N-grams, which model the word-to-word relationships, rather than only considering the predicates and arguments relationships. 3 1558 times or more (1 in 25 billion) are kept, and appear in the n-gram tables. All n-grams with lower counts are discarded. Co-occurrence probabilities can be calculated directly from the N-gram counts. 3.2 Web-derived N-gram features 3.2.1 PMI Previous work on noun compounds bracketing has used adjacency model (Resnik, 1993) and dependency model (Lauer, 1995) to compute association statistics between pairs of words. In this paper we generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) p(“x”)p(“y”) (3) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks."
P11-1156,lin-etal-2010-new,0,0.0114179,"means that if pages indexed by Google doubles, then so do the bigrams or trigrams frequencies. Therefore, the estimate becomes stable when the number of indexed pages grows unboundedly. Some details are presented in Cilibrasi and Vitanyi (2007). 5 Related Work Our approach is to exploit web-derived selectional preferences to improve the dependency parsing. The idea of this paper is inspired by the work of Suzuki et al. (2009) and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. 1563 Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP at"
P11-1156,J93-2004,0,0.0536453,"ndency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have ∗ Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − j|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 show"
P11-1156,D08-1017,0,0.269785,"ns. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them"
P11-1156,P06-1043,0,0.0537403,"on new domains? For Question I, we systematically assess the value of using web-scale data in state-of-the-art supervised dependency parsers. We compare dependency parsers that include or exclude selectional preference features obtained from web-scale corpus. To the best of our knowledge, none of the existing studies directly address long dependencies of dependency parsing by using web-scale data. 1557 Most statistical parsers are highly domain dependent. For example, the parsers trained on WSJ text perform poorly on Brown corpus. Some studies have investigated domain adaptation for parsers (McClosky et al., 2006; Daum´ e III, 2007; McClosky et al., 2010). These approaches assume that the parsers know which domain it is used, and that it has access to representative data in that domain. However, in practice, these assumptions are unrealistic in many real applications, such as when processing the heterogeneous genre of web texts. In this paper we incorporate the web-derived selectional preference features to design our parsers for robust opendomain testing. We conduct the experiments on the English Penn Treebank (PTB) (Marcus et al., 1993). The results show that web-derived selectional preference can i"
P11-1156,N10-1004,0,0.00491058,"atically assess the value of using web-scale data in state-of-the-art supervised dependency parsers. We compare dependency parsers that include or exclude selectional preference features obtained from web-scale corpus. To the best of our knowledge, none of the existing studies directly address long dependencies of dependency parsing by using web-scale data. 1557 Most statistical parsers are highly domain dependent. For example, the parsers trained on WSJ text perform poorly on Brown corpus. Some studies have investigated domain adaptation for parsers (McClosky et al., 2006; Daum´ e III, 2007; McClosky et al., 2010). These approaches assume that the parsers know which domain it is used, and that it has access to representative data in that domain. However, in practice, these assumptions are unrealistic in many real applications, such as when processing the heterogeneous genre of web texts. In this paper we incorporate the web-derived selectional preference features to design our parsers for robust opendomain testing. We conduct the experiments on the English Penn Treebank (PTB) (Marcus et al., 1993). The results show that web-derived selectional preference can improve the statistical dependency parsing,"
P11-1156,D07-1013,0,0.0159621,"Missing"
P11-1156,E06-1011,0,0.680264,"perating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have ∗ Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − j|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score1 relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note that the parsers provide very g"
P11-1156,P05-1012,0,0.700851,"ore importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance. 1 Introduction Dependency parsing is the task of building dependency links between words in a sentence, which has recently gained a wide interest in the natural language processing community. With the availability of large-scale annotated corpora such as Penn Treebank (Marcus et al., 1993), it is easy to train a high-performance dependency parser using supervised learning methods. However, current state-of-the-art statistical dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006; Hall et al., 2006) tend to have ∗ Correspondence author: jzhao@nlpr.ia.ac.cn lower accuracies for longer dependencies (McDonald and Nivre, 2007). The length of a dependency from word wi to word wj is simply equal to |i − j|. Longer dependencies typically represent the modifier of the root or the main verb, internal dependencies of longer NPs or PP-attachment in a sentence. Figure 1 shows the F1 score1 relative to the dependency length on the development set by using the graph-based dependency parsers (McDonald et al., 2005; McDonald and Pereira, 2006). We note tha"
P11-1156,W05-0603,0,0.023177,") and Pitler et al. (2010). The former uses the web-scale data explicitly to create more data for training the model; while the latter explores the webscale N-grams data (Lin et al., 2010) for compound bracketing disambiguation. Our research, however, applies the web-scale data (Google hits and Google V1) to model the word-to-word dependency relationships rather than compound bracketing disambiguation. 1563 Several previous studies have exploited the webscale data for word pair acquisition. Keller and Lapata (2003) evaluated the utility of using web search engine statistics for unseen bigram. Nakov and Hearst (2005) demonstrated the effectiveness of using search engine statistics to improve the noun compound bracketing. Volk (2001) exploited the WWW as a corpus to resolve PP attachment ambiguities. Turney (2007) measured the semantic orientation for sentiment classification using co-occurrence statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, part"
P11-1156,P08-1108,0,0.0429443,"ng labeled parent predictions. finding is that the N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspectiv"
P11-1156,W07-2201,0,0.242042,"Missing"
P11-1156,C10-1100,0,0.579173,"endency models by including the pointwise mutual information (Church and Hanks, 1900) between all pairs of words in the dependency tree: PMI(x, y) = log p(“x y”) p(“x”)p(“y”) (3) where p(“x y”) is the co-occurrence probabilities. When use the Google V1 corpus, this probabilities can be calculated directly from the N-gram counts, while using the Google hits, we send the queries to the search engine Google5 and all the search queries are performed as exact matches by using quotation marks.6 The value of these features is the PMI, if it is defined. If the PMI is undefined, following the work of (Pitler et al., 2010), we include one of two binary features: p(“x y”) = 0 or p(“x”) ∨ p(“y”) = 0 Besides, we also consider the trigram features be5 http://www.google.com/ Google only allows automated querying through the Google Web API, this involves obtaining a license key, which then restricts the number of queries to a daily quota of 1000. However, we obtained a quota of 20,000 queries per day by sending a request to api-support@google.com for research purposes. 6 PMI(“hit with”) xi -word=“hit”, xj -word=“with”, PMI(“hit with”) xi -word=“hit”, xj -word=“with”, xj -pos=“IN”, PMI(“hit with”) xi -word=“hit”, xi -"
P11-1156,D09-1058,0,0.315074,"are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-performing systems in Table 4. To facilitate c"
P11-1156,P08-1076,0,0.0604893,"shows the best performance. The other two are smaller ones, accuracies increase linearly with the log of the number of types in the auxiliary data set. Similar observations have been made by Pitler et al. (2010). We see that the relationship between accuracy and the number of Ngram is not monotonic for Google V1. The reason may be that Google V1 does not make detailed preprocessing, containing many mistakes in the corpus. Although Google hits is noisier, it has very much larger coverage of bigrams or trigrams. Some previous studies also found a log-linear relationship between unlabeled data (Suzuki and Isozaki, 2008; Suzuki et al., 2009; Bergsma et al., 2010; Pitler et al., 2010). We have shown that this trend continues well for dependency parsing by using web-scale data (NEWS and Google V1). 13 Google indexes about more than 8 billion pages and each contains about 1,000 words on average. # of tokens 3.2B 1,024.9B 8,000B θ 1 40 100 1 # of types 3.7B 3.4B - MST2 MST2+N-gram 0.95 Table 5: N-gram data, with total number of words in the original corpus (in billions, B). Following (Brants and Franz, 2006; Pitler et al., 2010), we set the frequency threshold to filter the data θ, and total number of unique N-g"
P11-1156,W03-3023,0,0.632545,"Missing"
P11-1156,W06-1604,0,0.015959,"statistics obtained from the search engines. Bergsma et al. (2010) created robust supervised classifiers via web-scale N-gram data for adjective ordering, spelling correction, noun compound bracketing and verb part-of-speech disambiguation. Our approach, however, extends these techniques to dependency parsing, particularly for long dependency relationships, which involves more challenging tasks than the previous work. Besides, there are some work exploring the wordto-word co-occurrence derived from the web-scale data or a fixed size of corpus (Calvo and Gelbukh, 2004; Calvo and Gelbukh, 2006; Yates et al., 2006; Drabek and Zhou, 2000; van Noord, 2007) for PP attachment ambiguities or shallow parsing. Johnson and Riezler (2000) incorporated the lexical selectional preference features derived from British National Corpus (Graff, 2003) into a stochastic unification-based grammar. Abekawa and Okumura (2006) improved Japanese dependency parsing by using the co-occurrence information derived from the results of automatic dependency parsing of large-scale corpora. However, we explore the webscale data for dependency parsing, the performance improves log-linearly with the number of parameters (unique N-gram"
P11-1156,D08-1059,0,0.0896167,"N-gram features derived from Google hits are slightly better than Google V1 due to the large N-gram coverage, we will discuss later. As a final note, all the comparisons between the integration of N-gram features and the baseline features in Table 3 are mildly significant using the Z-test of Collins et al. (2005) (p < 0.08). Type D C S Systems Yamada and Matsumoto (2003) McDonald et al. (2005) McDonald and Pereira (2006) Corston-Oliver et al. (2006) Hall et al. (2006) Wang et al. (2007) Carreras et al. (2008) GoldBerg and Elhadad (2010)† Ours Nivre and McDonald (2008)† Martins et al. (2008)† Zhang and Clark (2008) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) UAS 90.3 90.9 91.5 90.9 89.4 89.2 93.5 91.32 92.64 92.12 92.87 92.1 93.16 93.79 93.16 CM 38.7 37.5 42.1 37.5 36.4 34.4 40.41 46.61 44.37 45.51 45.4 47.15 Table 4: Comparison of our final results with other bestperforming systems on the whole Section 23. Type D, C and S denote discriminative, combined and semisupervised systems, respectively. † These papers were not directly reported the results on this data set, we implemented the experiments in this paper. To put our results in perspective, we also compare them with other best-perform"
P11-1156,A00-2021,0,\N,Missing
P11-1156,J90-1003,0,\N,Missing
P11-1156,P06-2041,0,\N,Missing
P13-1172,J93-2003,0,0.0512784,">−−−→(NN)←−−−<TC> Example: IPhone is a revolutionary smart phone pred subj Pattern#5: <OC>−−−→(VBE)←−−−<TC> Example: The quality of LCD is good Table 1: Some Examples of Used Syntactic Patterns 3.1.2 Unsupervised Word Alignment Model In this subsection, we present our method for capturing opinion relations using unsupervised word alignment model. Similar to (Liu et al., 2012), every sentence in reviews is replicated to generate a parallel sentence pair, and the word alignment algorithm is applied to the monolingual scenario to align a noun/noun phase with its modifiers. We select IBM-3 model (Brown et al., 1993) as the alignment model. Formally, given a sentence S = {w1 , w2 , ..., wn }, we have Pibm3 (A|S) ∝ N Y i=1 n(φi |wi ) N Y j=1 t(wj |waj )d(j|aj , N ) (1) where t(wj |waj ) models the co-occurrence information of two words in dataset. d(j|aj , n) models word position information, which describes the probability of a word in position aj aligned with a word in position j. And n(φi |wi ) describes the ability of a word for modifying (being modified by) several words. φi denotes the number of words 1757 that are aligned with wi . In our experiments, we set φi = 2. Since we only have interests on c"
P13-1172,W10-1701,0,0.0964131,"ey component of which we vary the methods between syntactic patterns and alignment model. Then we run the whole framework on the corpus with different size (from #500 to #1, 000, 000), domain (three domains) and language (Chinese and English) to empirically assess the performance variations and discuss which method is more effective. Furthermore, this paper naturally addresses another question: is it useful for opinion targets extraction when we combine syntactic patterns and word alignment model into a unified model? To this end, we employ a partially supervised alignment model (PSWAM) like (Gao et al., 2010; Liu et al., 2013). Based on the exquisitely designed high-precision syntactic patterns, we can obtain some precisely modified relations between words in sentences, which provide a portion of links of the full alignments. Then, these partial alignment links can be regarded as the constrains for a standard unsupervised word alignment model. And each target candidate would find its modifier under the partial supervision. In this way, the errors generated in standard unsupervised WAM can be corrected. For example in Figure 1, “kindly” and “courteous” are incorrectly regarded as the modifiers for"
P13-1172,I08-1038,0,0.195325,"given a certain amount of reviews. 3 Opinion Target Extraction Methodology To extract opinion targets from reviews, we adopt the framework proposed by (Liu et al., 2012), which is a graph-based extraction framework and 1756 has two main components as follows. 1) The first component is to capture opinion relations in sentences and estimate associations between opinion target candidates and potential opinion words. In this paper, we assume opinion targets to be nouns or noun phrases, and opinion words may be adjectives or verbs, which are usually adopted by (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008; Liu et al., 2012). And a potential opinion relation is comprised of an opinion target candidate and its corresponding modified word. 2) The second component is to estimate the confidence of each candidate. The candidates with higher confidence scores than a threshold will be extracted as opinion targets. In this procedure, we formulate the associations between opinion target candidates and potential opinion words in a bipartite graph. A random walk based algorithm is employed on this graph to estimate the confidence of each target candidate. In this paper, we fix the method in the second com"
P13-1172,D09-1159,0,0.18565,"al supervision. However, is this kind of combination always useful for opinion target extraction? To access this problem, we also make comparison between PSWAM based method and the aforementioned methods in the same corpora with different size, language and domain. The experimental results show the combination by using PSWAM can be effective on dataset with small and medium size. 1755 2 Related Work Opinion target extraction isn’t a new task for opinion mining. There are much work focusing on this task, such as (Hu and Liu, 2004b; Ding et al., 2008; Li et al., 2010; Popescu and Etzioni, 2005; Wu et al., 2009). Totally, previous studies can be divided into two main categories: supervised and unsupervised methods. In supervised approaches, the opinion target extraction task was usually regarded as a sequence labeling problem (Jin and Huang, 2009; Li et al., 2010; Ma and Wan, 2010; Wu et al., 2009; Zhang et al., 2009). It’s not only to extract a lexicon or list of opinion targets, but also to find out each opinion target mentions in reviews. Thus, the contextual words are usually selected as the features to indicate opinion targets in sentences. And classical sequence labeling models are used to trai"
P13-1172,D12-1123,1,0.111786,"et al., 2009; Qiu et al., 2011). However, the sentences in online reviews usually have informal writing styles including grammar mistakes, typos, improper punctuation etc., which make parsing prone to generate mistakes. As a result, the syntax-based methods which heavily depended on the parsing performance would suffer from parsing errors (Zhang et al., 2010). To improve the extraction performance, we can only employ some exquisite highprecision patterns. But this strategy is likely to miss many opinion targets and has lower recall with the increase of corpus size. To resolve these problems, Liu et al. (2012) formulated identifying opinion relations between words as an monolingual alignment process. A word can find its corresponding modifiers by using a word alignment 1754 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754–1763, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Figure 1: Mining Opinion Relations between Words using Partially Supervised Alignment Model model (WAM). Without using syntactic parsing, the noises from parsing errors can be effectively avoided. Nevertheless, we notice that the alignment mod"
P13-1172,C10-2090,0,0.119768,"rimental results show the combination by using PSWAM can be effective on dataset with small and medium size. 1755 2 Related Work Opinion target extraction isn’t a new task for opinion mining. There are much work focusing on this task, such as (Hu and Liu, 2004b; Ding et al., 2008; Li et al., 2010; Popescu and Etzioni, 2005; Wu et al., 2009). Totally, previous studies can be divided into two main categories: supervised and unsupervised methods. In supervised approaches, the opinion target extraction task was usually regarded as a sequence labeling problem (Jin and Huang, 2009; Li et al., 2010; Ma and Wan, 2010; Wu et al., 2009; Zhang et al., 2009). It’s not only to extract a lexicon or list of opinion targets, but also to find out each opinion target mentions in reviews. Thus, the contextual words are usually selected as the features to indicate opinion targets in sentences. And classical sequence labeling models are used to train the extractor, such as CRFs (Li et al., 2010), HMM (Jin and Huang, 2009) etc.. Jin et al. (2009) proposed a lexicalized HMM model to perform opinion mining. Both Li et al. (2010) and Ma et al. (2010) used CRFs model to extract opinion targets in reviews. Specially, Li et"
P13-1172,H05-1043,0,0.983947,"ing. So this task Therefore, identifying the aforementioned opinion relations between words is important for extracting opinion targets from reviews. To fulfill this aim, previous methods exploited the words co-occurrence information to indicate them (Hu and Liu, 2004a; Hu and Liu, 2004b). Obviously, these methods cannot obtain precise extraction because of the diverse expressions by reviewers, like long-span modified relations between words, etc. To handle this problem, several methods exploited syntactic information, where several heuristic patterns based on syntactic parsing were designed (Popescu and Etzioni, 2005; Qiu et al., 2009; Qiu et al., 2011). However, the sentences in online reviews usually have informal writing styles including grammar mistakes, typos, improper punctuation etc., which make parsing prone to generate mistakes. As a result, the syntax-based methods which heavily depended on the parsing performance would suffer from parsing errors (Zhang et al., 2010). To improve the extraction performance, we can only employ some exquisite highprecision patterns. But this strategy is likely to miss many opinion targets and has lower recall with the increase of corpus size. To resolve these probl"
P13-1172,J11-1002,0,0.22362,"Missing"
P13-1172,C10-1074,0,\N,Missing
P13-1172,H05-2017,0,\N,Missing
P13-1172,C10-2167,0,\N,Missing
P13-1173,P10-1041,0,0.0104156,"erns as contextual clues. Our approach is similar to (Wiebe and Riloff, 2005) and (Xu et al., 2013), all of which apply syntactic pattern learning and adopt self-learning strategy. However, the task of (Wiebe and Riloff, 2005) was to classify sentiment orientations in sentence level, while ours needs to extract more detailed information in term level. In addition, our method extends (Xu et al., 2013), and we give a more complete and in-depth analysis on the aforementioned problems in the first section. There were also many works employed graphbased method (Li et al., 2012; Zhang et al., 2010; Hassan and Radev, 2010; Liu et al., 2012), but none of previous works considered confidence of patterns in the graph. In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Li et al., 2010). The downside of supervised methods was the difficulty of obtaining annotated training data in practical applications. Also, classifiers trained 1765 on one domain often fail to give satisfactory results when shifted to another domain. Our method does not rely on annotated training data. terns are generated, we drop those patterns with frequency lower than"
P13-1173,P10-1060,0,0.0614549,"Missing"
P13-1173,W09-2307,0,0.0304995,"Missing"
P13-1173,D07-1114,0,0.19021,"alled Double Propagation which introduced eight heuristic syntactic rules. While manually defining syntactic patterns could be timeconsuming and error-prone, we learn syntactic patterns automatically from data. There have been extensive works on mining opinion words and opinion targets by syntactic pattern learning. Riloff and Wiebe (2003) performed pattern learning through bootstrapping while extracting subjective expressions. Zhuang et al. (2006) obtained various dependency relationship templates from an annotated movie corpus and applied them to supervised opinion words/targets extraction. Kobayashi et al. (2007) adopted a supervised learning technique to search for useful syntactic patterns as contextual clues. Our approach is similar to (Wiebe and Riloff, 2005) and (Xu et al., 2013), all of which apply syntactic pattern learning and adopt self-learning strategy. However, the task of (Wiebe and Riloff, 2005) was to classify sentiment orientations in sentence level, while ours needs to extract more detailed information in term level. In addition, our method extends (Xu et al., 2013), and we give a more complete and in-depth analysis on the aforementioned problems in the first section. There were also"
P13-1173,P09-1079,0,0.0125535,"gh frequency (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu et al., 2009; Zhu et al., 2009), and they often have difficulty in identifying the infrequent or long-tail opinion targets. 1764 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764–1773, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the problems stated above, this paper proposes a two-stage framework for mining opinion words and opinion targets. The underlying motivation is analogous to the novel idea “Mine the Easy, Classify the Hard” (Dasgupta and Ng, 2009). In our first stage, we propose a Sentiment Graph Walking algorithm to cope with the false opinion relation problem, which mines easy cases of opinion words/targets. We speculate that it may be helpful to introduce a confidence score for each pattern. Concretely, we create a Sentiment Graph to model opinion relations among opinion word/target/pattern candidates and apply random walking to estimate confidence of them. Thus, confidence of pattern is considered in a unified process. Patterns that often extract false opinion relations will have low confidence, and terms introduced by low-confiden"
P13-1173,C10-1074,0,0.1338,"random walking to estimate confidence of them. Thus, confidence of pattern is considered in a unified process. Patterns that often extract false opinion relations will have low confidence, and terms introduced by low-confidence patterns will also have low confidence accordingly. This could potentially improve the extraction accuracy. In the second stage, we identify the hard cases, which aims to filter out false opinion targets and extract long-tail opinion targets. Previous supervised methods have been shown to achieve stateof-the-art results for this task (Wu et al., 2009; Jin and Ho, 2009; Li et al., 2010). However, the big challenge for fully supervised method is the lack of annotated training data. Therefore, we adopt a self-learning strategy. Specifically, we employ a semi-supervised classifier to refine the target results from the first stage, which uses some highly confident target candidates as the initial labeled examples. Then opinion words are also refined. Our main contributions are as follows: • We propose a Sentiment Graph Walking algorithm to mine opinion words and opinion targets from reviews, which naturally incorporates confidence of syntactic pattern in a graph to improve extra"
P13-1173,W00-1213,0,0.0355731,"els of hyponyms in four WordNet (Miller, 1995) synsets “object”, “person”, “group” and “measure” into the GN corpus. Our idea was based on the fact that a term is more general when it sits in higher level in the WordNet hierarchy. Then inapplicable candidates were discarded and a 3071-word English 2 Note that the “positive” and “negative” here denote opinion targets and non-target terms respectively and they do not indicate sentiment polarities. 3 http://books.google.com/ngrams. 1767 GN corpus was created. Another Chinese GN corpus with 3493 words was generated in the similar way from HowNet (Gan and Wong, 2000). Generation of Labeled Examples. Let T = {Y+1 , Y−1 } denotes the initial labeled set, where N most highly confident target candidates but not in our GN corpora are regarded as the positive example set Y+1 , other N terms from GN corpora which are also top ranked in the target list are selected as the negative example set Y−1 . The reminder unlabeled candidates are denoted by T ∗ . Feature Representation for Classifier. Given T and T ∗ in the form of {(xi , yi )}. For a target candidate ti , xi = (o1 , . . . , on , p1 , . . . , pm )T represents its feature vector, where oj is the opinion word"
P13-1173,D12-1123,1,0.827279,". Our approach is similar to (Wiebe and Riloff, 2005) and (Xu et al., 2013), all of which apply syntactic pattern learning and adopt self-learning strategy. However, the task of (Wiebe and Riloff, 2005) was to classify sentiment orientations in sentence level, while ours needs to extract more detailed information in term level. In addition, our method extends (Xu et al., 2013), and we give a more complete and in-depth analysis on the aforementioned problems in the first section. There were also many works employed graphbased method (Li et al., 2012; Zhang et al., 2010; Hassan and Radev, 2010; Liu et al., 2012), but none of previous works considered confidence of patterns in the graph. In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Li et al., 2010). The downside of supervised methods was the difficulty of obtaining annotated training data in practical applications. Also, classifiers trained 1765 on one domain often fail to give satisfactory results when shifted to another domain. Our method does not rely on annotated training data. terns are generated, we drop those patterns with frequency lower than a threshold F . 3 T"
P13-1173,H05-1043,0,0.974257,"j-{mod}-(Prep)-{pcomp-n}-Noun”, but it doesn’t bear any sentiment orientation. We call such relations that match opinion patterns but express no opinion false opinion relations. Previous pattern learning algorithms (Zhuang et al., 2006; Kessler and Nicolov, 2009; Jijkoun et al., 2010) often extract opinion patterns by frequency. However, some high-frequency syntactic patterns can have very poor precision (Kessler and Nicolov, 2009). False Opinion Targets: In another case, the phrase “wonderful time” can be matched by an opinion pattern “Adj-{mod}-Noun”, which is widely used in previous works (Popescu and Etzioni, 2005; Qiu et al., 2009). As can be seen, this phrase does express a positive opinion but unfortunately “time” is not a valid opinion target for most domains such as MP3. Thus, false opinion targets are extracted. Due to the lack of ground-truth knowledge for opinion targets, non-target terms introduced in this way can be hardly filtered out. Long-tail Opinion Targets: We further notice that previous works prone to extract opinion targets with high frequency (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu et al., 2009; Zhu et al., 2009), and they often have difficulty in identifying the infrequen"
P13-1173,W03-1014,0,0.190424,"nion relations more precisely, subsequent research work exploited syntax information. Popescu and Etzioni (2005) used manually complied syntactic patterns and Pointwise Mutual Information (PMI) to extract opinion words/targets. Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules. While manually defining syntactic patterns could be timeconsuming and error-prone, we learn syntactic patterns automatically from data. There have been extensive works on mining opinion words and opinion targets by syntactic pattern learning. Riloff and Wiebe (2003) performed pattern learning through bootstrapping while extracting subjective expressions. Zhuang et al. (2006) obtained various dependency relationship templates from an annotated movie corpus and applied them to supervised opinion words/targets extraction. Kobayashi et al. (2007) adopted a supervised learning technique to search for useful syntactic patterns as contextual clues. Our approach is similar to (Wiebe and Riloff, 2005) and (Xu et al., 2013), all of which apply syntactic pattern learning and adopt self-learning strategy. However, the task of (Wiebe and Riloff, 2005) was to classify"
P13-1173,D09-1159,0,0.157668,"arget/pattern candidates and apply random walking to estimate confidence of them. Thus, confidence of pattern is considered in a unified process. Patterns that often extract false opinion relations will have low confidence, and terms introduced by low-confidence patterns will also have low confidence accordingly. This could potentially improve the extraction accuracy. In the second stage, we identify the hard cases, which aims to filter out false opinion targets and extract long-tail opinion targets. Previous supervised methods have been shown to achieve stateof-the-art results for this task (Wu et al., 2009; Jin and Ho, 2009; Li et al., 2010). However, the big challenge for fully supervised method is the lack of annotated training data. Therefore, we adopt a self-learning strategy. Specifically, we employ a semi-supervised classifier to refine the target results from the first stage, which uses some highly confident target candidates as the initial labeled examples. Then opinion words are also refined. Our main contributions are as follows: • We propose a Sentiment Graph Walking algorithm to mine opinion words and opinion targets from reviews, which naturally incorporates confidence of syntactic"
P13-1173,C10-2167,0,0.181247,"Missing"
P13-1173,H05-2017,0,\N,Missing
P13-1173,P12-1043,0,\N,Missing
P14-1030,P12-1036,0,0.260165,"opinion targets/words extraction? Is it beneficial to consider these two types of relations together for the extraction? To our best knowlIntroduction In opinion mining, extracting opinion targets and opinion words are two fundamental subtasks. Opinion targets are objects about which users’ opinions are expressed, and opinion words are words which indicate opinions’ polarities. Extracting them can provide essential information for obtaining fine-grained analysis on customers’ opinions. Thus, it has attracted a lot of attentions (Hu and Liu, 2004b; Liu et al., 2012; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012). 314 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 314–324, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics edge, these problems have seldom been studied before (see Section 2). 2) Ignoring word preference. When employing opinion relations to perform mutual reinforcing extraction between opinion targets and opinion words, previous methods depended on opinion associations among words, but seldom considered word preference. Word preference denotes a word’s preferred collocations. Intuitively, the confid"
P14-1030,H05-1043,0,0.621533,"its preferred collocations. It helps to improve the extraction precision. The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods. 1 1) Only considering opinion relations is insufficient. Previous methods mainly focused on employing opinion relations among words for opinion target/word co-extraction. They have investigated a series of techniques to enhance opinion relations identification performance, such as nearest neighbor rules (Liu et al., 2005), syntactic patterns (Zhang et al., 2010; Popescu and Etzioni, 2005), word alignment models (Liu et al., 2012; Liu et al., 2013b; Liu et al., 2013a), etc. However, we are curious that whether merely employing opinion relations among words is enough for opinion target/word extraction? We note that there are additional types of relations among words. For example, “LCD” and “LED” both denote the same aspect “screen” in TV set domain, and they are topical related. We call such relations between homogeneous words as semantic relations. If we have known “LCD” to be an opinion target, “LED” is naturally to be an opinion target. Intuitively, besides opinion relations,"
P14-1030,J11-1002,0,0.620036,"Missing"
P14-1030,D12-1123,1,0.285415,"extraction precision. The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods. 1 1) Only considering opinion relations is insufficient. Previous methods mainly focused on employing opinion relations among words for opinion target/word co-extraction. They have investigated a series of techniques to enhance opinion relations identification performance, such as nearest neighbor rules (Liu et al., 2005), syntactic patterns (Zhang et al., 2010; Popescu and Etzioni, 2005), word alignment models (Liu et al., 2012; Liu et al., 2013b; Liu et al., 2013a), etc. However, we are curious that whether merely employing opinion relations among words is enough for opinion target/word extraction? We note that there are additional types of relations among words. For example, “LCD” and “LED” both denote the same aspect “screen” in TV set domain, and they are topical related. We call such relations between homogeneous words as semantic relations. If we have known “LCD” to be an opinion target, “LED” is naturally to be an opinion target. Intuitively, besides opinion relations, semantic relations may provide additiona"
P14-1030,I08-1038,0,0.0987716,"nformal texts. However, all aforementioned methods only employed opinion relations for the extraction, but ignore considering semantic relations among homogeneous candidates. Moreover, they all ignored word preference in the extraction process. 3 The Proposed Method In this section, we propose our method in detail. We formulate opinion targets/words extraction as a co-ranking task. All nouns/noun phrases are regarded as opinion target candidates, and all adjectives/verbs are regarded as opinion word candidates, which are widely adopted by pervious methods (Hu and Liu, 2004a; Qiu et al., 2011; Wang and Wang, 2008; Liu et al., 2012). Then each candidate will be assigned a confidence and ranked, and the candidates with higher confidence than a threshold will be extracted as the results. Different from traditional methods, besides opinion relations among words, we additionally capture semantic relations among homogeneous candidates. To this end, a heterogeneous undirected graph G = (V, E) is constructed. V = V t ∪ V o denotes the vertex set, which includes opinion target candidates v t ∈ V t and opinion word candidates v o ∈ V o . E denotes the edge set, where eij ∈ E means that there is a relation betwe"
P14-1030,P13-1172,1,0.674507,"ion. The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods. 1 1) Only considering opinion relations is insufficient. Previous methods mainly focused on employing opinion relations among words for opinion target/word co-extraction. They have investigated a series of techniques to enhance opinion relations identification performance, such as nearest neighbor rules (Liu et al., 2005), syntactic patterns (Zhang et al., 2010; Popescu and Etzioni, 2005), word alignment models (Liu et al., 2012; Liu et al., 2013b; Liu et al., 2013a), etc. However, we are curious that whether merely employing opinion relations among words is enough for opinion target/word extraction? We note that there are additional types of relations among words. For example, “LCD” and “LED” both denote the same aspect “screen” in TV set domain, and they are topical related. We call such relations between homogeneous words as semantic relations. If we have known “LCD” to be an opinion target, “LED” is naturally to be an opinion target. Intuitively, besides opinion relations, semantic relations may provide additional rich clues for i"
P14-1030,C10-2090,0,0.0164493,"s confidence would mainly absorb the contributions from its word preferences rather than its all neighbors with opinion relations, which may be beneficial for improving extraction precision. We perform experiments on real-world datasets from different languages and different domains. Results show that our approach effectively improves extraction performance compared to the state-of-the-art approaches. 2 Related Work There are many significant research efforts on opinion targets/words extraction (sentence level and corpus level). In sentence level extraction, previous methods (Wu et al., 2009; Ma and Wan, 2010; Li et al., 2010; Yang and Cardie, 2013) mainly aimed to identify all opinion target/word mentions in sentences. They regarded it as a sequence labeling task, where several classical models were used, such as CRFs (Li et al., 2010) and SVM (Wu et al., 2009). This paper belongs to corpus level extraction, and aims to generate a sentiment lexicon and a target list rather than to identify mentions in senFigure 1: Heterogeneous Graph: OC means opinion word candidates. T C means opinion target candidates. Solid curves and dotted lines respectively mean semantic relations and opinion relations betw"
P14-1030,D09-1159,0,0.00653552,"zed. A candidate’s confidence would mainly absorb the contributions from its word preferences rather than its all neighbors with opinion relations, which may be beneficial for improving extraction precision. We perform experiments on real-world datasets from different languages and different domains. Results show that our approach effectively improves extraction performance compared to the state-of-the-art approaches. 2 Related Work There are many significant research efforts on opinion targets/words extraction (sentence level and corpus level). In sentence level extraction, previous methods (Wu et al., 2009; Ma and Wan, 2010; Li et al., 2010; Yang and Cardie, 2013) mainly aimed to identify all opinion target/word mentions in sentences. They regarded it as a sequence labeling task, where several classical models were used, such as CRFs (Li et al., 2010) and SVM (Wu et al., 2009). This paper belongs to corpus level extraction, and aims to generate a sentiment lexicon and a target list rather than to identify mentions in senFigure 1: Heterogeneous Graph: OC means opinion word candidates. T C means opinion target candidates. Solid curves and dotted lines respectively mean semantic relations and opin"
P14-1030,P13-1161,0,0.0642834,"e contributions from its word preferences rather than its all neighbors with opinion relations, which may be beneficial for improving extraction precision. We perform experiments on real-world datasets from different languages and different domains. Results show that our approach effectively improves extraction performance compared to the state-of-the-art approaches. 2 Related Work There are many significant research efforts on opinion targets/words extraction (sentence level and corpus level). In sentence level extraction, previous methods (Wu et al., 2009; Ma and Wan, 2010; Li et al., 2010; Yang and Cardie, 2013) mainly aimed to identify all opinion target/word mentions in sentences. They regarded it as a sequence labeling task, where several classical models were used, such as CRFs (Li et al., 2010) and SVM (Wu et al., 2009). This paper belongs to corpus level extraction, and aims to generate a sentiment lexicon and a target list rather than to identify mentions in senFigure 1: Heterogeneous Graph: OC means opinion word candidates. T C means opinion target candidates. Solid curves and dotted lines respectively mean semantic relations and opinion relations between two candidates. Thus, to resolve thes"
P14-1030,D10-1006,0,0.701566,"s between two opinion target candidates. E oo ⊂ E represents the semantic relations between two opinion word candidates. E to ⊂ E represents the opinion relations between opinion target candidates and opinion word candidates. Based on different relation types, we used three t t o o matrices Mtt ∈ R|V |×|V |, Moo ∈ R|V |×|V | t o and Mto ∈ R|V |×|V |to record the association weights between any two vertices, respectively. Section 3.4 will illustrate how to construct them. In terms of considering semantic relations among words, our method is related with several approaches based on topic model (Zhao et al., 2010; Moghaddam and Ester, 2011; Moghaddam and Ester, 2012a; Moghaddam and Ester, 2012b; Mukherjee and Liu, 2012). The main goals of these methods weren’t to extract opinion targets/words, but to categorize all given aspect terms and sentiment words. Although these models could be used for our task according to the associations between candidates and topics, solely employing semantic relations is still one-sided and insufficient to obtain expected performance. Furthermore, there is little work which considered these two types of relations globally (Su et al., 2008; Hai et al., 2012; Bross and Ehri"
P14-1030,C10-1074,0,\N,Missing
P14-1030,H05-2017,0,\N,Missing
P14-1030,C10-2167,0,\N,Missing
P14-1032,W08-0336,0,0.0315664,"Missing"
P14-1032,J93-1003,0,0.0961909,"sed on the observation that product features are often commented on by similar syntactic structures, it is natural to use patterns to capture common syntactic constituents around product features. Popescu and Etzioni (2005) designed some syntactic patterns to search for product feature candidates and then used Pointwise Mutual Information (PMI) to remove noise terms. Qiu et al. (2009) proposed eight heuristic syntactic rules to jointly extract product features and sentiment lexicons, where a bootstrapping algorithm named Double 337 specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let λ(t) denotes the LRT score of a product feature candidate t, Propagation was applied to expand a given seed set. Zhang et al. (2010) improved Qiu’s work by adding more feasible syntactic patterns, and the HITS algorithm (Kleinberg, 1999) was employed to rank candidates. Moghaddam and Ester (2010) extracted product features by automatical opinion pattern mining. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Wu et al. (2009) proposed a phrase level dependency parsing for mining aspects and featu"
P14-1032,H05-1043,0,0.0877431,"ilt to capture lexical semantic clue. At the same time, a semi-supervised convolutional neural model (Collobert et al., 2011) is employed to encode contextual semantic clue. Finally, the two kinds of semantic clues are com2 Related Work In product feature mining task, Hu and Liu (2004) proposed a pioneer research. However, the association rules they used may potentially introduce many noise terms. Based on the observation that product features are often commented on by similar syntactic structures, it is natural to use patterns to capture common syntactic constituents around product features. Popescu and Etzioni (2005) designed some syntactic patterns to search for product feature candidates and then used Pointwise Mutual Information (PMI) to remove noise terms. Qiu et al. (2009) proposed eight heuristic syntactic rules to jointly extract product features and sentiment lexicons, where a bootstrapping algorithm named Double 337 specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let λ(t) denotes the LRT score of a product feature candidate t, Propagation was applied to expand a given seed set. Zhang et al. (2010) improved Qiu’s work by adding more feasible syntactic patterns, and th"
P14-1032,P11-1016,0,0.0835646,"Missing"
P14-1032,P10-1040,0,0.303536,"to find the fm tuner in a very similar case in Example 1(a), where the product is mentioned by using player instead of mp3. Similarly, it may also fail on Example 1(b), just with have replaced by support. In essence, syntactic pattern is 336 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336–346, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics bined by a Label Propagation algorithm. In the proposed method, words are represented by continuous vectors, which capture latent semantic factors of the words (Turian et al., 2010). The vectors can be unsupervisedly trained on large scale corpora, and words with similar semantics will have similar vectors. This enables our method to be less sensitive to lexicon change, so that the data sparsity problem can be alleviated . The contributions of this paper include: • It uses semantics of words to encode contextual clues, which exploits deeper level information than syntactic constituents. As a result, it mines product features more accurately than syntaxbased methods. • It exploits semantic similarity between words to capture lexical clues, which is shown to be more effect"
P14-1032,P12-1043,0,0.0293802,"Missing"
P14-1032,D12-1123,1,0.817518,"ity seeds, we employ a Domain Relevance Measure (DRM) (Jiang and Tan, 2010), which combines term frequency with a domain3.2.1 Learning Word Embedding for Semantic Representation Given a sequence of training words W = {w1 , w2 , ..., wm }, the goal of the Skip-gram model is to learn a continuous vector space EB = {e1 , e2 , ..., em }, where ei is the word embedding of wi . The training objective is to maximize the 1 Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus. 2 The df (t) part of the original DRM is slightly modified because we want a tf × idf -like scheme (Liu et al., 2012). 338 average log probability of using word wt to predict a surrounding word wt+j , to rank candidates. This could potentially improve the ability of mining infrequent product features. Formally, we create a semantic similarity graph m X X 1 G = (V, E, W ), where V = {Vs ∪ Vc } is the ˆ = argmax EB log p(wt+j |wt ; et ) vertex set, which contains the labeled seed set Vs et ∈EB m t=1 −c≤j≤c,j6=0 and the unlabeled candidate set Vc ; E is the edge (3) set which connects every vertex pair (u, v), where where c is the size of the training window. Basiu, v ∈ V ; W = {wuv : cos(EBu , EBv )} is a call"
P14-1032,D09-1159,0,0.0907747,"Double 337 specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let λ(t) denotes the LRT score of a product feature candidate t, Propagation was applied to expand a given seed set. Zhang et al. (2010) improved Qiu’s work by adding more feasible syntactic patterns, and the HITS algorithm (Kleinberg, 1999) was employed to rank candidates. Moghaddam and Ester (2010) extracted product features by automatical opinion pattern mining. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Wu et al. (2009) proposed a phrase level dependency parsing for mining aspects and features of products. As discussed in the first section, syntactic patterns often suffer from data sparsity. Furthermore, most pattern-based methods rely on term frequency, which have the limitation of finding infrequent but important product features. A recent research (Xu et al., 2013) extracted infrequent product features by a semi-supervised classifier, which used word-syntactic pattern co-occurrence statistics as features for the classifier. However, this kind of feature is still sparse for infrequent candidates. Our metho"
P14-1032,P13-1173,1,0.843268,"ndidates. Moghaddam and Ester (2010) extracted product features by automatical opinion pattern mining. Zhuang et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to supervised movie feature extraction. Wu et al. (2009) proposed a phrase level dependency parsing for mining aspects and features of products. As discussed in the first section, syntactic patterns often suffer from data sparsity. Furthermore, most pattern-based methods rely on term frequency, which have the limitation of finding infrequent but important product features. A recent research (Xu et al., 2013) extracted infrequent product features by a semi-supervised classifier, which used word-syntactic pattern co-occurrence statistics as features for the classifier. However, this kind of feature is still sparse for infrequent candidates. Our method adopts a semantic word representation model, which can train dense features unsupervisedly on a very large corpus. Thus, the data sparsity problem can be alleviated. 3 λ(t) = pk11 (1 − p1 )n1 −k1 pk22 (1 − p2 )n2 −k2 (1) where k1 and k2 are the frequencies of t in the review corpus R and a background corpus1 B, n1 and n2 are the total number of terms"
P14-1032,C10-2167,0,0.207185,"Missing"
P14-1032,de-marneffe-etal-2006-generating,0,\N,Missing
P14-1032,H05-2017,0,\N,Missing
P15-1017,C12-1033,0,0.0799281,"Missing"
P15-1017,P09-2093,0,0.0476394,"s and structure-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues (such as sequences and parse trees) into feature vectors. Ahn (2006) uses the lexical features(e.g., full word, pos tag), syntactic features (e.g., dependency features) and externalknowledge features(WordNet) to extract the event. Inspired by the hypothesis of “One Sense Per Dis174 course”(Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the cross-event and cross-entity inference for the ACE event task. Although these approaches achieve high performance, featurebased methods suffer from the problem of selecting a suitable feature set when converting the classification clues into feature vectors. In structure-based methods, researchers treat event extraction as the task of predicting the structure of the event in a sentence. McClosky et al. (2011) casted the problem of biomedical event extraction as a dependency parsing problem. Li et al. (2013) presented a joint framew"
P15-1017,P11-1113,0,0.19037,"der the following sentence with an ambiguous word beats: S1: Obama beats McCain. S2: Tyson beats his opponent . In S1, beats is a trigger of type Elect. However, in S2, beats is a trigger of type Attack, which is more common than type Elect. Because of the ambiguity, a traditional approach may mislabel beats in S1 as a trigger of Attack. However, if we have the priori knowledge that Obama and McCain are presidential contenders, we have ample evidence to predict that beats is a trigger of type Elect. We call these knowledge lexical-level clues. To represent such features, the existing methods (Hong et al., 2011) often rely on human ingenuity, which is a time-consuming process and lacks generalization. Furthermore, traditional lexical features in previous methods are a one-hot representation, which may suffer from the data sparsity problem and may not be able to adequately capture the semantics of the words (Turian et al., 2010). To identify events and arguments more precisely, previous methods often captured contextual features, such as syntactic features, which aim to understand how facts are tied together from a larger field of view. For example, in S3, there are two events that share three argumen"
P15-1017,P14-1062,0,0.0101148,"tence-level features. The semantic interactions between the predicted trigger words and argument candidates are crucial for argument classification. Therefore, we propose three types of input that the DMCNN uses to capture these important clues: Figure 2 assumes that word embedding has size dw = 4, position embedding has size dp = 1 and event-type embedding has size de = 1. Let xi ∈ Rd be the d-dimensional vector representation corresponding to the i-th word in the sentence, where d = dw + dp ∗ 2 + de . A sentence of length n is represented as follows: • Context-word feature (CWF): Similar to Kalchbrenner et al. (2014) and Collobert et al. (2011), we take all the words of the whole sentence as the context. CWF is the vector of each word token transformed by looking up word embeddings. x1:n = x1 ⊕ x2 ⊕ ... ⊕ xn (3) where ⊕ is the concatenation operator. Thus, combined word embedding, position embedding and event-type embedding transform an instance as a matrix X ∈ Rn×d . Then, X is fed into the convolution part. • Position feature (PF): It is necessary to spec170 3.2.2 reserve more valuable information without missing the max-pooling value. As shown in Figure 2, the feature map output cj is divided into thre"
P15-1017,P08-1030,0,0.286991,"tences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Current state-of-the-art methods (Li et al., 2014; Li et al., 2013; Hong et al., 2011; Liao and Grishman, 2010; Ji and Grishman, 2008) often use a set of elaborately designed features that are extracted by textual analysis and linguistic 167 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 167–176, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics In Baghdad , a cameraman died when an American tank fired on the Palestine Hotel. det nsubj amod det prep_in advmod nsubj nn det prep_on advcl Figure 1: Event mentions and syntactic parser results of S3. The upper side shows two ev"
P15-1017,W13-3214,0,0.0425143,"Missing"
P15-1017,D14-1181,0,0.00995286,"Missing"
P15-1017,W06-0901,0,0.657785,"sentence may contain more than one event, using only the most important information to represent a sentence, as in the traditional CNN, will miss valuable clues. To resolve this problem, we propose a DMCNN to extract the sentence-level features. The DMCNN uses a dynamic multi-pooling layer to obtain a maximum value for each part of a sentence, which is split by event triggers and event arguments. Thus, the DMCNN is expected to capture more valuable clues compared to traditional CNN methods. • Event-type feature (EF): The event type of a current trigger is valuable for argument classification (Ahn, 2006; Hong et al., 2011; Liao and Grishman, 2010; Li et al., 2013), so we encode event type predicted in the trigger classification stage as an important clue for the DMCNN, as in the PF. 3.2.1 Input This subsection illustrates the input needed for a DMCNN to extract sentence-level features. The semantic interactions between the predicted trigger words and argument candidates are crucial for argument classification. Therefore, we propose three types of input that the DMCNN uses to capture these important clues: Figure 2 assumes that word embedding has size dw = 4, position embedding has size dp ="
P15-1017,P13-1008,0,0.523129,"d may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Current state-of-the-art methods (Li et al., 2014; Li et al., 2013; Hong et al., 2011; Liao and Grishman, 2010; Ji and Grishman, 2008) often use a set of elaborately designed features that are extracted by textual analysis and linguistic 167 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 167–176, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics In Baghdad , a cameraman died when an American tank fired on the Palestine Hotel. det nsubj amod det prep_in advmod nsubj nn det prep_on advcl Figure 1: Event ment"
P15-1017,D14-1198,0,0.213835,"Missing"
P15-1017,P10-1081,0,0.448939,"dering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods. 1 Introduction Event extraction is an important and challenging task in Information Extraction (IE), which aims to discover event triggers with specific types and their arguments. Current state-of-the-art methods (Li et al., 2014; Li et al., 2013; Hong et al., 2011; Liao and Grishman, 2010; Ji and Grishman, 2008) often use a set of elaborately designed features that are extracted by textual analysis and linguistic 167 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 167–176, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics In Baghdad , a cameraman died when an American tank fired on the Palestine Hotel. det nsubj amod det prep_in advmod nsubj nn det prep_on advcl Figure 1: Event mentions and syntactic parser results of S3. The"
P15-1017,P11-1163,0,0.0826368,"Missing"
P15-1017,P10-1040,0,0.015305,"trigger of Attack. However, if we have the priori knowledge that Obama and McCain are presidential contenders, we have ample evidence to predict that beats is a trigger of type Elect. We call these knowledge lexical-level clues. To represent such features, the existing methods (Hong et al., 2011) often rely on human ingenuity, which is a time-consuming process and lacks generalization. Furthermore, traditional lexical features in previous methods are a one-hot representation, which may suffer from the data sparsity problem and may not be able to adequately capture the semantics of the words (Turian et al., 2010). To identify events and arguments more precisely, previous methods often captured contextual features, such as syntactic features, which aim to understand how facts are tied together from a larger field of view. For example, in S3, there are two events that share three arguments as shown in Figure 1. From the dependency relation of nsubj between the argument cameraman and trigger died, we can induce a Victim role to cameraman in the Die event. We call such information sentence-level clues. However, the argument word cameraman and its trigger word fired are in different clauses, and there is n"
P15-1017,P95-1026,0,0.543966,"es have been explored for event extraction. Nearly all of the ACE event extraction use supervised paradigm. We further divide supervised approaches into feature-based methods and structure-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues (such as sequences and parse trees) into feature vectors. Ahn (2006) uses the lexical features(e.g., full word, pos tag), syntactic features (e.g., dependency features) and externalknowledge features(WordNet) to extract the event. Inspired by the hypothesis of “One Sense Per Dis174 course”(Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the cross-event and cross-entity inference for the ACE event task. Although these approaches achieve high performance, featurebased methods suffer from the problem of selecting a suitable feature set when converting the classification clues into feature vectors. In structure-based methods, researchers treat event extraction as the task of predicting the structur"
P15-1017,C14-1220,1,0.29368,"ing such features depends heavily on the performance of pre-existing NLP systems, which could suffer from error propagation. S3: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. To correctly attach cameraman to fired as a Target argument, we must exploit internal semantics over the entire sentence such that the Attack event results in Die event. Recent improvements of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and semantics between words within a sentence (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Zeng et al., 2014) for NLP tasks. CNNs typically use a max-pooling layer, which applies a max operation over the representation of an entire sentence to capture the most useful information. However, in event extraction, one sentence may contain two or more events, and these events may share the argument with different roles. For example, there are two events in S3, namely, the Die event and Attack event. If we use a traditional max-pooling layer and only keep the most important information to represent the sentence, we may obtain the information that depicts “a cameraman died” but miss the information about “Am"
P15-1017,P14-1023,0,\N,Missing
P15-1060,D12-1123,1,0.85578,"dependent and identically distributed. We define the following novel log-likelihood function ln LS , with four forms of regularization corresponding to the four kinds of priors: Figure 3: Prior Feature Extraction values and assign them an aspect prior probability pA,vk , indicating their general probability of being an aspect word. This TF-IDF approach is motivated by the following intuitions: the most frequently mentioned candidates in reviews have the highest probability of being an opinion target and false target words are non-domain specific and frequently appear in a general text corpus (Liu et al., 2012; Liu et al., 2013). For all adjective words, if the words are also included in the online sentiment resource SentiWordNet2 , we assign prior probability ps,vk to suggest that these words are generally recognized as sentiment words. Apart from these general priors, we obtain a small amount of fine-grained information as another type of prior knowledge. This fine-grained prior knowledge serves to indicate the probability of a known aspect word belonging to a specific aspect, denoted as pAj ,vk and an identified sentiment word bearing positive or negative sentiment, denoted as pSj ,vk . For inst"
P15-1060,N10-1122,0,0.55953,"roll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and McDonald, 2008; Zhao et al., 2010). Still, these methods have several important drawbacks. First, inaccurate approximations of the distribution over topics may reduce the computational accuracy. Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by inAspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these"
P15-1060,P10-2050,0,0.0288242,"ation consists in judging whether an opinionated review expresses an overall positive or negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identif"
P15-1060,P12-1036,0,0.626488,"negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and M"
P15-1060,W02-1011,0,0.0243323,"riors for words in reviews and incorporate this information into the learning process of our Sentiment-Aspect Extraction model. We regularize our model based on these priors to constrain the aspect modeling and improve its accuracy. Figure 3 provides an example of how such priors can be applied to a sentence, with φi representing the prior knowledge. Research has found that most aspect words are nouns (or noun phrases), and sentiment is often expressed with adjectives. This additional information has been utilized in previous work on aspect extraction (Hu and Liu, 2004; Benamara et al., 2007; Pang et al., 2002). Inspired by this, we first rely on Part of Speech (POS) Tagging to identify nouns and adjectives. For all noun words, we first calculate their term frequency (TF) in the review corpus, and then compute their inverse document frequency (IDF) from an external Google n-gram corpus1 . Finally, we rank their TF∗IDF Wjk hj vbk j=1 k=1 K X F X (6) Under this architecture, this equation can be explained as the conditional probability from visible unit k to hidden unit j (softmax of words to aspect or sentiment). According to Eq. 6, the conditional probability for the k-th word feature towards the j-"
P15-1060,D10-1101,0,0.0209744,"in online reviews. Methods following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect identification are generally based on standard sequence labeling techniques. The downside of supervised learning is its requirement of large amounts of hand-labeled training data to provide enough information for aspect and opinion identification. Subsequent studies have proposed unsupervised learning methods, especially LDA-based topic modeling, to classify aspects of comments. Specific variants inc"
P15-1060,H05-1043,0,0.0238611,"products or services, on the Internet, especially in the course of e-commerce activities. Analyzing online reviews not only helps customers obtain useful product information, but also provide companies with feedback to enhance their products or service quality. Aspect-based opinion mining enables people to consider much more finegrained analyses of vast quantities of online reviews, perhaps from numerous different merchant sites. Thus, automatic identification of aspects of entities and relevant sentiment polarities in Big Data is a significant and urgent task (Liu, 2012; Pang and Lee, 2008; Popescu and Etzioni, 2005). Identifying aspect and analyzing sentiment words from reviews has the ultimate goal of discerning people’s opinions, attitudes, emotions, etc. towards entities such as products, services, organizations, individuals, events, etc. In this context, aspect-based opinion mining, also known as feature-based opinion mining, aims at extracting and summarizing particular salient aspects of entities and determining relevant sentiment polarities ∗ Corresponding Author: Kang Liu (kliu@nlpr.ia.ac.cn) 616 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th I"
P15-1060,P09-1026,0,0.0245404,"els presented previously. Overall, our main contributions are as follows: 4. Last but not the least, this RBM model is capable of jointly modeling aspect and sentiment information together. 2 Related Work We summarize prior state-of-the-art models for aspect extraction. In their seminal work, Hu and Liu (2004) propose the idea of applying classical information extraction to distinguish different aspects in online reviews. Methods following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect i"
P15-1060,D07-1114,0,0.0138766,"following their approach exploit frequent noun words and dependency relations to extract product features without supervision (Zhuang et al., 2006; Liu et al., 2005; Somasundaran and Wiebe, 2009). These methods work well when the aspect is strongly associated with a single noun, but obtain less satisfactory results when the aspect emerges from a combination of low frequency items. Additionally, rule-based methods have a common shortcoming in failing to group extracted aspect terms into categories. Supervised learning methods (Jin et al., 2009; Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007) such as Hidden Markov Models, one-class SVMs, and Conditional Random Fields have been widely used in aspect information extraction. These supervised approaches for aspect identification are generally based on standard sequence labeling techniques. The downside of supervised learning is its requirement of large amounts of hand-labeled training data to provide enough information for aspect and opinion identification. Subsequent studies have proposed unsupervised learning methods, especially LDA-based topic modeling, to classify aspects of comments. Specific variants include the Multi-Grain LDA"
P15-1060,C08-1135,0,0.00873552,"esses an overall positive or negative opinion. Regarding aspect identification, previous methods can be divided into three main categories: rule-based, supervised, and topic model-based methods. For instance, association rule-based methods (Hu and Liu, 2004; Liu et al., 1998) tend to focus on extracting product feature words and opinion words but neglect connecting product features at the aspect level. Existing rule-based methods typically are not able to group the extracted aspect terms into categories. Supervised (Jin et al., 2009; Choi and Cardie, 2010) and semisupervised learning methods (Zagibalov and Carroll, 2008; Mukherjee and Liu, 2012) were introduced to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and"
P15-1060,D10-1006,0,0.802407,"d to resolve certain aspect identification problems. However, supervised training requires handlabeled training data and has trouble coping with domain adaptation scenarios. Hence, unsupervised methods are often adopted to avoid this sort of dependency on labeled data. Latent Dirichlet Allocation, or LDA for short, (Blei et al., 2003) performs well in automatically extracting aspects and grouping corresponding representative words into categories. Thus, a number of LDA-based aspect identification approaches have been proposed in recent years (Brody and Elhadad, 2010; Titov and McDonald, 2008; Zhao et al., 2010). Still, these methods have several important drawbacks. First, inaccurate approximations of the distribution over topics may reduce the computational accuracy. Second, mixture models are unable to exploit the co-occurrence of topics to yield high probability predictions for words that are sharper than the distributions predicted by inAspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This mod"
P15-1060,H05-2017,0,\N,Missing
P15-1067,D13-1136,0,0.0189846,"Missing"
P15-1067,D12-1110,0,0.00990307,"Missing"
P16-1122,N10-1145,0,0.240893,"attentive models, the attention mechanism under RNN is not well studied. In this work, we analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively. Then we present three new RNN models that add attention information before RNN hidden representation, which shows advantage in representing sentence and achieves new stateof-art results in answer selection task. 1 Introduction Answer selection (AS) is a crucial subtask of the open domain question answering (QA) problem. Given a question, the goal is to choose the answer from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013). Traditional AS models are based on lexical features such as parsing tree edit distance. Neural networks based models are proposed to represent the meaning of a sentence in a vector space and then compare the question and answer candidates in this hidden space (Wang and Nyberg, 2015; Feng et al., 2015), which have shown great success in AS. However, these models represent the question and sentence separately, which may ignore the information subject to the question when representing the answer. For example, given a candidate answer: Michael Jordan abruptly retired from Chic"
P16-1122,J81-4005,0,0.477561,"Missing"
P16-1122,D13-1044,0,0.0865284,"en a question and a set of candidate sentences, one should choose the best sentence from a candidate sentence set that can answer the question. Previous works usually stuck in employing feature engineering, linguistic tools, or external resources. For example, Yih et al. (2013) use semantic features from WordNet to enhance lexical features. Wang and Manning (2007) try to compare the question and answer sentence by their syntactical matching in parse trees. Heilman and Smith (Heilman and Smith, 2010) try to fulfill the matching using minimal edit sequences between their dependency parse trees. Severyn and Moschitti (2013) automate the extraction of discriminative tree-edit features over parsing trees. 1289 While these methods show effectiveness, they might suffer from the availability of additional resources and errors of many NLP tools such as dependency parsing. Recently there are many works use deep learning architecture to represent the question and answer in a same hidden space, and then the task can be converted into a classification or learning-to-rank problem (Feng et al., 2015; Wang and Nyberg, 2015). With the development of attention mechanism, Tan et.al(2015) propose an attention-based RNN models wh"
P16-1122,D13-1170,0,0.00133313,"n based RNN models. (2) We propose three inner attention based RNN models and achieve new state-of-the-art results in answer selection. (3) We use Occam’s Razor to regulate the attention weights which shows advantage in long sentence representation. 2 Related Work Recent years, many deep learning framework has been developed to model the text in a vector space, and then use the embedded representations in this space for machine learning tasks. There are many neural networks architectures for this representation such as convolutional neural networks(Yin et al., 2015), recursive neural networks(Socher et al., 2013) and recurrent neural networks(Mikolov et al., 2011). In this work we propose Inner Attention based RNN (IARNN) for answer selection, and there are two main works which we are related to. 2.1 Attention based Models Many recent works show that attention techniques can improve the performance of machine learning models (Mnih et al., 2014; Zheng et al., 2015). In attention based models, one representation is built with attention (or supervision) from other representation. Weston et al (2014) propose a neural networks based model called Memory Networks which uses an external memory to store the kn"
P16-1122,P15-2116,0,0.345988,"Missing"
P16-1122,D07-1003,0,0.69627,"Missing"
P16-1122,D15-1237,0,0.504874,"models are free from this problem and distribute nearly uniform (orange line) in a sentence. 6000 7000 position in a sentence 8000 9000 10000 end Figure 6: Bi-directional OARNN attention distribution, the horizontal axis is the postion of the word in a sentence that has been normalized from 1 to 10000. where a+ is ground truth answer candidate and a− stands for negative one, the scalar M is a predefined margin. When training result saturates after 50 epoches, we get the attention weight distribution (i.e. sq in Equation 2). The experiment is conducted on two answer selection datasets: WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007). The normalized attention weights is reported in Figure 5. However, the above model use only forward LSTM to build hidden state representation, the attention bias problem may attribute to the biased answer distribution: the useful part of the answer to the question sometimes may located at the end of the sentence. So we try OARNN in bidirectional architecture, where the forward LSTM and backward LSTM are concatenated for hidden representation, The bidirectional attention based LSTM attention distribution is shown in Figure 6. Analysis: As is shown in Figure 5 an"
P16-1122,N13-1106,0,0.120704,"Missing"
P16-1122,P13-1171,0,0.0684648,"e sentence (or document) representation. Commonly there are two ways to get attention from source sentence, either by the whole sentence representation (which they call attentive) or word by word attention (called impatient). 2.2 Answer Selection Answer selection is a sub-task of QA and many other tasks such as machine comprehension. Given a question and a set of candidate sentences, one should choose the best sentence from a candidate sentence set that can answer the question. Previous works usually stuck in employing feature engineering, linguistic tools, or external resources. For example, Yih et al. (2013) use semantic features from WordNet to enhance lexical features. Wang and Manning (2007) try to compare the question and answer sentence by their syntactical matching in parse trees. Heilman and Smith (Heilman and Smith, 2010) try to fulfill the matching using minimal edit sequences between their dependency parse trees. Severyn and Moschitti (2013) automate the extraction of discriminative tree-edit features over parsing trees. 1289 While these methods show effectiveness, they might suffer from the availability of additional resources and errors of many NLP tools such as dependency parsing. Re"
P16-1201,W06-0901,0,0.0223113,"ackle with it, our basic ED approach follows representation-based paradigm, which has been demonstrated effective in the cross-domain situation (Nguyen and Grishman, 2015). Related Work Event extraction is an increasingly hot and challenging research topic in NLP. Many approaches have been proposed for this task. Nearly all the existing methods on ACE event task use supervised paradigm. We further divide them into featurebased methods and representation-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the comb"
P16-1201,P98-1013,0,0.1115,"rom frames to event-types. Finally, we improve the performance of event detection and achieve a new state-of-the-art result by using the events automatically detected from FN. 1 Introduction In the ACE (Automatic Context Extraction) event extraction program, an event is represented as a structure consisting of an event trigger and a set of arguments. This paper tackles with the event detection (ED) task, which is a crucial component in the overall task of event extraction. The goal of ED is to identify event triggers and their corresponding event types from the given documents. FrameNet (FN) (Baker et al., 1998; Fillmore et al., 2003) is a linguistic resource storing considerable information about lexical and predicateargument semantics. In FN, a frame is defined as a composition of a Lexical Unit (LU) and a set of Frame Elements (FE). Most frames contain a set of exemplars with annotated LUs and FEs (see Figure 2 and Section 2.2 for details). From the above definitions of events and frames, it is not hard to find that the frames defined in FN share highly similar structures as the events defined in ACE. Firstly, the LU of a Frame plays a similar role as the trigger of an event. ACE defines the trig"
P16-1201,P15-1017,1,0.792201,"with 150,000 annotated exemplars. Eight relations are defined between frames in FN, but in this paper we only use the following three of them because the others do not satisfy our hypotheses (see section 4.2): Inheritance: A inherited from B indicates that A must correspond to an equally or more specific fact about B. It is a directional relation. See also: A and B connected by this relation indicates that they are similar frames. Perspective on: A and B connected by this relation means that they are different points-of-view about the same fact (i.e. Receiving vs. Transfer). event detection (Chen et al., 2015; Nguyen and Grishman, 2015). Nguyen and Grishman (2015) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN inclu"
P16-1201,J14-1002,0,0.0257536,"e fact (i.e. Receiving vs. Transfer). event detection (Chen et al., 2015; Nguyen and Grishman, 2015). Nguyen and Grishman (2015) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN include question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), textual entailment (Burchardt et al., 2009) and paraphrase recognition (Pad´o and Lapata, 2005). This is the first work to explore the application of FN to event detection. 2.3 Alike to existing work, we model event detection (ED) as a word classification task. In the ED task, each word in the given sentence is treated as a candidate trigger and the goal is to classify each of these candidates into one of 34 classes (33 event types plus a NA class). However, in t"
P16-1201,C04-1100,0,0.0228391,"2015). Nguyen and Grishman (2015) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN include question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), textual entailment (Burchardt et al., 2009) and paraphrase recognition (Pad´o and Lapata, 2005). This is the first work to explore the application of FN to event detection. 2.3 Alike to existing work, we model event detection (ED) as a word classification task. In the ED task, each word in the given sentence is treated as a candidate trigger and the goal is to classify each of these candidates into one of 34 classes (33 event types plus a NA class). However, in this work, as we assumed that the LU of a frame is analogical to the trigger of an event, we only treat the"
P16-1201,P14-1136,0,0.0125522,"-of-view about the same fact (i.e. Receiving vs. Transfer). event detection (Chen et al., 2015; Nguyen and Grishman, 2015). Nguyen and Grishman (2015) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN include question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), textual entailment (Burchardt et al., 2009) and paraphrase recognition (Pad´o and Lapata, 2005). This is the first work to explore the application of FN to event detection. 2.3 Alike to existing work, we model event detection (ED) as a word classification task. In the ED task, each word in the given sentence is treated as a candidate trigger and the goal is to classify each of these candidates into one of 34 classes (33 event types plus a NA cl"
P16-1201,P15-2060,0,0.619422,"ated exemplars. Eight relations are defined between frames in FN, but in this paper we only use the following three of them because the others do not satisfy our hypotheses (see section 4.2): Inheritance: A inherited from B indicates that A must correspond to an equally or more specific fact about B. It is a directional relation. See also: A and B connected by this relation indicates that they are similar frames. Perspective on: A and B connected by this relation means that they are different points-of-view about the same fact (i.e. Receiving vs. Transfer). event detection (Chen et al., 2015; Nguyen and Grishman, 2015). Nguyen and Grishman (2015) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN include question answering (Naray"
P16-1201,P11-1113,0,0.149267,"-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the combinational features of triggers and arguments. Liu et al. (2016) proposed a global inference approach to employ both latent local and global information for event detection. In representation-based methods, candidate event mentions are represented by embedding, which typically are fed into neural networks. Two similarly related work has been proposed on 3 3.1 Basic Event Detection Model Model We employ a simple three-layer (a input layer, a hidden layer and a so"
P16-1201,P08-1030,0,0.227743,"topic in NLP. Many approaches have been proposed for this task. Nearly all the existing methods on ACE event task use supervised paradigm. We further divide them into featurebased methods and representation-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the combinational features of triggers and arguments. Liu et al. (2016) proposed a global inference approach to employ both latent local and global information for event detection. In representation-based methods, candidate event mentions are represented by embedding, wh"
P16-1201,D14-1181,0,0.00150818,"o et al., 2003; Erhan et al., 2010). This paper uses unsupervised learned word embeddings as the source of base features. We use the Skipgram model (Mikolov et al., 2013) to learn word embeddings on the NYT corpus5 . Given a sentence, we concatenate the embedding vector of the candidate trigger and the average embedding vector of the words in the sentence as the input to our model. We train the model using a simple optimization technique called stochastic gradient descent (SGD) over shuffled mini-batches with the Adadelta update rule (Zeiler, 2012). Regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012). The experiments show that this simple model is surprisingly effective for event detection. 4 Event Detection in FrameNet To detect events in FN, we first learned the basic ED model based on ACE labeled corpus and then employ it to generate initial judgements (possible event types with confidence values) for each sentence in FN. Then, we apply a set of constraints for global inference based on the PSL model. 4.1 Probabilistic Soft Logic PSL is a framework for collective, probabilistic reasoning in relational domains (Kimmig et al., 2012; Bach et al., 2013). Similar to Ma"
P16-1201,P13-1008,0,0.227668,"nvert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the combinational features of triggers and arguments. Liu et al. (2016) proposed a global inference approach to employ both latent local and global information for event detection. In representation-based methods, candidate event mentions are represented by embedding, which typically are fed into neural networks. Two similarly related work has been proposed on 3 3.1 Basic Event Detection Model Model We employ a simple three-layer (a input layer, a hidden layer and a soft-max output layer) Artificial Neural Networks (ANNs) (Hagan et al., 1996) to model the ED"
P16-1201,H05-1108,0,0.0491284,"Missing"
P16-1201,D07-1002,0,0.0161106,"5) employed Convolutional Neural Networks (CNNs) to automatically extract sentence-level features for event detection. Chen et al. (2015) proposed dynamic multi-pooling operation on CNNs to capture better sentence-level features. FrameNet is a typical resource for framesemantic parsing, which consists of the resolution of predicate sense into a frame, and the analysis of the frame’s participants (Thompson et al., 2003; Giuglea and Moschitti, 2006; Hermann et al., 2014; Das et al., 2014). Other tasks which have been studied based on FN include question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007), textual entailment (Burchardt et al., 2009) and paraphrase recognition (Pad´o and Lapata, 2005). This is the first work to explore the application of FN to event detection. 2.3 Alike to existing work, we model event detection (ED) as a word classification task. In the ED task, each word in the given sentence is treated as a candidate trigger and the goal is to classify each of these candidates into one of 34 classes (33 event types plus a NA class). However, in this work, as we assumed that the LU of a frame is analogical to the trigger of an event, we only treat the LU annotated in the give"
P16-1201,P95-1026,0,0.19917,"lenging research topic in NLP. Many approaches have been proposed for this task. Nearly all the existing methods on ACE event task use supervised paradigm. We further divide them into featurebased methods and representation-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the combinational features of triggers and arguments. Liu et al. (2016) proposed a global inference approach to employ both latent local and global information for event detection. In representation-based methods, candidate event mentions are repr"
P16-1201,P10-1081,0,0.118674,"ed methods and representation-based methods. In feature-based methods, a diverse set of strategies has been exploited to convert classification clues into feature vectors. Ahn (2006) uses the lexical features(e.g., full word), syntactic features (e.g., dependency features) and externalknowledge features(WordNet (Miller, 1995)) to extract the event. Inspired by the hypothesis of One Sense Per Discourse (Yarowsky, 1995), Ji and Grishman (2008) combined global evidence from related documents with local decisions for the event extraction. To capture more clues from the texts, Gupta and Ji (2009), Liao and Grishman (2010) and Hong et al. (2011) proposed the crossevent and cross-entity inference for the ACE event task. Li et al. (2013) proposed a joint model to capture the combinational features of triggers and arguments. Liu et al. (2016) proposed a global inference approach to employ both latent local and global information for event detection. In representation-based methods, candidate event mentions are represented by embedding, which typically are fed into neural networks. Two similarly related work has been proposed on 3 3.1 Basic Event Detection Model Model We employ a simple three-layer (a input layer,"
P16-1201,C98-1013,0,\N,Missing
P16-1201,P09-2093,0,\N,Missing
P17-1019,P16-1154,0,0.469683,"nd-to-end models for KB-based question answering, such as GenQA (Yin et al., 2016), were able to retrieve facts from KBs with neural models. Unfortunately, they cannot copy SUs from the question in generating answers. Moreover, they could not deal with complex questions which need to utilize multiple facts. In addition, existing approaches for conversational (Dialogue) systems are able to generate natural utterances (Serban et al., 2016; Li et al., 2016) in sequence-tosequence learning (Seq2Seq). But they cannot interact with KB and answer information-inquired questions. For example, CopyNet (Gu et al., 2016) is able to copy words from the original source in generating the target through incorporating copying mechanism in conventional Seq2Seq learning, but they cannot retrieve SUs from external memory (e.g. KBs, Texts, etc.). • We propose a neural network based model, named as COREQA, by incorporating copying and retrieving mechanism in Seq2Seq learning. In our knowledge, it is the first end-to-end model that could answer complex questions in a natural way. • We implement experiments on both synthetic and real-world datasets. The experimental results demonstrate that the proposed model could be mo"
P17-1019,P16-1014,0,0.183291,"s, however, some words in sequences are “no-meaning” symbols and it is improper to encode them in encoding and decoding processes. For example, generating the response “Of course, read” for replying the message “Can you read the word ‘read’?” should not consider the meaning of the second “read”. By incorporating the copying mechanism, the decoder could directly copy the sub-sequences of source into the target (Vinyals et al., 2015). The basic approach is to jointly predict the indexes of the target word in the fixed vocabulary and/or matched positions in the source sequences (Gu et al., 2016; Gulcehre et al., 2016). The Attention Mechanism The prediction model of classical decoders for each target word yi share the same context vector c. However, a fixed vector is not enough to obtain a better result on generating a long targets.The attention mechanism in the decoding can dynamically choose context ct at each time step (Bahdanau et al., 2014), for example, representing ct as the weighted sum of the source states {ht }, ct = XLX eρ(st−1 ,hi ) αti hi ; αti = P ρ(s ,h0 ) t−1 i i=1 i0 e The Copying Mechanism 3 COREQA To generate natural answers for information inquired questions, we should first recognize k"
P17-1019,D16-1127,0,0.0186877,"om the value of “gender”). And we even need to deal with some morphological variants (e.g. “Singapore” in KB but “Singaporean” in answer). Although existing end-to-end models for KB-based question answering, such as GenQA (Yin et al., 2016), were able to retrieve facts from KBs with neural models. Unfortunately, they cannot copy SUs from the question in generating answers. Moreover, they could not deal with complex questions which need to utilize multiple facts. In addition, existing approaches for conversational (Dialogue) systems are able to generate natural utterances (Serban et al., 2016; Li et al., 2016) in sequence-tosequence learning (Seq2Seq). But they cannot interact with KB and answer information-inquired questions. For example, CopyNet (Gu et al., 2016) is able to copy words from the original source in generating the target through incorporating copying mechanism in conventional Seq2Seq learning, but they cannot retrieve SUs from external memory (e.g. KBs, Texts, etc.). • We propose a neural network based model, named as COREQA, by incorporating copying and retrieving mechanism in Seq2Seq learning. In our knowledge, it is the first end-to-end model that could answer complex questions in"
P17-1019,D12-1035,0,0.0381274,"Missing"
P17-1019,P14-1090,0,0.0272405,"Missing"
P17-1019,P15-1128,0,0.0243085,"Missing"
P17-1019,W16-0106,0,0.0404104,"sually are predicted using a (conditional) language model (e.g. “born” in Figure 1); 2) the major entities/phrases are selected from the source question (e.g. “Jet Li”); 3) the answering entities/phrases are retrieved from the corresponding KB (e.g. “Beijing”). In addition, some words or phrases even need to be inferred from related knowledge (e.g. “He” should be inferred from the value of “gender”). And we even need to deal with some morphological variants (e.g. “Singapore” in KB but “Singaporean” in answer). Although existing end-to-end models for KB-based question answering, such as GenQA (Yin et al., 2016), were able to retrieve facts from KBs with neural models. Unfortunately, they cannot copy SUs from the question in generating answers. Moreover, they could not deal with complex questions which need to utilize multiple facts. In addition, existing approaches for conversational (Dialogue) systems are able to generate natural utterances (Serban et al., 2016; Li et al., 2016) in sequence-tosequence learning (Seq2Seq). But they cannot interact with KB and answer information-inquired questions. For example, CopyNet (Gu et al., 2016) is able to copy words from the original source in generating the"
P17-1019,P16-5005,0,0.0202229,"er (not the input word embedding), Wpr ∈ R(dh +di +df )×do (di , dh and df indicate the size of input word vector, RNN decoder hidden state and fact representation respectively), and cqt and ckbt are the temporary memory of reading MQ and MKB at time t (see Section 3.4.3). Copy-mode: The score for “copying” the word xj from question Q is calculated as ψco (yt = xj ) = DN N2 (hj , st , histQ ) , where DN N2 is a neural network function with a two-layer MLP and histQ ∈ RLX is an accumulated vector which record the attentive history for each word in question (similar with the coverage vector in (Tu et al., 2016)). Retrieve-mode: The score for “retrieving” the entity word vj from retrieval facts (“Object” part) is calculated as ψre (yt = vj ) = DN N3 (fj , st , histKB ) , where DN N3 is also a neural network function and histKB ∈ RLF is an accumulated vector which record the attentive history for each fact in candidate facts. p(yt |st , yt−1 , MQ , MKB ) = ppr (yt |st , yt−1 , ct ) · pm (pr|st , yt−1 )+ pco (yt |st , yt−1 , MQ ) · pm (co|st , yt−1 )+ (2) pre (yt |st , yt−1 , MKB ) · pm (re|st , yt−1 ) where pr, co and re stand for the predict-mode, the copy-mode and the retrieve-mode, respectively, pm"
P17-1021,D13-1160,0,0.664994,"only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural n"
P17-1021,D14-1067,0,0.627295,"e-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbase"
P17-1021,P13-1042,0,0.0164781,"s are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of de"
P17-1021,J84-3009,0,0.10831,"Missing"
P17-1021,P15-1026,0,0.527104,"-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of"
P17-1021,P15-1033,0,0.00797898,"n different words of the same question. The extent of attention can be measured by the relatedness between each word representation hj and an answer aspect embedding ei . We propose the following formulas to calculate the weights. KB Embedding Matrix ?? ?6 ?? ?? ?? ?? Figure 2: The architecture of the proposed crossattention based neural network. Note that only one aspect(in orange color) is depicted for clarity. The other three aspects follow the same way. be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long sentences. Note that if we use unidirectional LSTM, the outcome of a specific word contains only the information of the words before it, whereas the words after it are not taken into account. To avoid this, we employ bidirectional LSTM as Bahdanau (2015) does, which consists of both forward and backward networks. The forward LSTM handles the question from left to right, and the backward LSTM processes in the reverse order. Thus, we could acquire two hidden state sequences, one − → − → − → from the forward one (h1 , h2 , ..., hn ) and the other ← − ← − ← − f"
P17-1021,P16-1122,1,0.323366,"tion mechanism has been widely used in different areas. Bahdanau et al. (2015) first applied attention model in NLP. They improved 228 References the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. Wang et al. (2016) proposed an inner attention mechanism that the attention was imposed directly to the input. And their experiment on answer selection showed the advantage of inner attention compared with traditional attention methods. Yin et al. (2016) tackled simple question answering by an attentive convolutional neural network. They stacked an attentive max-pooling above convolution layer to model the relationship between predicates and question patterns. Our approach differs from previous work in that we use attentions to help represent questions dynamically, not generating current word from vocabulary as"
P17-1021,D11-1142,0,0.0123893,"ry networks framework (Sukhbaatar et al., 2015), and achieves the state-of-the-art performance of endto-end methods. Our approach employs bidirectional LSTM, cross-attention model and global KB information. From the results, we observe that our approach achieves the best performance of all the end-to-end methods on WebQuestions. Bordes et al. (2014b; 2014a; 2015) all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions. Also note that Bordes et al. (2015) uses additional training data such as Reverb (Fader et al., 2011) and their original dataset SimpleQuestions. Dong et al. (2015) employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect to the words in the question. Besides, our approach employs the global KB information. So, we believe that the results faithfully show that the proposed approach is more effective than the other competitive methods. Model Analysis In this part, we further discuss the impacts of the components of our model. Table 2 indicates the effectiveness of different parts in the model. Methods es the last hidden state as the q"
P17-1021,C16-1226,0,0.0424091,"2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of the questions and"
P17-1021,P16-1220,0,0.128615,"2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). Different from previous methods, NNbased methods represent both of the questions and"
P17-1021,D14-1071,0,0.0745986,"KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. The most similar work to ours is Dong et al. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectively. The difference is that our approach uses cross-attention mechanism for each unique answer aspect, so the question representation is not fixed to only three types. Moreover, we utilize the global KB information. Xu et al. (2016a; 2016"
P17-1021,D13-1161,0,0.0155682,"such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, wi"
P17-1021,P14-1090,0,0.199154,"Missing"
P17-1021,Q16-1010,0,0.0486223,"Missing"
P17-1021,P15-1128,0,0.348258,"h the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers. Recently, with the progress of deep learning, neural network-based (NN-b"
P17-1021,D15-1044,0,0.0237223,"ssive power of Semantic Web data while at the same time hiding their complexity behind an intuitive and easy-to-use interface. At the same time the 5.2 Attention-based Model The attention mechanism has been widely used in different areas. Bahdanau et al. (2015) first applied attention model in NLP. They improved 228 References the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. Wang et al. (2016) proposed an inner attention mechanism that the attention was imposed directly to the input. And their experiment on answer selection showed the advantage of inner attention compared with traditional attention methods. Yin et al. (2016) tackled simple question answering by an attentive convolutional neural network. They stacked an attentive max-pooling above convolution layer to model the relationship between pred"
P17-1021,P14-2105,0,0.0745915,"imilarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. The most similar work to ours is Dong et al. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectiv"
P17-1021,P16-2033,0,0.0279007,"Missing"
P17-1021,C16-1164,0,0.445608,"Missing"
P17-1021,P09-1110,0,0.0250529,"Seaborne, 2008). However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years. Given natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from"
P17-1034,P12-2034,0,0.186749,"Missing"
P17-1034,E14-1030,0,0.0528341,"am. Subsequent work devoted most 367 Features LF LF+BF LF+BF abundant efforts to explore effective features and spammerlike clues. Linguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features. Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage. Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features. Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective t"
P17-1034,D15-1038,0,0.0212702,"Missing"
P17-1034,P13-2039,0,0.0204841,"forms effectively in the cold-start spam detection task. 2 Related Work Jindal and Liu (2008) make the first step to detect review spam. Subsequent work devoted most 367 Features LF LF+BF LF+BF abundant efforts to explore effective features and spammerlike clues. Linguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features. Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage. Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the"
P17-1034,D16-1187,0,0.152247,"in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage. Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features. Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective than the RNN and the traditional discrete manual linguistic features. Hovy (2016) used N-gram generative models to produce reviews and evaluated their effectiveness. P 54.5 63.4 69.1 R 71.1 52.6 63.5 F1 61.7 57.5 66.2 A 55.9 61.1 67.5 R 80.8 61.2 78.2 F1 64.6 59.6 65.7 A 55.8 58.5 59.1 (a) Hotel Features LF LF+BF LF+BF abundant P 53.8 58.1 56.6"
P17-1034,P16-1094,0,0.0300613,". Nevertheless, there is ample textual and behavioral information contained in the abundant reviews posted by the existing reviewers (Figure 1). We could employ behavioral information of existing similar reviewers to a new reviewer to approximate his behavioral features. We argue that a reviewer’s individual characteristics such as background information, motivation, and interactive behavior style have a great influence on a reviewer’s textual and behavioral information. So the textual information and the behavioral information of a reviewer are correlated with each other (similar argument in Li et al. (2016)). For example, the students of the college are likely to choose the youth hostel during summer vacation and tend to comment the room price in their reviews. But the financial analysts on a business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews. To augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews. There are several ways to model the textual information of the review spam, such"
P17-1034,P14-1147,0,0.0903066,"2 Related Work Jindal and Liu (2008) make the first step to detect review spam. Subsequent work devoted most 367 Features LF LF+BF LF+BF abundant efforts to explore effective features and spammerlike clues. Linguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features. Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage. Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguis"
P17-1034,P16-2057,0,0.0254323,"n book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features. Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective than the RNN and the traditional discrete manual linguistic features. Hovy (2016) used N-gram generative models to produce reviews and evaluated their effectiveness. P 54.5 63.4 69.1 R 71.1 52.6 63.5 F1 61.7 57.5 66.2 A 55.9 61.1 67.5 R 80.8 61.2 78.2 F1 64.6 59.6 65.7 A 55.8 58.5 59.1 (a) Hotel Features LF LF+BF LF+BF abundant P 53.8 58.1 56.6 (b) Restaurant Table 1: SVM classification results across linguistic features (LF, bigrams here (Mukherjee et al., 2013b)), behavioral features (BF: RL, RD, MCS (Mukherjee et al., 2013b)) and behavioral features with abundant behavioral information (BF abundant). Both training and testing use balanced data (50:50). tecting review sp"
P17-1034,D10-1021,0,0.0278585,"Missing"
P17-1034,C12-2131,0,0.0581856,"Missing"
P17-1034,P11-1032,0,0.548247,"are likely to choose the youth hostel during summer vacation and tend to comment the room price in their reviews. But the financial analysts on a business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews. To augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews. There are several ways to model the textual information of the review spam, such as Unigram (Mukherjee et al., 2013c), POS (Ott et al., 2011) and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003). We employ the CNN (Convolutional Neural Network) to model the review text, which has been proved that it can capture complex global semantic information that is difficult to express using traditional discrete manual features (Ren and Zhang, 2016). Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer. An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s beha"
P17-1034,C16-1014,0,0.543623,"he new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews. There are several ways to model the textual information of the review spam, such as Unigram (Mukherjee et al., 2013c), POS (Ott et al., 2011) and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003). We employ the CNN (Convolutional Neural Network) to model the review text, which has been proved that it can capture complex global semantic information that is difficult to express using traditional discrete manual features (Ren and Zhang, 2016). Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer. An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s behavioral features as the new reviewers’ features (detailed in Section 5.3). However, there is abundant behavioral information in the review graph (Figure 1), it is difficult for the traditional discrete manual behavioral features to record the global behavioral information (Wang et al., 2016). Moreover, the traditio"
P17-1034,D13-1170,0,0.00402091,"Missing"
P17-1034,D16-1083,1,0.728552,"rete manual features (Ren and Zhang, 2016). Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer. An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s behavioral features as the new reviewers’ features (detailed in Section 5.3). However, there is abundant behavioral information in the review graph (Figure 1), it is difficult for the traditional discrete manual behavioral features to record the global behavioral information (Wang et al., 2016). Moreover, the traditional features can not capture the reviewer’s individual characteristics, because there is no explicit characteristic tag available in the review system (experiH H B C R R R R Figure 1: Part of review graph simplified from Yelp. ments in Section 5.3). So, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem. By encoding the review graph structure (Figure 1), the proposed model can record the global footprints of the existing reviewers in an unsupervised"
P17-1038,W06-0901,0,0.877637,"Missing"
P17-1038,P98-1013,0,0.155226,"arguments in events and roles of CVTs as the roles of arguments play in the event, respectively. According to the statistics of the Freebase released on 23th April, 2015, there are around 1885 CVTs and around 14 million CVTs instances. After filtering out useless and meaningless CVTs, such as CVTs about user profiles and website information, we select 21 types of CVTs with around 3.8 million instances for experiments, which mainly involves events about education, military, sports and so on. FrameNet3 is a linguistic resource storing information about lexical and predicate argument semantics (Baker et al., 1998). FrameNet contains more than 1, 000 frames and 10, 000 Lexical Units (LUs). Each frame of FrameNet can be taken as a semantic frame of a type of events (Liu et al., 2016). Each frame has a set of lemmas with part of speech tags that can evoke the frame, which are called LUs. For example, appoint.v is a LU of Appointing frame in FrameNet, which can be mapped to people.appointment events in Freebase. And a LUs of the frame plays a similar role as the trigger of an event. Thus we use FrameNet to detect triggers in our automatically data labeling process. Wikipedia4 that we used was released on J"
P17-1038,D13-1160,0,0.107616,"Missing"
P17-1038,P11-1098,0,0.0152802,"Missing"
P17-1038,P15-1017,1,0.753394,"Missing"
P17-1038,D14-1198,0,0.197879,"Missing"
P17-1038,N13-1104,0,0.0476628,"Missing"
P17-1038,P13-1008,0,0.741449,"mapped these types of events manually and we add them into ACE training corpus in two ways. (1) we delete the human annotated ACE data for these mapped event types in ACE dataset and add our automatically labeled data to remainder ACE training data. We call this Expanded Data (ED) as ED Only. (2) We directly add our automatically labeled data of mapped event types to ACE training data and we call this training data as ACE+ED. Then we use such data to train the same event extraction model (DMCNN) and evaluate them on the ACE testing data set. Following (Nguyen et al., 2016; Chen et al., 2015; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set. And we use the same evaluation metric P, R, F as ACE task defined. We select three baselines trained with ACE data. (1) Li’s structure, which is the best reported structured-based system (Li et al., 2013). (2) Chen’s DMCNN, which is the best reported CNN-based system (Chen et al., 2015). (3) Nguyen’s JRNN, which is the state-ofthe-arts system (Nguyen et al., 2016). The results are shown in Table 4. Compared with all models, DMCNN trained"
P17-1038,P16-1201,1,0.82113,"here are around 1885 CVTs and around 14 million CVTs instances. After filtering out useless and meaningless CVTs, such as CVTs about user profiles and website information, we select 21 types of CVTs with around 3.8 million instances for experiments, which mainly involves events about education, military, sports and so on. FrameNet3 is a linguistic resource storing information about lexical and predicate argument semantics (Baker et al., 1998). FrameNet contains more than 1, 000 frames and 10, 000 Lexical Units (LUs). Each frame of FrameNet can be taken as a semantic frame of a type of events (Liu et al., 2016). Each frame has a set of lemmas with part of speech tags that can evoke the frame, which are called LUs. For example, appoint.v is a LU of Appointing frame in FrameNet, which can be mapped to people.appointment events in Freebase. And a LUs of the frame plays a similar role as the trigger of an event. Thus we use FrameNet to detect triggers in our automatically data labeling process. Wikipedia4 that we used was released on January, 2016. All 6.3 million articles in it are used in our experiments. We use Wikipedia because it is relatively up-to-date, and much of the information in Freebase is"
P17-1038,P11-1055,0,0.0212291,"ge is conducted, which aims to assign arguments to the event and identify their corresponding roles. We call this stage as argument classification. We employ two similar Dynamic Multi-pooling Convolutional Neural Networks with Multi-instance Learning (DMCNNs-MIL) for above two stages. The Dynamic Multi-pooling Convolutional Neural Networks (DMCNNs) is the best reported CNN-based model for event extraction (Chen et al., 2015) by using human-annotated training data. However, our automatically labeled data face a noise problem, which is a intrinsic problem of using DS to construct training data (Hoffmann et al., 2011; Surdeanu et al., 2012). In order to alleviate the wrong label problem, we use Multi-instance Learning (MIL) for two DMCNNs. Because the second stage is more complicated and limited in space, we take the MIL used in arguments classification as an example and describes as follows: We define all of the parameters for the stage of argument classification to be trained in DMCNNs as θ. Suppose that there are T bags {M1 , M2 , ..., MT } and that the  i-th bag contains qi instances (sentences) Mi = m1i , m2i , ..., mqi i , the objective of multi-instance learning is to predict the labels of the uns"
P17-1038,P11-1163,0,0.061963,"Missing"
P17-1038,P11-1113,0,0.811392,"Missing"
P17-1038,P09-1113,0,0.257676,"Missing"
P17-1038,P16-1025,0,0.0529364,"Missing"
P17-1038,P15-2060,0,0.418243,"Missing"
P17-1038,P08-1030,0,0.70382,"Missing"
P17-1038,N16-1034,0,0.537655,"life.marry events in ACE2005 dataset. We mapped these types of events manually and we add them into ACE training corpus in two ways. (1) we delete the human annotated ACE data for these mapped event types in ACE dataset and add our automatically labeled data to remainder ACE training data. We call this Expanded Data (ED) as ED Only. (2) We directly add our automatically labeled data of mapped event types to ACE training data and we call this training data as ACE+ED. Then we use such data to train the same event extraction model (DMCNN) and evaluate them on the ACE testing data set. Following (Nguyen et al., 2016; Chen et al., 2015; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set. And we use the same evaluation metric P, R, F as ACE task defined. We select three baselines trained with ACE data. (1) Li’s structure, which is the best reported structured-based system (Li et al., 2013). (2) Chen’s DMCNN, which is the best reported CNN-based system (Chen et al., 2015). (3) Nguyen’s JRNN, which is the state-ofthe-arts system (Nguyen et al., 2016). The results are shown in Table 4. Co"
P17-1038,D12-1069,0,0.0144758,"Missing"
P17-1038,D12-1042,0,0.00878712,"aims to assign arguments to the event and identify their corresponding roles. We call this stage as argument classification. We employ two similar Dynamic Multi-pooling Convolutional Neural Networks with Multi-instance Learning (DMCNNs-MIL) for above two stages. The Dynamic Multi-pooling Convolutional Neural Networks (DMCNNs) is the best reported CNN-based model for event extraction (Chen et al., 2015) by using human-annotated training data. However, our automatically labeled data face a noise problem, which is a intrinsic problem of using DS to construct training data (Hoffmann et al., 2011; Surdeanu et al., 2012). In order to alleviate the wrong label problem, we use Multi-instance Learning (MIL) for two DMCNNs. Because the second stage is more complicated and limited in space, we take the MIL used in arguments classification as an example and describes as follows: We define all of the parameters for the stage of argument classification to be trained in DMCNNs as θ. Suppose that there are T bags {M1 , M2 , ..., MT } and that the  i-th bag contains qi instances (sentences) Mi = m1i , m2i , ..., mqi i , the objective of multi-instance learning is to predict the labels of the unseen bags. In stage of ar"
P17-1038,P10-1040,0,0.0166672,"e trigger words for each event type. 3.3 Trigger Word Filtering and Expansion We can obtain an initial verbal trigger lexicon by above trigger word detection. However, this initial trigger lexicon is noisy and merely contains verbal triggers. The nominal triggers like marriage are missing. Because the number of nouns in one sentence is usually larger than that of verbs, it is hard to use TR to find nominal triggers. Thus, we propose to use linguistic resource FrameNet to filter noisy verbal triggers and expand nominal triggers. As the success of word embedding in capturing semantics of words (Turian et al., 2010), we employ word embedding to map the events in Freebase to frames in FrameNet. Specifically, we use the average word embedding of all words in i-th Freebase event type name ei and word embedding of k-th lexical units of j-th frame ej,k to compute the semantic similarity. Finally, we select the frame contains max similarity of ei and ej,k as the mapped frame, which can be formulated as follows: f rame(i) = arg max(similarity(ei , ej,k )) (7) j Then, we filter the verb, which is in initial verbal trigger word lexicon and not in the mapping frame. And we use nouns with high confidence in the map"
P17-1038,P16-1123,0,0.0109397,"Missing"
P17-1038,D15-1203,1,0.290413,"Missing"
P17-1164,W06-0900,0,0.0583782,"du/software/ 5 Related Work Event detection is an increasingly hot and challenging research topic in NLP. Generally, existing approaches could roughly be divided into two groups. The first kind of approach tackled this task under the supervision of annotated triggers and entities, but totally ignored annotated arguments. The majority of existing work followed this paradigm, which includes feature-based methods and representationbased methods. Feature-based methods exploited a diverse set of strategies to convert classification clues (i.e., POS tags, dependency relations) into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Gupta and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Liu et al., 2016b). Representation-based methods typically represent candidate event mentions by embeddings and feed them into neural networks (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a; Nguyen and Grishman, 2016). The second kind of approach, on the contrast, tackled event detection and argument extraction simultaneously, which is called joint approach (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013, 2014; Venugopal et al., 2014; Nguyen"
P17-1164,P15-1017,1,0.88051,"are event arguments. The correct type of the event triggered by “fired ” in this case is End-Position. However, it might be easily misidentified as Attack because “fired ” is a multivocal word. In this case, if we consider the phrase “former protege”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art"
P17-1164,P98-1013,0,0.348285,"Specifically, in training procedure, we first construct gold attentions for each trigger candidate based on annotated arguments. Then, treating gold attentions as the supervision to train the attention mechanism, we learn attention and event detector jointly both in supervised manner. In testing procedure, we use the ED model with learned attention mechanisms to detect events. In the experiment section, we systematically conduct comparisons on a widely used benchmark dataset ACE20051 . In order to further demonstrate the effectiveness of our approach, we also use events from FrameNet (FN) (F. Baker et al., 1998) as extra training data, as the same as Liu et al. (2016a) to alleviate the data-sparseness problem for ED to augment the performance of the proposed approach. The experimental results demonstrate that the proposed approach is effective for ED task, and it outperforms state-of-the-art approaches with remarkable gains. To sum up, our main contributions are: (1) we analyze the problem of joint models on the task of ED, and propose to use the annotated argument information explicitly for this task. (2) to achieve this goal, we introduce a supervised attention based ED model. Furthermore, we syste"
P17-1164,P09-2093,0,0.0325485,"e trigger word and the other bold words are event arguments. The correct type of the event triggered by “fired ” in this case is End-Position. However, it might be easily misidentified as Attack because “fired ” is a multivocal word. In this case, if we consider the phrase “former protege”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respec"
P17-1164,P11-1113,0,0.921572,"he other bold words are event arguments. The correct type of the event triggered by “fired ” in this case is End-Position. However, it might be easily misidentified as Attack because “fired ” is a multivocal word. In this case, if we consider the phrase “former protege”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared wi"
P17-1164,P08-1030,0,0.690342,"entence, “fired ” is the trigger word and the other bold words are event arguments. The correct type of the event triggered by “fired ” in this case is End-Position. However, it might be easily misidentified as Attack because “fired ” is a multivocal word. In this case, if we consider the phrase “former protege”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for even"
P17-1164,D14-1181,0,0.00357226,"Missing"
P17-1164,P14-1038,0,0.0166946,"iu et al. (2016a) detected events from FrameNet based on the observation that frames in FN are analogous to events in ACE 1795 5 https://github.com/subacl/acl16 (lexical unit of a frame ↔ trigger of an event, frame elements of a frame ↔ arguments of an event). All events they published are also frames in FN. Thus, we treat frame elements annotated in FN corpus as event arguments. Since frames generally contain more frame elements than events, we only use core6 elements in this work. Moreover, to obtain entity information, we use RPI Joint Information Extraction System7 (Li et al., 2013, 2014; Li and Ji, 2014) to label ACE entity mentions. Experimental Results We use the events from FN as extra training data and keep the development and test datasets unchanged.Table 4 presents the experimental results. Methods ANN ANN-S1 ANN-S2 ANN +FrameNet ANN-S1 +FrameNet ANN-S2 +FrameNet P 69.9 81.4 78.0 72.5 80.1 76.8 R 60.8 62.4 66.3 61.7 63.6 67.5 F1 65.0 70.8 71.7 66.7 70.9 71.9 Table 4: Experimental results on ACE 2005 corpus. “+FrameNet” designates the systems that are augmented by events from FrameNet. From the results, we observe that: 1). With extra training data, ANN achieves significant improvements"
P17-1164,D14-1198,0,0.181628,"Missing"
P17-1164,P13-1008,0,0.608567,"e”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art pipeline systems, both join1789 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789–1798 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org"
P17-1164,P10-1081,0,0.822659,"research topic in NLP. Generally, existing approaches could roughly be divided into two groups. The first kind of approach tackled this task under the supervision of annotated triggers and entities, but totally ignored annotated arguments. The majority of existing work followed this paradigm, which includes feature-based methods and representationbased methods. Feature-based methods exploited a diverse set of strategies to convert classification clues (i.e., POS tags, dependency relations) into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Gupta and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Liu et al., 2016b). Representation-based methods typically represent candidate event mentions by embeddings and feed them into neural networks (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a; Nguyen and Grishman, 2016). The second kind of approach, on the contrast, tackled event detection and argument extraction simultaneously, which is called joint approach (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013, 2014; Venugopal et al., 2014; Nguyen et al., 2016). Joint approach is proposed to capture internal and external dependencies of events"
P17-1164,P16-1201,1,0.801923,"event triggered by “fired ” in this case is End-Position. However, it might be easily misidentified as Attack because “fired ” is a multivocal word. In this case, if we consider the phrase “former protege”, which serves as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art pipeline systems, both join1789 Proceedings"
P17-1164,D16-1249,0,0.0383889,"rds. To achieve this goal, we propose two strategies to construct gold attention vectors: S1: only pay attention to argument words. That is, all argument words in the given context obtain the same attention, whereas other words get no attention. For candidates without any annotated arguments in context (such as negative samples), we force all entities to average the whole attention. Figure 2 illustrates the details, where α∗ is the final gold attention vector. to ED, the words around them are also helpful. And the nearer a word is to arguments, the more attention it should obtain. Inspired by Mi et al. (2016), we use a gaussian distribution g(·) to model the attention distribution of words around arguments. In detail, given an instance, we first obtain the raw attention vector α in the same manner as S1 (see figure 2). ′ Then, we create a new vector α with all points initialized with zero, and for each αi = 1, we ′ update α by the following algorithm: Algorithm 1: Updating α ′ for k ∈ {−w, ..., 0, ..., w} do ′ ′ αi+k = αi+k + g(|k|, µ, σ) end where w is the window size of the attention mechanism and µ, σ are hyper-parameters of the gaussian distribution. Finally, we normal′ ize α to obtain the tar"
P17-1164,N16-1034,0,0.651498,"osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art pipeline systems, both join1789 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789–1798 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1164 ED AE Symbolic Methods H"
P17-1164,D14-1090,0,0.517295,"as an argument (Role = P osition) of the target event, we would have more confidence in predicting it as an End-Position event. Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016). Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED. Table 1 illustrates our observations. Li et al. (2013) and Nguyen et al. (2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively. Compared with state-of-the-art pipeline systems, both join1789 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789–1798 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1164 ED"
P17-1164,P14-2136,0,0.0363178,"introduces the non-consecutive convolution to capture non-consecutive k-grams for event detection. It is the best reported representation-based approach on this task. Table 3 presents the experimental results on ACE 2005 corpus. From the table, we make the following observations: 1). ANN performs unexpectedly poorly, which indicates that unsupervised-attention mechanisms do not work well for ED. We believe the reason is that the training data of ACE 2005 corpus is insufficient to train a precise attention in an unsupervised manner, considering that data sparseness is an important issue of ED (Zhu et al., 2014; Liu et al., 2016a). 2). With argument information employed via supervised attention mechanisms, both ANN-S1 and ANN-S2 outperform ANN with remarkable gains, which illustrates the effectiveness of the proposed approach. 3). ANN-S2 outperforms ANN-S1, but the latter achieves higher precision. It is not difficult to understand. On the one hand, strategy S1 only focuses on argument words, which provides accurate information to identify event type, thus ANN-S1 could achieve higher precision. On the other hand, S2 focuses on both arguments and words around them, which provides more general but noi"
P17-1164,P15-2060,0,0.740377,"Missing"
P17-1164,D16-1085,0,0.519844,"Missing"
P17-1164,D09-1016,0,0.152513,"detection is an increasingly hot and challenging research topic in NLP. Generally, existing approaches could roughly be divided into two groups. The first kind of approach tackled this task under the supervision of annotated triggers and entities, but totally ignored annotated arguments. The majority of existing work followed this paradigm, which includes feature-based methods and representationbased methods. Feature-based methods exploited a diverse set of strategies to convert classification clues (i.e., POS tags, dependency relations) into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Gupta and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Liu et al., 2016b). Representation-based methods typically represent candidate event mentions by embeddings and feed them into neural networks (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a; Nguyen and Grishman, 2016). The second kind of approach, on the contrast, tackled event detection and argument extraction simultaneously, which is called joint approach (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013, 2014; Venugopal et al., 2014; Nguyen et al., 2016). Joint approach is proposed to capture"
P17-1164,N10-1123,0,0.0626273,"ation clues (i.e., POS tags, dependency relations) into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Gupta and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Liu et al., 2016b). Representation-based methods typically represent candidate event mentions by embeddings and feed them into neural networks (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a; Nguyen and Grishman, 2016). The second kind of approach, on the contrast, tackled event detection and argument extraction simultaneously, which is called joint approach (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013, 2014; Venugopal et al., 2014; Nguyen et al., 2016). Joint approach is proposed to capture internal and external dependencies of events, including trigger-trigger, argument-argument and trigger-argument dependencies. Theoretically, both ED and AE are expected to benefit from joint methods because triggers and arguments are jointly considered. However, in practice, existing joint methods usually only make remarkable improvements to AE, but insignificant to ED. Different from them, this work investigates the exploitation of argument information to improve the performance of ED."
P17-1164,C98-1013,0,\N,Missing
P18-1047,P14-1038,0,0.707525,"ntifies the semantic relations between two pre-assigned entities. Although great progresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elaborate features to construct the bridge between these two subtasks. Similar to other natural language processing (NLP) tasks, they need complicated feature engineering and heavily rely on pre-existing NLP tools for feature extraction. 506 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 506–514 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics vecto"
P18-1047,P16-1105,0,0.51388,"everal times when it needs to participate in different triplets. Therefore, our model could handle the triplet overlap issue and deal with both of EntityPairOverlap and SingleEntityOverlap sentence types. Moreover, since extracting entities and relations in a single end2end neural network, our model could extract entities and relations jointly. The main contributions of our work are as follows: Recently, with the success of deep learning on many NLP tasks, it is also applied on relational facts extraction. Zeng et al. (2014); Xu et al. (2015a,b) employed CNN or RNN on relation classification. Miwa and Bansal (2016); Gupta et al. (2016); Zhang et al. (2017) treated relation extraction task as an end-to-end (end2end) tablefilling problem. Zheng et al. (2017) proposed a novel tagging schema and employed a Recurrent Neural Networks (RNN) based sequence labeling model to jointly extract entities and relations. Nevertheless, the relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. Such phenomenon makes aforementioned methods, whatever deep learning based models and traditional feature engineering based joint models, always fail to extract relation"
P18-1047,D14-1200,0,0.50354,"al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elaborate features to construct the bridge between these two subtasks. Similar to other natural language processing (NLP) tasks, they need complicated feature engineering and heavily rely on pre-existing NLP tools for feature extraction. 506 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 506–514 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics vector. Then, the decoder reads in this vector and generates triplets directly. To generate a triplet, firstly, the decoder generate"
P18-1047,P11-1056,0,0.641717,"as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as < Chicago, country, U nitedStates &gt;. So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between two pre-assigned entities. Although great progresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elaborate features to construct the bridge between these two subtasks. Similar to other natural language processing (NLP) tasks, they need complicated feature engineering and heavily rely on pre-existing NLP"
P18-1047,D15-1062,0,0.36213,"ntroduction Recently, to build large structural knowledge bases (KB), great efforts have been made on extracting relational facts from natural language texts. A relational fact is often represented as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as < Chicago, country, U nitedStates &gt;. So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between two pre-assigned entities. Although great progresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elabo"
P18-1047,P16-1004,0,0.0361548,"relation Born_in Located_in ? ? Khartoum Contains …… Khartoum Entity Copy Copied entity 's capital Sudan , Khartoum in officials unnerved 's list the of News existence ? ? Capital ? 0.8 ? Sudan Decoder ? ? Capital ? GO ? ? capital 's Sudan , Khartoum in officials unnerved existence 's list the of News Attention Vector ? Encoder Figure 2: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence. adopted for some NLP tasks. Dong and Lapata (2016) presented a method based on an attentionenhanced and encoder-decoder model, which encodes input utterances and generates their logical forms. Gu et al. (2016); He et al. (2017) applied copy mechanism to sentence generation. They copy a segment from the source sequence to the target sequence. By giving a sentence without any annotated entities, researchers proposed several methods to extract both entities and relations. Pipeline based methods, like Zelenko et al. (2003) and Chan and Roth (2011), neglected the relevance of entity extraction and relation prediction. To resolve this problem, seve"
P18-1047,D15-1206,0,0.526954,"ntroduction Recently, to build large structural knowledge bases (KB), great efforts have been made on extracting relational facts from natural language texts. A relational fact is often represented as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as < Chicago, country, U nitedStates &gt;. So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between two pre-assigned entities. Although great progresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elabo"
P18-1047,P17-1017,0,0.113109,"produced by distant supervision method (Riedel et al., 2010). This dataset consists of 1.18M sentences sampled from 294k 1987-2007 New York Times news articles. There are 24 valid relations in total. In this paper, we treat this dataset as supervised data as the same as Zheng et al. (2017). We filter the sentences with more than 100 words and the sentences containing no positive triplets, and 66195 sentences are left. We randomly select 5000 sentences from it as the test set, 5000 sentences as the validation set and the rest 56195 sentences are used as train set. The second is WebNLG dataset (Gardent et al., 2017). It is originally created for Natural Language Generation (NLG) task. This dataset contains 246 valid relations. In this dataset, a instance including a group of triplets and several standard sentences (written by human). Every standard sentence contains all triplets of this instance. We on4.3 Baseline and Evaluation Metrics We compare our models with NovelTagging model (Zheng et al., 2017), which conduct the best performance on relational facts extraction. We directly run the code released by Zheng et al. (2017) to acquire the results. Following Zheng et al. (2017), we use the standard micro"
P18-1047,P16-1154,0,0.20793,"ce ? ? Capital ? 0.8 ? Sudan Decoder ? ? Capital ? GO ? ? capital 's Sudan , Khartoum in officials unnerved existence 's list the of News Attention Vector ? Encoder Figure 2: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence. adopted for some NLP tasks. Dong and Lapata (2016) presented a method based on an attentionenhanced and encoder-decoder model, which encodes input utterances and generates their logical forms. Gu et al. (2016); He et al. (2017) applied copy mechanism to sentence generation. They copy a segment from the source sequence to the target sequence. By giving a sentence without any annotated entities, researchers proposed several methods to extract both entities and relations. Pipeline based methods, like Zelenko et al. (2003) and Chan and Roth (2011), neglected the relevance of entity extraction and relation prediction. To resolve this problem, several joint models have been proposed. Early works (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) need complicated process of feature engineering and"
P18-1047,C10-2160,0,0.737967,"gresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) designed several elaborate features to construct the bridge between these two subtasks. Similar to other natural language processing (NLP) tasks, they need complicated feature engineering and heavily rely on pre-existing NLP tools for feature extraction. 506 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 506–514 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics vector. Then, the decoder reads in this vector and generates triplets directly. To generat"
P18-1047,C16-1239,0,0.45871,"ds to participate in different triplets. Therefore, our model could handle the triplet overlap issue and deal with both of EntityPairOverlap and SingleEntityOverlap sentence types. Moreover, since extracting entities and relations in a single end2end neural network, our model could extract entities and relations jointly. The main contributions of our work are as follows: Recently, with the success of deep learning on many NLP tasks, it is also applied on relational facts extraction. Zeng et al. (2014); Xu et al. (2015a,b) employed CNN or RNN on relation classification. Miwa and Bansal (2016); Gupta et al. (2016); Zhang et al. (2017) treated relation extraction task as an end-to-end (end2end) tablefilling problem. Zheng et al. (2017) proposed a novel tagging schema and employed a Recurrent Neural Networks (RNN) based sequence labeling model to jointly extract entities and relations. Nevertheless, the relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. Such phenomenon makes aforementioned methods, whatever deep learning based models and traditional feature engineering based joint models, always fail to extract relational triplets precisely"
P18-1047,C14-1220,1,0.926312,"pped entity pair. Introduction Recently, to build large structural knowledge bases (KB), great efforts have been made on extracting relational facts from natural language texts. A relational fact is often represented as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as < Chicago, country, U nitedStates &gt;. So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between two pre-assigned entities. Although great progresses have been made (Hendrickx et al., 2010; Zeng et al., 2014; Xu et al., 2015a,b), they all assume that the entities are identified beforehand and neglect the extraction of entities. To extract both of entities and relations, early works(Zelenko et al., 2003; Chan and Roth, 2011) adopted a pipeline manner, where they first conduct entity recognition and then predict relations between extracted entities. However, the pipeline framework ignores the relevance of entity identification and relation prediction (Li and Ji, 2014). Recent works attempted to extract entities and relations jointly. Yu and Lam (2010); Li and Ji (2014); Miwa and Sasaki (2014) desig"
P18-1047,P17-1019,1,0.848646,".8 ? Sudan Decoder ? ? Capital ? GO ? ? capital 's Sudan , Khartoum in officials unnerved existence 's list the of News Attention Vector ? Encoder Figure 2: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence. adopted for some NLP tasks. Dong and Lapata (2016) presented a method based on an attentionenhanced and encoder-decoder model, which encodes input utterances and generates their logical forms. Gu et al. (2016); He et al. (2017) applied copy mechanism to sentence generation. They copy a segment from the source sequence to the target sequence. By giving a sentence without any annotated entities, researchers proposed several methods to extract both entities and relations. Pipeline based methods, like Zelenko et al. (2003) and Chan and Roth (2011), neglected the relevance of entity extraction and relation prediction. To resolve this problem, several joint models have been proposed. Early works (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014) need complicated process of feature engineering and heavily depends o"
P18-1047,D17-1182,0,0.215794,"Missing"
P18-1047,S10-1006,0,0.0661697,"Missing"
P18-1047,P17-1113,0,0.568747,"EntityPairOverlap and SingleEntityOverlap sentence types. Moreover, since extracting entities and relations in a single end2end neural network, our model could extract entities and relations jointly. The main contributions of our work are as follows: Recently, with the success of deep learning on many NLP tasks, it is also applied on relational facts extraction. Zeng et al. (2014); Xu et al. (2015a,b) employed CNN or RNN on relation classification. Miwa and Bansal (2016); Gupta et al. (2016); Zhang et al. (2017) treated relation extraction task as an end-to-end (end2end) tablefilling problem. Zheng et al. (2017) proposed a novel tagging schema and employed a Recurrent Neural Networks (RNN) based sequence labeling model to jointly extract entities and relations. Nevertheless, the relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. Such phenomenon makes aforementioned methods, whatever deep learning based models and traditional feature engineering based joint models, always fail to extract relational triplets precisely. Generally, according to our observation, we divide the sentences into three types according to triplet overlap degree, in"
P18-4009,P15-1017,1,0.900406,"line DCFEE system9 . 5 Related Work The current EE approaches can be mainly classified into statistical methods, pattern-based method and hybrid method (Hogenboom et al., 2016). 12 Example of a pattern for a freeze event: (Frozen institution(ORG)+, Trigger word(TRI)+, Shareholder names(NAME)+,time) 54 Statistical method can be divided into two categories: traditional machine learning algorithm based on feature extraction engineering (Ahn, 2006), (Ji and Grishman, 2008), (Liao and Grishman, 2010), (Reichart and Barzilay, 2012) and neural network algorithm based on automatic feature extraction (Chen et al., 2015), (Nguyen et al., 2016), (Liu et al., 2017). The pattern method is usually used in industry because it can achieve higher accuracy, but meanwhile a lower recall. In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary (Chen et al., 2017), (Gu et al., 2016). Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and Morik, 2008), (Bjorne et al., 2010). To our best knowledge, there is no system that automatically generates labeled d"
P18-4009,P10-1081,0,0.248432,"experimental results show that the effectiveness of SEE and DEE, the acceptable precision Figure 5: A screen shot of the online DCFEE system9 . 5 Related Work The current EE approaches can be mainly classified into statistical methods, pattern-based method and hybrid method (Hogenboom et al., 2016). 12 Example of a pattern for a freeze event: (Frozen institution(ORG)+, Trigger word(TRI)+, Shareholder names(NAME)+,time) 54 Statistical method can be divided into two categories: traditional machine learning algorithm based on feature extraction engineering (Ahn, 2006), (Ji and Grishman, 2008), (Liao and Grishman, 2010), (Reichart and Barzilay, 2012) and neural network algorithm based on automatic feature extraction (Chen et al., 2015), (Nguyen et al., 2016), (Liu et al., 2017). The pattern method is usually used in industry because it can achieve higher accuracy, but meanwhile a lower recall. In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary (Chen et al., 2017), (Gu et al., 2016). Hybrid event-extraction methods combine statistical methods and pattern-based methods together (Jungermann and"
P18-4009,P16-1154,0,0.0524912,"Missing"
P18-4009,P08-1030,0,0.184611,"event knowledge base. The experimental results show that the effectiveness of SEE and DEE, the acceptable precision Figure 5: A screen shot of the online DCFEE system9 . 5 Related Work The current EE approaches can be mainly classified into statistical methods, pattern-based method and hybrid method (Hogenboom et al., 2016). 12 Example of a pattern for a freeze event: (Frozen institution(ORG)+, Trigger word(TRI)+, Shareholder names(NAME)+,time) 54 Statistical method can be divided into two categories: traditional machine learning algorithm based on feature extraction engineering (Ahn, 2006), (Ji and Grishman, 2008), (Liao and Grishman, 2010), (Reichart and Barzilay, 2012) and neural network algorithm based on automatic feature extraction (Chen et al., 2015), (Nguyen et al., 2016), (Liu et al., 2017). The pattern method is usually used in industry because it can achieve higher accuracy, but meanwhile a lower recall. In order to improve recall, there are two main research directions: build relatively complete pattern library and use a semi-automatic method to build trigger dictionary (Chen et al., 2017), (Gu et al., 2016). Hybrid event-extraction methods combine statistical methods and pattern-based metho"
P18-4009,P17-1164,1,0.900331,"Missing"
P18-4009,D15-1203,1,0.805938,"obtain these textual data from Sohu securities net10 . Method of data generation: annotation data consists of two parts: sentence-level data generated by labeling the event trigger and event arguments in the event mention; document-level data generated by labeling the event mention from the document-level announcement. Now the question is, how to find the event triggers. Event arguments and event mention that correspond to the structured event knowledge database are summarized from a mass of announcements. DS has proved its effectiveness in automatically labeling data for Relation Extraction (Zeng et al., 2015) and Event Extraction (Chen et al., 2017). Inspired by DS, we assume that one sentence contains the most event arguments and driven by a specific trigger is likely to be an event mention in an announcement. And arguments occurring in the event mention are 10 2.2 Event Extraction (EE) Figure 4 depicts the overall architecture of the EE system proposed in this paper which primarily involves the following two components: The sentence-level Event Extraction (SEE) purposes to http://q.stock.sohu.com/cn/000001/gsgg.shtml 52 S1 :… .… Sn :… pledge to CITIC... Sn+1:The pledge period is 12 months. … ext"
P18-4009,W06-0901,0,0.80415,"g.xiao, jzhao,}@nlpr.ia.ac.cn Abstract in Figure 1, an EE system is expected to discover an Equity Freeze event mention (E1 itself) triggered by frozen and extract the corresponding five arguments with different roles: Nagafu Ruihua (Role=Shareholder Name), 520,000 shares (Role=Num of Frozen Stock), People’s Court of Dalian city (Role=Frozen Institution), May 5,2017 (Role=Freezing Start Date) and 3 years (Role=Freezing End Date). Extracting event instances from texts plays a critical role in building NLP applications such as Information Extraction (IE), Question Answer (QA) and Summarization (Ahn, 2006). Recently, researchers have built some English EE systems, such as EventRegistry7 and Stela8 . However, in financial domain, there is no such effective EE system, especially in Chinese. Financial events are able to help users obtain competitors’ strategies, predict the stock market and make correct investment decisions. For example, the occurrence of an Equity Freeze event will have a bad effect on the company and the shareholders should make correct decisions quickly to avoid the losses. In business domain, official announcements released by companies represent the occurrence of major events"
P18-4009,P17-1038,1,0.731308,"ities net10 . Method of data generation: annotation data consists of two parts: sentence-level data generated by labeling the event trigger and event arguments in the event mention; document-level data generated by labeling the event mention from the document-level announcement. Now the question is, how to find the event triggers. Event arguments and event mention that correspond to the structured event knowledge database are summarized from a mass of announcements. DS has proved its effectiveness in automatically labeling data for Relation Extraction (Zeng et al., 2015) and Event Extraction (Chen et al., 2017). Inspired by DS, we assume that one sentence contains the most event arguments and driven by a specific trigger is likely to be an event mention in an announcement. And arguments occurring in the event mention are 10 2.2 Event Extraction (EE) Figure 4 depicts the overall architecture of the EE system proposed in this paper which primarily involves the following two components: The sentence-level Event Extraction (SEE) purposes to http://q.stock.sohu.com/cn/000001/gsgg.shtml 52 S1 :… .… Sn :… pledge to CITIC... Sn+1:The pledge period is 12 months. … extract event arguments and event triggers f"
P19-1367,P17-1110,0,0.0203787,"xl , xh )] j=1 Lo −1 XLo l Ll = log[p(yjl |y<j , xr , xl , xh , Yh )] j=1 Lo −1 XLo r Lr = log[p(yjr |y<j , xr , xl , xh , Yh , Yl )] j=1 Lo (12) where the three negative log-likelihoods (Lh , Ll and Lr ) are losses for different-level targeted outputs. Yh and Yl are output embedding sequences in the high-level decoder and low-level decoder, respectively. Finally, the sum of different losses in three decoders is considered as the total losses L. 4 Experiment 4.1 Datasets There are large-scale message-response pairs on social websites, which consist of informational text from different topics (Chen et al., 2017). Our experimental data comes from two public corpus: English “Twitter”6 and Chinese “Weibo” (Shang et al., 2015b). In order to improve the quality of datasets, some noisy message-response pairs are filtered (e.g., containing too many punctuations or emoticons), and the datasets are randomly split into Train/Dev/Test by a proportion (9:0.5:0.5). 3778 6 https://github.com/Marsan-Ma-zz/chat corpus 4.2 Implementation Details Models In order to make our model comparable with typical existing methods, we keep the same experimental parameters for VPN and comparative methods. We set the vocabulary si"
P19-1367,D14-1179,0,0.00906966,"Missing"
P19-1367,W14-4012,0,0.020538,"Missing"
P19-1367,P16-1154,0,0.0870906,"are also decoded by LSTM with 600 dimensions. The total losses are minimized by an Adam optimizer (Kingma and Ba, 2015) with 0.0001 learning rate. Particularly, the size of lowlevel clusters and high-level clusters are 3400 and 340, respectively, which are significantly smaller than the size of raw words (34000), and these clusters are also represented by 300-dimensional vectors. Finally, we implemented all models with the TensorFlow. 4.3 Evaluation Metrics Evaluation for generative responses is a challenging and under-researching problem (Novikova et al., 2017). Similar to (Li et al., 2016b; Gu et al., 2016), we borrow two well-established automatic evaluation metrics from machine translation and text summarization: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)7 , which could be leveraged to analyze the co-occurrences of n-gram between the generated responses and references. In addition to automatic evaluations, we also leverage manual evaluations to enhance the evaluations. Following previous studies (He et al., 2017; Qian et al., 2018; Liu et al., 2018), we employ three metrics for manual evaluations as follows. 1) Fluency (Flu.): measuring the grammaticality and fluency of generated respo"
P19-1367,P17-1019,1,0.859989,"he TensorFlow. 4.3 Evaluation Metrics Evaluation for generative responses is a challenging and under-researching problem (Novikova et al., 2017). Similar to (Li et al., 2016b; Gu et al., 2016), we borrow two well-established automatic evaluation metrics from machine translation and text summarization: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)7 , which could be leveraged to analyze the co-occurrences of n-gram between the generated responses and references. In addition to automatic evaluations, we also leverage manual evaluations to enhance the evaluations. Following previous studies (He et al., 2017; Qian et al., 2018; Liu et al., 2018), we employ three metrics for manual evaluations as follows. 1) Fluency (Flu.): measuring the grammaticality and fluency of generated responses, where too short responses are regarded as lack of fluency. 2) Consistency (Con.): measuring whether the generated responses are consistent with the inputs or not. 3) Informativeness (Inf.): measuring whether the response provides informative (knowledgeable) contents or not. 4.4 Table 1: Overall performance on Twitter and Weibo datasets. Note that the first three lines are only onepass decoding, and the fourth line"
P19-1367,N16-1014,0,0.151357,"and output (response) are represented by multi-level vocabularies (e.g., raw words, low-level clusters and high-level clusters) and then processed by multi-pass encoder and decoder. Introduction As one of the long-term goals in AI and NLP, automatic conversation devotes to constructing automatic dialogue systems to communicate with humans (Turing, 1950). Benefited from large-scale human-human conversation data available on the Internet, data-driven dialog systems have attracted increasing attention of both academia and industry (Ritter et al., 2011; Shang et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,c, 2017). Recently, a popular approach to build dialog engines is to learn a response generation model within an encoder-decoder framework such as sequence-to-sequence (Seq2Seq) model (Cho et al., 2014a). In such a framework, an encoder transforms the source sequence into hidden vectors, and a decoder generates the targeted sequence based on the encoded vectors and previously generated words. In this process, the encoder and decoder share a vocabulary (word list)1 , and the targeted words are typically performed by a softmax classifier over the vocabulary word-byword. However, such typical S"
P19-1367,P16-1094,0,0.157736,"and output (response) are represented by multi-level vocabularies (e.g., raw words, low-level clusters and high-level clusters) and then processed by multi-pass encoder and decoder. Introduction As one of the long-term goals in AI and NLP, automatic conversation devotes to constructing automatic dialogue systems to communicate with humans (Turing, 1950). Benefited from large-scale human-human conversation data available on the Internet, data-driven dialog systems have attracted increasing attention of both academia and industry (Ritter et al., 2011; Shang et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,c, 2017). Recently, a popular approach to build dialog engines is to learn a response generation model within an encoder-decoder framework such as sequence-to-sequence (Seq2Seq) model (Cho et al., 2014a). In such a framework, an encoder transforms the source sequence into hidden vectors, and a decoder generates the targeted sequence based on the encoded vectors and previously generated words. In this process, the encoder and decoder share a vocabulary (word list)1 , and the targeted words are typically performed by a softmax classifier over the vocabulary word-byword. However, such typical S"
P19-1367,D16-1127,0,0.171615,"and output (response) are represented by multi-level vocabularies (e.g., raw words, low-level clusters and high-level clusters) and then processed by multi-pass encoder and decoder. Introduction As one of the long-term goals in AI and NLP, automatic conversation devotes to constructing automatic dialogue systems to communicate with humans (Turing, 1950). Benefited from large-scale human-human conversation data available on the Internet, data-driven dialog systems have attracted increasing attention of both academia and industry (Ritter et al., 2011; Shang et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,c, 2017). Recently, a popular approach to build dialog engines is to learn a response generation model within an encoder-decoder framework such as sequence-to-sequence (Seq2Seq) model (Cho et al., 2014a). In such a framework, an encoder transforms the source sequence into hidden vectors, and a decoder generates the targeted sequence based on the encoded vectors and previously generated words. In this process, the encoder and decoder share a vocabulary (word list)1 , and the targeted words are typically performed by a softmax classifier over the vocabulary word-byword. However, such typical S"
P19-1367,D17-1230,0,0.0314973,"Missing"
P19-1367,W04-1013,0,0.0259869,"ze of lowlevel clusters and high-level clusters are 3400 and 340, respectively, which are significantly smaller than the size of raw words (34000), and these clusters are also represented by 300-dimensional vectors. Finally, we implemented all models with the TensorFlow. 4.3 Evaluation Metrics Evaluation for generative responses is a challenging and under-researching problem (Novikova et al., 2017). Similar to (Li et al., 2016b; Gu et al., 2016), we borrow two well-established automatic evaluation metrics from machine translation and text summarization: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)7 , which could be leveraged to analyze the co-occurrences of n-gram between the generated responses and references. In addition to automatic evaluations, we also leverage manual evaluations to enhance the evaluations. Following previous studies (He et al., 2017; Qian et al., 2018; Liu et al., 2018), we employ three metrics for manual evaluations as follows. 1) Fluency (Flu.): measuring the grammaticality and fluency of generated responses, where too short responses are regarded as lack of fluency. 2) Consistency (Con.): measuring whether the generated responses are consistent with the inputs"
P19-1367,C16-1316,0,0.0132465,"Gu et al. (2016) proposed CopyNet, which is able to copy words from the source message. External knowledge bases were also leveraged to extend the vocabulary (Qian et al., 2018; Zhou et al., 2018; Ghazvininejad et al., 2018). Moreover, Xing et al. (2017) incorporated topic words into Seq2Seq frameworks, where topic words are obtained from a pre-trained LDA model (Blei et al., 2003). Wu et al. (2018b) changed the static vocabulary mechanism by a dynamic vocabulary, which jointly learns vocabulary selection and response generation. We also borrow the idea from studies beyond one-pass decoding. Mou et al. (2016) designed backward and forward sequence generators. Xia et al. (2017) proposed deliberation networks on sequence generation beyond one-pass decoding, where the first-pass decoder generates a raw word sequence, and then the second decoder delivers a refined word sequence based on the raw word sequence. Furthermore, Su et al. (2018) presented hierarchical decoding with linguistic patterns on data-to-text tasks. However, there has been no unified frameworks to solve the issues of fixed vocabulary and onepass decoding. Differently, we propose multi-pass encoding and decoding with multi-level vocab"
P19-1367,D17-1238,0,0.0436673,"Missing"
P19-1367,P02-1040,0,0.103587,"earning rate. Particularly, the size of lowlevel clusters and high-level clusters are 3400 and 340, respectively, which are significantly smaller than the size of raw words (34000), and these clusters are also represented by 300-dimensional vectors. Finally, we implemented all models with the TensorFlow. 4.3 Evaluation Metrics Evaluation for generative responses is a challenging and under-researching problem (Novikova et al., 2017). Similar to (Li et al., 2016b; Gu et al., 2016), we borrow two well-established automatic evaluation metrics from machine translation and text summarization: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004)7 , which could be leveraged to analyze the co-occurrences of n-gram between the generated responses and references. In addition to automatic evaluations, we also leverage manual evaluations to enhance the evaluations. Following previous studies (He et al., 2017; Qian et al., 2018; Liu et al., 2018), we employ three metrics for manual evaluations as follows. 1) Fluency (Flu.): measuring the grammaticality and fluency of generated responses, where too short responses are regarded as lack of fluency. 2) Consistency (Con.): measuring whether the generated responses are consi"
P19-1367,D11-1054,0,0.0489421,"d networks for response generation. The dialogue input (context) and output (response) are represented by multi-level vocabularies (e.g., raw words, low-level clusters and high-level clusters) and then processed by multi-pass encoder and decoder. Introduction As one of the long-term goals in AI and NLP, automatic conversation devotes to constructing automatic dialogue systems to communicate with humans (Turing, 1950). Benefited from large-scale human-human conversation data available on the Internet, data-driven dialog systems have attracted increasing attention of both academia and industry (Ritter et al., 2011; Shang et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,c, 2017). Recently, a popular approach to build dialog engines is to learn a response generation model within an encoder-decoder framework such as sequence-to-sequence (Seq2Seq) model (Cho et al., 2014a). In such a framework, an encoder transforms the source sequence into hidden vectors, and a decoder generates the targeted sequence based on the encoded vectors and previously generated words. In this process, the encoder and decoder share a vocabulary (word list)1 , and the targeted words are typically performed by a softmax classif"
P19-1367,P15-1152,0,0.0393653,"Missing"
P19-1367,D17-1235,0,0.0139018,"ve response generation, and it has been widely applied in response generation tasks. For example, Shang et al. (2015a) presented neural recurrent encoderdecoder frameworks for short-text response gener8 In the Discussion Section, all evaluations are based on tokens (IDs) for unifying, so the performances of raw-word decoder on Chinese Weibo dataset are different from the ones (character level) in Table 1. ation with attention mechanisms (Bahdanau et al., 2015). Li et al. (2016b) introduced persona-based neural response generation to obtain consistent responses for similar inputs to a speaker. Shao et al. (2017) added a self-attention to generate long and diversified responses in Seq2Seq learning. In this study, we focus on two important problems in response generation: one fixed vocabulary and one-pass decoding. Our work is inspired by following researches to alleviate issues on the fixed vocabulary. Gu et al. (2016) proposed CopyNet, which is able to copy words from the source message. External knowledge bases were also leveraged to extend the vocabulary (Qian et al., 2018; Zhou et al., 2018; Ghazvininejad et al., 2018). Moreover, Xing et al. (2017) incorporated topic words into Seq2Seq frameworks,"
P19-1367,N18-2010,0,0.0268849,"ned from a pre-trained LDA model (Blei et al., 2003). Wu et al. (2018b) changed the static vocabulary mechanism by a dynamic vocabulary, which jointly learns vocabulary selection and response generation. We also borrow the idea from studies beyond one-pass decoding. Mou et al. (2016) designed backward and forward sequence generators. Xia et al. (2017) proposed deliberation networks on sequence generation beyond one-pass decoding, where the first-pass decoder generates a raw word sequence, and then the second decoder delivers a refined word sequence based on the raw word sequence. Furthermore, Su et al. (2018) presented hierarchical decoding with linguistic patterns on data-to-text tasks. However, there has been no unified frameworks to solve the issues of fixed vocabulary and onepass decoding. Differently, we propose multi-pass encoding and decoding with multi-level vocabularies to deal with the above two problems simultaneously. 6 Conclusion and Future Work In this study, we tackle the issues of one fixed vocabulary and one-pass decoding in response generation tasks. To this end, we have introduced vocabulary pyramid networks, in which dialogue input and output are represented by multi-level voca"
P19-1418,P18-1071,0,0.0276548,"Missing"
P19-1418,P16-1004,0,0.0447818,"Missing"
P19-1418,P18-1068,0,0.412107,"e encoder-decoder framework similar to machine translation. The distinguishing difference of semantic parsing, however, is in its target sequences, which are token sequences of well-formed semantic representations. SQL language and lambda expressions are typical examples of SP targets. The “SELECT..FROM..WHERE” pattern in SQL and the paired parentheses in lambda expressions are consequences of underlying grammars. However, standard Seq2Seq models ignore the patterns and may give ill-formed results. To better model the grammatical and semantical constraints, many decoding methods were devised. Dong and Lapata (2018) proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets. Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (Krishnamurthy et al., 2017; Chen et al., 2018; Yin and Neubig, 2018). There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (Yu et al., 2018a,b). However, the aforementioned methods still have the following key issue. They explic"
P19-1418,P18-1069,0,0.102755,"rated immediately with no doubt. But for tokens seen less often, the model may be pondering and less confident, and it will be better to carry out more computations. In this way, it is unnecessary to pre-build any intermediate supervision for training, such as preprocessed sketches (Dong and Lapata, 2018) and predesigned grammars (Yin and Neubig, 2018), which must be manually redesigned for an unseen kind of target language. Furthermore, we use the model uncertainty estimates to reflect its prediction confidence. Although different uncertainty estimates have been explored in semantic parsing (Dong et al., 2018), we use Dropout (Srivastava et al., 2014) as the uncertainty signal (Gal and Ghahramani, 2016) due 4265 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4265–4270 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Pondering Mode to its simplicity, and use policy gradient algorithm to guide the model search. Our contributions are thus three-fold. depth2 encoder attention • We introduce the adaptive decoding mechanism into semantic parsing, which is well rid of intermediate representations and easily adaptabl"
P19-1418,P09-1069,0,0.0149437,"eq2Seq (Dong and Lapata, 2016) Seq2Tree (Dong and Lapata, 2016) JL16 (Jia and Liang, 2016) TranX (Yin and Neubig, 2018) Coarse2fine (Dong and Lapata, 2018) 84.6 87.1 89.3 88.2 88.2 84.2 84.6 83.3 86.2 87.7 AdaNSP (ours) - halting module 88.9 86.1 88.6 85.7 Table 1: Results on GeoQuery and ATIS datasets 4 Related Work Semantic Parsing. CCG or alignment-based Parsers (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Wong and Mooney, 2006, 2007) try to model the correlation between semantic tokens and lexical meaning of natural language sentences. Methods based on dependency trees (Ge and Mooney, 2009; Liang et al., 2011; Reddy et al., 2016) otherwise convert outputs from an existing syntactic parser into semantic representations, which can be easily adopted in languages with much fewer resources than English. Recently neural semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al.,"
P19-1418,P18-1170,0,0.047562,"Missing"
P19-1418,P17-1097,0,0.0145263,"resources than English. Recently neural semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al., 2017; Groschwitz et al., 2018; Wang et al., 2018). Moreover, learning algorithms in SP like structural learning and maximum marginal likelihood are combined with reinforcement algorithms (Guu et al., 2017; Iyyer et al., 2017; Misra et al., 2018). Adaptive Computing. Adaptive Computation Times (ACT) was first proposed to adaptively learn the depth of RNN models from data (Graves, 2016). Skip-RNN (Campos et al., 2018) used a similar idea to equip a skipping mechanism with existing RNN cells, which adaptively skip some recurrent blocks along the computational graph and thus saved many computations. BlockDrop (Wu et al., 2018) also introduced the REINFORCE algorithm to jointly learn a dropping policy and discard some blocks of the ResNet by the policy network. Recently, Dehghani et al. (2019) prop"
P19-1418,P17-1167,0,0.0735331,"Missing"
P19-1418,P16-1002,0,0.0196199,"Parsers (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Wong and Mooney, 2006, 2007) try to model the correlation between semantic tokens and lexical meaning of natural language sentences. Methods based on dependency trees (Ge and Mooney, 2009; Liang et al., 2011; Reddy et al., 2016) otherwise convert outputs from an existing syntactic parser into semantic representations, which can be easily adopted in languages with much fewer resources than English. Recently neural semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al., 2017; Groschwitz et al., 2018; Wang et al., 2018). Moreover, learning algorithms in SP like structural learning and maximum marginal likelihood are combined with reinforcement algorithms (Guu et al., 2017; Iyyer et al., 2017; Misra et al., 2018). Adaptive Computing. Adaptive Computation Times (ACT) was first proposed to adaptively learn the depth of RNN mod"
P19-1418,D17-1160,0,0.0737855,"OM..WHERE” pattern in SQL and the paired parentheses in lambda expressions are consequences of underlying grammars. However, standard Seq2Seq models ignore the patterns and may give ill-formed results. To better model the grammatical and semantical constraints, many decoding methods were devised. Dong and Lapata (2018) proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets. Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (Krishnamurthy et al., 2017; Chen et al., 2018; Yin and Neubig, 2018). There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (Yu et al., 2018a,b). However, the aforementioned methods still have the following key issue. They explicitly require the expertise to design intermediate representations or model structures, which is not ideal or acceptable for scenarios with Domain Specific Languages (DSL) or new representations because of domain alterations and the incompleteness of the expert knowledge. To follow the successful idea and"
P19-1418,D10-1119,0,0.0295629,"Missing"
P19-1418,D11-1140,0,0.045759,"Missing"
P19-1418,P16-1127,0,0.0231619,"and Collins, 2005, 2007; Kwiatkowski et al., 2010; Wong and Mooney, 2006, 2007) try to model the correlation between semantic tokens and lexical meaning of natural language sentences. Methods based on dependency trees (Ge and Mooney, 2009; Liang et al., 2011; Reddy et al., 2016) otherwise convert outputs from an existing syntactic parser into semantic representations, which can be easily adopted in languages with much fewer resources than English. Recently neural semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al., 2017; Groschwitz et al., 2018; Wang et al., 2018). Moreover, learning algorithms in SP like structural learning and maximum marginal likelihood are combined with reinforcement algorithms (Guu et al., 2017; Iyyer et al., 2017; Misra et al., 2018). Adaptive Computing. Adaptive Computation Times (ACT) was first proposed to adaptively learn the depth of RNN models from data (Grave"
P19-1418,P11-1060,0,0.0188659,"ta, 2016) Seq2Tree (Dong and Lapata, 2016) JL16 (Jia and Liang, 2016) TranX (Yin and Neubig, 2018) Coarse2fine (Dong and Lapata, 2018) 84.6 87.1 89.3 88.2 88.2 84.2 84.6 83.3 86.2 87.7 AdaNSP (ours) - halting module 88.9 86.1 88.6 85.7 Table 1: Results on GeoQuery and ATIS datasets 4 Related Work Semantic Parsing. CCG or alignment-based Parsers (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Wong and Mooney, 2006, 2007) try to model the correlation between semantic tokens and lexical meaning of natural language sentences. Methods based on dependency trees (Ge and Mooney, 2009; Liang et al., 2011; Reddy et al., 2016) otherwise convert outputs from an existing syntactic parser into semantic representations, which can be easily adopted in languages with much fewer resources than English. Recently neural semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al., 2017; Groschwitz et"
P19-1418,P17-1041,0,0.021033,"kens. We instead to propose an adaptive decoding method to avoid such intermediate representations. The decoder is guided by model uncertainty and automatically uses deeper computations when necessary. Thus it can predict tokens adaptively. Our model outperforms the state-of-the-art neural models and does not need any expertise like predefined grammar or sketches in the meantime. 1 Introduction Semantic Parsing (SP) maps a natural language utterance into a formal language, which is crucial in abundant tasks, such as question answering (Zettlemoyer and Collins, 2005, 2007) and code generation (Yin and Neubig, 2017). The prevailing neural semantic parsers view semantic parsing as a sequence transduction task, and adopt the encoder-decoder framework similar to machine translation. The distinguishing difference of semantic parsing, however, is in its target sequences, which are token sequences of well-formed semantic representations. SQL language and lambda expressions are typical examples of SP targets. The “SELECT..FROM..WHERE” pattern in SQL and the paired parentheses in lambda expressions are consequences of underlying grammars. However, standard Seq2Seq models ignore the patterns and may give ill-form"
P19-1418,D18-1266,0,0.0117718,"al semantic parsers, especially under the encoder-decoder framework, also sprang up (Dong and Lapata, 2016, 2018; Jia and Liang, 2016; Xiao et al., 2016). To make the model aware of the underlying grammar of targets, people try to exert constraints on the decoder side by sketches, typing, grammars and runtime execution guides (Dong and Lapata, 2018; Krishnamurthy et al., 2017; Groschwitz et al., 2018; Wang et al., 2018). Moreover, learning algorithms in SP like structural learning and maximum marginal likelihood are combined with reinforcement algorithms (Guu et al., 2017; Iyyer et al., 2017; Misra et al., 2018). Adaptive Computing. Adaptive Computation Times (ACT) was first proposed to adaptively learn the depth of RNN models from data (Graves, 2016). Skip-RNN (Campos et al., 2018) used a similar idea to equip a skipping mechanism with existing RNN cells, which adaptively skip some recurrent blocks along the computational graph and thus saved many computations. BlockDrop (Wu et al., 2018) also introduced the REINFORCE algorithm to jointly learn a dropping policy and discard some blocks of the ResNet by the policy network. Recently, Dehghani et al. (2019) proposed Universal Transformers (UT) as an al"
P19-1418,D18-2002,0,0.0212448,"heses in lambda expressions are consequences of underlying grammars. However, standard Seq2Seq models ignore the patterns and may give ill-formed results. To better model the grammatical and semantical constraints, many decoding methods were devised. Dong and Lapata (2018) proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets. Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (Krishnamurthy et al., 2017; Chen et al., 2018; Yin and Neubig, 2018). There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (Yu et al., 2018a,b). However, the aforementioned methods still have the following key issue. They explicitly require the expertise to design intermediate representations or model structures, which is not ideal or acceptable for scenarios with Domain Specific Languages (DSL) or new representations because of domain alterations and the incompleteness of the expert knowledge. To follow the successful idea and overcome the above issue, we introduce a n"
P19-1418,Q16-1010,0,0.027704,"Missing"
P19-1418,N18-2093,0,0.0151572,"l the grammatical and semantical constraints, many decoding methods were devised. Dong and Lapata (2018) proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets. Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (Krishnamurthy et al., 2017; Chen et al., 2018; Yin and Neubig, 2018). There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (Yu et al., 2018a,b). However, the aforementioned methods still have the following key issue. They explicitly require the expertise to design intermediate representations or model structures, which is not ideal or acceptable for scenarios with Domain Specific Languages (DSL) or new representations because of domain alterations and the incompleteness of the expert knowledge. To follow the successful idea and overcome the above issue, we introduce a novel adaptive decoding mechanism. Inspired by adaptive computing (Graves, 2016), pervasive tokens in training data will be generated immediately with no doubt. But"
P19-1418,N06-1056,0,0.177016,"Missing"
P19-1418,P07-1121,0,0.0987755,"Missing"
P19-1418,D18-1193,0,0.0208483,"l the grammatical and semantical constraints, many decoding methods were devised. Dong and Lapata (2018) proposed to generate tokens of an intermediate sketch first, followed by decoding into final formal targets. Others chose to gradually build abstract syntax trees using a transition-based paradigm, and tokens are generated at the tree leaves or in the middle of the transitions (Krishnamurthy et al., 2017; Chen et al., 2018; Yin and Neubig, 2018). There are also some decoders comprised of several submodules which are intended to generate different parts of the semantic output, respectively (Yu et al., 2018a,b). However, the aforementioned methods still have the following key issue. They explicitly require the expertise to design intermediate representations or model structures, which is not ideal or acceptable for scenarios with Domain Specific Languages (DSL) or new representations because of domain alterations and the incompleteness of the expert knowledge. To follow the successful idea and overcome the above issue, we introduce a novel adaptive decoding mechanism. Inspired by adaptive computing (Graves, 2016), pervasive tokens in training data will be generated immediately with no doubt. But"
P19-1418,D07-1071,0,0.0696049,"Missing"
P19-1418,N15-1162,0,0.0305194,"Missing"
