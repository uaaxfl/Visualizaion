1988.tmi-1.13,P85-1037,0,0.0233711,"Missing"
1988.tmi-1.13,A88-1021,0,0.024644,"who answers questions from the computer or edits his machine's translation results. Most research and development has been devoted to this type. In the latter, the user is responsible for translation, while the computer provides him or her with the necessary tools, e.g., a quick-retrieval electronic dictionary or an easy-to-use word-processor. Although the effects of the second type of translation have been broadly identified, there has been little research on what kinds of function are necessary. While research on electronic dictionaries is thriving in the computational linguistic community [wachowicz86,walker87, chodorow85,tsurumaru86,nakamura87,jensen88], retrieval from conventional electronic dictionaries, as from printed dictionaries, is restricted, because it is done by matching a key word against entry words. In the next section, we argue that it is very useful to have the capacity for flexible retrieval of texts from a bi-lingual dictionary or from a translation database accumulated by the user or other users. For this purpose, we propose a new retrieval mechanism, based on syntax-matching driven by generalization rules. The outline is as follows: 1. Input any text, including not only individual words but also phrases, and sentences, as"
1988.tmi-1.13,C86-1105,0,0.0127787,"who answers questions from the computer or edits his machine's translation results. Most research and development has been devoted to this type. In the latter, the user is responsible for translation, while the computer provides him or her with the necessary tools, e.g., a quick-retrieval electronic dictionary or an easy-to-use word-processor. Although the effects of the second type of translation have been broadly identified, there has been little research on what kinds of function are necessary. While research on electronic dictionaries is thriving in the computational linguistic community [wachowicz86,walker87, chodorow85,tsurumaru86,nakamura87,jensen88], retrieval from conventional electronic dictionaries, as from printed dictionaries, is restricted, because it is done by matching a key word against entry words. In the next section, we argue that it is very useful to have the capacity for flexible retrieval of texts from a bi-lingual dictionary or from a translation database accumulated by the user or other users. For this purpose, we propose a new retrieval mechanism, based on syntax-matching driven by generalization rules. The outline is as follows: 1. Input any text, including not only individual words but also phrases, and sentences, as"
1993.tmi-1.7,P90-1004,0,0.0163828,"oups of conventional methods for disambiguation of ppattachment. The EBD aims to achieve a high success rate with only a simple computation mechanism like SBD mentioned in section 2.2.3. 2.2.1 Syntactic Methods Right association which attaches a prepositional phrase immediately to its right and minimal attachment which does so in a manner in which the fewest syntactic rules are employed are well known.[1] They are advantageous in that they are simple and general and do not require knowledge other than syntactic knowledge. Recently, however, it is reported that their success rates are not good.[2,3] 2.2.2 Method based on Syntactic Rules and a Dictionary Many systems specify the preference of pp-attachment in syntactic rules and a dictionary using semantic markers4. This method is effective against obligatory cases. However, it is not useful for optional cases exemplified in sentence (1) nor for adnominal usage. 2.2.3 Statistical Methods Recently, the construction and use of very large corpora has been on the upswing[4] and a variety of statistical NLP research projects have been introduced. Here we outline two. Method based on Cooccurrence Frequency Several methods based on cooccurrence"
1993.tmi-1.7,P91-1030,0,0.0135287,"oups of conventional methods for disambiguation of ppattachment. The EBD aims to achieve a high success rate with only a simple computation mechanism like SBD mentioned in section 2.2.3. 2.2.1 Syntactic Methods Right association which attaches a prepositional phrase immediately to its right and minimal attachment which does so in a manner in which the fewest syntactic rules are employed are well known.[1] They are advantageous in that they are simple and general and do not require knowledge other than syntactic knowledge. Recently, however, it is reported that their success rates are not good.[2,3] 2.2.2 Method based on Syntactic Rules and a Dictionary Many systems specify the preference of pp-attachment in syntactic rules and a dictionary using semantic markers4. This method is effective against obligatory cases. However, it is not useful for optional cases exemplified in sentence (1) nor for adnominal usage. 2.2.3 Statistical Methods Recently, the construction and use of very large corpora has been on the upswing[4] and a variety of statistical NLP research projects have been introduced. Here we outline two. Method based on Cooccurrence Frequency Several methods based on cooccurrence"
1993.tmi-1.7,C92-2097,1,0.903821,"osed analogy-based translation[6] in order to overcome problems inherent in conventional machine translation. Since the late 1980s, several organizations have begun research along this line. First, it was applied to the translation of parts of sentences such as noun phrases including our first model, Example-Based Machine Translation (EBMT) and high quality translation was demonstrated.[7,8,9]. Several methods to translate a whole sentence have been proposed including our second model, Transfer-Driven Machine Translation (TDMT) and are currently under investigation from various points of view.[7, 10, 11, 12, 13, 14, 15, 16, 17] In this paper, semantic distance calculation is applied for structural disambiguation and called EBD. 3.2 Semantic Distance Calculation Here, we explain one of the basic techniques of EBMT and EBD, i.e., calculation of the semantic distance of two expressions.[8,9] The input, I, and the source part of the example, E, are n-tuples of words, lk and Ek, respectively. The semantic distance between expressions, d(l,E), is the sum of the semantic distances between words, d(lk,Ek) , multiplied by weights, wk as shown in formula (1). 5 In fact, they collect cooccurrence data of the form ""verb surface"
1993.tmi-1.7,C92-2115,0,0.108992,"osed analogy-based translation[6] in order to overcome problems inherent in conventional machine translation. Since the late 1980s, several organizations have begun research along this line. First, it was applied to the translation of parts of sentences such as noun phrases including our first model, Example-Based Machine Translation (EBMT) and high quality translation was demonstrated.[7,8,9]. Several methods to translate a whole sentence have been proposed including our second model, Transfer-Driven Machine Translation (TDMT) and are currently under investigation from various points of view.[7, 10, 11, 12, 13, 14, 15, 16, 17] In this paper, semantic distance calculation is applied for structural disambiguation and called EBD. 3.2 Semantic Distance Calculation Here, we explain one of the basic techniques of EBMT and EBD, i.e., calculation of the semantic distance of two expressions.[8,9] The input, I, and the source part of the example, E, are n-tuples of words, lk and Ek, respectively. The semantic distance between expressions, d(l,E), is the sum of the semantic distances between words, d(lk,Ek) , multiplied by weights, wk as shown in formula (1). 5 In fact, they collect cooccurrence data of the form ""verb surface"
1993.tmi-1.7,1992.tmi-1.15,0,0.200835,"osed analogy-based translation[6] in order to overcome problems inherent in conventional machine translation. Since the late 1980s, several organizations have begun research along this line. First, it was applied to the translation of parts of sentences such as noun phrases including our first model, Example-Based Machine Translation (EBMT) and high quality translation was demonstrated.[7,8,9]. Several methods to translate a whole sentence have been proposed including our second model, Transfer-Driven Machine Translation (TDMT) and are currently under investigation from various points of view.[7, 10, 11, 12, 13, 14, 15, 16, 17] In this paper, semantic distance calculation is applied for structural disambiguation and called EBD. 3.2 Semantic Distance Calculation Here, we explain one of the basic techniques of EBMT and EBD, i.e., calculation of the semantic distance of two expressions.[8,9] The input, I, and the source part of the example, E, are n-tuples of words, lk and Ek, respectively. The semantic distance between expressions, d(l,E), is the sum of the semantic distances between words, d(lk,Ek) , multiplied by weights, wk as shown in formula (1). 5 In fact, they collect cooccurrence data of the form ""verb surface"
1993.tmi-1.7,1992.tmi-1.14,0,0.0423508,"osed analogy-based translation[6] in order to overcome problems inherent in conventional machine translation. Since the late 1980s, several organizations have begun research along this line. First, it was applied to the translation of parts of sentences such as noun phrases including our first model, Example-Based Machine Translation (EBMT) and high quality translation was demonstrated.[7,8,9]. Several methods to translate a whole sentence have been proposed including our second model, Transfer-Driven Machine Translation (TDMT) and are currently under investigation from various points of view.[7, 10, 11, 12, 13, 14, 15, 16, 17] In this paper, semantic distance calculation is applied for structural disambiguation and called EBD. 3.2 Semantic Distance Calculation Here, we explain one of the basic techniques of EBMT and EBD, i.e., calculation of the semantic distance of two expressions.[8,9] The input, I, and the source part of the example, E, are n-tuples of words, lk and Ek, respectively. The semantic distance between expressions, d(l,E), is the sum of the semantic distances between words, d(lk,Ek) , multiplied by weights, wk as shown in formula (1). 5 In fact, they collect cooccurrence data of the form ""verb surface"
1993.tmi-1.7,1992.tmi-1.4,0,0.0337004,"osed analogy-based translation[6] in order to overcome problems inherent in conventional machine translation. Since the late 1980s, several organizations have begun research along this line. First, it was applied to the translation of parts of sentences such as noun phrases including our first model, Example-Based Machine Translation (EBMT) and high quality translation was demonstrated.[7,8,9]. Several methods to translate a whole sentence have been proposed including our second model, Transfer-Driven Machine Translation (TDMT) and are currently under investigation from various points of view.[7, 10, 11, 12, 13, 14, 15, 16, 17] In this paper, semantic distance calculation is applied for structural disambiguation and called EBD. 3.2 Semantic Distance Calculation Here, we explain one of the basic techniques of EBMT and EBD, i.e., calculation of the semantic distance of two expressions.[8,9] The input, I, and the source part of the example, E, are n-tuples of words, lk and Ek, respectively. The semantic distance between expressions, d(l,E), is the sum of the semantic distances between words, d(lk,Ek) , multiplied by weights, wk as shown in formula (1). 5 In fact, they collect cooccurrence data of the form ""verb surface"
1993.tmi-1.7,C92-2101,0,0.0320298,"hat EBD in TDMT can properly deal with many kind of structural ambiguity in Japanese. EBD is easily incorporated into conventional parsers by using EBD as a subroutine. 1) During parsing, by augmenting rules for pp-attachment, or, 2) After parsing, by traversing parse trees, 3-tuples, ""x p y"", are passed to the EBD subroutine. This integration can utilize the parser's global constraints, such as no cross modification, and lexical constraint, such as the obligatory case of the verb's case frame, thus improving correct-rate(C). In our experiments, the examples are collected by hand. Kaji et al. [20] proposed a method that automates collection of bilingual examples from a corpus. The authors consider that automation of example collection is indispensable for large-scale implementation and plan to devise a method in the near future. In order to disambiguate pp-attachment, Jensen et a/.[21] proposed a method that parses definition sentences in a dictionary and extracts necessary information by patternmatching against the parse trees. Their method requires many rules that extract semantic relations and reliability factors from the parse tree pattern for each preposition, relation by relation"
1993.tmi-1.7,J87-3005,0,0.0261756,"are passed to the EBD subroutine. This integration can utilize the parser's global constraints, such as no cross modification, and lexical constraint, such as the obligatory case of the verb's case frame, thus improving correct-rate(C). In our experiments, the examples are collected by hand. Kaji et al. [20] proposed a method that automates collection of bilingual examples from a corpus. The authors consider that automation of example collection is indispensable for large-scale implementation and plan to devise a method in the near future. In order to disambiguate pp-attachment, Jensen et a/.[21] proposed a method that parses definition sentences in a dictionary and extracts necessary information by patternmatching against the parse trees. Their method requires many rules that extract semantic relations and reliability factors from the parse tree pattern for each preposition, relation by relation. However, the EBD mechanism can deal with every preposition in a uniform way. EBD attaches importance to the individuality of linguistic phenomena. For the time being, it does not aim to extract any abstract knowledge from examples, but to make use of examples directly. To generalize examples"
1993.tmi-1.7,C92-1029,0,0.0229681,"tural disambiguation16 based on semantic distance in TDMT, which translates 12 The causes of failures are intricate. Here, the authors determine countermeasures under the principle that any change in the thesaurus and semantic distance definition is to be avoided. 13 We have shown that, in general, the smaller the distance, the better the quality[8]. 14 We have shown that, in general, the more examples we have, the better the quality[8]. 15 In the case of ambiguous scope caused by coordinate conjunctions, it is necessary to integrate EBD with a parsing method like the one proposed by Kurohashi[22] which is based on dynamic programming. 16 In TDMT, tiebreaking has not been implemented. - 88 - - Japanese spoken sentence into English. The experimental results demonstrated that EBD in TDMT can properly deal with many kind of structural ambiguity in Japanese. EBD is easily incorporated into conventional parsers by using EBD as a subroutine. 1) During parsing, by augmenting rules for pp-attachment, or, 2) After parsing, by traversing parse trees, 3-tuples, ""x p y"", are passed to the EBD subroutine. This integration can utilize the parser's global constraints, such as no cross modification, a"
1993.tmi-1.7,C92-2107,0,0.0157623,"proposed a method that parses definition sentences in a dictionary and extracts necessary information by patternmatching against the parse trees. Their method requires many rules that extract semantic relations and reliability factors from the parse tree pattern for each preposition, relation by relation. However, the EBD mechanism can deal with every preposition in a uniform way. EBD attaches importance to the individuality of linguistic phenomena. For the time being, it does not aim to extract any abstract knowledge from examples, but to make use of examples directly. To generalize examples[23] or to compress the example database with no drop in system performance is of importance from the standpoint of space and time complexity. 5.2 Semantic Granularity As explained below, thesauri give appropriate semantic granularity for problems of natural language processing. In the pp-attachment problem, compared with SBD which utilizes the frequency of word cooccurrence (3-tuples of words), EBD which utilizes examples (3-tuples of words) and a thesaurus can achieve higher correct-rate(C) with far fewer examples (a much smaller corpus) as shown in section 4.3.2. This implies that best match ba"
1999.mtsummit-1.34,C96-1070,0,0.132529,"Missing"
1999.mtsummit-1.34,C96-1075,0,0.0608382,"Missing"
1999.mtsummit-1.34,P89-1013,0,0.0183586,"Missing"
1999.mtsummit-1.34,C88-2118,0,0.0291956,"Missing"
1999.mtsummit-1.34,P91-1024,1,0.749748,"Missing"
1999.mtsummit-1.34,P98-2233,1,0.883543,"Missing"
1999.mtsummit-1.34,W97-0404,0,0.260944,"Missing"
1999.mtsummit-1.34,W99-0207,1,\N,Missing
1999.mtsummit-1.34,C98-2228,1,\N,Missing
2001.mtsummit-papers.3,C96-1070,0,0.0318176,"Missing"
2001.mtsummit-papers.3,C92-2067,0,0.862792,"translated sentences have been manually ranked by native speakers of target languages. Such subjective evaluation by ranking, however, is taxing on both time and resources (King, 1996). The developers of TDMT would like to evaluate their MT system under development more frequently; therefore if automatic evaluation methods are inexpensive, fast, sufficiently accurate for them to assess whether or not the current version of their MT system is improved, then these automatic evaluation methods will prove beneficial. Conventional approaches to automatic evaluation include methods (Thompson, 1991; Su, 1992; Takezawa et al., 1999; Sugaya et al., 1999; Yasuda et al., 2000; Yasuda et al., 2001) that automatically assign one of several ranks (Sumita et al., 1999; Nagao & Tsujii, 1985) such as A, B, C, and D to MT output according to a single edit distance between an MT output and a correct translation example. The single edit distance can be differently designed, but changing its design makes assigning a certain rank more accurate, but another rank less accurate. For examples, EDi (i = 1, 5, 9, or 13) in Figure 1 differ from each other in its design. For ED1, the combination of edit operators: eith"
2001.mtsummit-papers.3,1999.mtsummit-1.34,1,0.851374,"Missing"
2001.mtsummit-papers.3,1999.mtsummit-1.44,0,0.027287,"Missing"
2001.mtsummit-papers.3,2001.mtsummit-papers.67,0,0.619385,"Missing"
2002.tmi-papers.20,J93-2003,0,0.170703,"by applying hierarchical phrase alignment. The hierarchical phrase alignment is a method to align bilingual sentences phrase-by-phrase employing the partial parse results. Based on the hierarchical phrase alignment, a translation model is trained on a chunked corpus by converting hierarchically aligned phrases into a sequence of chunks. The second method transforms the bilingual correspondence of the phrase alignments into that of translation model. Both of our approaches yield better quality of the translaiton model. 1 Introduction A statistical machine translation (SMT), first introduced by Brown et al. (1993), represents a translation process as a noisy channel model that consists of a source-channel model, a translation model and a prior, language model of target language texts. This transformed the problem of machine translation into a maximum posteriori solution to the source-channel paradigm. The translation model is based on word-for-word translation and limited to allow only one channel source word to be aligned from a channel target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In"
2002.tmi-papers.20,C92-2101,0,0.0830307,"nger input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset of all the alignmen"
2002.tmi-papers.20,J99-4005,0,0.129977,"he search problem is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arriv"
2002.tmi-papers.20,W01-1405,0,0.0146108,"t word, B( f ), relative to the center of the previous source word, k. – d&gt;1 ( j − j0 |B( f )) : Distortion probability for non-head words. The position of a non-head word j is determined by the word class and relative to the previous target word generated from the same source word ( j0 ). • NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Pr"
2002.tmi-papers.20,P00-1056,0,0.826996,"el target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In addition, the training procedure relies on the EM algorithm, which can converge to an optimal solution but does not assured the global maximum parameter assignment. Furthermore, the translation models are represented by the numbers of parameters, so that easily suffered from the overfitting problem. In order to overcome these problems, simpler models, such as word-for-word translation models (Brown et al. 1993) or HMM models (Och & Ney 2000), have been introduced to determine the initial parameters and to bootstrap the training. This paper describes two methods to overcome the above problems by using hierarchical phrase alignment (Imamura 2001). Hierarchical phrase alignment (HPA) is a method to align bilingual texts phrase-by-phrase from partial parse results. One method converts the hierarchically aligned phrasal texts into a pair of sequences of chunks of words, treating the word-forword translation model as a chunk-for-chunk translation model. The second method computes the parameters for the translation model from the comput"
2002.tmi-papers.20,W99-0604,0,0.0797365,"y of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Problem Training for the various parameters, t, n, p1 , d1 , d&gt;1 relies on the EM algorithm, which optimizes the log-likelihood of the model over a given bilingual corpus. The EM algorithm can find an optimal solution, although it cannot assure finding the globally best one. As the number of parameters is larger than those of speech it will become e"
2002.tmi-papers.20,W01-1408,0,0.209859,"em is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arrived in Kyoto — — — k"
2002.tmi-papers.20,1999.mtsummit-1.34,1,0.807335,"Missing"
2002.tmi-papers.20,C00-2135,0,0.102903,"+train model is better for longer input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset"
2002.tmi-papers.20,P01-1030,0,\N,Missing
2002.tmi-tutorials.1,W99-0905,0,\N,Missing
2002.tmi-tutorials.1,1987.mtsummit-1.11,0,\N,Missing
2002.tmi-tutorials.1,1995.tmi-1.28,0,\N,Missing
2002.tmi-tutorials.1,P98-1069,0,\N,Missing
2002.tmi-tutorials.1,C98-1066,0,\N,Missing
2002.tmi-tutorials.1,C92-2101,0,\N,Missing
2002.tmi-tutorials.1,C00-2131,0,\N,Missing
2002.tmi-tutorials.1,P95-1033,0,\N,Missing
2002.tmi-tutorials.1,C96-1078,0,\N,Missing
2002.tmi-tutorials.1,J97-2004,0,\N,Missing
2002.tmi-tutorials.1,1993.tmi-1.5,0,\N,Missing
2002.tmi-tutorials.1,C96-1030,0,\N,Missing
2003.mtsummit-papers.1,P02-1038,0,0.0122726,"Both of these automatic evaluators were reported to be able to closely simulate the human evaluation of MT systems. BLEU (Papineni et al., 2001) was shown to correlate highly to two human evaluation measures of translation, adequacy and fluency, which are graded from 1 (very bad) to 5 (very good). RED (Akiba et al., 2001) was shown to approximate the performance of an MT system, as measured by the ATR standard of MT evaluation. Although many researchers have insisted on the quality of their novel approaches for developing MT systems by the evaluation results of BLEU (Yamada and Knight, 2002; Och and Ney, 2002; Marcu and Wong, 2002) in their papers, we have doubts over whether the automatic evaluators, including BLEU, are perfectly or significantly reliable as a basis to form conclusions in research papers. As the regression results in (Papineni et al., 2001) indicate, for example, a small but distinct gap exists between the evaluation results by BLEU and those by human evaluators. This gap highlights the possibility that BLEU may cause some misleading conclusions. To prevent either developers or researchers of MT from drawing misleading conclusions when using the automatic evaluators, it is essent"
2003.mtsummit-papers.1,2001.mtsummit-papers.62,1,0.882564,"T evaluation. In the same way, nine Japanese translations were evaluated by nine EJ translators. Each translation was finally assigned the median rank among its nine ranks. Table 2 shows how the median ranks become more consistent as the number of human evaluators increased. Each fractional number is the correlation coefficient between two independent vectors of median ranks, each of which were calculated from one of two disjoint sets of human evaluators selected randomly. Even in the case 6 Some researchers classify these as example-based MT systems. Figure 1: Results of a paired comparison (Sugaya et al., 2001) between translations by an MT system and those by a person whose TOEIC score is known. “X < Y ” denotes that Y is superior to X. Each number, such as 420, denotes that person’s TOEIC score. SAT-0203 Person-685 D3 -0203 < < < Person-420 TDMT-0203 Person-965 < < Person-540 Person-820 < < where the number of human evaluators is four, the correlation coefficient reached the large number of 0.946. The median ranks among the nine ranks are used in this paper, and have a high correlation coefficient of at least 0.946. 3.2 Design of Experiment This section gives three critical points to consider in t"
2003.mtsummit-papers.1,1999.mtsummit-1.34,1,0.827808,"makes it easier for the MT developers to evaluate whether their new ideas for constructing MT systems are effective. As a criterion for human evaluation, this paper adopts the ATR standard of MT evaluation 1 , which was proposed for the human evaluation of S2SMT systems. Two promising automatic evaluators were simultaneously proposed: BLEU (bilingual evaluation understudy (Papineni et al., 2001)) and RED (Ranker based on Edit Distances (Akiba et al., 2001)) as rapid MT evaluation methods. BLEU was designed to evaluate the performance of MT systems on a test 1 ATR’s standard of MT evaluation (Sumita et al., 1999) is defined as follows: (A) Perfect: no problems in either information or grammar; (B) Fair: easy-to-understand, with either some unimportant information missing or flawed grammar; (C) Acceptable: broken, but understandable with effort; (D) Nonsense: important information has been translated incorrectly. suite. On the other hand, RED was designed to evaluate a translation segment such as an utterance or a sentence. As mentioned in (Akiba et al., 2001), RED can also be used to evaluate the performance of MT systems by summing all of the evaluation results of translations for the test suite. Bot"
2003.mtsummit-papers.1,W01-1401,1,0.836602,"riven Machine Translation) is a pattern-based MT system6 using hand-coded syntactic transfer rules (Furuse and Iida, 1996). Each version of HPAT (Hierarchical Phrase Alignmentbased Translation) is a pattern-based system using automatically-generated syntactic transfer (Imamura, 2002). Each version of SAT (Statistical ATR Translator) is an SMT (Statistical machine translation) system using hierarchical phrase alignment (Watanabe et al., 2002). Each version of D 3 (DPmatch Driven transDucer) is an example-based MT system using online-generated translation patterns, which are close to templates (Sumita, 2001). Human evaluation results: Nine English translations of each Japanese sentence by the JE MTs were simultaneously shown to each of nine JE translators that were native speakers of English, to keep the evaluation results consistent. These translations were then evaluated according to the ATR standard of MT evaluation. In the same way, nine Japanese translations were evaluated by nine EJ translators. Each translation was finally assigned the median rank among its nine ranks. Table 2 shows how the median ranks become more consistent as the number of human evaluators increased. Each fractional num"
2003.mtsummit-papers.1,W02-0701,1,0.828443,"Missing"
2003.mtsummit-papers.1,W02-0700,0,0.101227,"Missing"
2003.mtsummit-papers.1,takezawa-etal-2002-toward,1,0.812751,"ber of reference translations affect the accuracy of the ordering of MT systems? 3. How accurately does each automatic evaluator order the same type of MT systems? 3.1 Experiment Resources The authors evaluated the two automatic evaluators to answer the following questions: This section describes the experiment resources used: test suite, evaluated MTs, reference translations and human evaluation results. Test Suite: The test suite used consists of three hundred and forty-five pairs of English and Japanese sentences, which were randomly selected from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). BTEC contains a variety of expressions used in a number of situations related to overseas travel. Reference Translations: The authors asked five native speakers of Japanese who are familiar with English to translate English sentences in the test suite in three ways. Consequently, there were sixteen reference translations in Japanese, including Japanese sentences in the test suite. The reference translations in English for the test suite were prepared in the same way as those in Japanese. Evaluated MTs: The MT systems used were the nine EJ MT systems of three types and the nine JE MT systems"
2003.mtsummit-papers.1,2001.mtsummit-papers.3,1,0.902656,"learning translation knowledge from parallel corpora, MT developers are requiring rapid MT evaluation methods now more than ever. A rapid MT evaluation method makes it easier for the MT developers to evaluate whether their new ideas for constructing MT systems are effective. As a criterion for human evaluation, this paper adopts the ATR standard of MT evaluation 1 , which was proposed for the human evaluation of S2SMT systems. Two promising automatic evaluators were simultaneously proposed: BLEU (bilingual evaluation understudy (Papineni et al., 2001)) and RED (Ranker based on Edit Distances (Akiba et al., 2001)) as rapid MT evaluation methods. BLEU was designed to evaluate the performance of MT systems on a test 1 ATR’s standard of MT evaluation (Sumita et al., 1999) is defined as follows: (A) Perfect: no problems in either information or grammar; (B) Fair: easy-to-understand, with either some unimportant information missing or flawed grammar; (C) Acceptable: broken, but understandable with effort; (D) Nonsense: important information has been translated incorrectly. suite. On the other hand, RED was designed to evaluate a translation segment such as an utterance or a sentence. As mentioned in (Akiba"
2003.mtsummit-papers.1,C96-1070,0,0.0310161,"Missing"
2003.mtsummit-papers.1,2002.tmi-papers.20,1,0.838777,"3 (0110, 0204, 0209), and three versions of JE SAT (0110, 0204, 0209). TDMT, HPAT, SAT, and D3 are different types of MT systems (Sumita, 2002). Each version of TDMT (Transfer Driven Machine Translation) is a pattern-based MT system6 using hand-coded syntactic transfer rules (Furuse and Iida, 1996). Each version of HPAT (Hierarchical Phrase Alignmentbased Translation) is a pattern-based system using automatically-generated syntactic transfer (Imamura, 2002). Each version of SAT (Statistical ATR Translator) is an SMT (Statistical machine translation) system using hierarchical phrase alignment (Watanabe et al., 2002). Each version of D 3 (DPmatch Driven transDucer) is an example-based MT system using online-generated translation patterns, which are close to templates (Sumita, 2001). Human evaluation results: Nine English translations of each Japanese sentence by the JE MTs were simultaneously shown to each of nine JE translators that were native speakers of English, to keep the evaluation results consistent. These translations were then evaluated according to the ATR standard of MT evaluation. In the same way, nine Japanese translations were evaluated by nine EJ translators. Each translation was finally a"
2003.mtsummit-papers.1,P02-1039,0,0.0122844,"tions for the test suite. Both of these automatic evaluators were reported to be able to closely simulate the human evaluation of MT systems. BLEU (Papineni et al., 2001) was shown to correlate highly to two human evaluation measures of translation, adequacy and fluency, which are graded from 1 (very bad) to 5 (very good). RED (Akiba et al., 2001) was shown to approximate the performance of an MT system, as measured by the ATR standard of MT evaluation. Although many researchers have insisted on the quality of their novel approaches for developing MT systems by the evaluation results of BLEU (Yamada and Knight, 2002; Och and Ney, 2002; Marcu and Wong, 2002) in their papers, we have doubts over whether the automatic evaluators, including BLEU, are perfectly or significantly reliable as a basis to form conclusions in research papers. As the regression results in (Papineni et al., 2001) indicate, for example, a small but distinct gap exists between the evaluation results by BLEU and those by human evaluators. This gap highlights the possibility that BLEU may cause some misleading conclusions. To prevent either developers or researchers of MT from drawing misleading conclusions when using the automatic evalu"
2003.mtsummit-papers.1,2002.tmi-papers.9,0,0.0153537,"ake a vector so that each element corresponds to the same MT output. Correlation # of human evaluators (N ) 1 2 3 4 0.736 0.896 0.901 0.946 sions of JE D3 (0110, 0204, 0209), and three versions of JE SAT (0110, 0204, 0209). TDMT, HPAT, SAT, and D3 are different types of MT systems (Sumita, 2002). Each version of TDMT (Transfer Driven Machine Translation) is a pattern-based MT system6 using hand-coded syntactic transfer rules (Furuse and Iida, 1996). Each version of HPAT (Hierarchical Phrase Alignmentbased Translation) is a pattern-based system using automatically-generated syntactic transfer (Imamura, 2002). Each version of SAT (Statistical ATR Translator) is an SMT (Statistical machine translation) system using hierarchical phrase alignment (Watanabe et al., 2002). Each version of D 3 (DPmatch Driven transDucer) is an example-based MT system using online-generated translation patterns, which are close to templates (Sumita, 2001). Human evaluation results: Nine English translations of each Japanese sentence by the JE MTs were simultaneously shown to each of nine JE translators that were native speakers of English, to keep the evaluation results consistent. These translations were then evaluated"
2003.mtsummit-papers.47,C00-1019,0,0.0225145,"ribe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characterized by disregarding functi"
2003.mtsummit-papers.47,1999.mtsummit-1.37,0,0.0277591,"ated We describe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characterized by disre"
2003.mtsummit-papers.47,W02-0718,0,0.028516,"tyle differences between the input sentence and the example corpus. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from Yuji Matsumoto Nara Institute of Science and Technology 8916-5 Takayama, Ikoma Nara 630-0101 matsu@is.aist-nara.ac.jp those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to the input sentence and modifies their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungrammatical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. One is that the translation accuracy drastically drops as input sentenc"
2003.mtsummit-papers.47,W01-1401,1,0.922553,"nslated Translated We describe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characteri"
2003.mtsummit-papers.47,takezawa-etal-2002-toward,1,0.906734,"nt sentences to an input sentence are defined as follows. A sentence that shares the main meaning with the input sentence despite missing some unimportant information. It does not contain information additional to that in the input sentence. Table 2: Cross Perplexity a sufficiently large volume of examples. These texts do not come from real speech but are directly written by imaging speech. They rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 2002). Perplexity is used as a metric for how well a language model derived from a training set matches a test"
2003.mtsummit-papers.47,A00-1006,1,0.874376,"Missing"
2003.mtsummit-papers.54,C02-1076,1,0.940109,"“o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternative strategy taken after the framework of example-based machine translation: Retrieve a translation example from a parallel corpus whose source part is similar to the input sentence, then slightly modify the target part of the example"
2003.mtsummit-papers.54,J93-2003,0,0.0952076,"ine translation framework. The decoder presented here is based on the greedy approach to the decoding problem, but the search is initiated from a similar translation extracted from a bilingual corpus. The experiments on multilingual translations showed that the proposed method was far superior to a word-by-word generation beam search algorithm. 1 Introduction E = NULL0 show1 me2 The framework of statistical machine translation formulates the problem of translating a sentence in a language J into another language E as the maximization problem of the conditional probability Eˆ = argmaxE P(E|J) (Brown et al., 1993). The application of the Bayes Rule resulted in Eˆ = argmaxE P(E)P(J|E). The former term P(E) is called a language model, representing the likelihood of E. The latter term P(J|E) is called a translation model, representing the generation probability from E into J. Under this concept, Brown et al. (1993) presented a translation model where a source sentence is mapped to a target sentence with the notion of word alignment1 . Although it has been successfully applied to similar language pairs, such as French– English and German–English, little success has been achieved for drastically different l"
2003.mtsummit-papers.54,C00-1019,0,0.0234736,"earch process from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is,"
2003.mtsummit-papers.54,P98-2158,0,0.0894867,"Missing"
2003.mtsummit-papers.54,P01-1030,0,0.476307,"to merging statistical and example-based machine translation (Nagao, 1984). J= A=( 1 The source/target sentences are the channel model’s source/target that correspond to the translation system’s output/input. the3 uindo1 no2 shinamono3 7 0 4 one4 o4 0 in5 the6 window7 mise5 tekudasai6 1 1 ) Figure 1: Example of word alignment Given an input, the decoder first finds some translation examples whose source part is similar to the input. Second, it modifies the retrieved translation using the greedy search algorithm, a hill-climbing approach to find a (hopefully good) translation as introduced by Germann et al. (2001). Translation experiments were carried out between four different languages pairs, Chinese, English, Japanese and Korean, and it was verified that in any directions, the proposed decoder was superior to the beam search based decoder. The following section briefly reviews the word alignment based statistical machine translation, then proposes the example-based decoder. Section 4 shows experiments on various language pairs, followed by discussion. 2 Statistical Machine Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A,"
2003.mtsummit-papers.54,W02-1018,0,0.0187635,"with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased translation model (Yamada and Knight, 2001) or the phrase-based translation model (Marcu and Wong, 2002), the more quality improvement will be expected. Furthermore, the example-based decoder can be adapted to the error correction framework with more accurate translation models. The greedy decoding process can be initiated from the translations from other systems, such as a rule-based MT system or an example-based MT system, instead of from translation examples extracted solely from a bilingual corpus. 6 Conclusion This paper presented an example-based decoding algorithm for statistical machine translation, which can offer both of the benefits of example-based and statistical machine translation"
2003.mtsummit-papers.54,P01-1050,0,0.354659,".4/42.8 10.5/63.4 16.3/51.7 14.5/39.5 14.0/51.5 19.5/42.5 15.0/32.5 non-matched 28.2/38.6 30.3/24.9 25.7/29.5 39.1/26.8 33.4/26.2 31.8/32.8 37.6/31.4 35.8/28.1 50.3/10.9 31.9/28.7 31.3/31.9 27.7/12.3 total 21.4/48.8 23.1/24.9 20.0/37.6 31.0/34.1 27.8/29.4 24.7/36.9 28.4/42.2 29.2/36.1 38.2/20.6 24.9/37.6 26.7/36.1 22.7/20.2 tions, not from merely guessed sentences by an input string. In general, the greedy method is strongly influenced by the initial state of the search, but our method provides the strong bias especially required for long distance language pairs, such as Japanese and English. Marcu (2001) proposed a slightly different approach in which translation examples are extracted phrase-by-phrase into a translation memory and the search process is initiated by concatenating the phrases found in the memory. Both share similar input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: 銀行 の 前 で バッグ を ひったくら れ まし た i was robbed of my bag in front of the bank my bag was stolen in the front o"
2003.mtsummit-papers.54,P02-1038,0,0.12391,"ne of the easiest problems, while the English-to-Chinese will be the hardest direction to translate. 4.3 Evaluation The translation experiments were carried out on 510 sentences selected randomly from the test set. Two decoding algorithms were tested. One is the example-based decoder as explained in Section 3, and the other is a left-to-right output generation beam search algorithm as presented by Watanabe and Sumita (2002). The metrics for this evaluation were as follows: WER: Word-error-rate, which penalizes the edit distance (insertion/deletion/substitution) against reference translations (Och and Ney, 2002). English Japanese 167,163 980,790 1,148,428 15,641 21,896 5,547 9,220 35.35 24.06 Korean 1,269,888 13,395 4,191 20.34 PER: Position independent WER, which penalizes without considering positional disfluencies (Och and Ney, 2002). BLEU: BLEU score, which computes the ratio of the n-gram for the translation results found in reference translations (Papineni et al., 2002). Contrary to the above error metrics, the higher scores indicates better translations. SE: Subjective evaluation ranks ranging from A to D (A : perfect, B : fair, C : acceptable and D : nonsense), judged by a native speaker. The"
2003.mtsummit-papers.54,W01-1408,0,0.424899,"tistical and example-based machine translation (Nagao, 1984). J= A=( 1 The source/target sentences are the channel model’s source/target that correspond to the translation system’s output/input. the3 uindo1 no2 shinamono3 7 0 4 one4 o4 0 in5 the6 window7 mise5 tekudasai6 1 1 ) Figure 1: Example of word alignment Given an input, the decoder first finds some translation examples whose source part is similar to the input. Second, it modifies the retrieved translation using the greedy search algorithm, a hill-climbing approach to find a (hopefully good) translation as introduced by Germann et al. (2001). Translation experiments were carried out between four different languages pairs, Chinese, English, Japanese and Korean, and it was verified that in any directions, the proposed decoder was superior to the beam search based decoder. The following section briefly reviews the word alignment based statistical machine translation, then proposes the example-based decoder. Section 4 shows experiments on various language pairs, followed by discussion. 2 Statistical Machine Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A,"
2003.mtsummit-papers.54,P02-1040,0,0.0729813,"search algorithm as presented by Watanabe and Sumita (2002). The metrics for this evaluation were as follows: WER: Word-error-rate, which penalizes the edit distance (insertion/deletion/substitution) against reference translations (Och and Ney, 2002). English Japanese 167,163 980,790 1,148,428 15,641 21,896 5,547 9,220 35.35 24.06 Korean 1,269,888 13,395 4,191 20.34 PER: Position independent WER, which penalizes without considering positional disfluencies (Och and Ney, 2002). BLEU: BLEU score, which computes the ratio of the n-gram for the translation results found in reference translations (Papineni et al., 2002). Contrary to the above error metrics, the higher scores indicates better translations. SE: Subjective evaluation ranks ranging from A to D (A : perfect, B : fair, C : acceptable and D : nonsense), judged by a native speaker. The scores are evaluated by the ratio of A ranked sentences, A+B for either A or B ranks, and A+B+C for either A, B or C ranks. We have evaluated only a languageto-English and a language-to-Japanese assuming that they are translations for Japanese-toEnglish and English-to-Japanese, respectively 2. For all the languages, 16 reference translations were created for the non-s"
2003.mtsummit-papers.54,W01-1402,0,0.0162967,"from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased t"
2003.mtsummit-papers.54,W01-1401,1,0.800277,"problem by combining two different unit sizes, by allowing a decoder to initiate the greedy search process from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assign"
2003.mtsummit-papers.54,takezawa-etal-2002-toward,1,0.936794,"be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternative strategy taken after the framework of example-based machine translation: Retrieve a translation example from a parallel corpus whose source part is similar to the input sentence, then slightly modify the target part of the example"
2003.mtsummit-papers.54,C00-2123,0,0.032521,"ted by the structural differences: i.e., English takes an SVO structure while Japanese usually takes the form of SOV. In addition, insertion and deletion occur very frequently as seen in the example. For instance, there exists no corresponding Japanese morphemes for “the3 ” and “the6 ”. Therefore, they should be inserted when translating from Japanese. Similarly, Japanese morphemes “no2 ” and “o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-bas"
2003.mtsummit-papers.54,C02-1050,1,0.906411,"ccur very frequently as seen in the example. For instance, there exists no corresponding Japanese morphemes for “the3 ” and “the6 ”. Therefore, they should be inserted when translating from Japanese. Similarly, Japanese morphemes “no2 ” and “o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternativ"
2003.mtsummit-papers.54,P01-1067,0,0.0538691,"ed similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased translation model (Yamada and Knight, 2001) or the phrase-based translation model (Marcu and Wong, 2002), the more quality improvement will be expected. Furthermore, the example-based decoder can be adapted to the error correction framework with more accurate translation models. The greedy decoding process can be initiated from the translations from other systems, such as a rule-based MT system or an example-based MT system, instead of from translation examples extracted solely from a bilingual corpus. 6 Conclusion This paper presented an example-based decoding algorithm for statistical machine translation, which can offer both of the"
2004.iwslt-evaluation.2,J90-2002,0,0.724612,"imal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, briefly explain each system, and then compare them. Finally, we ex1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technolog"
2004.iwslt-evaluation.2,W01-1401,1,0.863919,"on is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in co"
2004.iwslt-evaluation.2,P91-1024,1,0.754066,"source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track. bilingual corpus bilingual dictionary thesaurus grammar D3 travel domain (20K) in-house in-house N.A. HPAT travel domain (20K) in-house in-house in-house D3 achieves a go"
2004.iwslt-evaluation.2,C94-1015,0,0.0154509,"is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4. 13 plain the selector used to determine the best from multiple translations based on SMT models. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial"
2004.iwslt-evaluation.2,P03-1057,1,0.787621,"s. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven tran"
2004.iwslt-evaluation.2,C02-1076,1,0.866736,"7.00 70.00 77.60 83.40 16.60 HPAT 38.60 59.80 77.40 83.40 16.60 SELECT 59.80 73.00 82.40 87.80 12.20 DIFF. +2.80 +3.00 +4.80 +4.40 -4.40 Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”). 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the fluency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a s"
2004.iwslt-evaluation.2,hogan-frederking-1998-evaluation,0,0.0713962,"e drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its fluency improves as the size of corpus increases. 2.4. Results 2.4.1. Selecting Effect 2.5. Discussion As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element transRelated works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging apGood: easy to understand, with either some unimportant information missing or flawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly. 15 translations with additional constraints [17, 18, 19]:  P (¯fi |¯ ea i ) P (f |e) ≈ Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus. S S,A S,A,B S,A,B,C D D3 34.80 47.40 62.60 73.40 26.60 HPAT 25.20 44.20 70.40 80.40 19"
2004.iwslt-evaluation.2,P01-1050,0,0.0620593,"age and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sent"
2004.iwslt-evaluation.2,P01-1030,0,0.0810071,"nces. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihoo"
2004.iwslt-evaluation.2,2003.mtsummit-papers.54,1,0.856638,"s regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihood of the phrase-segmen"
2004.iwslt-evaluation.2,N03-1017,0,0.00448749,"ndling long Japanese input. The latter was attributed to the fact that we tuned our parameter to mWER and we exploited phrase models as well. Table 8: Evaluation - IWSLT Chinese-to-English supplied task. System Top Our Bottom (15) where count(¯ e, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences. mWER 45.59 46.99 61.69 Fluency 38.20 38.20 25.04 Adequacy 33.38 29.50 29.06 4. Other Features of C3 This section introduces another feature of C3: paraphrasing and filtering corpora, which are not used in the IWSLT04 task but are useful for boosting MT performance. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. Specifically, theis variety makes it m"
2004.iwslt-evaluation.2,2003.mtsummit-papers.53,0,0.0727491,"Missing"
2004.iwslt-evaluation.2,W03-1001,0,0.0212046,"Missing"
2004.iwslt-evaluation.2,W04-3216,0,0.0140311,"regarded as the distortion tion 5, the term P (f |¯f , e probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f : ¯, e) ∝ P (f |¯f , e P (¯f |f )  P (¯fj |f ) ≈ Equation 12 can be regarded as a Hidden Markov Model in ¯ j in the lattice F ¯ is treated as an which each source phrase F ¯ observation emitted from a state Ei , a target phrase, in the ¯ as shown in Figure 2. lattice E, (8) (9) j The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To briefly sum"
2004.iwslt-evaluation.2,J03-1002,0,0.00549884,"rocedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (14) To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained fixed during the HMM iterations. 3.8. Results The results appear strange in two points: (1) Our proposal didn’t work well for the Japanese-to-English track but did work well for the Chinese-to-English track; (2) Our proposal achieved high fluency but marked low adequacy. 3.6. Phrase Segment Induction Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-s"
2004.iwslt-evaluation.2,P02-1038,0,0.0308911,"e segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure ge"
2004.iwslt-evaluation.2,P03-1021,0,0.00794031,"an be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input"
2004.iwslt-evaluation.2,W02-1021,0,0.0253988,"λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (1"
2004.iwslt-evaluation.2,shimohata-sumita-2002-automatic,1,0.850547,"xtract good transfer patterns for HPAT, and to estimate the parameters for SMT. 3.7. Decoder The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created o"
2004.iwslt-evaluation.2,2002.tmi-tutorials.2,0,0.02481,"st translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created on the normalized target sentences had a reduced word-error rate [31]. Finch et al. [32] demonstrated that the expansion of refer"
2004.iwslt-evaluation.2,C04-1017,1,0.875016,"Missing"
2004.iwslt-evaluation.2,E03-1029,1,0.895696,"Missing"
2004.iwslt-evaluation.2,W02-1611,1,\N,Missing
2004.iwslt-evaluation.2,watanabe-etal-2002-statistical,1,\N,Missing
2004.tmi-1.12,W03-0318,1,0.764349,"duction Although machine translation (MT) technology has been undergoing development for several decades, its performance does not yet satisfy users’ needs. Modifying an input sentence into a more translatable one, known as “pre-editing,” is an important means of improving MT performance. (Bernth & Gdaniec 2001) provided a guideline for the manual pre-editing of input sentences. (Mitamura & Nyberg 2001) proposed a controlled language that is advantageous for MT. They also proposed a rewriting tool named KANTOO that supports an author in matching free input sentences to a controlled language. (Doi & Sumita 2003) proposed an automatic pre-editing method that splits long input sentences. All of the previous works of pre-editing deal with partial modiﬁcation of an input sentence. In this paper, we propose a novel pre-editing technique that incorporates similar sentence retrieval in MT to improve the translation of hard-to-translate1 input sentences. The retrieval method has the advantage of relying only on a monolingual corpus, which is easy to prepare on large scale. Figure 1 shows an overview of our proposal. An input sentence 1 A “hard-to-translate” sentence refers to a sentence whose translation qua"
2004.tmi-1.12,P02-1040,0,0.091785,"experiment among three methods and two additional conditions. 3.1 Overview of Automatic Evaluation of Machine Translation In recent years, research on automatic evaluation of MT has become increasingly active. The basic idea of automatic evaluation is that a translation to be tested obtains a higher score as it shares more common parts with several reference translations. There are three basic methods for measuring similarity between a test translation and reference translations: the common N-gram, the word error rate (WER), and the position-independent word error rate (PER). The BLEU method (Papineni et al. 2002), which is one of the major methods, uses the common N-gram method. The similarity score is based on a common N-gram ratio of a test translation to the reference translations. The value of BLEU similarity ranges from 0 to 1. The higher the BLUE score is, the more similar a test sentence is. The method uses a brevity penalty to penalize short-length sentences. The NIST method (NIST 2002), which is also widely used, is a revised version of the BLEU method. The other two methods, WER and PER, are also often used (Tillmann et al. 1997). WER is a length-normalized Levenshtein distance. This metric"
2004.tmi-1.12,W01-1401,1,0.857073,"translation quality of retrieved sentences is better than that of the original sentences. Our approach requires a translation quality measure to determine whether an input is hard-to-translate. Some MT systems can measure their translation quality by themselves. (Ueﬃng et al. 2003) proposed a method to estimate a conﬁdence measure for statistical MT based on word graphs and N-best lists. The low-conﬁdence translations correspond to hardto-translate input sentences. Example-based MT systems can estimate their translation quality from the similarity distance between input and example sentences (Sumita 2001). The parsing result of an input sentence is also useful. The proposed method for measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a comparative study of three methods: common Ngram, common word sequence, and common word set. (Section 3) Furthermore, we add two additional conditions to improve retrieval precision. (Section 4) We describe an experiment on applying similar sentence retrieval to a Japanese-to-English MT system in Section 5. 2 Experimental Data We focus"
2004.tmi-1.12,1999.mtsummit-1.34,1,0.879999,"measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a comparative study of three methods: common Ngram, common word sequence, and common word set. (Section 3) Furthermore, we add two additional conditions to improve retrieval precision. (Section 4) We describe an experiment on applying similar sentence retrieval to a Japanese-to-English MT system in Section 5. 2 Experimental Data We focused on travel conversation, which is a major target domain in speech translation (Sumita et al. 1999) (Wahlster 2000). We used two types of Japanese corpora: a corpus based on machine-aided dialog (MAD) and the basic travel expression corpus (BTEC). The MAD corpus was used as a collection of inputs and the BTEC as a candidate corpus. The MAD corpus contains 2,135 diﬀerent sentences. These sentences are a collection of transcribed utterances that were spoken in several travel situations. Therefore, the corpus reﬂects the characteristics of spoken language. The sentences are divided into two parts: one containing 437 sentences and the other 1,698 sentences. The former part was used in 2 3 The c"
2004.tmi-1.12,takezawa-etal-2002-toward,1,0.810282,"a corpus to be retrieved. an experiment to ﬁnd a method for similar sentence retrieval (Sections 3 and 4). The latter part was used in an experiment applying similar sentence retrieval to MT (Section 5). The BTEC is a collection of edited colloquial travel expressions often found in phrasebooks. It contains 116,773 diﬀerent sentences. The MAD corpus is derived from transcribed utterances, while the BTEC is not. Although both corpora contain expressions frequently used in travel conversation, they have diﬀerent characteristics in terms of average number of words in a sentence 4 and perplexity (Takezawa et al. 2002). We used the BTEC as the candidate corpus because its sentences are shorter and more grammatical than those in the MAD corpus. Consequently, these sentences are more suitable for application to MT. 3 Method for Measuring Similarity between Two Sentences Our method for measuring similarity is based on research into automatic evaluation of MT results. Similarity measurement in automatic evaluation only depends on reference sentences5 and does not require any other knowledge. In addition, many research attempts have demonstrated that automatic evaluation is strongly correlated to human evaluatio"
2004.tmi-1.12,2003.mtsummit-papers.52,0,0.0324055,"slation by Similar Sentence Retrieval can be classiﬁed as hard-to-translate or not by an MT system. If a given input sentence is hard to translate, the similar sentence retrieval function searches for the most similar sentence from a translatable sentence corpus2 and provides it to the MT system. MT performance can be improved if the translation quality of retrieved sentences is better than that of the original sentences. Our approach requires a translation quality measure to determine whether an input is hard-to-translate. Some MT systems can measure their translation quality by themselves. (Ueﬃng et al. 2003) proposed a method to estimate a conﬁdence measure for statistical MT based on word graphs and N-best lists. The low-conﬁdence translations correspond to hardto-translate input sentences. Example-based MT systems can estimate their translation quality from the similarity distance between input and example sentences (Sumita 2001). The parsing result of an input sentence is also useful. The proposed method for measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a compara"
2005.iwslt-1.5,J90-2002,0,0.320474,"aining data conditions showed the potential of the proposed hybrid approach and revealed new directions in how to improve the current system performance. 1. Introduction Corpus-based approaches to machine translation (MT) have achieved much progress over the last decades. There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. Despite a high performance on average, these approaches can often produce translations with severe errors. However, different MT engines not always do the same error. Due to the particular output characteristics of each MT engine, quite different translation hypotheses are produced. Thus, combining multiple MT systems can boost the system performance by exploiting the strengths of each MT engine. W"
2005.iwslt-1.5,2003.mtsummit-papers.54,1,0.79351,"-based SMT engine [MSEP] (cf. Section 2.1.3), an SMT engine based on syntactic transfer [HPATR2] (cf. Section 2.1.4), an EBMT engine that incorporates word-level SMT methods [HPATR] (cf. Section 2.1.5), an EBMT engine based on hierarchical phrase alignments [HPAT] (cf. Section 2.1.6), an DP-match-driven EBMT engine [D3 ] (cf. Section 2.1.7), and a translation memory system [EM] (cf. Section 2.1.8). The translation knowledge of the eight MT systems is automatically acquired from a parallel corpus. The characteristics of the element MTs are summarized in Table 1. 2.1.1. SAT SAT is an SMT system [3]. The decoder searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In SAT, the search is initiated from Preprocessing Translation Tagger Selection MT1 Hypothesis MTm Hypothesis 1 Input SELECTOR Chunker Parser Resources m TM LM 1 Output TM LM n Statistical Models (LM, TM) Thesaurus Dictionary Corpus (monolingual) Corpus (bilingual) Figure 1: System outline Table 1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest"
2005.iwslt-1.5,P01-1030,0,0.0372404,"1: Features of element MT engines U nit Coverage Quality Speed Resources SAT SMT PBHMTM sentence&word wide excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source lang"
2005.iwslt-1.5,2004.iwslt-evaluation.2,1,0.800582,"excellent modest corpus phrase wide good slow corpus MSEP HPATR2 HPATR phrase wide good slow corpus, chunker phrase wide good modest corpus, parser phrase wide good modest corpus, parser similar translation examples retrieved from a parallel corpus. The similarity measure used here is a combination of an editdistance and tf/idf criteria as seen in the information retrieval framework [4]. The retrieved translations are modified by using a greedy search approach to find better translations [5]. 2.1.2. PBHMTM PBHMTM is a statistical MT system that is based on a phrase-based HMM translation model [6]. The model directly structures the phrase-based SMT approach in a Hidden Markov structure. The probability P (f |e) of translating a foreign source sentence f into a target language sentence e using noisy channel modeling is approximated by introduc¯, to explicitly capture ing two new hidden variables, ¯f and e the phrase translation relationship: X P(f |¯ f, ¯ e, e)P(¯ f |¯ e, e)P(¯ e|e) (1) P (f |e) = ¯ f ,¯ e The first term represents the probability that a phrasesegmented source language sentence ¯f can be reordered and generated as the source text of f (Phrase Segmentation Model). The se"
2005.iwslt-1.5,W02-1021,0,0.0245308,"nted target language sentence e (Phrase Ngram Model). ¯ and ¯f are expanded If the phrase segmented sentences e ¯ ¯ then the apinto corresponding lattice structures E and F, proximation of the proposed models can be regarded as a Hidden Markov Model in which each source phrase in the EBMT HPAT D3 phrase wide good fast corpus, parser, thesaurus sentence narrow excellent fast corpus, thesaurus, bilingual dictionary EM sentence narrow excellent fast corpus ¯ is treated as an observation emitted from a state, a lattice F ¯ target phrase, in the lattice E. The decoder is a word-graph-based decoder [7], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the word-based trigram language model and the class 5-gram model. The second pass uses the A* strategy to search for the best path for translation on the generated word-graph. 2.1.3. MSEP MSEP is a phrase-based SMT system that utilizes morphosyntactic informat"
2005.iwslt-1.5,2005.mtsummit-papers.35,1,0.73862,"p1 ), we incorporate the following features into the loglinear translation model: • Class-based n-gram Q model: P r(eI1 ) = i Pr(ei |ci )P r(ci |ci−1 1 ) • Length model: P r(l|eI1 , f1J ), whereby l is the length (number of words) of a translated target sentence. • Phrase matching score: The translated target sentence is matched with phrase translation examples that are extracted from a parallel corpus based on bidirectional word alignment of phrase translation pairs. A score is derived based on the number of matches. 2.1.4. HPATR2 HPATR2 is a statistical MT system based on syntactic transfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The"
2005.iwslt-1.5,C04-1015,1,0.7436,"π:π∈E A characteristic of HPATR2 is that not only word translations but also the translation of multi-word sequences is carried out by the syntactic transfer. Parsing hypotheses, which are multi-word sequences connected by context-free grammar rules, are created. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic d"
2005.iwslt-1.5,2002.tmi-papers.9,1,0.840155,"ansfer [9]. The translation model of HPATR2 is defined as an inside probability of two parse trees, which is used to create probabilistic context-free grammar rules. The system searches for the best translation that maximizes the product of the following probabilities, where F, E are a source and a target parse trees, and θ, π are context-free grammar rules of the source and the target language, respectively. • Probability of Source Tree Model Y P (F) = P (θ) (3) θ:θ∈F • Probability of Target Tree Model Y P (E) = P (π) 2.1.6. HPAT HPAT is an example-based MT system based on syntactic transfer [11]. The most important knowledge in HPAT are transfer rules, which define the correspondences between source and target patterns. The transfer rules can be regarded as synchronized context-free grammar rules. When the system translates an input sentence, the sentence is first parsed by using the source side of the transfer rules. Next, a tree structure of the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which"
2005.iwslt-1.5,P03-1057,1,0.847084,"the target language is generated by mapping the source grammar rules to the corresponding target rules. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples of the transfer rules. In general, the automatic acquisition process generates many redundant rules. To avoid this problem, HPAT optimizes the transfer rules by removing redundant rules (feedback cleaning, [12]) in order to increase an automatic evaluation score. 2.1.7. D3 (4) π:π∈E • Probability of Tree-mapping Model Y P (F|E)P (E|F) = P (θ|π)P (π|θ) selection is not performed during transfer, but all possible word translation candidates are generated. In a second step, an SMT module using a lexicon model and an n-gram language model is exploited to search for the best translation that maximizes the product of the probabilities. Therefore, HPATR selects the best translation among the output of example-based MT using models of statistical MT from the viewpoints of adequacy of word translation and fl"
2005.iwslt-1.5,W01-1401,1,0.85314,"ted. The best hypothesis (parse tree and translation) is selected based on the above models. Therefore, HPATR2 is an MT system that contains features of phrase-based SMT as well as syntax-based SMT. 2.1.5. HPATR HPATR is an extension of the example-based HPAT system (cf. Section 2.1.6) that incorporates a word-based statistical MT system [10]. Similar to HPAT, an EBMT module based on syntactic transfer is used to generate translation candidates that have minimum semantic distances. However, word D3 (DP-match Driven transDucer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved"
2005.iwslt-1.5,P91-1024,1,0.660049,"Ducer) is an EBMT system that exploits DP-matching between word sequences [13]. In the translation process, D3 retrieves the most similar source sentence from a parallel corpus for an input sentence. The similarity is calculated based on the counts of insertion, deletion, and substitution operations, where the total is normalized by the sum of the lengths of the word sequences. Substitution considers the semantic distance between two substituted words and is defined as the division of K, the level of the least common abstraction in the thesaurus of two words, by N, the height of the thesaurus [14]. According to the difference between the input sentence and the retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the"
2005.iwslt-1.5,C02-1076,1,0.892551,"retrieved source sentence, the translation of the retrieved source sentence is modified by using dictionaries. 2.1.8. EM EM is a translation memory system that matches a given source sentence against the source language parts of translation examples extracted from a parallel corpus. In case an exact match can be achieved, the corresponding target language sentence will be used. Otherwise, the system fails to output a translation. 2.2. Selection of the Best MT Engine Output We use an SMT-based method of automatically selecting the best translation among outputs generated by multiple MT systems [15]. This approach scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the average TM·LM score of one MT output is significantly higher than those of another MT output. The SELECTOR algorithm is summarized in Figure 2. (1) proc SELECTOR( Input, Corpus, n, M T 1 , . . . , M T m ) ; (2) begin (3) (∗ initalize statistical models ∗) (4) for each i in {1, . . . , n} do (5) Corpusi ← subset(Corpus) ; (6) T Mi ← translation-model(Corpusi ) ; (7) LMi ← language-model(Corpusi ) ; ("
2005.mtsummit-ebmt.15,J93-2003,0,0.00436194,"end NULL another another hotel NULL  Lexicon Model |hotel) p( Greedy Decoding for SMT In this section, we explain the outline of SMT and greedy decoding in short. 2.1 Statistical Machine Translation Statistical machine translation formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem: argmaxT p(S|T ) ∗ p(T ), (1) where p(S|T ) is called a translation model (T M ), representing the generation probability from T into S, and p(T ) is called a language model (LM ), which represents the likelihood of the target language (Brown et al., 1993). During the translation process (decoding), a statistical score based on T M and LM is assigned to each translation. In this paper, we call this score TM·LM. The translation with the highest TM·LM score is selected as the output. We used the IBM-4 translation model (Brown et al., 1993) in the experiments in Section 4, which consists of probabilities for word translations (lexicon model), the number of source words produced by a target word (fertility model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequence"
2005.mtsummit-ebmt.15,P01-1020,0,0.0223056,"tance between the given hypotheses and those of other MT engines, whereby the edit-distance is defined as the sum of the costs of insertion, deletion, and substitution operations required to map one word sequence into the other (Wagner, 1974). – differences in the length of a given initial translation hypothesis toward the shortest/longest initial translation hypothesis. Moreover, we added also statistical features and syntactic/semantic features for the experiments described in this paper, some of which were used in previous research on the automatic evaluation of machine translation output (Corston-Oliver et al., 2001). (→ i would prefer the hyatt regency please and if possible i want a single room) (initial translation hypothesis) MT1 : i ’m asked do i want to stay to room single room MT2 : i ’ll send a hyatt i ’d like to stay in a single room MT3 : i want to stay at the single room which asks you for the hyatt regency hotel MT4 : i want to stay at a single room in which it asks for the hyatt regency hotel MT5 : i want to stay at the single room which you may ask for hyatt regency hotel with • Perplexity of the source language input and the initial translation hypothesis calculated on the basis of trigram"
2005.mtsummit-ebmt.15,P01-1030,0,0.0246424,"), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algorithm Figure 2 illustrates the decoding algorithm, which is described in detail in"
2005.mtsummit-ebmt.15,2002.tmi-papers.9,0,0.0373438,"Missing"
2005.mtsummit-ebmt.15,P01-1050,0,0.0195919,"a high performance on average, the greedy decoding approach can often produce translations with severe errors. A major problem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples ret"
2005.mtsummit-ebmt.15,N04-4003,1,0.783157,"roblem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text corpus. However, the sequential decoding of multiple decoder seeds is computationally expensive. In"
2005.mtsummit-ebmt.15,quirk-2004-training,0,0.049879,"Missing"
2005.mtsummit-ebmt.15,C92-2067,0,0.0202221,"orpus Statistics word corpus sentence lang uage tokens count J 1,114,186 BTEC 162,318 952,300 E 62,529 J MAD 4,894 57,500 E word words per types sentence 6.9 18,781 5.9 12,404 10.0 2,607 10.3 2,158 The BTEC corpus was used for the acquisition of translation knowledge (training set) and the MAD corpus was used for the training of the decision tree classifier. In addition, we used 502 sentences from the MAD corpus reserved for evaluation purposes as the test set. 4.1.2 Evaluation Metrics For the evaluation, we used the following automatic scoring measure and human assessment. • Word Error Rate (Su et al., 1992) (WER), which penalizes edit operations against reference translations.. • Translation Accuracy (Sumita et al., 1999) (ABC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged by a native speaker. Hereafter, we use the total count of translations ranked A, B, or C as the ABC score. 4 Parts of the BTEC corpus were used in the International Workshop of Spoken Language Translation (http://www.slt.atr.jp/IWSLT2004/) and will be made publicly available through GSK (http://www.gsk.or.jp). 122 In contrast to WER, higher ABC scores indicate bet"
2005.mtsummit-ebmt.15,1999.mtsummit-1.34,1,0.814857,"4 57,500 E word words per types sentence 6.9 18,781 5.9 12,404 10.0 2,607 10.3 2,158 The BTEC corpus was used for the acquisition of translation knowledge (training set) and the MAD corpus was used for the training of the decision tree classifier. In addition, we used 502 sentences from the MAD corpus reserved for evaluation purposes as the test set. 4.1.2 Evaluation Metrics For the evaluation, we used the following automatic scoring measure and human assessment. • Word Error Rate (Su et al., 1992) (WER), which penalizes edit operations against reference translations.. • Translation Accuracy (Sumita et al., 1999) (ABC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged by a native speaker. Hereafter, we use the total count of translations ranked A, B, or C as the ABC score. 4 Parts of the BTEC corpus were used in the International Workshop of Spoken Language Translation (http://www.slt.atr.jp/IWSLT2004/) and will be made publicly available through GSK (http://www.gsk.or.jp). 122 In contrast to WER, higher ABC scores indicate better translations. For the automatic scoring measure we utilized up to 16 human reference translations. 4.2 Translatio"
2005.mtsummit-ebmt.15,W01-1401,1,0.890601,"Missing"
2005.mtsummit-ebmt.15,takezawa-etal-2002-toward,1,0.829973,"ng two Japanese(J)-English(E) parallel corpora of the travel domain. 2 The translation models are trained using the GIZA++ toolkit, http://www.fjoch.com 3 The language models are trained using the CMUCambridge Statistical Language Modeling Toolkit v2, http://mi.eng.cam.ac.uk/∼prc14/toolkit.html • Basic Travel Expression Corpus (BTEC) The BTEC corpus is a large collection of sentences4 that bilingual travel experts consider useful for people going to or coming from countries with different languages. The BTEC sentences are not transcriptions of actual interactions, but were written by experts (Takezawa et al., 2002). • Machine Aided Dialogue Corpus (MAD) The MAD corpus is a collection of dialogues between a native speaker of Japanese and a native speaker of English that is mediated by a speech-to-speech translation system (Kikui et al., 2003). The statistics of the corpora are given in Table 3, where word token refers to the number of words in the corpus and word type refers to the vocabulary size. Since the MAD corpus consists of dialogues, it contains more complex and compound sentences as well as filled pauses, resulting in longer sentences that are more difficult to translate. Table 3: Corpus Statist"
2005.mtsummit-ebmt.15,C00-2123,0,0.02089,"lity model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algorithm Figure 2 illustrates the decoding"
2005.mtsummit-ebmt.15,P97-1047,0,0.0296968,"ords produced by a target word (fertility model), word insertions (generation model), and word order changes (distortion model). LM is based on the frequency of consecutive word sequences (n-gram). The T M and LM probabilities are trained automatically from a parallel text corpus. Figure 1 gives an example for the process of transferring a Japanese source sentence into an English target sentence and illustrates which translation knowledge is captured by the respective statistical models mentioned above. 2.2 Greedy Decoding Various decoding algorithms have been proposed, including stack-based (Wang and Waibel, 1997), beam search (Tillmann and Ney, 2000), and greedy decoding (Germann et al., 2001). This paper concentrates on the greedy decoding approach described in details in Section 2.2.1. The local optima problem of this approach is illustrated in Section 2.2.2. 118 x could could recommend another another hotel         Distortion Model p(src_pos=3|trg_pos=5) [source]         Language Model (LM): p(could you recommend another hotel) = p(could) p(you |could) p(recommend |could you) p(another |you recommend) p(hotel |recommend another) Figure 1: Statistical Models 2.2.1 Algor"
2005.mtsummit-ebmt.15,2003.mtsummit-papers.54,1,0.8222,"ance on average, the greedy decoding approach can often produce translations with severe errors. A major problem of the greedy decoding approach is that the translation output depends 117 on the initial translation hypothesis to start the search, which may lead to a local optimum translation but not to the global optimum translation. Therefore, the selection of the starting point is crucial to avoid local optima in the search. Previous methods addressed this problem by creating an initial translation hypothesis based on translation examples obtained from a parallel text corpus (Marcu, 2001), (Watanabe and Sumita, 2003) or by using diverse starting points generated by multiple translation engines (Paul et al., 2004). Combining multiple MT systems has the advantage of exploiting the strengths of each MT engine. Quite different initial translation hypotheses are produced due to particular output characteristics of each MT engine. Therefore, larger parts of the search space can be explored while avoiding local optima problems of the search algorithm. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text c"
2005.mtsummit-ebmt.7,2002.tmi-papers.9,0,\N,Missing
2005.mtsummit-ebmt.7,C92-4203,0,\N,Missing
2005.mtsummit-ebmt.7,W02-1021,0,\N,Missing
2005.mtsummit-ebmt.7,W01-1401,1,\N,Missing
2005.mtsummit-ebmt.7,P02-1040,0,\N,Missing
2005.mtsummit-ebmt.7,P91-1024,1,\N,Missing
2005.mtsummit-ebmt.7,P03-1057,1,\N,Missing
2005.mtsummit-ebmt.7,1999.mtsummit-1.34,1,\N,Missing
2005.mtsummit-ebmt.7,rapp-2002-part,0,\N,Missing
2005.mtsummit-papers.35,2004.iwslt-evaluation.1,0,0.0116257,"hose products of the probabilities are the highest. 6 Experiments We evaluate the proposed method through Japanese to English translation. 6.1 Experimental Settings Corpora: The corpus used here is the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER"
2005.mtsummit-papers.35,J93-2003,0,0.0122806,"by combining phrase (multiword sequence) translation and phrase reordering without syntax. On the other hand, SMT based on tree-to-tree mapping, which involves syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reord"
2005.mtsummit-papers.35,2003.mtsummit-papers.6,0,0.0195235,"Reduce parsing failure is a task that must be accomplished to improve translation quality. 6.2.3 Translation Quality According to N -best Size Figure 6 shows the changes in the multiple word error rates according to the n-best size in the case of IWSLT. The translation quality by Pharaoh improved along with the expansion of the beam width. In the proposed methods, the quality was nearly fixed to the n-best size. Generally, beam search decoders decode from the 273 Related Work Statistical machine translation that employs syntax has been proposed as outlined below. Yamada and Knight (2001) and Charniak et al. (2003) proposed translation and language models in which the input sentence is mapped to the output parse tree. Even though they only used parse trees for one side, while we use both sides, a syntax-based language model would improve the fluency of translation. Graehl and Knight (2004) and Melamed (2004) proposed theoretical models that employ parse trees of source and target languages. Our proposed method is a realization of these methods. On the other hand, Koehn et al. (2003), Vogel et al. (2003), and Zens and Ney (2004) proposed phrase-based statistical MTs that do not use syntactic information."
2005.mtsummit-papers.35,A00-2018,0,0.0157509,"Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IW"
2005.mtsummit-papers.35,N04-1014,0,0.145709,"d from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents a practical method of statistical MT based on syntactic transfer, which is a kind of tree-to-tree mapping. Syntactic transfer has been widely used in machine translation, and it is suitable for a language pair whose respective structures are different (e.g., languages formed by SVO and SOV). An advantage of our method is that not only hierarchical reordering but also the flat phrases han267 dled in phrase-based SMT can be directly applied to the translat"
2005.mtsummit-papers.35,N03-1017,0,0.0394759,"Missing"
2005.mtsummit-papers.35,P04-1083,0,0.231027,"tion to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents a practical method of statistical MT based on syntactic transfer, which is a kind of tree-to-tree mapping. Syntactic transfer has been widely used in machine translation, and it is suitable for a language pair whose respective structures are different (e.g., languages formed by SVO and SOV). An advantage of our method is that not only hierarchical reordering but also the flat phrases han267 dled in phrase-based SMT can be directly applied to the translation. The rest of"
2005.mtsummit-papers.35,niessen-etal-2000-evaluation,0,0.0182495,"The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IWSLT. Step 1 Step 2 VP X PP 12 ji ni VP Y V Y VB de leave exit start VP X Y NP ni V 12 ji Step 3 de VP X PP at 12 o’clock at noon to 12 o’clock leave at 12 o’clock leave at noon exit at 12 o’clock : VP Y VB leave exit start Step 4 VP X NP 12 o’clock noon VP VP 12 ji ni de leave at 12 o’clock leave 12 o’clock leave noon exit 12 o’clock : VP leave at 12 o’clock leave at noon leave 12 o’clock exit at 12 o’clock leave noon : (pruning) VP leave at"
2005.mtsummit-papers.35,P02-1038,0,0.0461785,"argmax P (e|f )2 e = argmax P (e)P (f |e)P (e|f ). e (4) According to the above representation, the final score of the translation model is given by multiplying the translation model from source to target (P (e|f ), forward translation model) and the translation model from target to source (P (f |e), backward translation model). Then, we obtain the score of the bidirectional translation model by applying Equation 3 to these models as follows. P (f |e)P (e|f ) =  P (F  |E  )P (E  |e) F  ,E  ·  P (E  |F  )P (F  |f ) E  ,F  ≈  Equation 5 is nearly equal to the log linear model (Och and Ney, 2002), in which the feature functions are probabilities of source/target tree models and tree mapping models, and the weights of the models are uniform. 3.3 Inside Probability The source and target tree models can be regarded as probabilistic context-free grammar (PCFG). Namely, nodes in the tree are generated independently of each other, and the probability of the tree is computed by the product of the probabilities of a parent node generating a child node sequence (i.e., the inside probability). P (F|f ) = P (E|e) = • Since the model includes the source tree model, a correct parse tree of the sou"
2005.mtsummit-papers.35,J03-1002,0,0.0284406,"here is a similar approach to acquisition of alignment templates (Och and Ney, 2004), which extracts phrases based on the continuity of word alignment. The phrase alignment in this paper uses not only the continuity of word alignment but also the constraints of parse trees. Namely, only phrases that are a part of the source and target trees are extracted. Figure 3 shows an example of this phrase alignment. The phrases are extracted as follows. 1. First, perform word alignment in both directions (source to target and target to source). We use Viterbi alignment of IBM model 4 learned by GIZA++ (Och and Ney, 2003). 2. Next, extract sure alignments, i.e., those that agree with Viterbi alignments in both directions. The alignments that do not agree in both directions are regarded as possible alignments. 270 For example, by focusing on the sure alignments (2) and (3) in Figure 3, the pair (NP → 12 o’clock) and (NP → 12 ji) is extracted as a bilingual phrase because it only contains (2) and (3) (i.e., it does not contain the sure alignments (1), (4), (5), and (6)). However, focusing on the sure alignments (4) and (5), there are no sub-trees that contain only them and thus do not contain (1), (2), (3), and"
2005.mtsummit-papers.35,J04-4002,0,0.0269972,"ional translation model can be computed in a bottom-up manner, so it can also naturally be applied to a bottom-up parser that parses two languages. 4 Training In the training phase, we assume that parse trees are given in advance in order to reduce complexity. Therefore, the problems in the training phase are (1) extracting corresponding nodes between bilingual trees (phrase alignment) and (2) estimating probabilities of the source and target tree models and the tree-mapping model. 4.1 Phrase Alignment The phrase alignment used here is a similar approach to acquisition of alignment templates (Och and Ney, 2004), which extracts phrases based on the continuity of word alignment. The phrase alignment in this paper uses not only the continuity of word alignment but also the constraints of parse trees. Namely, only phrases that are a part of the source and target trees are extracted. Figure 3 shows an example of this phrase alignment. The phrases are extracted as follows. 1. First, perform word alignment in both directions (source to target and target to source). We use Viterbi alignment of IBM model 4 learned by GIZA++ (Och and Ney, 2003). 2. Next, extract sure alignments, i.e., those that agree with Vi"
2005.mtsummit-papers.35,P02-1040,0,0.0732214,"rkshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing, and a rule-based phrase structure parser developed in-house was used for Japanese parsing in the training phase. Word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) formed the language model. Evaluation Metrics: We used BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and mWER (multiple Word Error Rate, (Nießen et al., 2000)) 2 In these experiments, we arranged numerical words into numbers (e.g., “fifty/CD one/CD” → ‘51/CD’), so the number of words is different from that of IWSLT. Step 1 Step 2 VP X PP 12 ji ni VP Y V Y VB de leave exit start VP X Y NP ni V 12 ji Step 3 de VP X PP at 12 o’clock at noon to 12 o’clock leave at 12 o’clock leave at noon exit at 12 o’clock : VP Y VB leave exit start Step 4 VP X NP 12 o’clock noon VP VP 12 ji ni de leave at 12 o’clock leave 12 o’clock leave noon exit 12 o’clock : VP leave at 12 o’clock"
2005.mtsummit-papers.35,1999.mtsummit-1.34,1,0.790879,"it at 12 o’clock leave noon : (pruning) VP leave at 12 o’clock Figure 4: Example of Decoding Set Name BTEC (Training) IWSLT (Training) Test Item # of Sentences # of Words # of Diff. Words # of Sentences # of Words # of Diff. Words # of Sentences # of Words # of Diff. Words Japanese English 152,170 1,178,419 1,103,600 16,686 10,669 20,000 188,533 182,018 8,652 6,133 500 — 4,018 — 888 — Table 2: Corpus Size metrics for the automatic evaluation. For the subjective evaluation, an English native classified the translations into the four ranks of A: Perfect, B: Fair, C: Acceptable, and D: Nonsense (Sumita et al., 1999). Note that a lower score denotes a better translation in the mWER metric. 6.2 Results 6.2.1 Translation Quality First, we measured the translation quality of the proposed method. The results are shown in Table 3. In order to measure the effect of syntactic transfer and flat phrases independently, two alternative methods were applied: • Flat phrases were excluded from the translation model (w/o phrases). Only the most primitive rules were applied to decoding. • Decoding was performed without syntactic information. We used the phrase-based beam 272 search decoder, Pharaoh, developed by USC ISI"
2005.mtsummit-papers.35,takezawa-etal-2002-toward,1,0.781689,"merged in Step 4. Through this process, not only the output sequence but also 271 the syntactic labels of the input and output are acquired, so the decoder can parse and transfer the higher structure. If the parsing of the input (or output) sentence fails, the decoder extracts partial translations from its agenda and sequentially outputs the translations whose products of the probabilities are the highest. 6 Experiments We evaluate the proposed method through Japanese to English translation. 6.1 Experimental Settings Corpora: The corpus used here is the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. The corpus size is shown in Table 2. IWSLT in Table 2 is a corpus used in the evaluation campaign of the International Workshop on Spoken Language Translation (Akiba et al., 2004), which is a subset of BTEC. The test set is the same as that of IWSLT. 2 Training: Word alignment was acquired from the Viterbi alignment of IBM model 4 using GIZA++ (Och and Ney, 2003). Charniak (2000)’s parser was used for English parsing,"
2005.mtsummit-papers.35,2003.mtsummit-papers.53,0,0.0914521,"es syntactic information, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear."
2005.mtsummit-papers.35,P01-1067,0,0.13895,"ce can be applied to parsing. Reduce parsing failure is a task that must be accomplished to improve translation quality. 6.2.3 Translation Quality According to N -best Size Figure 6 shows the changes in the multiple word error rates according to the n-best size in the case of IWSLT. The translation quality by Pharaoh improved along with the expansion of the beam width. In the proposed methods, the quality was nearly fixed to the n-best size. Generally, beam search decoders decode from the 273 Related Work Statistical machine translation that employs syntax has been proposed as outlined below. Yamada and Knight (2001) and Charniak et al. (2003) proposed translation and language models in which the input sentence is mapped to the output parse tree. Even though they only used parse trees for one side, while we use both sides, a syntax-based language model would improve the fluency of translation. Graehl and Knight (2004) and Melamed (2004) proposed theoretical models that employ parse trees of source and target languages. Our proposed method is a realization of these methods. On the other hand, Koehn et al. (2003), Vogel et al. (2003), and Zens and Ney (2004) proposed phrase-based statistical MTs that do not"
2005.mtsummit-papers.35,N04-1033,0,0.0826958,"tion, is theoretical, so its features remain unclear from the viewpoint of a practical system. The SMT proposed in this paper translates phrases with hierarchical reordering based on the bilingual parse tree. In our experiments, the best translation was obtained when both phrases and syntactic information were used for the translation process. 1 Introduction Statistical machine translation (SMT), originally proposed by Brown et al. (1993), has evolved from word-level translation to phrase-level (multi-word, i.e., flat phrases in this paper) translation (Koehn et al., 2003; Vogel et al., 2003; Zens and Ney, 2004). In phrase-based SMT, the cost of reordering words is reduced because the word order in a phrase is locally changed before translation. However, reordering phrases is also necessary for accurate translation. Most phrase-based SMT systems reorder phrases on a flat structure. Another approach, statistical machine translation based on tree-to-tree mapping, explicitly involves syntactic information and hierarchically reordered words (Graehl and Knight, 2004; Melamed, 2004). However, these proposals are theoretical, and thus their features on a practical system remain unclear. This paper presents"
2006.iwslt-evaluation.12,C02-1076,1,0.827325,"ntactic transfer; and EM by exact match. For the OPEN track, only TATR was used; for the CSTAR track, a hybrid system using three engines and Selector was used. See Figure 1. 5.4. Selector 5.1. TATR In order to select the best translation among outputs generated by multiple MT systems, we employ an SMT-based method that scores MT outputs by using multiple language (LM) and translation model (TM) pairs trained on different subsets of the training data. It uses a statistical test to check whether the obtained TM·LM scores of one MT output are significantly higher than those of another MT output [9]. Given an input sentence, m translation hypotheses are produced by the component MT engines (m = 1 for this evaluation), whereby n different TM·LM scores are assigned to each hypothesis. In order to check whether the highest scoring hypothesis is significantly better then the other MT outputs, a multiple comparison test based on the Kruskal-Wallis test is used [10]. If one of the MT outputs is significantly better, this output is selected. Otherwise, the output of the MT engine that performs best on a development set is selected. TATR is a phrase-based SMT system built within the framework of"
2006.iwslt-evaluation.12,J03-1002,0,0.0511227,"Missing"
2006.iwslt-evaluation.12,2006.iwslt-evaluation.15,0,0.0505268,"Missing"
2006.iwslt-evaluation.12,2005.mtsummit-papers.35,1,0.807212,"Missing"
2006.iwslt-evaluation.12,2005.iwslt-1.5,1,0.8833,"Missing"
2006.iwslt-evaluation.12,N06-2049,1,0.870022,"ation. Our main translation engine for this year’s IWSLT evaluation, TATR, is also a phrase-based SMT. The hybrid multiple engine approach, that was used last year [1], was used again this year. But we replaced the 2005 SMTs (PBHMTM and SAT) with TATR, partly for simplification reasons. In addition to TATR, two other engines are included in this year’s hybrid system: HPATR3, a SMT based on syntactic transfer; and EM, the translation memory based on exact match. We employed new approaches for pre-processing, postprocessing, and language modeling. We used subword-based Chinese word segmentation [2]. This word segmentation achieved the highest F-score rate for the second Sighan test data, and can recognize numerical expressions and foreign names. We built a conversion model to implement capitalization and punctuation by using the maximum entropy principle and the conditional random field (CRF) approach, which can integrate long-range features to enhance performance. We applied sentence-splitting techniques to all languages. This approach significantly improved CE and JE translation. 2. Preprocessing 2.1. Arabic segmentation Of the released data, we threw away all end-of-utterance markers"
2006.iwslt-evaluation.12,N06-2013,0,\N,Missing
2006.iwslt-papers.8,takezawa-kikui-2004-comparative,0,0.146526,"Missing"
2006.iwslt-papers.8,2006.iwslt-evaluation.12,1,\N,Missing
2007.iwslt-1.15,N06-2049,1,0.896044,"Missing"
2007.iwslt-1.15,W06-1626,0,0.0485171,"/MX is/AL CEO/AU of/AL a/AL British/IU company/AL. The CRF tagging model is expressed by the following equation: 1 http://www.speech.sri.com/projects/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calcula"
2007.iwslt-1.15,D07-1054,1,0.871459,"e in the additional corpus (Tanaka corpus, Yomiuri News corpus, SLDB corpus, and Beijing Olympic corpus included in ChineseLDC) was calculated 3. Only those sentences for which the perplexity was lower than 100 were used as training sentences. After the selection process we were left with 40K sentences from the supplied corpus and 117K additional sentences from the external corpora, giving us a total of 157K sentences for training. 2.4. Modeling Issues Word-trigram language models were used, these were smoothed using Knesser-Ney discounting. In addition, topicdependent models were constructed [1]. We built bilingual cluster-based models from 157K bilingual training sentence pairs. The sentence pairs were clustered into 10 sub-corpora. These sub-corpora intutitively represent sub-domains of the main corpus. The motivation behind this strategy was to build models specific to these sub-domains and then predict the sub-domain of the text to be translated, and use the appropriate model for the translation process. A strong improvement was demonstrated using this technique for all language pairs in the IWSLT06 evaluation campaign. In this year’s campaign we only apply this technique to the"
2007.iwslt-1.15,P07-2046,1,0.884488,"a method for re-segmenting the tokens in the confusion network. 3. Chinese-English 3.1. Corpora We used the supplied corpus in combination with the Beijing Olympic Corpus, and other corpora provided by the LDC. These corpora and their respective sizes are shown in Table 1. 3.2. Lemmatization Data sparseness is one of the key factors that degrade statistical machine translation (SMT). Especially for a translation task like IWSLT, where collecting a large amount of indomain data is very expensive. One method to reduce the translation degradation caused by this approach is by using lemmatization [2]. Lemmatization is shallow morphological analysis, which uses single a lexical entry to replace a whole range of derived inflected words. For example, the three words: “doing”, “did” and “done”, can be replaced by one word: “do”. In fact, they should all be mapped to the same Chinese target word during alignment. It is easy to see that as a result, the process reduces the number of types observed in the data, thereby easing the problems associate with sparse data, and in Chinese at least we expect the process to preserve as much of the semantic information as possible. We used Moses to impleme"
2007.iwslt-1.15,W07-0717,0,0.0516508,"he only difference in training with lemmatization from that without is the alignment factor. The former uses Chinese surface words and English lemmas as the alignment factor, but the latter uses Chinese surface words and English surface words. Therefore, the lemmatized English is only used in the word alignment stage of the training. All the other aspects of the training process are the same for both the lemmatized translation training and nonlemmatized training. 3.3. Translation model combination Linear interpolation of translation models has been shown to be effective in machine translation [2, 3]. In this campaign we apply this approach as the main means of integrating models built from the external resources with the primary models built from the supplied corpus. More formally, we use the following equation for model combination: p(e|f ) = α1 p1 (e|f ) + α2 p2 (e|f ) where p1 and p2 are two models to be integrated, and the weight α1 and α2 must sum to unity. We did not use automatic optimization methods to select the α1 and α2 . Instead, we hand-selected the values by evaluating the performance of multiple runs on the development data. We consider this approach reasonable since the s"
2007.iwslt-1.15,D07-1090,0,0.0387915,"/srilm 6. Hit-rate-based Skip n-gram Rescoring This section describes the re-scoring of the statistical MT decoder hypotheses based on skip n-gram counts extracted from a large-scale corpus consisting of collections of webpages. In order to handle very large amounts of training data to build language models, recent research focuses on distributed language modeling that use a two-pass approach to store corpora in suffix arrays and serve raw counts [6, 7] or a single-pass approach that provides smoothed probabilities 2 http://www.chasen.org/˜taku/software/CRF++ using simple smoothing techniques [8]. Although such approaches are to be preferred when available, the computational and hardware requirements are still immense and not always practicable. In order to make use of very large training corpora with fewer resources, we use a method based on n-gram occurrence counts . The hit-rate of a word sequence is defined to be: X HitRate(w1L ) = δ(wij ) (2) i,j;i<j δ(wij ) =  1 0 : f (wij ) &gt; 0 : f (wij ) = 0 The hit-rate counts can be easily calculated, even for very large training corpora like the Web-Corpus introduced in Section 6.1. For the IWSLT experiments, we calculated the hitrate feat"
2007.iwslt-1.15,W08-0334,1,\N,Missing
2007.iwslt-1.15,P02-1040,0,\N,Missing
2007.iwslt-1.15,N07-1061,0,\N,Missing
2007.iwslt-1.15,W05-0909,0,\N,Missing
2007.iwslt-1.15,P07-2045,0,\N,Missing
2007.iwslt-1.15,J03-1002,0,\N,Missing
2007.iwslt-1.15,W08-0335,1,\N,Missing
2007.iwslt-1.15,2006.iwslt-evaluation.12,1,\N,Missing
2007.iwslt-1.15,I05-3017,0,\N,Missing
2007.iwslt-1.15,P03-1021,0,\N,Missing
2007.mtsummit-papers.48,P97-1017,0,0.224293,"Missing"
2007.mtsummit-papers.48,W06-3114,0,0.0311269,"at regardless of training corpus size, it is better to try all of the candidates in a surrogate list to compare the results. We have also conducted an experiment to conﬁrm this. Experiment Setting For experiment, we use ATR’s BTEC(Basic Travel Expression Corpus) (Kikui et al., 2003). This corpus contains basic expression for travel. Table 1 shows the size of this corpus. We tagged the Japanese corpus using Chasen4 and the English corpus using an in-house tagger at ATR and made both translation and language models for SMT system with these. These models were created using the training toolkit (Koehn and Monz, 2006) with GIZA++ (Och and Ney, 2003), mkcls (Och, 1999) and SRILM (Stolcke, 2002). We made two special test sets from the original test set to clarify the diﬀerences between the proposed method and the baseline method. First, sentences were collected from the Japanese test set which included words tagged as “place-general” by Chasen. Those words were then replaced with the 4 http://chasen-legacy.sourceforge.jp/ JE EJ Figure 6: A example of additional phrase table for the baseline method untrained words “カーディフ/kadifu (cardiﬀ)”, “ ポーツマス/potsumasu (portsmouth)” and “ベル ファスト/berufasuto (belfast)”. Sec"
2007.mtsummit-papers.48,N03-1017,0,0.0327672,"Missing"
2007.mtsummit-papers.48,J03-1002,0,0.00438287,"ze, it is better to try all of the candidates in a surrogate list to compare the results. We have also conducted an experiment to conﬁrm this. Experiment Setting For experiment, we use ATR’s BTEC(Basic Travel Expression Corpus) (Kikui et al., 2003). This corpus contains basic expression for travel. Table 1 shows the size of this corpus. We tagged the Japanese corpus using Chasen4 and the English corpus using an in-house tagger at ATR and made both translation and language models for SMT system with these. These models were created using the training toolkit (Koehn and Monz, 2006) with GIZA++ (Och and Ney, 2003), mkcls (Och, 1999) and SRILM (Stolcke, 2002). We made two special test sets from the original test set to clarify the diﬀerences between the proposed method and the baseline method. First, sentences were collected from the Japanese test set which included words tagged as “place-general” by Chasen. Those words were then replaced with the 4 http://chasen-legacy.sourceforge.jp/ JE EJ Figure 6: A example of additional phrase table for the baseline method untrained words “カーディフ/kadifu (cardiﬀ)”, “ ポーツマス/potsumasu (portsmouth)” and “ベル ファスト/berufasuto (belfast)”. Second, sentences from the Japanese"
2007.mtsummit-papers.48,E99-1010,0,0.0156011,"of the candidates in a surrogate list to compare the results. We have also conducted an experiment to conﬁrm this. Experiment Setting For experiment, we use ATR’s BTEC(Basic Travel Expression Corpus) (Kikui et al., 2003). This corpus contains basic expression for travel. Table 1 shows the size of this corpus. We tagged the Japanese corpus using Chasen4 and the English corpus using an in-house tagger at ATR and made both translation and language models for SMT system with these. These models were created using the training toolkit (Koehn and Monz, 2006) with GIZA++ (Och and Ney, 2003), mkcls (Och, 1999) and SRILM (Stolcke, 2002). We made two special test sets from the original test set to clarify the diﬀerences between the proposed method and the baseline method. First, sentences were collected from the Japanese test set which included words tagged as “place-general” by Chasen. Those words were then replaced with the 4 http://chasen-legacy.sourceforge.jp/ JE EJ Figure 6: A example of additional phrase table for the baseline method untrained words “カーディフ/kadifu (cardiﬀ)”, “ ポーツマス/potsumasu (portsmouth)” and “ベル ファスト/berufasuto (belfast)”. Second, sentences from the Japanese test set were coll"
2007.mtsummit-papers.48,P02-1040,0,0.082229,"t for each of the categories. Also, to ensure the advantage of the use of the top N high-frequency words, ﬁve modiﬁed source sentences are made by replacing the untrained words with the top ﬁve high-frequency words from the surrogate lists and the best translation result is chosen comparing scores as shown in equation 2. This is done if the untrained word is the only one in the source sentence to avoid any complexity of calculation. We use Cleopatra made at ATR for the decoding, which is compatible with Pharaoh. Result Table shows results of an automatic evaluation. Values are scores of BLEU (Papineni et al., 2002). As the table indicates, the proposed method outperforms the baseline method with exception to the English to Japanese place name test set. However, these BLEU scores are obtained by using only one reference set and have wide ﬂuctuation, in contrast to using multiple references. To investigate further, diﬀerences between the translations produced by the two methods were checked using Testset no. of sentences Baseline Proposed Baseline Proposed place 106 39.87 44.14 42.09 41.91 person 60 36.87 40.18 21.87 25.53 Table 2: Automatic evaluation(baseline vs. posed) prohuman evaluators. Table shows"
2007.mtsummit-ucnlg.5,N03-1017,0,0.0383282,"ion experiments (BLEU score). tures. We can approximate Eq. 4 by regarding its denominator as constant. The translation results (ˆ e) are then obtained by eˆ(f, λM 1 ) = argmaxe M ∑ λi hi (e, f ) (5) i=1 Figure 5: Results of statistical machine translation experiments (NIST score). 4. Lexical weighting probability from source target to language weight = 0.2) 5. Phrase penalty (weight = 0.2) 6. Word penalty (weight = −1.0) 7. Distortion weight (weight = 0.5) 4.2 Experimental Conditions 8. Target language model probability (weight = 0.5) 4.2.1 Features We used eight features (Och and Ney, 2003; Koehn et al., 2003) and their weights for the translations. 1. Phrase translation probability from source language to target language (weight = 0.2) 2. Phrase translation probability from target language to source language (weight = 0.2) 3. Lexical weighting probability from source language to target language (weight = 0.2) 35 According to a previous study, Minimum Error Rate Training (MERT) (Och, 2003), which is the optimization of feature weights by maximizing the BLEU score on the development set can improve the performance of a system. However, the range of improvements is not stable because the MERT algorit"
2007.mtsummit-ucnlg.5,A94-1010,0,0.140616,"el, but also its applications, such as statistical machine translation (SMT) or ASR. The reason for this is that a large amount of training data tends to yield a large language model and applications then have to deal with this model. We propose a method of selecting the training set by selecting a number of appropriate training sentences from a training corpus to solve the problem of an expanded language model with increased training load. This method enables an adequate training set to be selected from a large corpus using a small indomain development set and the sentence clustering method (Carter, 1994). We can make the language model compact without degrading performance because this method effectively reduces the size of the set for training the language model. This compact language model can outperform a language model trained on the entire original corpus. This method is especially effective for domains where it is difficult to enlarge the corpus, such as in spoken language corpora (Kikui et al., 2003). The main approach to recovering undersupply of indomain corpus has been to use very large domainclose or out-of-domain corpus for the language model training (NIST, 2006). In such case, t"
2007.mtsummit-ucnlg.5,J03-1002,0,0.0100774,"al machine translation experiments (BLEU score). tures. We can approximate Eq. 4 by regarding its denominator as constant. The translation results (ˆ e) are then obtained by eˆ(f, λM 1 ) = argmaxe M ∑ λi hi (e, f ) (5) i=1 Figure 5: Results of statistical machine translation experiments (NIST score). 4. Lexical weighting probability from source target to language weight = 0.2) 5. Phrase penalty (weight = 0.2) 6. Word penalty (weight = −1.0) 7. Distortion weight (weight = 0.5) 4.2 Experimental Conditions 8. Target language model probability (weight = 0.5) 4.2.1 Features We used eight features (Och and Ney, 2003; Koehn et al., 2003) and their weights for the translations. 1. Phrase translation probability from source language to target language (weight = 0.2) 2. Phrase translation probability from target language to source language (weight = 0.2) 3. Lexical weighting probability from source language to target language (weight = 0.2) 35 According to a previous study, Minimum Error Rate Training (MERT) (Och, 2003), which is the optimization of feature weights by maximizing the BLEU score on the development set can improve the performance of a system. However, the range of improvements is not stable bec"
2007.mtsummit-ucnlg.5,P03-1021,0,0.0251507,"Word penalty (weight = −1.0) 7. Distortion weight (weight = 0.5) 4.2 Experimental Conditions 8. Target language model probability (weight = 0.5) 4.2.1 Features We used eight features (Och and Ney, 2003; Koehn et al., 2003) and their weights for the translations. 1. Phrase translation probability from source language to target language (weight = 0.2) 2. Phrase translation probability from target language to source language (weight = 0.2) 3. Lexical weighting probability from source language to target language (weight = 0.2) 35 According to a previous study, Minimum Error Rate Training (MERT) (Och, 2003), which is the optimization of feature weights by maximizing the BLEU score on the development set can improve the performance of a system. However, the range of improvements is not stable because the MERT algorithm uses random numbers while searching optimum weights. As previously mentioned, we used fixed weights instead of weights optimized by MERT to remove its unstable effects and simplify evaluation. 4.2.2 Language Model Table 3: Language model size. We used a modified Kneser-Ney (Chen and Goodman, 1998) 3-gram language model for the experiments explained in this section because modified"
2007.mtsummit-ucnlg.5,P02-1040,0,0.0799065,"Missing"
2007.mtsummit-ucnlg.5,2004.tmi-1.9,0,0.109283,"Missing"
2007.mtsummit-ucnlg.5,N06-2049,1,0.811065,"ge model for the experiments explained in this section because modified Kneser-Ney smoothing tended to perform better than the Good-Turing language model in this translation task. Normalized n training-set size 1 0.4 2 3 1 1.0 2 3 gram # of entries gram gram gram gram gram gram 704 K 12 M 12 M 1509 K 23 M 27 M 4.2.3 Corpus We used the bilingual data listed in Table 2 for the statistical machine-translation experiments to train the translation model. We first aligned the bilingual sentences for preprocessing using the Champollion tool (Ma, 2006). We then segmented Chinese words using Achilles (Zhang et al., 2006). We used the preprocessed data to train the phrase-based translation model using GIZA++ (Och and Ney, 2003) and PHARAOH tools (Koehn et al., 2003). For the language model training, we used the data listed in Table 1. 4.3 Experimental results Figures 4 and 5 plot the results for the statistical machine translation experiments that used the final language model obtained with our method. The horizontal axis is the same as that in Fig. 3, and the vertical axis represents the automatic metric of translation quality (BLEU score (Papineni et al., 2002) in Fig. 4, and NIST score (NIST, 2002) in Fig."
2007.tmi-papers.19,2001.mtsummit-papers.3,1,0.893267,"ers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utiliz"
2007.tmi-papers.19,W05-0909,0,0.0288594,"cceptable Translation Nonsense Table 2: Automatic Evaluation Metrics BLEU: NIST: METEOR: GTM: WER: PER: TER: the geometric mean of n-gram precision of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discrimina"
2007.tmi-papers.19,2004.tmi-1.8,0,0.153171,"automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, word, part-of-speech) and semantic (thesaususbased semantic class) matches were used to compare MT system outputs with reference translations and to approximate human scores of acceptability directly. (Kulesza and Shieber, 2004) trained a binary SVM classifier based on automatic scoring features in order to distinguish between “human-produced” and “machine-generated” translations of newswire data instead of predicting human judgments directly. The approach proposed in this paper also utilizes a supervised learning method to predict human assessments of translation quality, but differs in the following two aspects: (1) Reduction of Classification Perplexity: The decomposition of a multiclass classification task into a set of binary classification problems reduces the complexity of the learning task resulting in higher"
2007.tmi-papers.19,N07-1006,0,0.115946,"Missing"
2007.tmi-papers.19,niessen-etal-2000-evaluation,0,0.0624038,"tive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binar"
2007.tmi-papers.19,2001.mtsummit-papers.46,0,0.0107964,"anslation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguis"
2007.tmi-papers.19,P02-1040,0,0.0751806,"Missing"
2007.tmi-papers.19,quirk-2004-training,0,0.0918346,"system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various learning approaches for the multiclass classification problem for a very small data set in the domain of technical documentation. (Akiba et al., 2001) utilized DT classifiers trained on multiple edit-distance features where combinations of lexical (stem, w"
2007.tmi-papers.19,2006.amta-papers.25,0,0.0596957,"cores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from human-evaluated MT system outputs. The work described in (Quirk, 2004) uses statistical measures to estimate confidence on the word/phrase level and gathers systemspecific features about the translation process itself to train binary classifiers. Empirical thresholds on automatic evaluation scores are utilized to distinguish between good and bad translations. He also investigates the feasabil155 ity of various"
2007.tmi-papers.19,1999.mtsummit-1.34,1,0.721734,"uality of a translation have been proposed. In this paper, human assessments of translation quality with respect to the fluency, the adequacy and the acceptability of the translation are investigated. Fluency indicates how natural the evaluation segment sounds to a native speaker of English. For adequacy, the evaluator was presented with the source language input as well as a “gold standard” translation and has to judge how much of the information from the original translation is expressed in the translation (White et al., 1994). Acceptability judges how easy-to-understand the translation is (Sumita et al., 1999). The fluency, adequacy and acceptability judgments consist of one of the grades listed in Table 1. The high cost of such human evaluation metrics has triggered a huge interest in the development of automatic evaluation metrics for machine translation. Table 2 introduces some metrics that are widely used in the MT research community. 3 Prediction of Human Assessments Most of the previously proposed approaches to predict human assessments of translation quality utilize supervised learning methods like decision trees (DT), support vector ma5 4 3 2 1 acceptability Perfect Translation Good Transla"
2007.tmi-papers.19,2003.mtsummit-papers.51,0,0.0312584,"ion of the system output with respect to reference translations. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002) a variant of BLEU using the arithmetic mean of weighted n-gram precision values. Scores are positive with 0 being the worst possible (Doddington, 2002) calculates unigram overlaps between a translation and reference texts using various levels of matches (exact, stem, synonym). Scores range between 0 (worst) and 1 (best) (Banerjee and Lavie, 2005) measures the similarity between texts by using a unigram-based Fmeasure. Scores range between 0 (worst) and 1 (best) (Turian et al., 2003) Word Error Rate: the minimal edit distance between the system output and the closest reference translation divided by the number of words in the reference. Scores are positive with 0 being the best possible (Niessen et al., 2000) Position independent WER: a variant of WER that disregards word ordering (Och and Ney, 2001) Translation Edit Rate: a variant of WER that allows phrasal shifts (Snover et al., 2006) chines (SVM), or perceptrons to learn discriminative models that are able to come closer to human quality judgments. Such classifiers can be trained on a set of features extracted from hu"
2007.tmi-papers.19,1994.amta-1.25,0,0.116906,"Missing"
2008.iwslt-evaluation.11,W08-0334,1,0.872755,"ation task, we integrated two strategies for pivot translation by linear interpolation. 1. Introduction This paper describes the NICT/ATR SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2008 evaluation campaign. We participated in the following translation tasks: Chinese–English (Challenge Task), English–Chinese (Challenge Task), Chinese–English (BTEC Task), Chinese–Spanish (BTEC Task), and Chinese– English–Spanish (PIVOT Task). Although our theme for each task was different, our systems were based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapte"
2008.iwslt-evaluation.11,P07-2045,0,0.0149744,"e based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese tran"
2008.iwslt-evaluation.11,J03-1002,0,0.00224261,"a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various fact"
2008.iwslt-evaluation.11,P03-1021,0,0.0223499,"r can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the param"
2008.iwslt-evaluation.11,P02-1040,0,0.0774265,"The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the parameters tuned on devset (open experiments). The BLEU scores were calculated based on Chinese character n-grams. When calculating BLEU scores, we removed out-of-vocabulary (OOV) words from the machine translated text and ignored punctuation. 1 We used 3-gram language models for performing MERT and used 5gra"
2008.iwslt-evaluation.11,W08-0335,1,0.86956,"Missing"
2008.iwslt-evaluation.11,N06-2049,1,0.880518,"Missing"
2008.iwslt-evaluation.11,I05-3017,0,0.087631,"Missing"
2008.iwslt-evaluation.11,D07-1054,1,0.880592,"Missing"
2008.iwslt-evaluation.11,N07-1061,1,0.865684,"of the language X to language Y SMT system by using corpus X of the “X-E corpus” and the newly created corpus Y’. After developing the models, as described above, we remove all the phrase table entries that have OOV words on the target side of the phrase table. We will call the system developed above the X2Y’ system and the strategy PseudoCorpusY. In this system, the target side of the phrase table is not completely reliable. For training these systems, we develop and use a language model using corpus Y of the “Y-E corpus.” 4.4. Phrase Table Composition This strategy was introduced by Utiyama [12]. In order to implement this strategy, we first develop the X2E system using the “X-E corpus” and the E2Y system using the “Y-E corpus.” Then, we compose a new phrase table from the phrase tables of the X2E and E2Y systems. For the purpose of integrating two models, we extend this strategy to include the lexicalized reordering model. 4.5. Linear Interpolation This strategy is used to develop new models from those described above, by linear interpolation: the phrase translation model and the lexicalized reordering model. First, we interpolate two PseudoCorpus models. These models are de- 82 - n"
2008.iwslt-evaluation.11,W05-0909,0,0.115895,"Missing"
2009.iwslt-evaluation.12,W08-0334,1,\N,Missing
2009.iwslt-evaluation.12,P07-2045,0,\N,Missing
2009.iwslt-evaluation.12,P03-1021,0,\N,Missing
2009.iwslt-evaluation.12,J03-1002,0,\N,Missing
2009.mtsummit-papers.18,J93-2003,0,0.029315,"e to mine parallel texts from mixedlanguage web pages. We define a mixedlanguage web page as a web page consisting of (at least) two languages. We mined Japanese-English parallel texts from mixedlanguage web pages. We presented the statistics for extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and langu"
2009.mtsummit-papers.18,W08-0334,1,0.880155,"Missing"
2009.mtsummit-papers.18,W07-0717,0,0.0203036,"ure 1 and Table 4 show that the performance of the SMT systems trained with our extracted sentence alignments are inferior to that of the SMT system trained with the IWSLT training data. A likely reason is that the extracted alignments are out-ofdomain data with respect to the IWSLT testsets. In the following, we show that the extracted alignments are useful for improving the performance of the SMT system trained with the IWSLT training data, even though these alignments are not best suited to the testsets. 4.3 Interpolation of models We linearly interpolated8 language and translation models (Foster and Kuhn, 2007) to improve the performance of the SMT system. 4.3.1 Interpolation of language models We first interpolated language models (LMs). We interpolated the language model made from 900,000 sentences in Section 4.1 (hereafter LM(900k)) and that made from the IWSLT training data in Section 4.2 (hereafter LM(IWSLT)). The weight of LM(IWSLT) was 0.1, 0.2, ..., 0.9. In addition to these interpolated language models, we used the translation model made from the IWSLT training data in Section 4.2 for all of the weights. The figures in the 0.1, ..., 0.9 rows in Table 5 show the BLEU scores for set2, ..., se"
2009.mtsummit-papers.18,W04-3208,0,0.161396,"005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studie"
2009.mtsummit-papers.18,J93-1004,0,0.340799,"es into noisy parallel text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs"
2009.mtsummit-papers.18,kawahara-kurohashi-2006-case,1,0.855263,"Missing"
2009.mtsummit-papers.18,P07-2045,0,0.0122038,"Missing"
2009.mtsummit-papers.18,2005.mtsummit-papers.11,0,0.0791756,"chine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung"
2009.mtsummit-papers.18,ma-cieri-2006-corpus,0,0.0118588,"extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002"
2009.mtsummit-papers.18,1999.mtsummit-1.79,0,0.219583,"llel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For exa"
2009.mtsummit-papers.18,J05-4003,0,0.437791,"uropean languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts fro"
2009.mtsummit-papers.18,J03-1002,0,0.00533169,"000 sentence alignments were effective for multi-lingual natural language processing. Based on the statistics presented in Sections 3.2 and 3.3, we concluded that we extracted a clean parallel corpus from the original web corpus. 4 Machine translation experiments We verify the usefulness of the extracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1"
2009.mtsummit-papers.18,P03-1021,0,0.0112609,"system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testsets to evaluate the performance of the SMT system in terms of BLEU scores. Hereafter,"
2009.mtsummit-papers.18,P02-1040,0,0.079854,"tracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testse"
2009.mtsummit-papers.18,J03-3002,0,0.231052,"ese include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two"
2009.mtsummit-papers.18,P06-1062,0,0.286421,"gh these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For example, Shi et al. (2006) have aligned parall"
2009.mtsummit-papers.18,steinberger-etal-2006-jrc,0,0.0659403,"Missing"
2009.mtsummit-papers.18,P03-1010,1,0.869044,"he Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to o"
2009.mtsummit-papers.18,2007.mtsummit-papers.63,1,0.758763,"es for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith"
2009.mtsummit-papers.18,C94-2175,0,0.099231,"text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs when |J |∼ |E|. Conseq"
2009.mtsummit-posters.10,2007.mtsummit-papers.63,1,0.920528,"tistical machine translation (SMT) experiments with the corpus and conﬁrmed that the corpus is useful for SMT. 1 Introduction Multilingual parallel corpora are required to support many tasks in natural language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Ja"
2009.mtsummit-posters.10,2005.mtsummit-papers.11,0,0.0444829,"al language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Japan {mutiyama,eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce."
2009.mtsummit-posters.10,tiedemann-nygaard-2004-opus,0,0.0174316,",eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce. However, there are a lot of translated texts on the Web. Specially, open source manuals are translated into Japanese from English by volunteer translators. We collected such English and Japanese texts. Then, the sentences in collected texts were automatically aligned, resulting in a parallel corpus made from open source software manuals. Manuals of open source software has been used for making a parallel corpus named OPUS (Tiedemann and Nygaard 2004), which was made from OpenOfﬁce.org documentation1 , KDE manuals including KDE messages2 , and PHP manuals 3 . However, the JapaneseEnglish part of OPUS is not large. In contrast, we collected about 500 thousand sentence pairs. In addition, our work involved extensive human efforts to ensure the quality of our parallel corpus. The original and translated texts often proscribe copy, distribute, display, and make derivative works. Our target texts are open source software manuals. Such open source software manuals are often published under open licenses under which we can modify and distribute t"
2009.mtsummit-posters.10,J93-1004,0,0.306217,", newlines are not deleted because they are regarded as sentence ends. 4.3 Aligning sentences We use Utiyama and Isahara’s alignment method, because their method has been successfully used in aligning noisy Japanese-English parallel texts (Utiyama and Isahara, 2007). Below is a concise description of their algorithm. We begin by obtaining the maximum similarity sentence alignments. Let J and E be a Japanese text ﬁle and an English text ﬁle, respectively. We calculate the maximum similarity sentence alignments (J1 ,E1 ), (J2 ,E2 ), . . ., (Jm ,Em ), using a dynamic programming matching method (Gale and Church, 1993), where (Ji , Ei ) is a Japanese and English sentence alignment pair in J and E. We allow 1-ton, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning sentences. The similarity between Ji and Ei is calculated based on word overlap (i.e., number of word pairs from Ji and Ei that are translations of each other based on a bilingual dictionary with 450,000+ entries). The similarity between a Japanese document, J, and an English document, E, (noted AVSIM(J,E)) is calculated using: ∑m AVSIM(J, E) = i=1 SIM(Ji , Ei ) m (1) A high AVSIM(J,E) value occurs when the sentence alignments in J and E take o"
2009.mtsummit-posters.10,P07-2045,0,0.0228859,"Missing"
2009.mtsummit-posters.10,P02-1040,0,0.0934553,"Missing"
2009.mtsummit-posters.10,P03-1021,0,0.016259,"ect without performing ﬁlling and painting operations. This includes BIOS settings, kernel conﬁguration and some simpliﬁcations in user land. This signal can be used to perform a remote checkpoint of a session. Xlib may choose to cache font data, loading it only as needed to draw text or compute text dimensions. Table 3: Example of parallel sentences formed to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papinei et al., 2002). The evaluation was done using a single reference. Tuning was performed using the standard technique developed by Och (Och, 2003). The test and development data were extracted from the aligned JF sentences. Each of test and development data consists of 500 sentences. In the following experiments, we simulated a situation where an SMT system was applied to help volunteer translators translate English JF documents into Japanese. We want to use all parallel sentences efﬁciently to help translators. This is a problem of domain adaptation. All of paralell sentences were translated from English to Japanese. Therefore we did MT experiments from English. In the ﬁrst experiment, we used all parallel sentences (excluding developm"
2009.mtsummit-posters.10,J03-1002,0,0.00480786,"Missing"
2009.mtsummit-posters.22,P07-2002,1,0.866754,"of documents every day, such as blogs, Wikipedia articles, open source software manuals, documents on nongovernmental organization (NGO) activities, and so on. These translations are read for pleasure, for practical purposes, for language learning, and many other reasons. Needless to say, volunteer translators contribute a great deal to the sharing and spreading of information around the world. Consequently, supporting their activities is a very important research issue. Volunteer translators translate a large number of documents everyday. However, they lack proper translation support tools (Abekawa and Kageura, 2007a). Thus, providing a good supporting environment should be of great assistance in improving volunteer translators’ efficiency and increasing the level of enjoyment they experience in translating. This is the motivation for our work in this paper. 2 Hosting volunteer translators Abekawa and Kageura (2007a) have developed a translation aid editor, QRedit, which has been experimentally provided to a limited number of volunteer translators. They report that QRedit is very effective for aiding their work, as described in Section 8. Based on the success of this translation aid editor, we have devel"
2009.mtsummit-posters.22,macklovitch-2006-transtype2,0,0.152808,"cement of translation candidate display [inside, left, right] of the SL area • Synchronized scroll [both directions, source→target, target→source, none] Figure 7 shows an example of a customized QRedit window. (See Figure 5 for a default window.) 8 User response As of 3 July, 2009 – three months after we made MNH and QRedit publicly available – there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. As quantitatively evaluating the benefit of using translation aid systems is technically a difficult task (cf. (Macklovitch, 2006)), and as we are dealing than before. We have not yet been able to obtain responses from group users, because they have only started using QRedit recently and thus have not accumulated sufficient experience to give an informed judgment on the system. We also have not been able to evaluate the usability of MNH because it is still under development and we are now improving its usability based on comments and suggestions from users. Figure 7: Customized QRedit window with volunteer translators who are not working on a “time is money” basis but rather wish to reduce the subjective burden of transl"
2009.mtsummit-posters.22,2001.mtsummit-papers.59,0,0.0378705,"uments; work that is concerned with hosting translated documents, often multilingually; and work that is addressed at aiding translators and translation communities. There are too many joint or collaborative online translation projects to mention here. GlobalVoices Online4 is perhaps one of the most well known, along with TUP5 (Translators United for Peace). Most projects do not provide translation aid facilities or collaborative working environments. They are rather projects defined by interested groups of people, using existing facilities. An example of the second category is Yakushite.net (Shimohata et al., 2001). It provides a collaborative translation environment in which users can use MT for translation, while contributing to collaborative terminology augmentation for the improvement of MT. Except for providing the MT engine, the translation aid functions are weak. Worldwide Lexicon (McConnell, 2007) is another example. Within the project a variety of mechanisms are provided that facilitate the sharing of translated documents world wide, with which one can (i) detect translated texts, if there are any; (ii) translate by oneself; (iii) subscribe to an RSS feed for translation; and (iv) use machine t"
2009.mtsummit-posters.22,P03-1010,1,0.803051,"ions and make derivative works public under certain conditions. Specifically, MNH uses four Creative Attribution Non-Commercial Share Alike This license lets others remix, tweak, and build upon the translator’s work non-commercially, as long as they credit the translator and license their new creations under identical terms. Translators can also use other licenses that are similar to the Creative Commons licenses listed above. In this way, translators can legally share their translations on MNH. These shared translations are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). Currently, MNH has a simple bilingual concordancer as shown in Figure 4. We plan to extend this concordancer in our future work because a bilingual concordancer is a very important tool for translation (Macklovitch et al., 2009). 6 High quality, comprehensive language resources Dictionaries and the web are the two main language resources that online volunteer translators use during translation. MNH, in cooperation with Sanseido, provides the “Grand Concise English Japanese Dictionary” (1) They are native-speakers of the target language (TL). (2) Most of them do not have a native-level comman"
2009.tc-1.4,P07-2002,1,0.458056,"Missing"
2010.iwslt-evaluation.18,N03-1017,0,0.00288212,"words by replacing them with known words that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous cont"
2010.iwslt-evaluation.18,P05-1033,0,0.0604414,"that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY alg"
2010.iwslt-evaluation.18,koen-2004-pharaoh,0,0.0968916,"aper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language m"
2010.iwslt-evaluation.18,P03-1021,0,0.0147245,"translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., i"
2010.iwslt-evaluation.18,J07-2003,0,0.0812834,"en in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-"
2010.iwslt-evaluation.18,W10-1760,1,0.791009,"on of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. We use an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches [6]. Word segmentations that are consistent with the phrasal segmentations of SMT translation models are learned from the SMT training corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn"
2010.iwslt-evaluation.18,P07-1040,0,0.0206781,"er the same surface string but differ only in the segmentation of the source language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorith"
2010.iwslt-evaluation.18,N04-1022,0,0.0213991,"language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm"
2010.iwslt-evaluation.18,2006.amta-papers.25,0,0.017203,"often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argm"
2010.iwslt-evaluation.18,P02-1034,0,0.0171555,"output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, w"
2010.iwslt-evaluation.18,P05-1012,0,0.0195886,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D07-1080,1,0.900751,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D08-1024,0,0.0536492,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,2008.iwslt-evaluation.13,1,0.855795,"n using the loss biased maximization in Equation 2 largely inspired by [14]. For the loss function lij and the underlying BLEU score b(·), we applied document scaled BLEU which computes BLEU by replacing one translation ei1 with another eij in a set of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignme"
2010.iwslt-evaluation.18,N03-1033,0,0.0440416,"t of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-c"
2010.iwslt-evaluation.18,N06-1014,0,0.0794113,"ir corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the"
2010.iwslt-evaluation.18,P08-1012,0,0.0154613,". As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from"
2010.iwslt-evaluation.18,P03-1054,0,0.00620711,"our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” la"
2010.iwslt-evaluation.18,P08-1067,0,0.0126776,"a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” last utterance from both speakers. The BOWs were collected from the n-best list of the tra"
2010.iwslt-evaluation.18,2009.iwslt-evaluation.12,1,0.830023,"DIALOG corpus, the BTEC corpus and the DEVSET corpus. All the data in the DEVSET for the BTEC task, using on single reference, was included for training. Only the devset for DIALOG was reserved for development testing. All of our experiment results presented in this paper are based on this testset. In total, we had around 35K sentence pairs for training. The devset used for MERT is sampled from all of the DEVSET for BTEC. In the last year’s IWSLT campaign, we introduced a devset sampling technique in which the development data were sampled from training data that are similar to the input text [24]. The similarity is measured by the BLEU using the input sentences as references. This year, we sampled from bilingual data with multiple reference translations, rather than from large amounts of DIALOG data with single reference translations, in order to avoid overfitting. We extracted 500 sentences for each translation direction. During MERT, only the training corpus for DIALOG and BTEC were used to train the translation model, but all of the data was used to build final translation model. Some pre-processing was also carried out on the corpus before training. First, in order to avoid ambigu"
2010.iwslt-evaluation.18,I08-4033,1,0.828764,"ces are split if multiple sentences are found in one line. At the end of translation, these multiple sentences are concatenated into a single line. We also did some normalization to the text. For English text, all the words were lowercased, any hyphens or commas were removed from between numeral words and tokenized using the standard tools provided by the Moses toolkit1 . The Chinese word segmentation originally provided contained inconsistencies and was not usable to build the translation model. The Chinese word segmentation was therefore redone using three methods: character-based, Achilles [25] and ICTCLAS2 . We will explain the usage of different segmentation standards in the next section. Basically, the numeral words in Chinese can be written either using Chinese characters or Arabic numbers. We converted all of the Arabic numbers to Chinese characters using a simple set of heuristics. Our translation model was built from data containing the punctuation for both source and target languages. In the official testing, the test data is provided without punctuation to remain consistent with the format of ASR output. So, before sending the test data for translation, we restored the punc"
2011.eamt-1.17,1995.iwpt-1.8,0,0.868516,"Missing"
2011.eamt-1.17,P96-1025,0,0.650243,"Missing"
2011.eamt-1.17,W03-0318,1,0.806022,"org/moses/. The Moses toolkit is a statistical machine translation system that allows automatic training of translation models for any language pair. Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 113120 Leuven, Belgium, May 2011 2 Domain Previous Work Research has been done on splitting long sentences into smaller segments in order to improve the translation. Splitting can be done either at the translation model training phase or the translation testing phase. Furuse et al. (1998) and Doi and Sumita (2003) tended to split speech output instead of text data. For speech output, the main issue is that one utterance may contain a few short sentences instead of one long sentence. Therefore, the main problem is splitting them into proper sentences for translation. However, since there is no punctuation in the speech data, it is difficult to locate the sentence boundaries, so, parsing results and word characteristics were used to determine the sentence boundaries. Kim and Ehara (1994) proposed using a rulebased method to split long sentences into multiple sentences. Furthermore, after splitting the se"
2011.eamt-1.17,P98-1070,0,0.0437221,"tem. 1 http://www.statmt.org/moses/. The Moses toolkit is a statistical machine translation system that allows automatic training of translation models for any language pair. Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 113120 Leuven, Belgium, May 2011 2 Domain Previous Work Research has been done on splitting long sentences into smaller segments in order to improve the translation. Splitting can be done either at the translation model training phase or the translation testing phase. Furuse et al. (1998) and Doi and Sumita (2003) tended to split speech output instead of text data. For speech output, the main issue is that one utterance may contain a few short sentences instead of one long sentence. Therefore, the main problem is splitting them into proper sentences for translation. However, since there is no punctuation in the speech data, it is difficult to locate the sentence boundaries, so, parsing results and word characteristics were used to determine the sentence boundaries. Kim and Ehara (1994) proposed using a rulebased method to split long sentences into multiple sentences. Furthermo"
2011.eamt-1.17,gerber-hovy-1998-improving,0,0.124918,"Missing"
2011.eamt-1.17,W04-1101,0,0.706743,"Missing"
2011.eamt-1.17,C94-1069,0,0.845532,"Missing"
2011.eamt-1.17,W09-0429,0,0.424345,"even when a sentence is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in Moses1 (Koehn and Haddow, 2009). Furthermore, a long sentence may include some long and complex noun phrases. These noun phrases should also be translated individually regardless of the context. We try to locate the area of these noun phrases by looking at the sequences of nouns in the sentences. We then bracket these noun phrases using the “zone” constraint in Moses. We use the NTCIR-8 (Fujii et al., 2010) Patent Translation shared task data between Japanese and English in our experiment. The sentences in the patent are always long and have complex structures, so even the humans have difficulties understanding if the texts"
2011.eamt-1.17,P07-2045,0,0.00517199,"atent corpus provided by the NTCIR8 Translation Campaign7 for Japanese and English translation. The training corpus contains about 3 million sentence pairs, the development set has 2,000 sentence pairs and the test set has 1,251 and 1,119 sentence pairs for J-E and E-J translation directions, respectively. We used Moses as a baseline system, with the following settings: • grow-diag-final-and heuristic • 5-gram language model, interpolated KneserNey discounting • msd-bidirectional-fe lexicalized reordering • distortion-limit = -1 (unlimited). 4.3 Reordering Constraint Marker The Moses decoder (Koehn et al., 2007; Koehn and Haddow, 2009) provides a way to specify the word reordering constraints. Two types of constraints were introduced: 1. Words within zones have to be translated without reordering with outside material. The distortion limit is set to unlimited, based on the findings in Kumai et al. (2008). Since Japanese 7 117 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html # of zone # of wall # of bracket zone # of clause Japanese 5604 914 280 2165 English 5696 434 347 1553 POS tag Japanese particle-case-compound particle-dependency particle-conjunctive particle-adverbializer auxiliary auxiliar"
2011.eamt-1.17,D10-1087,0,0.15808,"×q verb-main º-×q verb-auxiliary º- verb-suffix º auxiliary º-¨º-È particle-casecompound º-º particle-conjunctive º-º* particle-dependency º-íº= particle-adverbializer º-¨º-°`* particle-case-misc POS tag CC DT* EX IN PP RB RBR RBS WRB Description Head Position coordinating conjunction determiner existential there preposition or subordinating conjunction personal pronoun adverb adverb, comparative adverb, superlative Wh-adverb Table 3: POS tags used for split in English Table 2: POS tags used for split in Japanese is being done on inserting missing punctuation into the text (Murata et al., 2010; Guo et al., 2010). Similar to Kim and Ehara (1994), a rule-based approach is proposed to split a sentence into multiple clauses. First, the sentence is part-of-speech (POS) tagged by ChaSen using the IPAdic dictionary. In many cases, if there is a comma, the context before and after the comma may be independent and can be translated separately, making a comma a very important clue for locating splitting position candidates. However, not all commas are suitable to be used as split boundaries. We therefore combine the POS tags and commas as clues to determine the split position for long senten"
2011.eamt-1.17,niessen-etal-2000-evaluation,0,0.0420941,"For the zones (excluding bracket zones), the longest zone for Japanese is 25 words and the shortest is 2 words, with an average of 4.64 words. For English, the counts are 16 words, 2 words and 3.23 words, respectively. Table 7 shows the translation evaluation results. The English reference is lowercased and tokenized, and the Japanese reference is segmented by ChaSen. We evaluated the results using BLEU # of split 72 238 125 50 69 2 28 163 67 17 2 38 26 15 2 914 164 128 1 56 21 39 1 24 434 Table 6: Number of split condition by POS tags (Papineni et al., 2002), NIST (Doddington, 2002) and WER (Nießen et al., 2000). The baseline shows the results without using any reordering constraints. “Wall” shows the results where only the wall constraint was used and “zone” shows the results where only the zone constraint was used. Finally, the “wall+zone” shows the results where both the wall and zone constraints were used. From the results, we can see that by adding either zone or wall constraints, we can improve the quality in all three metrics. However, WER improves the most with the wall constraint. This means that wall constraint contributes more to control longdistance word reordering. By adding both constra"
2011.eamt-1.17,P02-1040,0,0.0888839,"Missing"
2011.eamt-1.17,W10-1762,0,0.017717,"e ending grammar in the middle of the sentence is usually not complete. Xu et al. (2005) proposed to separate a sentence pair into two sub-pairs based on a modified IBM Model 1. This process continues recursively over all the sub-pairs until their lengths are smaller than a given threshold. Finally, these sub-pairs are used for training a statistical translation model. The difference with our proposed method is that they split the sentences used for training, whereas we split only the input test data: they split the sentences using a bilingual corpus, but we split the sentences monolingually. Sudoh et al. (2010) proposed dividing the source sentence into small clauses using a syntactic parser. Then, a non-terminal symbol serves as a place-holder for the relative clause. However, they would also have to train a clause translation model which can translate the non-terminal symbols. They proposed a clause alignment method using a graph-based method to build the nonterminal corpus. The advantage of their method is that it can perform short and long distance reordering simultaneously. Travel News JST Patent Japanese English MIN MAX AVG MIN MAX AVG 2 162 10.75 2 116 9.46 1 132 28.77 1 135 26.52 1 250 30.98"
2011.eamt-1.17,P03-1010,0,0.0246833,"e and after the splitting points are independent, and we specify the translation zones as the boundary constraints. Our method is simple and does not require complicated processes like clause alignment, parsing, subject supplement or sentence ending completion. 3 Translation of Long Sentences in Patent Patent translation is a difficult task, as the sentences are usually very long and consist of many complex noun phrases. Table 1 shows the minimum, maximum and average sentence lengths in various domains for Japanese and English, such as the travel domain2 (Fordyce, 2007), JENAAD news articles (Utiyama and Isahara, 2003), JST3 scientific paper abstracts and patents (Fujii et al., 2010). For the word count statistics, the Japanese texts are segmented using ChaSen4 (Matsumoto et al., 2007) and the English texts are tokenized using a standard tokenizer provided by WMT workshop5 . As we can see, the patent text is much longer than any other domain and the maximum length is 3-4 times longer as well. For the 2 BTEC - http://iwslt07.fbk.eu/. http://www.jst.go.jp 4 http://chasen-legacy.sourceforge.jp/ 5 http://www.statmt.org/wmt08/scripts.tgz 3 114 Source Reference Baseline Split into multiple clauses Source Referenc"
2011.eamt-1.17,N10-1016,0,0.0223059,"ymbol serves as a place-holder for the relative clause. However, they would also have to train a clause translation model which can translate the non-terminal symbols. They proposed a clause alignment method using a graph-based method to build the nonterminal corpus. The advantage of their method is that it can perform short and long distance reordering simultaneously. Travel News JST Patent Japanese English MIN MAX AVG MIN MAX AVG 2 162 10.75 2 116 9.46 1 132 28.77 1 135 26.52 1 250 30.98 1 114 25.36 3 636 39.95 1 474 33.92 Table 1: Minimum/Maximum/Average sentence lengths in various domains Xiong et al. (2010) used Maximum Entropy Markov Models to learn the translation boundaries based on word alignments in hierarchical trees. The obtained beginning and ending translation boundaries are integrated into the decoder as soft constraints. A new feature is introduced to the decoder’s log linear model: translation boundary violation counting. This feature prefers the hypotheses that are consistent with the translation boundaries. Our research in this paper is different in the sense that we only want to split long sentences for translation where the context before and after the splitting points are indepe"
2011.eamt-1.17,2005.eamt-1.37,0,0.025338,"e used to determine the sentence boundaries. Kim and Ehara (1994) proposed using a rulebased method to split long sentences into multiple sentences. Furthermore, after splitting the sentences, they tried to identify a subject and inserted it into the subsequence sentences wherever needed. When a sentence is split, the ending grammar of the former part is changed so that its conjugation (tense, aspect, modality) matches the ending of the original complete sentence. This process is especially necessary for Japanese, where the ending grammar in the middle of the sentence is usually not complete. Xu et al. (2005) proposed to separate a sentence pair into two sub-pairs based on a modified IBM Model 1. This process continues recursively over all the sub-pairs until their lengths are smaller than a given threshold. Finally, these sub-pairs are used for training a statistical translation model. The difference with our proposed method is that they split the sentences used for training, whereas we split only the input test data: they split the sentences using a bilingual corpus, but we split the sentences monolingually. Sudoh et al. (2010) proposed dividing the source sentence into small clauses using a syn"
2011.iwslt-evaluation.5,2011.eamt-1.17,1,0.76364,"tion Case and punct No case and no punct BLEU 0.1190 0.1106 NIST 4.6929 4.7142 WER 0.7177 0.7523 PER 0.5746 0.5977 GTM 0.4844 0.4620 METEOR 0.4847 0.4503 TER 67.1430 71.8380 Table 1: The official results for the NICT system in terms of a variety of automatic evaluation metrics. ent sections of the sentence that could more effectively be translated separately. Since the input utterances are punctuated and contain spaces that indicate word boundaries we exploited this punctuation and space information as cues to determine the likely positions to delimit segmentation boundaries. In previous work [1] it has been shown that constraints of this type can be useful in managing the decoding of longer sentences. Constraining the search in the right way leaves a simpler problem for the machine translation decoder to solve, and one that can be performed considerably more efficiently than unconstrained decoding over the full search space.. The second strategy was to build the translation model for the system using two heterogeneous methods. The translation model is a key component in any phrase-based SMT system, and building this model using two different techniques can potentially bring benefits"
2011.iwslt-evaluation.5,W08-0336,0,0.0384903,"• Discontinuous (previous phrase-pair) • Swap (previous phrase-pair) 5. A word insertion penalty feature Based on a set of pilot experiments we decoded with no limit on the distances phrases could be moved in the reordering process during decoding. The base model above was augmented with an additional translation model feature intended to be an indicator of the quality/reliability of each phrase-pair; this feature will be explained later in Section 4.2. 2.2. Pre-processing The Chinese data supplied for this task was not segmented into words. We used the Stanford Chinese word segmentation tool [4, 5] with the Peking University (PKU) model to wordsegment this data. The English data was tokenized by applying a number of regular expressions to separate punctuation, 50 and split contractions such as “it’s” and “hasn’t” into two separate tokens. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post process"
2011.iwslt-evaluation.5,P03-1021,0,0.0477536,"lt in the same manner using the SRI language modeling toolkit [7]. 5-gram models were built for decoding the development and test data for evaluation, and 3-gram models were built for decoding during the parameter tuning process to speed up decoding. The language models were smoothed using modified KnesserNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering"
2011.iwslt-evaluation.5,P02-1040,0,0.0804854,"serNey smoothing. 2.4.2. Translation Model The translation model for the base system was built in the the standard manner using a 2-step process. First the training data To tune the values for the log-linear weights in our system, we use the standard minimum error-rate training procedure (MERT) [9]. The weights for the models were tuned using the development data supplied for the task. To perform the MERT tuning we used the publicly available ZMERT framework [10], and this allowed us to easily add and tune additional features into our models. The models were tuned with respect the BLEU metric [11]: ‘BLEU4 Closest’ that is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree."
2011.iwslt-evaluation.5,P03-1054,0,0.0202175,"hat is built into the tool. 3. Rule-based Decoding Constraints 3.1. Motivation Translating long and complex sentences has been a critical problem in machine translation. A standard phrase-based statistical machine translation system cannot solve the problem of word reordering in the target when the source sentence has a complex structure. A syntax-based machine translation system could solve the problem by running a parser on the source sentence in order to get the syntactic structure, but when a sentence is long and complex, the parser may fail to give a correct parse tree. Klein and Manning [12] have shown that the accuracy of parsing decreases as sentence length increases, and the parsing time increases dramatically. However, in this research, we found that even when a sentence is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almo"
2011.iwslt-evaluation.5,W09-0429,0,0.0596069,"is long and complex, it is possible to split a sentence into smaller units which can be translated separately with minor consideration of the context. The main problem here is locating the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a"
2011.iwslt-evaluation.5,C94-1069,0,0.0497907,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,1995.iwpt-1.8,0,0.0237346,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P96-1025,0,0.0947943,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,W04-1101,0,0.0282081,"ting the best locations for the split. We use linguistic information such part-of-speech (POS) tags and commas as clues to determine the split positions. After splitting a sentence into small clauses, the clauses are translated almost independently. This means that word reordering can only be done within a clause, not between clauses. This constraint can be specified using “wall” tag in MOSES (as in Koehn and Haddow [13]), and we implemented the same scheme in the OCTAVIAN decoder. 3.2. Methodology A large body of previous research has shown that punctuation is very useful when parsing a text [14, 15, 16, 17]. The comma is one such useful mark. Basically, a comma has two roles: as a delimiter to separate different syntactic types, or 51 as a separator to separate the elements of the same category type [18]. However, this information alone is not enough to distinguish whether the comma is suitable to be a split position for machine translation. A comma and the information around the comma could help to find a proper place for a split. Whether or not it is a proper place for a split depends upon if the information on the left and right sides of the comma are able to be translated independently. Punc"
2011.iwslt-evaluation.5,P11-1064,1,0.886228,"t out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised manner [26]. The technique is based on a Pitman-Yor process model. Bayesian models of this form have recently proved themselves useful in the field of natural language processing, as they typically offer benefits over more traditional techniques based on maximum likelihood. In particular, they model the data according to a power law distribution that is often observed in linguistic data. Moreover, by encouraging the re-use of parameters in the model during training, Bayesian models of this type will prefer to build very compact models with few parameters that have a tendency not to over-fit the data. In"
2011.iwslt-evaluation.5,J07-2003,0,0.148834,"Missing"
2011.iwslt-evaluation.5,D10-1087,0,0.0658909,"Missing"
2011.iwslt-evaluation.5,W00-1308,0,0.0546915,"Missing"
2011.iwslt-evaluation.5,N03-1033,0,0.0127114,"Missing"
2011.iwslt-evaluation.5,P00-1056,0,0.27701,"man here is an out-of-vocabulary word and is marked with a ‘|’ symbol.). Decoding constraints Unconstrained decoding Constrained decoding BLEU score 10.84 11.16 Table 4: The effect of re-ordering constraints on translation quality. 4. Bayesian Alignment 4.1. Motivation In a standard phrase-based statistical machine translation system (and in the base system we used in this shared evaluation), a two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a"
2011.iwslt-evaluation.5,P10-1147,0,0.0337203,"two-step alignment and extraction process is commonly used. In the first step, word-level alignment is performed both from source-to-target and from target-to-source using the publicly available GIZA++ [24] tool. In the second step, these two word-level alignments are combined and by means of a set of heuristics, a large set of bilingual phrase pairs that are consistent with these alignments are extracted. This approach although inelegant has proven itself to be highly effective in practice, and this is the reason for its pervasiveness. However, other approaches are possible. DeNero and Klein [25] point out that this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a solution to this, they proposed a supervised discriminative model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results. In our system we employ a related technique that is able to perform direct phrase-to-phrase alignment and extraction in a single unified framework in a fully unsupervised ma"
2011.iwslt-evaluation.5,J93-2003,0,\N,Missing
2011.iwslt-evaluation.5,D08-1076,0,\N,Missing
2011.iwslt-evaluation.5,P07-2045,0,\N,Missing
2011.iwslt-evaluation.5,2010.iwslt-evaluation.18,1,\N,Missing
2011.iwslt-papers.11,N09-2038,0,0.0345077,"translation performance for its test set, and another data set giving the highest. In the experiments, we compare two methods for selecting data to be manually translated from the field data. Both of them use source side language models for data selection, but in different manners. According to the experimental results, either or both of the methods show larger improvements compared to a random data selection. 1. Introduction As a result of the drastic technical innovation advances in spoken language processing, speech-to-speech translation systems are now starting to be used in actual fields [1, 2]. In addition to the development of basic technologies, the efficient usage of field data is also an important challenge to be addressed for system performance improvement. A speech-to-speech translation system consists mainly of three subsystems: an automatic speech recognition (ASR) subsystem, a machine translation (MT) subsystem and a speech synthesis subsystem. While the simplest and most effective usage of field data is to annotate (transcribe and translate) all of the field data and use this for ASR and MT training, annotation is expensive and time consuming. In this paper, we propose a"
2011.iwslt-papers.11,P03-1021,0,0.0102454,"es) and a non-OOV part (non-OOV sentences). In addition to the training data used for the “Baseline 1”, “Baseline 2” uses OOV sentences and their manual translation. The baseline selection is a simple random selection. And, upper bound uses all of the field data and its manual translation. Table 2 shows the details of data sets used for the annotation data selection experiments. For each of the data sets (Hokkaido and Kyushu), we randomly sampled 1000 sentences from the field data to be used for the development sets and test sets. These development sets are used for minimum error rate training[9] and annotation data selection. The test sets are for the MT performance evaluation. In the experiments, we evaluate selection performance by computing the BLEU score [10] of the SMT system trained on the selected sentences including manual translation of the selected sentences. For the translation model and language model training, we use MOSES [11] and the SRI language model toolkit [12]. For the language model setting, we used a modified Kneser-Ney [13] 5-gram language model. For the data selection, we use 3-gram language model and set n (the number of selected sentence in each iteration) t"
2011.iwslt-papers.11,P02-1040,0,0.0809305,"Missing"
2011.iwslt-papers.11,P07-2045,0,0.0328106,"Missing"
2011.iwslt-papers.11,I08-2088,1,0.847105,"bsystem, a machine translation (MT) subsystem and a speech synthesis subsystem. While the simplest and most effective usage of field data is to annotate (transcribe and translate) all of the field data and use this for ASR and MT training, annotation is expensive and time consuming. In this paper, we propose a method for selecting useful field data to be manually translated in sentence units. In previous studies on ASR [3], positive results were obtained by selectively annotating (transcribing) field data. There have also been many studies on domain adaptation research handling data selection [1, 4, 5, 6] in ASR and MT researches, However, there has been little research done from a data annotation1 point of view. Typical MT domain adaptation research handles the selection of productive training sentences from out-of-domain monolingual or parallel corpus. In the task setting, we can 1 Different from ASR, annotation means manual translation here. use source and target language information to select training sentences from the parallel corpora. However, only source language information is available in our annotation data selection task. In this paper, we propose two methods that use source side l"
2011.iwslt-papers.11,N03-1017,0,0.0137249,"ion is shown Fig. 2. In the adaptation experiments, we used data sets from two areas. One is the data set from Hokkaido, which gave the lowest speech translation performance on its test set. The other data set is from Kyushu, which gave the highest. Translation memory 1 No output yes Translation Memory 2 2.2. System configuration of MT system Fig. 3 gives a flowchart of the MT subsystem. As shown in the figure, the MT system consists of 2 main components: the statistical based machine translation (SMT) and a translation memory. For the SMT, we employed a log-linear model as a phrase-based SMT [7]. This model expresses the probability of a target-language word sequence (e) for a given source language word sequence (f ) given by (∑ ) M exp i=1 λi hi (e, f ) (∑ ) P (e|f ) = ∑ (1) M ′ e′ exp i=1 λi hi (e , f ) where hi (e, f ) is the feature function, λi is the feature function’s weight, and M is the number of features. We can approximate Eq. 1 by regarding its denominator as constant. The translation results (ˆ e) are then obtained by eˆ(f, λM 1 ) = argmaxe M ∑ λi hi (e, f ) (2) i=1 We used the following eight features [7] for the translations. 3. Lexical weighting probability from the s"
2011.mtsummit-papers.37,P05-1074,0,0.0256923,"TF , not simply find a paraphrase for the input sentence. There are also two minor differences. One difference is the calculation of P (f |f  ). P (f |f  ) can be represented as (1) How to select one of eij is not discussed in this paper because we focus on the retrieval part of TMs. 3 326 P (fp |fp ) where fp and fp is one of the set of paraphrase pairs of f and f  , respectively, and P (fp |fp ) is the paraphrase probability of fp given fp . Quirk et al. calculated P (fp |fp ) from monolingual parallel corpora. In contrast, we calculate P (fp |fp ) from bilingual parallel corpora (Bannard and Callison-Burch, 2005). Another difference lies in the implementation. They used an in-house decoder which was very much like a phrase-based SMT monotone decoder. We use weighted finite state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows"
2011.mtsummit-papers.37,N06-1003,0,0.0147483,"m TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one o"
2011.mtsummit-papers.37,P09-1053,0,0.0242961,"state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep"
2011.mtsummit-papers.37,C10-2043,0,0.0382665,"Missing"
2011.mtsummit-papers.37,D07-1103,0,0.0294813,"(1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 ). The purpose of this threshold is to keep highly-accurate paraphrase pairs. 3.4 Implementation using WFSTs We use WFSTs to retrieve sentences in a TM. Given an input sentence f , the best sentence fˆ in Equation (1) is represented in Equation (2"
2011.mtsummit-papers.37,P07-2045,0,0.00355409,"is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 )."
2011.mtsummit-papers.37,D09-1040,0,0.0434158,"Missing"
2011.mtsummit-papers.37,P10-2001,1,0.868418,"d by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one of eij as the translati"
2011.mtsummit-papers.37,W04-3219,0,0.0224,"Missing"
2011.mtsummit-papers.37,W03-0311,1,0.801179,"t, the paraphrase retrieval proposed in this paper will use the translations of the retrieved sentences without modification. For example, if “is there a beauty parlor?” is retrieved when “is there a salon?” is given as an input, we simply output 2 Previous TM systems could use a thesaurus to detect paraphrases. However, large scale thesauruses do not exist for most languages. In this paper, we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Parap"
2011.mtsummit-papers.37,2009.mtsummit-papers.14,0,0.102253,"we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in"
2011.mtsummit-papers.37,W01-1401,1,0.874099,"ntroduction Translation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input."
2011.mtsummit-papers.37,2004.iwslt-evaluation.11,0,0.180725,"anslation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input. This is because “is"
2011.mtsummit-papers.37,W10-3806,0,0.063305,"Missing"
2011.mtsummit-papers.40,C96-1009,0,0.116742,"Missing"
2011.mtsummit-papers.40,D07-1103,0,0.0336507,"English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exact test as ScoreF to measure the validity of each bilingual term candidate. The statistic used in Fisher’s exact test is deﬁned as ScoreF as follows. First, we obtain the contingency table, shown below, for a bilingual term candidate TJ,E consisting of Japanese term J and English term E C(J, E) C(E) − C(J, E) We use functions implemented in TermExtract 2 to extract term candidates. TermExtract is a Perl module for extracting terms. We slightly modiﬁed the POS patterns use"
2011.mtsummit-papers.40,P07-2045,0,0.00258597,"statistical measures. In addition, we compare three statistical measures for extracting bilingual terms. Note that Macken et al. (2008) have also used statistical measures to ﬁlter out invalid bilingual term candidates; however, they did not compare their statistical measures against other measures. 3 Bilingual term extraction (1) Extract the term candidates, which match speciﬁc part-of-speech(POS) patterns (e.g., a single noun or a noun sequence), from the Japanese and English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exa"
2011.mtsummit-papers.51,W06-2920,0,0.0180473,"ool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. The parser uses a lexicalized probabilistic CFG model. The model is based on the principle of maximum entropy. STANFORD Stanford’s parser (Klein and Manning, 2003). The parser uses an unlexicalized probabilistic CFG model. We used version 1.6.5. BERKELEY Berkeley’s parser (Petrov and Klein, 2007). The parser uses an unlexicalized probabilistic CFG model. We used release 1.1. 2.2 Dependency parser Owing to the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), research into dependency parsing have been active. Dependency structure is a tree structure in which a node is a word and an edge is the relation between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three dom"
2011.mtsummit-papers.51,W08-0309,0,0.0188608,"between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three domains. Sentence length is the number of words per English sentence. We used the IWSLT corpus (Eck and Hori, 2005) in the travel domain, the WMT08 News Commentary corpus (Callison-Burch et al., 2008) in the news domain, and the NTCIR-8 Patent machine translation corpus (Fujii et al., 2010) in the patent domain. Figure 1: Penn Treebank-style phrase structure Figure 2: Dependency tree structure We built a model using WSJ section 2 to 21 from Penn Treebank. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. 2.3 HPSG parser There is a parser based on the HPSG (Pollard and Sag, 1994) theory. HPSG-based parsers analyze not only phrase structure but also deeper structures, such as the arguments of a pre"
2011.mtsummit-papers.51,A00-2018,0,0.0904273,"was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such"
2011.mtsummit-papers.51,P08-1009,0,0.0388999,"hang et al., 2006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machin"
2011.mtsummit-papers.51,P97-1003,0,0.0771229,"when the parser was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the oth"
2011.mtsummit-papers.51,W06-1628,0,0.0507067,"Missing"
2011.mtsummit-papers.51,2005.iwslt-1.1,0,0.0135043,"e is a tree structure in which a node is a word and an edge is the relation between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three domains. Sentence length is the number of words per English sentence. We used the IWSLT corpus (Eck and Hori, 2005) in the travel domain, the WMT08 News Commentary corpus (Callison-Burch et al., 2008) in the news domain, and the NTCIR-8 Patent machine translation corpus (Fujii et al., 2010) in the patent domain. Figure 1: Penn Treebank-style phrase structure Figure 2: Dependency tree structure We built a model using WSJ section 2 to 21 from Penn Treebank. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. 2.3 HPSG parser There is a parser based on the HPSG (Pollard and Sag, 1994) theory. HPSG-based parsers analyze"
2011.mtsummit-papers.51,C96-1058,0,0.0903811,"Missing"
2011.mtsummit-papers.51,C96-1009,0,0.0109805,"ut instead used the noun phrases determined by using the parse results of a context document, a document that contains an input sentence. This method can determine noun phrases by considering document-level consistency. The method is as follows: 1. The method parses a context document containing an input sentence. Set Training Development Test Number of sentences 3.2 million 2,000 1,119 Table 3: Statistics for the NTCIR-8 English to Japanese patent translation task dataset. 2. The method extracts all noun phrases from the parse results. 3. The method ranks the noun phrases based on a C-value (Frantzi and Ananiadou, 1996) that gives high rank to phrases with high termhood from nested candidates. 4. The method searches the list of noun phrases (in order of rank) for expressions that appear in the input sentence. 5. The method determines the searched expression to be a noun phrase and adds zone tags if the expression does not conﬂict with existing zone tags. The C-value of a phrase p is expressed in the following equation: C-value(p) =  (l(p)−1) n(p)  (l(p)−1) t(p) n(p)− c(p) (c(p) = 0) (c(p) &gt; 0) where l(p) is the length of a phrase p, n(p) is the frequency of p in a document, t(p) is the total frequency of p"
2011.mtsummit-papers.51,2006.amta-papers.8,0,0.0137765,"se source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context containing the input"
2011.mtsummit-papers.51,P03-1054,0,0.0791817,"ain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introdu"
2011.mtsummit-papers.51,W04-3250,0,0.111841,"ined directly by the parsers. “Baseline” indicates the result that did not use a parser and zone tags. The ﬁve parsers other than COLLINS had improved the BLEU scores over the baseline BLEU score. From these results, it can be seen that the CHARNIAK, STANFORD, BERKELEY, MST, and ENJU parsers were effective for patent machine translation. The “**” mark in Table 4 and Table 5 denotes a statistical signiﬁcant difference at the signiﬁcance level of α = 0.01 and the “*” mark denotes a statistical signiﬁcant difference at the signiﬁcance level of α = 0.05 according to the bootstrap resampling test (Koehn, 2004). The difference between BERKELY and the top parsers STANFORD, MST, and ENJU was not signiﬁcant at α = 0.05 and the difference between BERKELEY and CHARNIAK was signiﬁcant at α = 0.05. From these results, it can be seen that BERKELEY, STANFORD, MST, and ENJU were especially effective for patent machine translation among the six parsers when the noun phrases in an input sentence were obtained by parsing the input sentence directly. COLLINS CHARNIAK STANFORD BERKELEY MST ENJU BLEU 39.42 39.58 39.64 39.76 39.54 39.68 Gains from without context +2.45** +0.34* +0.08 +0.16 +0.10 +0.28* Table 5: Comp"
2011.mtsummit-papers.51,P06-1077,0,0.0177111,"ntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context"
2011.mtsummit-papers.51,P09-1063,0,0.0313516,"Missing"
2011.mtsummit-papers.51,J93-2004,0,0.0391607,"hese results showed how their method was effective with each parser. The rest of this paper is organized as follows: In section 2, we show the six parsers that we compared. In section 3, we explain the method of comparison. In section 4, we discuss the experiment results from the NTCIR-8 patent translation task data. In section 5, we conclude this paper. 449 2 Parsers We focused on six well-known publicly available parsers. The parsers are categorized by method into three groups: probabilistic CFG parser, dependency parser, and HPSG parser. 2.1 Probabilistic CFG parser Owing to Penn Treebank (Marcus et al., 1993), there has been a lot of research into parsers based on probabilistic CFG that output phrase structures. Fig. 1 shows an example of a phrase structure. The ways to parameterize the probabilistic models vary. In this research, we used the following four parsers: COLLINS Collins’ (1997) parser. The parser uses a lexicalized probabilistic CFG model. The tool includes three models: model 1, 2, and 3. We used model 3. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. Th"
2011.mtsummit-papers.51,P08-1114,0,0.0689699,"006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluati"
2011.mtsummit-papers.51,E06-1011,0,0.0210896,"ts of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increa"
2011.mtsummit-papers.51,J08-1002,0,0.0409543,"ontaining the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increased. With globalization comes an increase in th"
2011.mtsummit-papers.51,P08-1006,0,0.142156,"he Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increased. With globalization comes an increase in the need for the international circulation of patent documents. It is, therefore, important to improve the quality of machine translation of patent sentences. Word ordering is the main issue in statistical machine translation of long patent sentences between language pairs with widely different word 448 There is a task-oriented evaluation (Miyao et al., 2008). Miyao et al. (2008) compared parsers based on the accuracy of identifying protein-protein interaction that used the parser’s output as features for machine learning models. This evaluation showed the effect of each parser for the protein-protein interaction task using biomedical papers. There is research that studied the relation between parse accuracy and translation quality (Quirk and Corston-Oliver, 2006). This showed the relationship between a parser’s training data size and the translation quality. They did not compare parsers, nor did they use a patent corpus. Research has also been do"
2011.mtsummit-papers.51,P11-2076,1,0.419665,"It is difﬁcult to select the appropriate parser for patent translation because the effects of each parser on patent translation are not clear. This paper provides comparative evaluation of several state-of-the-art parsers for English, focusing on the effects for patent machine translation from English to Japanese. We measured how much each parser contributed to improve translation quality when the parser was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation"
2011.mtsummit-papers.51,P02-1040,0,0.0828331,"translated by a phrase-based statistical machine translation with reordering constraints using syntax of input sentences. We translated from English to Japanese, whose word orders are widely different. In translation between languages with widely different word orders, it is difﬁcult to assign the proper word order, especially with 450 long input sentences. Input sentence syntax is useful in deciding a word order for the translated sentence. We parsed the input sentence and constrained the word order using these parsed results. The translation quality was measured using the 4-gram BLEU score (Papineni et al., 2002). There is a method that determines the noun phrases in an input sentence by using the parse results from document-level context that contains the input sentence. We applied this method and compared parsers based on the translation quality. We also examined the effects of this method on each parser and combinations of parsers. First, we show the issue of patent translation. Next, we explain the methods that deal with the issue by reordering constraints using syntax of input sentences. Finally, we explain the method that estimates noun phrases using document-level context. 3.1 Patent translatio"
2011.mtsummit-papers.51,N07-1051,0,0.234005,"entences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, de"
2011.mtsummit-papers.51,W06-1608,0,0.263524,"tent sentences. Word ordering is the main issue in statistical machine translation of long patent sentences between language pairs with widely different word 448 There is a task-oriented evaluation (Miyao et al., 2008). Miyao et al. (2008) compared parsers based on the accuracy of identifying protein-protein interaction that used the parser’s output as features for machine learning models. This evaluation showed the effect of each parser for the protein-protein interaction task using biomedical papers. There is research that studied the relation between parse accuracy and translation quality (Quirk and Corston-Oliver, 2006). This showed the relationship between a parser’s training data size and the translation quality. They did not compare parsers, nor did they use a patent corpus. Research has also been done on the relationship between four parsers and translation quality of syntax-based statistical machine translation (Zhang et al., 2006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translati"
2011.mtsummit-papers.51,H05-1059,0,0.0464321,"HPSG parser. 2.1 Probabilistic CFG parser Owing to Penn Treebank (Marcus et al., 1993), there has been a lot of research into parsers based on probabilistic CFG that output phrase structures. Fig. 1 shows an example of a phrase structure. The ways to parameterize the probabilistic models vary. In this research, we used the following four parsers: COLLINS Collins’ (1997) parser. The parser uses a lexicalized probabilistic CFG model. The tool includes three models: model 1, 2, and 3. We used model 3. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. The parser uses a lexicalized probabilistic CFG model. The model is based on the principle of maximum entropy. STANFORD Stanford’s parser (Klein and Manning, 2003). The parser uses an unlexicalized probabilistic CFG model. We used version 1.6.5. BERKELEY Berkeley’s parser (Petrov and Klein, 2007). The parser uses an unlexicalized probabilistic CFG model. We used release 1.1. 2.2 Dependency parser Owing to the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), research into dependency parsing have been active. Dependency"
2011.mtsummit-papers.51,N10-1016,0,0.0411628,"valuated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. More"
2011.mtsummit-papers.51,W08-0401,1,0.865812,"tent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine trans"
2011.mtsummit-papers.51,P08-1064,0,0.0117577,"no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context containing the input sentence to determine the noun phrases in the input sentence (Onis"
2012.iwslt-evaluation.15,D12-1037,0,0.0218568,"Missing"
2012.iwslt-evaluation.15,P07-2045,0,0.0268509,"Missing"
2012.iwslt-evaluation.15,P02-1040,0,\N,Missing
2012.iwslt-evaluation.15,N04-1022,0,\N,Missing
2012.iwslt-evaluation.15,P03-1021,0,\N,Missing
2012.iwslt-evaluation.15,J03-1002,0,\N,Missing
2012.iwslt-evaluation.15,2009.iwslt-evaluation.12,1,\N,Missing
2012.iwslt-evaluation.15,I05-3027,0,\N,Missing
2012.iwslt-evaluation.16,koen-2004-pharaoh,0,\N,Missing
2012.iwslt-evaluation.16,P04-1021,0,\N,Missing
2012.iwslt-evaluation.16,P07-2045,0,\N,Missing
2012.iwslt-evaluation.16,D09-1024,0,\N,Missing
2012.iwslt-evaluation.16,P09-1012,0,\N,Missing
2012.iwslt-evaluation.16,P12-1049,0,\N,Missing
2012.iwslt-evaluation.16,P11-1044,0,\N,Missing
2012.iwslt-evaluation.16,W11-3208,1,\N,Missing
2012.iwslt-evaluation.16,2010.iwslt-papers.7,1,\N,Missing
2012.iwslt-evaluation.16,2010.iwslt-evaluation.18,1,\N,Missing
2012.iwslt-evaluation.16,D08-1076,0,\N,Missing
2012.iwslt-evaluation.16,2012.eamt-1.60,0,\N,Missing
2013.mtsummit-papers.3,2012.iwslt-papers.6,0,0.0204834,"Missing"
2013.mtsummit-papers.3,2010.iwslt-papers.7,1,0.834779,"nd the work in this paper are the only romanization induction techniques reported in the literature to date. The advantage of these methods is that they can be applied to many different languages without the need for an existing romanization system, and can be optimized to fit a specific purpose. Furthermore, as we will show later, the strategy seems quite robust to noise in the data, and we were able to build effective systems from data that contained non-transliteration pairs. 4 Methodology Our method induces a romanization system directly from a non-parametric Bayesian bilingual alignment (Finch and Sumita, 2010) between source and target grapheme sequences. This model has been shown to align consistently, without a tendency to overfit the data, and is therefore suitable for both one-to-many and many-to-many alignment. We use Levenshtein distance (LD) to select an appropriate romanization from a set of candidates derived from the alignment. More formally, let S = (s1 , s2 , . . . , sI ) and T = (t1 , t2 , . . . , tI ) be corpora of source and target words respectively. Each si and ti are represented as sequences of graphemes in their respective writing systems. Let Π and Ω be sets of grapheme sequence"
2013.mtsummit-papers.3,W11-3208,1,0.788419,")] (2) ck ∈Cj Where D(ck ) is the cost in terms of Levenshtein distance from using romanization rule (oj , ck ). For a single occurrence of oj in the corpus, this cost is LD(ck , ψ(oj )), the Levenshtein distance between romanization candidate sequence ck and ψ(oj ), the target grapheme sequence aligned to oj . The expected value of this cost over the corpus is calculated according to: ∑ p(cl )LD(ck , cl ) (3) E[D(ck )] = l=1..K 5 Experiments 5.1 Inducing Japanese Romanization 5.1.1 Data For training and evaluation in our experiments we used the Japanese-English translation mining corpus of (Fukunishi et al., 2011). This corpus consists of 4339 Japanese-English word pairs extracted from Wikipedia interlanguage link titles, all of which are annotated as correct/incorrect transliteration pairs. 3800 of the word pairs were correct transliterations and 539 word pairs were noise. 5.1.2 Induced Systems We induced two different romanization systems from the data. The simplest method (Unigram) discovered romanizations for each individual kana character. A more sophisticated method learned romanizations for multiple sequences of kana (Ngram). Table 1 shows example romanization rules Kana カ ク グ ケ コ シ ジ ス ズ ゼ ツ ト"
2013.mtsummit-papers.3,W10-2405,0,0.0809689,"xisting systems. We will show later in this paper that in our chosen application, it is possible to induce a romanization system that is more effective than simply choosing from well-established existing schemes. 3 Related Work In many transliteration mining approaches (Aransa et al., 2012; Htun et al., 2012), romanization is required to compare words across languages, typically using normalized edit distance metrics. Statistical transliteration systems can be used, but these need large amounts of training data which may not be available. A simple system of automatic romanization was used by (Jiampojamarn et al., 2010) to great effect in the shared mining task of the NEWS2010 workshop. Their system allowed cross language comparison between word pairs in different scripts by aligning single characters in one script to either single Roman characters or to NULL. Their romanization rules roman20 ized by substitution with the most representative single character, or by deletion. Our work differs from theirs in that we are aiming to induce a full romanization involving multiple characters on the target side1 without the deletion of the characters to be romanized. The fact that romanization was only performed with"
2013.mtsummit-papers.3,W10-2403,0,0.0383865,"Missing"
2014.amta-researchers.9,P13-2068,0,0.023877,"Missing"
2014.amta-researchers.9,W09-2404,0,0.125788,", the well-developed techniques of SMT are mainly focused on the sentencelevel translation, i.e., the models are trained on the parallel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceed"
2014.amta-researchers.9,W12-3156,0,0.0734953,"loped techniques of SMT are mainly focused on the sentencelevel translation, i.e., the models are trained on the parallel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1:"
2014.amta-researchers.9,2012.eamt-1.60,0,0.0881271,"a baseline SMT system to introduce documentlevel information. In practice, if document-level information is no available, our approach degenerates to the baseline system (i.e. λtdoc = λst doc = 0); otherwise, the approach produces several sets of {λtdoc , λst }, which suggests that we should pay attention to the document-level doc features. 4 Experiment 4.1 Data and Settings We tested the proposed approach on French-to-English translation because this translation task has been handled well by state-of-the-art SMT systems. We used two different schemes. One is on the WIT3 corpus of TED talks4 (Cettolo et al., 2012), which contains a small training set with document-level parallel development set and test set. The other scheme is a relatively more realistic setting: using the Europarl corpus (Koehn, 2005) for model training and an in-domain development set for the weight tuning in the baseline SMT system. Then we selected document pairs from the Common Crawl (CC) Corpus5 of WMT2013 for document-level development and test set. The CC corpus has a lot of noise, with many document pairs only several sentences long – too short for our purposes. Thus, we selected relatively high-quality document pairs, with m"
2014.amta-researchers.9,J07-2003,0,0.749481,"ge of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is essentially a re-ranking process in a document-level decoding (Fig. 1). We first use an off-the-shelf baseline SMT system to translate a document sentence-bysentence, obtaining the m-best translation candidates for each sentence. The baseline SMT system can be trained and tuned in a standard way with sentence-level parallel data. Then, we conduct a decoding on the document level to find good combinations among the m-best candidate sentences. The search is realized in a cube-pruning way (Chiang, 2007). Here, we use good to mean that the combinations are good for both sentence- and document-level metrics under the Pareto optimality (Duh et al., 2012). As far as we know, this is the first attempt to apply document-level re-ranking in an SMT system. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 112 tk-2 tk-1 tk tk+1 tk+2 rk-2 rk-1 rk rk+1 rk+2 Figure 2: In Seval (BLEU), every translation t will be compared with its reference r. 3.2 Notation In the following description, we use D to denote an input document on the source-side language c"
2014.amta-researchers.9,P12-1001,0,0.310384,"evel translation, where both the training and decoding are on the sentence level. Then we introduce two document-level features, one using the word-embedding technique to model the semantic relation of context on the target side and the other using a token-type ratio to model the consistency in translation. With the two document-level features, we conduct a further decoding on the document level to get a better combination of sentencelevel translation within a document. As to the weights of the introduced features, we utilize a multi-objective learning approach based on the Pareto optimality (Duh et al., 2012) to simultaneously optimize the sentence-level and document-level metrics. The proposed approach requires no word-net or document-level parallel corpus for model training. Instead, it requires a vector list of the target-side vocabulary by word embedding and a small development set of parallel document pairs to tune the weights of document-level features. The remainder of the paper is organized as follows. In Section 2, we mention the related work around using document-level information in translation. In Section 3, we describe our proposed approach. Section 4 presents experimental results, an"
2014.amta-researchers.9,P12-2023,0,0.130415,"). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the document level. The app"
2014.amta-researchers.9,D11-1084,0,0.0913281,"er and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the"
2014.amta-researchers.9,W13-3302,0,0.013329,"allel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent resea"
2014.amta-researchers.9,D12-1108,0,0.0185143,"ts a documentlevel decoding on the target-side m-best candidate lists of source-side sentences to find a better combination (marked by the dashed zigzag arrow). is statistical and needs to be trained on monolingual or bilingual document-level data. Along with the feature of lexical cohesion, the topic is a sophisticated feature that must be supported by extra resources. Many approaches using document-level features require to modify the decoder of a baseline system to adapt to their features in decoding. Research mainly focusing on the decoding and tuning algorithm, such as the series work of Hardmeier et al. (2012) and Stymne et al. (2013), extends the traditional sentence-based SMT system to be able to collaborate with documentlevel features. As to our approach, the features used can model the lexical consistency as well as semantic relation at a certain level while not being as rigid as the features/operations on the very lexical level that many previous approaches use. We assume that these features, combined with the multi-objective tuning, will provide a robust and stable way to take advantage of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is es"
2014.amta-researchers.9,E14-1035,0,0.0703531,"many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the document level. The approach first uses an off-the-shelf SMT system"
2014.amta-researchers.9,D11-1125,0,0.0266109,").11 The word embedding is over a vocabulary of 130, 000 words, with 50-dimension vectors. In the document-level decoding algorithm, we set the margin in cube-pruning to [−10, 10] to enlarge the search space. The search generated 100 document-level candidates for re-ranking. t feature and the Deval calculation, we set window-size to 2. That is, the context was In the fdoc defined as the two preceding and two succeeding sentences. For weight tuning on the document level, the multi-objective tuning can be combined with any tuning algorithm, such as MERT (Och, 2003), MIRA (Chiang, 2012), or PRO (Hopkins and May, 2011). Our approach contains only two free weights, λtdoc and λst doc ; thus, we used a greedy search for them in (−1.0, 1.0), with step of 0.1, to avoid any possible search errors in the tuning phase. We took the consistency verification approach (Xiao et al., 2011) as the comparison approach in our experiments. Similar to our approach, this approach takes advantage of the mbest translation candidates and uses a further decoding step to polish the baseline sentence-level 8 http://www.speech.sri.com/projects/srilm/ 9 As well as the word alignment of the highest-scoring candidate. 10 http://ml.nec-l"
2014.amta-researchers.9,W04-3250,0,0.0953589,"iable, especially in scheme-2. Consequently, the consistency verification method does not perform well in scheme-2. In Tables 4 and 5, we show the experimental results of the proposed approach in scheme-1 and scheme-2, respectively. Different sets of weights on the frontier of Pareto optimality are listed,15 with their corresponding Seval and Deval on development set and Seval on test set (i.e., test set BLEU). The first rows, λtdoc = 0 and λst doc = 0 are the performance of the baseline SMT system for scheme-1 and scheme-2. We conduct a statistical significance test via the bootstrap method (Koehn, 2004) using bleu-kit16 . For each row, + and − mean the result is better or worse than the baseline, respectively: a single mark means the difference is on the level of p < 0.05 and a double mark means on the p < 0.01 level. For the overall performance, in scheme-1, the change of test set BLEU is in the range of [−0.01, +0.48] points compared to the baseline; in scheme-2, the range of change is in [−0.26, +0.56]. Because the Pareto frontier offers multiple weights rather than a deterministic, the change on test set BLEU we report here is a range rather than a deterministic value. 12 Xiao et al. (20"
2014.amta-researchers.9,2005.mtsummit-papers.11,0,0.00716137,"rwise, the approach produces several sets of {λtdoc , λst }, which suggests that we should pay attention to the document-level doc features. 4 Experiment 4.1 Data and Settings We tested the proposed approach on French-to-English translation because this translation task has been handled well by state-of-the-art SMT systems. We used two different schemes. One is on the WIT3 corpus of TED talks4 (Cettolo et al., 2012), which contains a small training set with document-level parallel development set and test set. The other scheme is a relatively more realistic setting: using the Europarl corpus (Koehn, 2005) for model training and an in-domain development set for the weight tuning in the baseline SMT system. Then we selected document pairs from the Common Crawl (CC) Corpus5 of WMT2013 for document-level development and test set. The CC corpus has a lot of noise, with many document pairs only several sentences long – too short for our purposes. Thus, we selected relatively high-quality document pairs, with moderate lengths of 40–60 sentences to compose the baseline. The data used in the two schemes and the detailed information are listed in Tables 1 and 2, respectively. In experiments, as previous"
2014.amta-researchers.9,W13-3303,0,0.0131583,"sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; E"
2014.amta-researchers.9,P03-1021,0,0.256712,"(doc) of scheme-1are an identical set. train dev. (snt.) dev. (doc.) test scheme-1 0.14M snt. 934 snt. 8 doc. / 934 snt. 11 doc. / 1, 664 snt. scheme-2 1.99M snt. 2, 000 snt. 14 doc. / 600 snt. 55 doc. / 2, 500 snt. model was an interpolated 5-gram model with modified Kneser-Ney discounting, trained by SRILM8 (Stolcke, 2002), on each scheme’s training data. In sentence-level decoding, the ttablelimit was 20; the stack size was 200; and the distortion-limit was 6, all of which followed the default settings of Moses’ decoder. The feature weights of the baseline PB SMT system were tuned by MERT (Och, 2003) to optimize the sentence-level development set BLEU (Papineni et al., 2002). The settings in tuning and translating on sentence-level were identical. For the document-level decoding of the proposed approach, we used the baseline system to generate a 1000-best translation candidate list for each sentence in a document. Each translation candidate was attached with the word alignment information in sentence-level decoding for the st fdoc calculation. Duplicate candidates in a 1000-best list were merged to one candidate taking t the highest score9 of the baseline SMT system. For the fdoc calculat"
2014.amta-researchers.9,J03-1002,0,0.00350855,"with moderate lengths of 40–60 sentences to compose the baseline. The data used in the two schemes and the detailed information are listed in Tables 1 and 2, respectively. In experiments, as previously described, a baseline SMT system was built from sentencelevel parallel data (the train row in Tables 1 and 2) and tuned on sentence-level development set (the dev. (snt.) row). We used the phrase-based statistical machine translation (PB SMT) system of Moses6 (Koehn et al., 2007) as the baseline SMT system. In model training, we used the grow-diag-final-and to symmetrize the output of GIZA++7 (Och and Ney, 2003). The maxphrase-length was set to 7 and the reordering model was msd-bidirectional-fe. The language 3 We always set λ snt to be positive. 4 https://wit3.fbk.eu/ λtdoc and λst doc can be either positive or negative. 5 http://www.statmt.org/wmt13/translation-task.html 6 http://www.statmt.org/moses/ 7 https://code.google.com/p/giza-pp/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 116 Table 1: Data used in experiment. train dev. (snt.) dev. (doc.) test scheme-1 WIT3 WIT3 WIT3 WIT3 scheme-2 Europarl WMT dev2006 CC CC Table 2: Number of sent"
2014.amta-researchers.9,P02-1040,0,0.0927951,"oc.) test scheme-1 0.14M snt. 934 snt. 8 doc. / 934 snt. 11 doc. / 1, 664 snt. scheme-2 1.99M snt. 2, 000 snt. 14 doc. / 600 snt. 55 doc. / 2, 500 snt. model was an interpolated 5-gram model with modified Kneser-Ney discounting, trained by SRILM8 (Stolcke, 2002), on each scheme’s training data. In sentence-level decoding, the ttablelimit was 20; the stack size was 200; and the distortion-limit was 6, all of which followed the default settings of Moses’ decoder. The feature weights of the baseline PB SMT system were tuned by MERT (Och, 2003) to optimize the sentence-level development set BLEU (Papineni et al., 2002). The settings in tuning and translating on sentence-level were identical. For the document-level decoding of the proposed approach, we used the baseline system to generate a 1000-best translation candidate list for each sentence in a document. Each translation candidate was attached with the word alignment information in sentence-level decoding for the st fdoc calculation. Duplicate candidates in a 1000-best list were merged to one candidate taking t the highest score9 of the baseline SMT system. For the fdoc calculation, we used a high-quality 10 English word embedding used in the SENNA tool"
2014.amta-researchers.9,W13-3308,0,0.0228699,"on the target-side m-best candidate lists of source-side sentences to find a better combination (marked by the dashed zigzag arrow). is statistical and needs to be trained on monolingual or bilingual document-level data. Along with the feature of lexical cohesion, the topic is a sophisticated feature that must be supported by extra resources. Many approaches using document-level features require to modify the decoder of a baseline system to adapt to their features in decoding. Research mainly focusing on the decoding and tuning algorithm, such as the series work of Hardmeier et al. (2012) and Stymne et al. (2013), extends the traditional sentence-based SMT system to be able to collaborate with documentlevel features. As to our approach, the features used can model the lexical consistency as well as semantic relation at a certain level while not being as rigid as the features/operations on the very lexical level that many previous approaches use. We assume that these features, combined with the multi-objective tuning, will provide a robust and stable way to take advantage of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is essentially a re-ranking pr"
2014.amta-researchers.9,W10-2602,0,0.22849,"ounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest featu"
2014.amta-researchers.9,N12-1046,0,0.0739994,"as begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexica"
2014.amta-researchers.9,2011.mtsummit-papers.13,0,0.251451,", recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only"
2014.amta-researchers.9,D13-1163,0,0.0464399,"l translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually r"
2014.amta-researchers.9,P07-2045,0,\N,Missing
2014.iwslt-evaluation.20,2012.eamt-1.60,0,0.0123428,"statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model. 1. Introduction In the IWSLT 2014 machine translation evaluation campaign, the NICT team participated in the TED [1] translation shared-task for Chinese-English. This paper describes the machine translation approach adopted for this campaign. Our system was a combination of phrase-based and hierarchical SMT systems. The combination was performed by reranking the n-best hypotheses from these systems. A loglinear model which used the hypothesis scores of the component systems as features was used to calculate the score used in reranking. Additional features were also added into the log-linear model, for example features from a neural network model, or talk-level language model scores. In addition to system co"
2014.iwslt-evaluation.20,P14-1129,0,0.135961,"mple features from a neural network model, or talk-level language model scores. In addition to system combination, we put emphasis on language modeling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual tra"
2014.iwslt-evaluation.20,J03-1002,0,0.00988323,"ling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models train"
2014.iwslt-evaluation.20,2005.mtsummit-papers.11,0,0.0128404,"rom this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models trained on the English LDC Gigaword dataset, a collection of approximately 4 billion words of international newswire text. 2.1. Pre-processing The English data was tokenized by applying the EUROPARL tokenizer [5]. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post processing based on experiments carried out for the 2010 IWSLT shared evaluation [6]. 2.2. Post-processing The output of the translation system was subject to the following post-processing steps which were carried out in the following order: 1. In all"
2014.iwslt-evaluation.20,P03-1021,0,0.0455445,"Missing"
2014.iwslt-evaluation.20,2013.iwslt-evaluation.1,0,0.0146617,"cores of the component systems (MERT) [10]. The weights for the models were tuned using the development data supplied for the task. 3.5. Evaluation We evaluated each of these systems on the IWSLT 2013 test set, and the results are shown in Table 3.5. The evaluation in all of the experiments in this report was carried out on tokenized, lowercase data, using the “multi-bleu.perl” evaluation script included in release version 2.1 of the MOSES toolkit. The systems are roughly comparable in performance, and about 1.5 BLEU percentage points higher than the caseinsensitive MOSES baseline reported in [11], we believe this can be explained by differences in the tokenization used for evaluation, and also by differences in the development sets used for tuning. We found that when tuned and evaluated on different data sets, the relative rankings of the systems may vary. 4. Methodology 4.1. Language Modeling 4.1.1. Neural Network Model We implemented the neural network joint models proposed in [2] and used the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale o"
2014.iwslt-evaluation.20,D13-1140,0,0.0203032,"d the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale of the neural network. We found the settings used in [2] gave rise the highest performance, and we therefore adopted these settings in our system. These settings were: 11-word source context, 3-word target context, 192-unit shared embedding layer, and two additional 512unit hidden layers. We set both input and output vocabulary size to 32000. The neural network was implemented using the NPLM toolkit [12]. The results are shown in Table 4.3. The gain using from this approach was approximately 0.5 BLEU points. This was lower than the gains reported in [2], however, in their experiments the neural network was directly integrated into the decoding process. We integrated monolingual neural network model into the OCTAVIAN decoder, however, the experiments were not completed due to time limitations. 4.1.2. Gigaword We combined language models trained on the source of the parallel TED corpus, and the Gigaword newswire corpus by linear interpolation. The interpolated language model was then used direc"
2014.iwslt-evaluation.20,P96-1041,0,0.370762,"Missing"
2014.iwslt-evaluation.20,P07-2045,0,0.0124426,"the following order: 1. In all experiments, the out of vocabulary words (OOVs) were passed through the translation process unchanged, some of these OOVs were Chinese and some English. For the primary submission, we took 139 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the decision to delete only those OOVs containing Chinese characters not included in the ASCII character set and leave words containing only ASCII characters in the output. 2. The output was de-tokenized using the de-tokenizer included with the MOSES toolkit [7]. 3. The output was re-cased using the re-casing tool supplied with the MOSES toolkit. We trained the recasing tool on cased text from the TED talk training data. 3. The Base Systems 3.1. Decoders Our submission used two SMT systems within a system combination framework; these systems were: 1. OCTAVIAN, an in-house phrase-based decoder. 2. A hierarchical version of the MOSES decoder [7]. The OCTAVIAN decoder used in these experiments is an in-house phrase-based statistical machine translation decoder that can operate in a similar manner to the publicly available MOSES decoder [7]. The base dec"
2014.iwslt-papers.5,N03-1017,0,0.00933623,"Missing"
2014.iwslt-papers.5,P07-2045,0,0.0182188,"Missing"
2014.iwslt-papers.5,2005.mtsummit-papers.11,0,0.0574603,"Missing"
2014.iwslt-papers.5,W12-4207,0,0.0512408,"Missing"
2014.iwslt-papers.5,N09-1028,0,0.0455172,"Missing"
2014.iwslt-papers.5,D12-1077,0,0.0272006,"Missing"
2014.iwslt-papers.5,C04-1073,0,0.107251,"Missing"
2014.iwslt-papers.5,P05-1066,0,0.116818,"Missing"
2014.iwslt-papers.5,C10-1043,0,0.0500847,"Missing"
2014.iwslt-papers.5,P03-1056,0,0.0731239,"Missing"
2014.iwslt-papers.5,P13-1045,0,0.0374312,"Missing"
2014.iwslt-papers.5,N03-1033,0,0.0314199,"Missing"
2014.iwslt-papers.5,2008.jeptalnrecital-long.17,0,0.133128,"Missing"
2014.iwslt-papers.5,J03-1002,0,0.0078425,"Missing"
2014.iwslt-papers.5,N04-4026,0,0.0740724,"Missing"
2014.iwslt-papers.5,P03-1021,0,0.080262,"Missing"
2014.iwslt-papers.5,P02-1040,0,0.0911959,"Missing"
2014.iwslt-papers.5,D10-1092,0,0.0676014,"Missing"
2014.iwslt-papers.5,Y13-1026,0,0.0235984,"Missing"
2014.iwslt-papers.8,N13-1023,0,0.0474077,"method to integrate them. 2. Related Work The work in this paper is based upon the stream decoder [1], an extension to a phrase-based statistical machine translation decoder that allows it to decode directly from continuous stream of tokens. We describe this methodology in more detail in Section 3. In [2] the prosody information in the speech signal was used to segment a continuous stream of speech input for translation. In their experiments, a silence duration of approximately 100ms was found to be suitable for segmentation. A number of diverse strategies for pre-segmentation were studied in [3]. They studied both non-linguistic techniques, that included ﬁxed-length segments, and a “hold-output” method. The hold-output method method is relevant to the research in this paper because it relies the same principle used by the stream decoder. It identiﬁes contiguous blocks of text that do not contain alignments to words outside them. An SVM was used to predict these blocks prior to the decoding process; the stream decoder operates by identifying similar structures during decoding. Their experimental results showed this method to be ineffective. Linguisticallymotivated segmentation techniq"
2014.iwslt-papers.8,P14-2090,0,0.337584,"this paper because it relies the same principle used by the stream decoder. It identiﬁes contiguous blocks of text that do not contain alignments to words outside them. An SVM was used to predict these blocks prior to the decoding process; the stream decoder operates by identifying similar structures during decoding. Their experimental results showed this method to be ineffective. Linguisticallymotivated segmentation techniques were also considered. Conjunctions, sentence boundaries and commas were investigated, with commas being the most effective segmentation cue in their investigation. In [4] a strategy for pre-segmentation based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation can be controlled 206 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. The automatic interpretation from English into Japanese has been studied in [5]. Their approach used heuristics to identify predicates that are lik"
2014.iwslt-papers.8,P07-2045,0,0.00667062,"In this alternative strategy, the stream decoder will undertake a new decoding pass in which it is forced to make a monotonic step as the ﬁrst step in the decoding process. Then, a state is selected from the best hypothesis using Algorthim 1. This process may also fail if the monotonic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level"
2014.iwslt-papers.8,P03-1021,0,0.00994997,"as the ﬁrst step in the decoding process. Then, a state is selected from the best hypothesis using Algorthim 1. This process may also fail if the monotonic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level BLEU each talk is considered to be a single sentence in the BLEU computation. For consistency only the talk level BLEU results ar"
2014.iwslt-papers.8,2001.mtsummit-papers.68,0,0.00881081,"onic step would lead to the violation of Lmin . In our implementation, we allow the decoder to violate Lmin only in this case. Our stream decoder was implemented within the framework of the OCTAVIAN decoder, a phrase-based statistical machine translation decoder that operates in a similar manner to the MOSES decoder [11]. The training procedure was quite typical: 5-gram language models were used, trained with modiﬁed Kneser-Ney smoothing; MERT [12] was used to train the log-linear weights of the models; the decoding was performed with no limit on the distortion. 4.3. Evaluation The BLEU score [13] was used to evaluate the machine translation quality in all our experiments. Where sentence segmentation was known we used both talk and sentencelevel BLEU, and for the experiments where true stream decoding was performed on a stream of tokens with no segmentation information, talk-level BLEU was used. In talk level BLEU each talk is considered to be a single sentence in the BLEU computation. For consistency only the talk level BLEU results are reported in this paper, but the results from the sentence-level BLEU experiments were similar in character. 1 http://www.ted.com 208 Proceedings of th"
2014.iwslt-papers.8,P06-2088,0,0.0770843,"eing the most effective segmentation cue in their investigation. In [4] a strategy for pre-segmentation based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation can be controlled 206 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. The automatic interpretation from English into Japanese has been studied in [5]. Their approach used heuristics to identify predicates that are likely to be invertible from a dependency structure derived from a phrase-structure parse of the English. They exploit the somewhat free word order of Japanese to re-order the Japanese tokens into an order that is appropriate for interpretation. The resulting word order may be a little dis-ﬂuent, but is nonetheless grammatically valid and is typical of the kind of compromise that needs to be made during interpretation. There are also some related studies in translation process research (for example, [6, 7]) that study in detail t"
2014.iwslt-papers.8,2008.iwslt-papers.5,0,0.0164548,"nterpretation. There are also some related studies in translation process research (for example, [6, 7]) that study in detail the process of human simultaneous interpretation. In [8] it was shown that the prediction and use of soft boundaries in the source language text, when used as reordering constraints can improve the quality of a speech translation system. 3. Stream Decoding The stream decoding strategy differs from approaches based on the pre-segmentation of the stream of input tokens in that the segmentation decisions are able to exploit information from the decoding process itself. In [9], it is stated that long segments of around 10-40 words are required in a presegmentation strategy in order to achieve performance close to the underlying machine translation system. These long segments give rise to long latencies, and the penalty for reducing the segment size in order to achieve acceptably latencies is typically severe. These issues have been addressed recently with more intelligent strategies for choosing the segmentation points [4], but nonetheless we believe the stream decoding approach deserves more attention in the literature, and merits further study for the following r"
2014.iwslt-papers.8,I05-3027,0,0.0448543,"ore the operation of and enhancements to the stream decoder we use the TED1 talks data sets from the IWSLT2014 campaign. We studied English to: Spanish, Italian, French, German and Chinese, and found the results on the set of European language pairs were mostly similar in character, and we therefore report results on only English-Spanish (a typical pair) and English-German (an exceptional pair) from this set. Statistics on the corpora are given in Table 1. The European language data was tokenized by the Stanford PTBTokenizer. The Chinese was segmented using the Stanford Chinese word segmenter [10] according to the Chinese Penn Treebank standard. 4.2. Decoder Test 1701 1397 1700 Table 1: Statistics on corpora using in the stream decoding experiments. The numbers given are in segments, representing individual subtitles, corresponding approximately to sentences. such state exists, in which case the algorithm returns s0 , and since the stream decoder is required to make an output, it must use an alternative strategy. In this alternative strategy, the stream decoder will undertake a new decoding pass in which it is forced to make a monotonic step as the ﬁrst step in the decoding process. Th"
2015.mtsummit-papers.1,P84-1068,0,0.64869,"Missing"
2015.mtsummit-papers.1,N12-1047,0,0.062571,"t (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary concern in this experiment is the effect of long distance relationship, in general, n-gram based metrics such as BLEU alone do not fully illustrate it. RIBES is therefore used alongside BLEU. RIBES is an automatic evaluation method based on r"
2015.mtsummit-papers.1,P05-1033,0,0.267749,"uilt a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabulary and phrases are commonly used in entire patent documents, and Corpus A is of a"
2015.mtsummit-papers.1,P05-1066,0,0.0440006,"most the entire gain in the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an ea"
2015.mtsummit-papers.1,N15-1105,0,0.107415,"Missing"
2015.mtsummit-papers.1,2012.amta-caas14.1,0,0.0123933,"tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across dist"
2015.mtsummit-papers.1,P13-2121,0,0.0327298,"entence pairs was randomly divided into development and test data respectively consisting of 1,000 English-Japanese claim sentence pairs. 4.2. Systems In this experiment, we regard the implementation of phrase-based SMT in the Moses toolkit (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary c"
2015.mtsummit-papers.1,I13-1147,0,0.0147867,"much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source a"
2015.mtsummit-papers.1,D10-1092,0,0.147645,"Missing"
2015.mtsummit-papers.1,P13-1048,0,0.0299936,"sive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translat"
2015.mtsummit-papers.1,N03-1017,0,0.0190389,"ble. We first parsed 200,000 patent sentences using the initial parsing model. We then built a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabula"
2015.mtsummit-papers.1,C94-2183,0,0.0489313,"cross distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. Ther"
2015.mtsummit-papers.1,E91-1054,0,0.456166,"tactic parsing (Isozaki et al., 2010b; de Gispert et al., 2015), with growing volumes of parallel patent corpora available, have brought significant improvements in the performance of statistical machine translation (SMT) for translating patent documents across distant language pairs (Goto et al., 2012; Goto et al., 2015). However, among various sentences within a patent document, patent claim sentences still pose difficulties for SMT resulting in low translation quality, despite their utmost legal importance. A patent claim sentence is written in a kind of sublanguage (Buchmann et al., 1984; Luckhardt, 1991) in the sense that it has the following two characteristics: (i) comprising a patent claim by itself with an extreme length and (ii) having a typical sentence structure composed of a fixed set of components irrespective of language, such as those illustrated in Figures 1 and 2. The difficulties in patent claim translation lie in these two characteristics. Regarding the first characteristic, the extreme lengths cause syntactic parsers to fail with consequent low Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 1 reordering accuracy. Regarding the second"
2015.mtsummit-papers.1,2006.eamt-1.24,0,0.551983,"Missing"
2015.mtsummit-papers.1,2012.amta-papers.20,0,0.0173078,"document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in whi"
2015.mtsummit-papers.1,P02-1040,0,0.0931704,"(2) Claim sentences are translated according to the sentence structure, producing structurally natural translation outputs. We manually extracted a set of language independent claim components. Moreover, using these components, we constructed a set of synchronous rules for English and Japanese to transfer the SSSS in the source language to the target language. The results of an experiment demonstrate these two major effects of our SSSS transfer method. Regarding the first effect, when used in conjunction with pre-ordering, our method improves translation quality by five points in BLEU score (Papineni et al., 2002), in both English-to-Japanese and Japanese-to-English translations. Regarding the second effect, gains in RIBES score (Isozaki et al., 2010a) of over 30 points are obtained, indicating that our SSSS transfer is effective in transferring an input sentence structure to the output sentence. Components Preamble Transitional phrase Body Element Element Element Example strings An apparatus, comprising: a pencil; an eraser attached to the pencil; and a light attached to the pencil. Figure 1. Example of an English patent claim (WIPO, 2014) Components Element Body Element Element Transitional phrase Pr"
2015.mtsummit-papers.1,P09-2004,0,0.0322502,"tant for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this di"
2015.mtsummit-papers.1,P06-1055,0,0.0259631,"“having:”, ”備える”〉 〈“wherein:”, ”ことを特徴とする”〉 〈“wherein:”, ”する”〉 3.2. Pre-ordering Another major issue in patent claim translation is that the extreme lengths cause syntactic parsers to fail with consequent low reordering accuracy. To evaluate the effect of introducing our SSSS transfer on the translation quality, we also implemented a pre-ordering tool using state-of-the-art techniques (Isozaki et al., 2010b; Goto et al., 2012; Goto et al., 2015). Our pre-ordering method is based on syntactic parsing. First, the input sentence is parsed into a binary tree structure by using the Berkeley Parser (Petrov et al., 2006). For example, when “He likes apples.” is inputted into our English-to-Japanese translation system, it is parsed as shown in Figure 7. Second, the nodes in the parse tree are reordered using a classifier. For example, according to the classifier's decision, the two children of the “VP” node, i.e., “VBZ” and “NP”, are swapped, whereas the order of the two children of the “S” node, i.e., “NP” and “VP”, is retained. Once such a decision is made for every node with two children (henceforth, binary mode), the word order of the entire sentence becomes very similar to that in Japanese, i.e., “He (kar"
2015.mtsummit-papers.1,P05-1034,0,0.049209,"n the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublang"
2015.mtsummit-papers.1,P13-2066,0,0.0184134,"re appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2"
2015.mtsummit-papers.1,2007.mtsummit-papers.63,1,0.700657,"abulary and phrases are commonly used in entire patent documents, and Corpus A is of a substantial size to cover a large portion of them. However, the claim-specific sentence structure would never be taken into account, as Corpus A does not contain any claim sentences. To bring claim-specific characteristics into the SMT training, even for the baseline systems, we also used Corpus B comprising 1.0 million parallel sentences of patent claims. These were automatically extracted from pairs of English and Japanese patent documents published between 1999 and 2012 using a sentence alignment method (Utiyama and Isahara, 2007). The concatenation of Corpora A and B was used to train baseline SMT systems, as well as those for our extensions. 2 Note that Goto et al. (2015) learned the SWAP/STRAIGHT classification problem jointly with the parsing source sentences. 3 https://www.cis.upenn.edu/~treebank/ 4 https://www2.nict.go.jp/out-promotion/techtransfer/EDR/index.html Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 7 Development and test data were constructed separately from the training data in the following manner. First, we randomly extracted English patent documents from p"
2015.mtsummit-papers.1,N09-2004,0,0.0270482,"s, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies"
2015.mtsummit-papers.1,C04-1073,0,0.141971,"Missing"
2015.mtsummit-papers.1,P14-2092,0,0.218298,"Missing"
2015.mtsummit-papers.1,P09-2035,0,0.0194868,"rall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2004; Murakami et al., 2009; Murakami et al., 2013) and sentence segmentation (Xiong et al., 2009; Jin and Liu, 2010) for dealing with long input sentences with complex structures. Our approach is similar to the above models in the sense that it incorporates structural information into SMT, but differs in that it uses sublanguage-specific sentence structures, rather than syntactically motivated structures. This results in significant improvement in translation quality for the claim sublanguage using only a handful of rules. 6. Conclusion In this paper, we described a method for transferring sublanguage-specific sentence structure for English-to-Japanese and Japanese-to-English patent clai"
2015.mtsummit-papers.4,P96-1041,0,0.203437,"ated to lemmatized words in the English sentences by using a Japanese-English dictionary. 4.2.2 Phrase-based SMT The phrase-based SMT experiments were performed with the two models, in which the parameters θ and θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same"
2015.mtsummit-papers.4,N12-1047,0,0.0198592,"θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same BPRM model with the autoencoder layer removed. 5 Results and Analysis Tables 2 and 3 present the results of the phrase pair extraction task, and Table 4 presents the results of the phrase-based SMT task. 2 ftp://"
2015.mtsummit-papers.4,D14-1179,0,0.1104,"Missing"
2015.mtsummit-papers.4,P14-1129,0,0.0179666,"otation: the left-side RNN is referred to as the source-side and the right-side RNN (we use overbars on the symbols to differentiate it) is referred to as the target-side. 2 Related work In this section, we review recent work on neural network phrase representation models. Continuous phrase representation models with a feed-forward neural network were studied in Schwenk (2007, 2012). The models estimated translation probabilities for unseen phrases with a continuous vector space of phrases. Le et al. (2012) proposed a similar approach to score phrase pairs using fixed-size inputs and outputs. Devlin et al. (2014) proposed a neural network joint model (NNJM) as an extension of the NNLM (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences o"
2015.mtsummit-papers.4,W08-0509,0,0.0213167,"h contains the most documents among the domains, according to International Patent Classification (IPC) code 1 . 720K sentence pairs from the documents published between 1993 to 2005 were used as the training data, and 2.0K and 0.5K sentence pairs randomly sampled from the 2006 and 2007 documents were used as the development and test data respectively. We also used the 2006 and 2007 documents to extract the similar phrases. Furthermore, we used the English and Japanese monolingual corpus in NTCIR-10 for the training of word representations. For the extraction of phrase pairs, we used MGIZA++ (Gao and Vogel, 2008) and growdiag-final-and heuristics of the Moses toolkit (Koehn et al., 2007). To facilitate effective learning, we used only phrase pairs that contained content words (i.e. had at least one noun or verb word in the phrase) and had a high translation probability (a threshold on the source-given-target conditional probability was used). We extracted phrase pairs from the training, development and test data. The training phrase-pairs were used for training the neural network models. The development phrase-pairs were used to control the training of the models. The model was trained for 4,000 itera"
2015.mtsummit-papers.4,P13-1031,0,0.017613,"est word types en ja 0.8K 0.9K 2.4K 2.1K Monolingual sent en ja 41M 81M Table 1: Data sets where θ¯ is the set of target-side parameters, and α and β are the hyper-parameters for the balance of each error. We also use an L1 regularization term in the objective function. In Equation (8), we group the semantic error Ec terms of source- and target-side (which use the summary vectors c and ¯ c) into one term, and arrange the terms according to error type. The parameters θ and θ¯ are optimized to minimize Equation (8) using the AdaGrad stochastic adaptive subgradient algorithm (Duchi et al., 2011; Green et al., 2013): θi = θi−1 − η Gi = Gi−1 + ∂J −1/2 G ∂θi−1 i ∂J 2 ∂θi−1 (9) (10) where η is the learning rate, i is the number of the training iterations, and G is the sum of the squares of the past gradients. θ and θ¯ are learned and updated in every iteration through the training data of phrase-pairs. The number of training iterations was determined using development data. 3.2 Word representations Word representations, in which words are represented as real-valued vectors (Bengio et al., 2003; Mikolov et al., 2013), serve as the inputs to our model. The word representations r are calculated as: ri = Lui ∈"
2015.mtsummit-papers.4,D13-1176,0,0.0437439,"dicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take the tree structure of their input into account. Kalchbrenner and Blunsom (2013) proposed recurrent continuous translation models based on recurrent language models (RLMs), which predict target words from an unbounded history of both source and target words with a conditional probability. In their implementation convolutional neural networks were used to model the source-side. Cho et al. (2014) proposed a gated recurrent unit which adaptively remembers and forgets its state based on the input signal to the unit. This model was used to score each phrase pair in the phrase table for SMT. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |"
2015.mtsummit-papers.4,W04-3250,0,0.318544,"Missing"
2015.mtsummit-papers.4,P07-2045,0,0.00600191,"atent Classification (IPC) code 1 . 720K sentence pairs from the documents published between 1993 to 2005 were used as the training data, and 2.0K and 0.5K sentence pairs randomly sampled from the 2006 and 2007 documents were used as the development and test data respectively. We also used the 2006 and 2007 documents to extract the similar phrases. Furthermore, we used the English and Japanese monolingual corpus in NTCIR-10 for the training of word representations. For the extraction of phrase pairs, we used MGIZA++ (Gao and Vogel, 2008) and growdiag-final-and heuristics of the Moses toolkit (Koehn et al., 2007). To facilitate effective learning, we used only phrase pairs that contained content words (i.e. had at least one noun or verb word in the phrase) and had a high translation probability (a threshold on the source-given-target conditional probability was used). We extracted phrase pairs from the training, development and test data. The training phrase-pairs were used for training the neural network models. The development phrase-pairs were used to control the training of the models. The model was trained for 4,000 iterations, and estimated parameters θ and θ¯ by evaluating the highest accuracy"
2015.mtsummit-papers.4,N03-1017,0,0.0605517,"representations (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012) based on neural networks have outperformed the previous stateof-the-art approaches. These language models map each word to a dense, low-dimensional, real-valued vector, and estimate the probability of words in a continuous space. Representations for phrases have been used in the context of Statistical Machine Translation (SMT). Zou et al. (2013) used phrasal representations for computing the distance between phrase pairs and added a feature based on this distance into the log-linear model of a phrase-based SMT system (Koehn et al., 2003). Their method learned bilingual word representations, and subsequently obtained the phrase-level representations by simply averaging word vectors. Continuous representations for phrases or sentences with neural networks – such as a feed-forward, recursive, or recurrent neural networks – have also been used in SMT. A phrase representation model using a feed-forward neural network for phrase-based SMT was proposed by Schwenk (2007, 2012), and achieved significant BLEU score improvements. Since the model directly projects feature vectors not from words but from phrases or sentences onto a contin"
2015.mtsummit-papers.4,N12-1005,0,0.0172388,"ur model is symmetric and does not differentiate between source- and target-side, we use the following notation: the left-side RNN is referred to as the source-side and the right-side RNN (we use overbars on the symbols to differentiate it) is referred to as the target-side. 2 Related work In this section, we review recent work on neural network phrase representation models. Continuous phrase representation models with a feed-forward neural network were studied in Schwenk (2007, 2012). The models estimated translation probabilities for unseen phrases with a continuous vector space of phrases. Le et al. (2012) proposed a similar approach to score phrase pairs using fixed-size inputs and outputs. Devlin et al. (2014) proposed a neural network joint model (NNJM) as an extension of the NNLM (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by usi"
2015.mtsummit-papers.4,D13-1054,0,0.0988804,"M (Bengio et al., 2003). The NNJM calculates the target-side word probability by using a target-side language model in combination with a context from the source-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences of a phrase to a continuous vector on each node of the tree recursively. Li et al. (2013) described an ITG reordering classifier which predicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take"
2015.mtsummit-papers.4,P11-2093,0,0.0217739,"Word2Vec toolkit (Mikolov et al., 2013). The size of the word representation vector n is usually determined empirically. 4 Experiments We conducted two experiments with the Bilingual Phrase Representation Model: a phrase-pair extraction task and a phrase-based SMT task. 4.1 Data and model parameters Both experiments were conducted on two English-Japanese (en-ja) corpora. One was from IWSLT 2007 (Fordyce, 2007) which is in the domain of spoken travel conversation and the other was a patent translation corpus from NTCIR-10 (Goto et al., 2013). The Japanese sentences were tokenized using KyTea (Neubig et al., 2011). Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 48 Table 1 provides statistics on each corpus. The “sent” column indicates the number of sentence pairs, and the “word types” columns of “en” and “ja” indicate the number of English and Japanese unique words. The “Monolingual” column indicates the size of the monolingual data for the training of the word representations described in Section 3.2. For IWSLT 2007, we used the training data for the training of the word representations. For NTCIR-10, we used about 723K sentence pairs belonging to the physics"
2015.mtsummit-papers.4,P02-1040,0,0.0922584,"with the two models, in which the parameters θ and θ¯ were estimated on DEV.1 and DEV.2 of the phrase-pair extraction experiment, using the phrase-pairs extracted using the Moses toolkit. We added the inverse of the error distance used in the ranking experiments, as a feature into the log-linear model of the Moses decoder. The 5-gram language models were built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996). For word and phrase alignments, we used MGIZA++ and grow-diag-final-and heuristics. To tune the weights with respect to the BLEU score (Papineni et al., 2002), we used n-best batch MIRA (Cherry and Foster, 2012). The distortion limit parameter was set to 10. We evaluated each model on BLEU using the NIST’s mteval-v13a.pl 2 script. Statistical significance testing of the BLEU differences was performed using paired bootstrap resampling (Koehn, 2004). For both experiments, we tested with two models. The first was the proposed Bilingual Phrase Representation Model (BPRM). The second was the same BPRM model with the autoencoder layer removed. 5 Results and Analysis Tables 2 and 3 present the results of the phrase pair extraction task, and Table 4 presen"
2015.mtsummit-papers.4,C12-2104,0,0.0382569,"Missing"
2015.mtsummit-papers.4,D11-1014,0,0.0337723,"the last hidden layer hl is the summary representation c (c ≡ hl ). A shared semantic representation of both source and target is required, and therefore c and ¯ c are trained jointly using error signals based on the distance between them. The error is calculated using the semantic error defined in Equation (6). Equation (7) is the sum of the reconstruction error distance between each input rk in the source-side phrase and the autoencoder’s reconstruction ak . The autoencoder is used for learning representations of words (Chandar A P et al., 2014), phrases (Zhang et al., 2015), and sentences (Socher et al., 2011; Li et al., 2013). The target-side errors were calculated in the same manner. The objective function J is the sum of the total error distance from the source and target RNNs, and is represented by using Equations (5), (6), and (7) as: J = αEo (r|o; θ) + Ec (¯ c|c; θ) + βEa (r|a; θ) + λ∥θ∥ ¯ ¯ + βEa (¯ ¯ + λ∥θ∥ ¯ +αEo (¯ r|¯ o; θ) + Ec (c|¯ c; θ) r|¯ a; θ) ¯ = 2 · Ec (¯ c|c; θ, θ) ¯ +α(Eo (r|o; θ) + Eo (¯ r|¯ o; θ)) ¯ +β(Ea (r|a; θ) + Ea (¯ r|¯ a; θ)) ¯ +λ(∥θ∥ + ∥θ∥) Proceedings of MT Summit XV, vol.1: MT Researchers' Track (8) Miami, Oct 30 - Nov 3, 2015 |p. 47 Data IWSLT 2007 NTCIR-10 Traini"
2015.mtsummit-papers.4,P14-1011,0,0.0319151,"ce-side. The NNJM requires a maximum length for the source-side phrases. These approaches employ feed-forward neural networks and are constrained to operate on phrases of limited length. The use of recursive neural networks addresses the fixed-size issue by using a tree structure of phrases and sentences. The recursive neural network maps features from subsequences of a phrase to a continuous vector on each node of the tree recursively. Li et al. (2013) described an ITG reordering classifier which predicted phrase reorderings in SMT that was able to exploit syntactic and semantic information, Zhang et al. (2014, 2015) proposed bilingually-constrained recursive autoencoders, which generated phrasal embeddings for machine translation by learning to minimize the semantic distance between translation equivalents, and maximizing the distance between non-translation pairs. In contrast to the work on recursive networks, it is also possible to create continuous phrase representations with RNNs. Here, simpler models are possible that do not need take the tree structure of their input into account. Kalchbrenner and Blunsom (2013) proposed recurrent continuous translation models based on recurrent language mod"
2015.mtsummit-papers.4,D13-1141,0,0.0290209,"epresentations of word, phrase, and sentence which alleviate issues of sparsity have successfully been used in a number of natural language processing tasks. Language models with continuous word representations (Bengio et al., 2003; Mikolov et al., 2010; Mikolov, 2012) based on neural networks have outperformed the previous stateof-the-art approaches. These language models map each word to a dense, low-dimensional, real-valued vector, and estimate the probability of words in a continuous space. Representations for phrases have been used in the context of Statistical Machine Translation (SMT). Zou et al. (2013) used phrasal representations for computing the distance between phrase pairs and added a feature based on this distance into the log-linear model of a phrase-based SMT system (Koehn et al., 2003). Their method learned bilingual word representations, and subsequently obtained the phrase-level representations by simply averaging word vectors. Continuous representations for phrases or sentences with neural networks – such as a feed-forward, recursive, or recurrent neural networks – have also been used in SMT. A phrase representation model using a feed-forward neural network for phrase-based SMT"
2016.amta-researchers.7,D11-1033,0,0.501687,"ction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of mult"
2016.amta-researchers.7,W07-0702,0,0.0173464,"development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both likelihood and automati"
2016.amta-researchers.7,2011.iwslt-evaluation.18,0,0.549189,"ion is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this met"
2016.amta-researchers.7,N12-1047,0,0.183885,"should be computed from the probability distribution of the phrases, we treat it as a hyper-parameter. In other words, empty values are set experimentally to maximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The los"
2016.amta-researchers.7,P11-2031,0,0.0242386,"a-En translation. If we treat these values as probabilities, they are exp(−7) ≈ 0.0009 and exp(−6) ≈ 0.0025, respectively. Evaluation Metrics We used word BLEU, the translation edit rate (TER) (Snover et al., 2006), Meteor (English only) (Denkowski and Lavie, 2014), and the rank-based intuitive bilingual evaluation score (RIBES; Japanese only) (Isozaki et al., 2010) as the evaluation metrics. 5 http://research.nii.ac.jp/ntcir/index-ja.html 6 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S The MultEval tool (Clark et al., 2011)7 was used for statistical testing8 with the signiﬁcance level set to p &lt; 0.05. The mean scores of ﬁve runs were used to reduce instability in optimization. Although we used multiple metrics, for simplicity we will describe the results using BLEU. Comparison Methods We compared various methods using the single-domain model as the baseline. We used the following conventional methods, which have been described in Section 2. • Corpus Concatenation: A corpus-concatenated model was constructed using all domain data. Optimization and testing were performed using the development and test sets of each"
2016.amta-researchers.7,2012.amta-papers.4,0,0.359509,"translated them while changing the weight vectors of the linear and log-linear interpolations. Although they used perplexities as objective functions to estimate the weights, optimization algorithms, such as minimum error rate training (MERT) (Och, 2003), have been used recently to estimate weight vectors (Foster et al., 2010). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Feature augmentation (Daum´e, 2007) is a domain adaptation method used in machine learning that simultaneously optimizes the weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the feature vector h(e, f ). The ﬁrst is model adaptation, which modiﬁes trained submodels, and the second is corpus ﬁltering, which trains models using adapted corpora. The ﬁll-up method (Bisazza et al., 2011), translation model combination (Sennrich, 2012), and instance weighting (Foster et al., 2010; Matsoukas et al., 2009) are well-known model adaptation methods. The ﬁ"
2016.amta-researchers.7,P07-1033,0,0.247024,"Missing"
2016.amta-researchers.7,W14-3348,0,0.0410445,"Missing"
2016.amta-researchers.7,W08-0334,1,0.802845,"in Adaptation for Statistical Machine Translation Domain adaptation is applied when the target domain (in-domain) data are insufﬁcient but data from another domain (out-domain) are available in sufﬁcient quantities. Domain adaptation in machine translation aims to improve the translation quality of in-domain texts using both in-domain and out-domain data. There are two types of domains: those that are predeﬁned, such as “News” and “Web,” and those that are artiﬁcially created via automatic clustering. Even when using automatic clustering, the translation quality can be improved in some cases (Finch and Sumita, 2008; Sennrich et al., 2013). However, in this study, we have used predeﬁned domains. Corpus Concatenation The simplest approach to achieving domain adaptation for SMT is training the model using a concatenated corpus of in- and out-domain data. We refer to this method as corpus concatenation. The trained model is optimized using development (held-out) data of the in-domain. In machine learning, a model trained on a concatenated corpus has features that are intermediate between the in- and the out-domains. Therefore, model accuracy is also generally intermediate between models trained individually"
2016.amta-researchers.7,D10-1044,0,0.200207,"ear models. 1 Introduction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneou"
2016.amta-researchers.7,W07-0717,0,0.230586,"using standard log-linear models. 1 Introduction Machine translation is used for translating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two me"
2016.amta-researchers.7,P13-2121,0,0.0344768,"rom ASPEC-JE. Translation System Each source sentence was preordered using an in-house preordering system (Section 4.5 of Goto et al. (2015)) trained for general-purpose. The same preordering system was applied to all domains. In addition, all Japanese sentences, including the test sets, were segmented into words in advance using the MeCab morphological analyzer (Kudo et al., 2004). The phrase tables and lexicalized reordering models were trained using the default settings in the Moses toolkit. The 5-gram language models were learned from the target side of the training sentences using KenLM (Heaﬁeld et al., 2013). Multi-domain KBMIRA, described in Section 3.3.1, was used for optimization. A clone of the Moses decoder was used for decoding. The settings were the same as the default values in Moses, i.e., phrase table limit = 20, distortion limit = 6, and the beam width was 200. When the decoder selected phrase pair candidates, 1) phrase pairs were ﬁrst obtained from all phrase tables, 2) a likelihood of each phrase was computed in accordance with the augmented feature space, and 3) the highest 20 pairs were selected. Empty Value The empty value described in Section 3.2.1 was set empirically. From integ"
2016.amta-researchers.7,D11-1125,0,0.0266107,"ough an empty value is a type of unknown probability and should be computed from the probability distribution of the phrases, we treat it as a hyper-parameter. In other words, empty values are set experimentally to maximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimizatio"
2016.amta-researchers.7,Q13-1035,0,0.205305,"Missing"
2016.amta-researchers.7,D10-1092,0,0.0488247,"Missing"
2016.amta-researchers.7,W14-3627,0,0.0154271,"domain from the outdomain corpora on the basis of cross-entropy difference (i.e., modiﬁed Moore-Lewis ﬁltering). Then, they trained the models using the in-domain corpus with additional sentences. Corpus ﬁltering adapts not only phrase tables but also all submodels used in the translator. However, the ideal number of additional sentences cannot be estimated in advance. Another Approach Another approach that does not require changing the likelihoods is connecting two translators in series. A translation result generated by the out-domain translator is re-translated by the in-domain translator (Jeblee et al., 2014). This method treats the generation of domain-speciﬁc translation as error correction. 3 Multi-domain Adaptation 3.1 Feature Augmentation Feature augmentation is used to adapt feature weights to domains in machine learning. The feature space is segmented into the following subspaces: common, out-domain (source domain), and in-domain (target domain). In-domain features are copied to the in-domain and common spaces, and out-domain features are copied to the out-domain and common spaces. The adapted weight vector is obtained by optimizing the entire space. The in- and out-domain features deployed"
2016.amta-researchers.7,P07-2045,0,0.0137197,"corporating Corpus-Concatenated Model and SingleDomain Models where hc and hi denote the feature vectors of the common and the domain-speciﬁc spaces, respectively. All features are deployed to the common space, but only features that match the domain are copied to the domain space. hc hi = = Φ(f, e)  Φ(f, e) ∅ (3) if domain(f ) = i otherwise (4) where Φ(f, e) denotes the subvector that stores the model scores and so on. It is equal to h(f, e) if no feature augmentation is applied. We obtain the weight vector by optimizing this feature matrix. We use the default features of the Moses toolkit (Koehn et al., 2007) (15 dimensions) in the experiments reported in Section 4. The number of dimensions in the augmented feature space is 15 in the common space and 14 in each of the domain spaces1 . Clark et al. (2012) applied feature augmentation to machine translation from Arabic to English (with News and Web domains) and Czech to English (six domains, e.g., Fiction). Only a corpus-concatenated model was used to obtain features so that feature functions were not changed to reﬂect the different domains. 3.2 Core of Proposed Methods 3.2.1 Corpus-Concatenated Model and Single-domain Models In machine translation,"
2016.amta-researchers.7,N03-1017,0,0.0634641,"_S this space as that used in the standard log-linear model. This can be realized via a slight modiﬁcation of existing translation systems. Both methods use a corpus-concatenated model, which covers multiple domains and contains few unknown words, and single-domain models, which are accurate in their speciﬁc domains. In addition, we tune the hyper-parameter of the multiple-model combination. With appropriate settings, state-of-the-art domain adaptation can be realized even when using standard log-linear models. In this study, we use phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003, 2007) with preordering. The remainder of this paper is organized as follows. Section 2 brieﬂy reviews domain adaptation in machine translation. Section 3 explains our proposed methods in detail. Section 4 discusses the characteristics of our methods through experiments, and Section 5 concludes the paper. 2 Domain Adaptation for Statistical Machine Translation Domain adaptation is applied when the target domain (in-domain) data are insufﬁcient but data from another domain (out-domain) are available in sufﬁcient quantities. Domain adaptation in machine translation aims to improve the translati"
2016.amta-researchers.7,W07-0733,0,0.0412336,"ximize the BLEU score of a development corpus2 . 3.3 Optimization 3.3.1 Joint Optimization One merit of feature augmentation in machine learning is that conventional algorithms can be used for optimization because feature augmentation operates only in the feature space. Machine translation uses optimization algorithms such as MERT (Och, 2003), pairwise ranking optimization (PRO) (Hopkins and May, 2011), and k-best batch MIRA (KBMIRA) (Cherry and Foster, 2012). We employ KBMIRA in this paper because it is appropriate for high-dimensional optimization3 . 2 Moses assigns -100 as the empty value (Koehn and Schroeder, 2007; Birch et al., 2007). As we describe in Section 4.2, this is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both li"
2016.amta-researchers.7,W04-3230,0,0.0271748,"were provided by the international conference NTCIR-8, and the test set was provided by NTCIR-95 . • ASPEC: An Asian scientiﬁc paper excerpt corpus (Nakazawa et al., 2016)6 . We used a million sentences of high-conﬁdence translation from ASPEC-JE. Translation System Each source sentence was preordered using an in-house preordering system (Section 4.5 of Goto et al. (2015)) trained for general-purpose. The same preordering system was applied to all domains. In addition, all Japanese sentences, including the test sets, were segmented into words in advance using the MeCab morphological analyzer (Kudo et al., 2004). The phrase tables and lexicalized reordering models were trained using the default settings in the Moses toolkit. The 5-gram language models were learned from the target side of the training sentences using KenLM (Heaﬁeld et al., 2013). Multi-domain KBMIRA, described in Section 3.3.1, was used for optimization. A clone of the Moses decoder was used for decoding. The settings were the same as the default values in Moses, i.e., phrase table limit = 20, distortion limit = 6, and the beam width was 200. When the decoder selected phrase pair candidates, 1) phrase pairs were ﬁrst obtained from all"
2016.amta-researchers.7,D09-1074,0,0.0775839,"e weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the feature vector h(e, f ). The ﬁrst is model adaptation, which modiﬁes trained submodels, and the second is corpus ﬁltering, which trains models using adapted corpora. The ﬁll-up method (Bisazza et al., 2011), translation model combination (Sennrich, 2012), and instance weighting (Foster et al., 2010; Matsoukas et al., 2009) are well-known model adaptation methods. The ﬁll-up method changes feature values. If a phrase is contained in an in-domain phrase table, the feature values in that table are used. Otherwise, the feature values in the out-domain phrase table are used. Translation model combination generates a new phrase table by combining two translation probabilities of in- and out- domains. The weights of the combination are determined using each feature function to minimize the perplexity on a development set. Instance weighting modiﬁes each model parameter in the phrase table to discriminate between the i"
2016.amta-researchers.7,P03-1021,0,0.104288,"els. The overall likelihood is computed by the following equation: log P (e|f ) ∝ w · h(e, f ) (1) where h(e, f ) is a feature vector and w is a weight vector of the feature functions. Then, a domain-speciﬁc translation is generated by changing the weight vector w of each domain. For example, Foster and Kuhn (2007) trained single-domain PBSMT models and translated them while changing the weight vectors of the linear and log-linear interpolations. Although they used perplexities as objective functions to estimate the weights, optimization algorithms, such as minimum error rate training (MERT) (Och, 2003), have been used recently to estimate weight vectors (Foster et al., 2010). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Feature augmentation (Daum´e, 2007) is a domain adaptation method used in machine learning that simultaneously optimizes the weight vector of each domain (cf., Section 3.1). Clark et al. (2012) applied it to machine translation as a type of log-linear interpolation; however, they only adapted the weight vectors of a model. Model Adaptation There are basically two approaches to achieve domain adaptation by changing the featur"
2016.amta-researchers.7,P02-1040,0,0.0965662,"is is extremely small and produces low BLEU scores. 3 Another reason is that the BLEU score of a baseline system was the highest in our preliminary experiments. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S A major difference between general machine learning and optimization in machine translation is in the loss functions. The loss functions of machine learning algorithms use likelihood output by decoders. In contrast, the optimization algorithms employed in machine translation use both likelihood and automatic evaluation scores, such as BLEU (Papineni et al., 2002). Automatic evaluation scores are computed by comparing system outputs with their reference translations over the entire document. In fact, MERT and KBMIRA contain BLEU scores of the development set in their loss functions4 . This means that BLEU scores must be computed for each domain to optimize multiple domains. To solve this problem, we modify the KBMIRA algorithm. The modiﬁcations to Algorithm 1 proposed by Cherry and Foster (2012) are as follows. 1. The variable BG that maintains BLEU statistics (such as the number of n-gram matches) is extended to the D-dimensional array, where D denote"
2016.amta-researchers.7,E12-1055,0,0.194504,"ating a variety of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this method uses an opti"
2016.amta-researchers.7,P13-1082,0,0.272476,"of text types, including speech. However, it remains challenging to appropriately translate texts across all domains and only a limited number of domains have been targeted. The most promising approach to improve translation quality is to train the translator on massive bilingual corpora. However, collecting such corpora is challenging and expensive in several domains. Domain adaptation, which improves target domain quality by using data from another domain, has been proposed as a solution (Foster and Kuhn, 2007; Foster et al., 2010; Axelrod et al., 2011; Bisazza et al., 2011; Sennrich, 2012; Sennrich et al., 2013). This technique is important when applying machine translation to practical tasks. This paper presents methods of domain adaptation for statistical machine translation (SMT) that assume multiple domains. The proposed methods combine multiple models using log-linear interpolation. These are simple yet effective approaches to take advantage of multiple domains based on feature augmentation (Daum´e, 2007), a domain adaptation technique used in machine learning. We propose the following two methods. 1. Simultaneous optimization of multiple domains: this method uses an optimizer extended to multip"
2016.amta-researchers.7,2006.amta-papers.25,0,0.039511,"Missing"
2016.amta-researchers.7,D13-1112,0,0.0512636,"Missing"
2020.acl-main.324,N19-1388,0,0.065454,"e attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of divers"
2020.acl-main.324,D18-1549,0,0.307174,"UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language"
2020.acl-main.324,W19-5301,0,0.0735941,"Missing"
2020.acl-main.324,C18-1263,0,0.0276317,"e pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et a"
2020.acl-main.324,P15-1166,0,0.0246013,"7.58 25.05 14.09 9.75 25.84 10.90 23.80 10.07 13.09 28.82 12.41 15.79 19.57 27.59 16.62 11.05 28.56 12.77 25.25 10.92 14.33 32.38 14.78 15.47 19.28 26.79 15.62 10.57 27.78 12.03 25.52 11.11 14.33 31.28 13.83 15.93 20.00 27.80 17.21 11.58 28.62 13.12 25.98 11.22 15.17 32.43 15.30 Average 15.61 17.15 19.13 18.63 19.53 Table 5: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the English to non-English language pairs. The other columns show results from Table 2. 7 Related Work Multilingual NMT has attracted much attention in the machine translation community. Dong et al. (2015) first extended NMT from the translation of a single language pair to multiple language pairs, using a shared encoder and multiple decoders and 3532 Corpus SM MUNMT +FT LBKD +FT Cs-En De-En Es-En Et-En Fi-En Fr-En Hu-En It-En Lt-En Lv-En Ro-En Tr-En 20.62 21.31 25.53 19.48 7.62 25.86 14.48 24.33 1.72 0.95 28.52 12.99 20.09 21.95 25.37 19.60 7.19 25.41 14.54 24.77 14.04 14.90 28.38 15.65 21.50 22.41 26.24 21.61 8.06 26.30 15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19."
2020.acl-main.324,N16-1101,0,0.0218044,"15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19.48 Average 16.95 19.32 20.55 20.47 21.19 Table 6: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) an"
2020.acl-main.324,R19-1050,0,0.0409383,"Missing"
2020.acl-main.324,N16-1162,0,0.0752553,"Missing"
2020.acl-main.324,Q17-1024,0,0.0420139,"Missing"
2020.acl-main.324,P07-2045,0,0.00631129,"n of the MUNMT and LBUNMT models, respectively, after encoding M j (Xi1 ) generated by the previous MUNMT model in the L1 → Lj direction. X j (M 1 (Xij )) and LB j (M 1 (Xij )) denote the softened Lj sentence probability distribution of the MUNMT and LBUNMT models, respectively, after encoding M 1 (Xij ) generated by the previous MUNMT model in the Lj → L1 direction. 5 5.1 Experiments Datasets To establish an MUNMT system, we considered 13 languages from WMT monolingual news crawl datasets: Cs, De, En, Es, Et, Fi, Fr, Hu, It, Lt, Lv, Ro, and Tr. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 words. We then used a shared vocabulary for all languages, with 80,000 sub-word tokens based on BPE (Sennrich et al., 2016b). The statistics of the data are presented in Table 1. For Cs,De,En, we randomly extracted 50M monolingual news crawl data after cleaning; For other languages, we used all news crawl data after cleaning as shown in Table 1. Language Cs De En Es Et Fi Fr Hu It Lt Lv Ro Tr Sentences Words Sub-words 50.00M 50.00M 50.00M 36.33M 3.00M 15.31M 50.0"
2020.acl-main.324,C18-1054,0,0.023553,"ing of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem o"
2020.acl-main.324,W18-6309,0,0.0284044,"ns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has be"
2020.acl-main.324,W19-5330,1,0.703395,") concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pretraining in the initialization step, to achieve remarkable results with some similar language pairs. Lample and Conneau (2019) achieved better UNMT performance by introducing a pretrained language model. Sun et al. (2019, 2020) proposed to train UNMT with cross-lingual language representation agreement, to further improve UNMT performance. Moreover, an unsupervised translation task that evaluated in the WMT19 news translation task (Barrault et al., 2019) attracted many researchers to participate (Marie et al., 2019; Li et al., 2019). For Multilingual UNMT, Xu et al. (2019) exploited multiple auxiliary languages for jointly boosting UNMT models via the Polygon-Net framework. Sen et al. (2019) proposed an MUNMT scheme that jointly trains multiple languages with a shared encoder and multiple decoders. In contrast with their use of multiple decoders, we have constructed a simpler MUNMT model with one encoder and one decoder. Further, we have extended the four or five languages used in their work to thirteen languages, for training our MUNMT model. 8 Conclusion and Future Work In this paper, we have introduc"
2020.acl-main.324,D18-1039,0,0.0403498,"LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs tra"
2020.acl-main.324,P18-1006,0,0.0664673,"(2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bilingual word embeddings, denoising autoencoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. Lample et al. (2018b) concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pr"
2020.acl-main.324,P19-1117,0,0.0411944,"rom Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using"
2020.acl-main.324,W18-6327,0,0.027368,"nglish to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe e"
2020.acl-main.324,P19-1297,0,0.455981,"mple and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a few pioneer studies. For example, Xu et al. (2019) and Sen et al. (2019) proposed a multilingual scheme that jointly trains multiple languages with multiple decoders. However, the performance of their MUNMT is much worse than our re-implemented individual baselines (shown in Tables 2 and 3) and the scale of their study is modest (i.e., 4-5 languages). In this paper, we empirically introduce an unified framework to translate among thirteen languages (including three language families and six language branches) using a single encoder and single decoder, making use of multilingual data to improve UNMT for all languages. On the basis of these empirical findings, we pr"
2020.acl-main.324,P18-1005,0,0.0216021,"bulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and revertin"
2020.acl-main.324,P16-1009,0,0.571282,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P16-1162,0,0.832951,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P19-1119,1,0.531451,"to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and reverting it with this encoder. This pretrained m"
2020.acl-main.324,D19-1089,0,0.0734585,"Missing"
2020.acl-main.324,P19-1583,0,0.0192416,"Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bil"
2020.acl-main.324,W19-5325,0,\N,Missing
2020.acl-main.34,P05-1066,0,0.156159,"Missing"
2020.acl-main.34,P17-1012,0,0.0231835,"ule (ATT), and the second sub-layer is a position-wise fully connected feed-forward network (FNN). A residual connection (He et al., 2016) is applied between the sub-layers, and layer normalization (LN) (Ba et al., 2016) is performed. Formally, the l-th identical layer of this stack is as follows: l l−1 l−1 l−1 H = LN(ATTle (Ql−1 ) e , Ke , Ve ) + H l l Hl = LN(FFNle (H ) + H ). (1) l−1 l−1 {Ql−1 e , Ke , Ve } are query, key, and value vectors that are transformed from the (l-1)-th layer Hl−1 . For example, {Q0 , K 0 , V 0 } are packed from the H0 learned by the positional encoding mechanism (Gehring et al., 2017). Similarly, the decoder is composed of a stack of L identical layers. Compared with the stacked encoder, it contains an additional attention sublayer to compute alignment weights for the output of the encoder stack HL : l l−1 l−1 l−1 Si = LN(ATTld (Ql−1 i , Ki , Vi ) + Si ), l l L Cli = LN(ATTlc (Si , KL e , Ve ) + Si ), (2) Sli = LN(FFNld (Cli ) + Cli ), l−1 l−1 where Ql−1 are query, key, and d , Kd , and Vd value vectors, respectively, that are transformed from the (l-1)-th layer Sl−1 in time-step i. L {KL e , Ve } are transformed from the L-th layer of the encoder. The top layer of the dec"
2020.acl-main.34,N19-1122,0,0.0409928,"Missing"
2020.acl-main.34,D19-1088,0,0.0341217,"Missing"
2020.acl-main.34,N19-4009,0,0.0250087,"Missing"
2020.acl-main.34,P16-1162,0,0.0697775,"to further improve translation performance. 5 5.1 Experiments Setup The proposed methods were evaluated on the WMT14 English-to-German (EN-DE), WMT14 English-to-French (EN-FR), and WMT17 Chineseto-English (ZH-EN) tasks. The EN-DE corpus consists of 4M sentence pairs, the ZH-EN corpus of 22M sentence pairs, and the EN-FR corpus of 36M sentence pairs. We used the case-sensitive 4gram BLEU score as evaluation metric. The results of the newstest2014 test sets are reported for the EN-DE and EN-FR tasks, and the newstest2017 test set is reported for the ZH-EN task. The byte pair encoding algorithm (Sennrich et al., 2016) was applied to encode all sentences to limit the size of the vocabulary to 40K. The other configurations were identical to those in (Vaswani et al., 2017). The poposed models were implemented by using 360 EN-DE BLEU #Speed #Param Existing NMT systems Trans.base (Vaswani et al., 2017) 27.3 N/A 65.0M +Context-Aware SANs (Yang et al., 2019a) 28.26 N/A 106.9M +Convolutional SANs (Yang et al., 2019b) 28.18 N/A 88.0M +BIARN (Hao et al., 2019) 28.21 N/A 97.4M Trans.big (Vaswani et al., 2017) 28.4 N/A 213.0M +Context-Aware SANs (Yang et al., 2019a) 28.89 N/A 339.6M +Convolutional SANs (Yang et al., 2"
2020.acl-main.34,P07-1090,0,0.0422181,"tent or function words with UNK in a source sentence. Figure 1 shows that the BLEU scores of the test set decreased much ∗ Corresponding author Transformer (base) -Function Words -Content Words 20 more substantially when parts of content words were randomly replaced with UNK on the WMT14 English-to-German task, which is in line with the findings in He et al. (2019)’s work. To address this limitation, we propose a content word-aware NMT model that exploits the results of translation using a sequence of content words learned by a simple content word recognition method. Inspired by the works of (Setiawan et al., 2007, 2009; Zhang and Zhao, 2013), we first divide words in a sentence into content words and other function words depending on term frequencyinverse document frequency (TF-IDF) constraints. Two methods are designed to utilize the sequence of content word on the source and target sides: 1) We encode the content words of the source sentence as a new source representation, and learn an additional content word context vector based on it to improve translation performance; 2) A specific loss for content words of the target sentence is introduced to compensate for the original training objection, to ob"
2020.acl-main.34,P09-1037,0,0.0880705,"Missing"
2020.acl-main.34,N19-1407,0,0.0385394,"Missing"
2020.acl-main.44,2005.mtsummit-papers.11,0,0.14949,". Therefore, an extension of the original Zipf’s/power law requires at least two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English words from Europarl (Koehn, 2005) 1 and Chinese characters of Academia Sinica from the data of Sproat and Emerson (2003).2 2 Proposed and Related Formulation Under a logarithmic form, the Zipf’s law states that x + y = C, where (x, y) = (log r, log f ), and C is roughly a constant. We further investigate the 1 http://www.statmt.org/europarl/v8/ europarl.tgz 2 http://sighan.cs.uchicago.edu/ bakeoff2005/data/icwb2-data.zip 460 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 460–464 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 0 1 2 3 4 5 English Word Chinese"
2020.acl-main.44,P07-2045,0,0.00616617,"bg el fr β + γnorm bg cs da de el en es et fi fr hu it lt lv nl pl pt ro sk sl sv β γnorm α Experiment and Discussion We used the proposed formulation to fit data of various European languages and typical Asian languages. The Europarl corpus (Koehn, 2005) and data from the Second International Chinese Word Segmentation Bakeoff (ICWB2) (Sproat and Emerson, 2003) were mentioned in Section 1. We also used English-Japanese patent data from the 7th NTCIR Workshop (Fujii et al., 2008). The Europarl data and English data from NTCIR were lower-cased and tokenized using the toolkit provided by MOSES5 (Koehn et al., 2007). Fitting was performed under a logarithmic scale using the fit function6 in gnuplot.7 Specifically, relation-frequency data were used to fit (α, β, γ) and C in y = C −αx−β log10 (10x +10γ ). For the initialization, (α, β, γ) = (1, 1, rmax 2 ) and C = 3γ were applied. Table 1 lists the fitting results for all the languages8 in the Europarl corpus. The (α, β, γ) with Greek (el), English (en), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl), and"
2020.acl-main.44,W98-1218,0,0.083013,". The simple proportionality of the Zipf’s/power law can be observed on randomly generated textual data (Li, 1992) and it only roughly depicts the r-f relation in real textual data. A two-parameter generalization of the Zipf’s/power law is the Zipf-Mandelbrot law, where f ∝ (r + β)−α (Mandelbrot, 1965). Li et al. (2010) considered the reversed rank of rmax +1−r, where rmax is the maximum of ranking index, and proposed a two-parameter formulation of f ∝ r−α (rmax + 1 − r)β . As a straightforward observation, the coefficients of proportionality should be distinguished for common and rear words (Powers, 1998; Li et al., 2010). Therefore, an extension of the original Zipf’s/power law requires at least two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English wor"
2020.acl-main.44,W03-1719,0,0.0340993,"st two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English words from Europarl (Koehn, 2005) 1 and Chinese characters of Academia Sinica from the data of Sproat and Emerson (2003).2 2 Proposed and Related Formulation Under a logarithmic form, the Zipf’s law states that x + y = C, where (x, y) = (log r, log f ), and C is roughly a constant. We further investigate the 1 http://www.statmt.org/europarl/v8/ europarl.tgz 2 http://sighan.cs.uchicago.edu/ bakeoff2005/data/icwb2-data.zip 460 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 460–464 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 0 1 2 3 4 5 English Word Chinese Character Artificial gi00 = 0 gi0 − gi−1 . xi − xi−1 (1) exp(bc − d) ∝ r−α (r + γ)−β ,"
2020.acl-srw.37,D11-1033,0,0.301577,"a selection, pre-training and fine-tuning. 3.1 Data Pre-processing Blindly pre-training a NMT model on vast amounts of monolingual data belonging to the assisting languages and LOI might improve translation quality slightly. However, divergences between the languages, especially their scripts (Hermjakob et al., 2018) and also the distributions of data between different training phases is known to impact the final result. Motivated by past works on using related languages (Dabre et al., 2017), orthography mapping/unification (Hermjakob et al., 2018; Chu et al., 2012) and data selection for MT (Axelrod et al., 2011), we propose to improve the efficacy of pre-training by reducing data and language divergence. 3.1.1 Script Mapping Previous research has shown that enforcing shared orthography (Sennrich et al., 2016b; Dabre et al., 2015) has a strong positive impact on translation. Following this, we propose to leverage existing script mapping rules1 or script unification mechanisms to, at the very least, maximize the possibility of cognate sharing and thereby bringing the assisting language closer to the LOI. This should strongly impact languages such as Hindi, Punjabi and Bengali belonging to the same fami"
2020.acl-srw.37,chu-etal-2012-chinese,1,0.614354,"ASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a v"
2020.acl-srw.37,Y15-1033,1,0.921893,"languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Metho"
2020.acl-srw.37,Y17-1038,1,0.933618,"velopment of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target langu"
2020.acl-srw.37,W11-2123,0,0.0396903,"g a denoised auto-encoder model. The NMT model is pre-trained with the MASS task, until convergence, jointly for both the source and target languages. Thereafter training is resumed on the parallel corpus, a step known as fine-tuning (Zoph et al., 2016). 4 Experimental Settings We conducted experiments on Japanese–English (Ja–En) translation in a variety of simulated lowresource settings using the “similar” assisting lan281 guage pairs Chinese (Zh) and French (Fr) and the “distant” assisting language pairs Russian (Ru) and Arabic (Ar). 4.1 as linguistically). 3. Data selection: We used KenLM (Heafield, 2011) to train 5-gram LMs on in-domain data for LM scoring based data selection and use ASPEC dev set for length distribution based data selection. Datasets We used the official ASPEC Ja–En parallel corpus (Nakazawa et al., 2016) provided by WAT 20192 . The official split consists of 3M, 1790 and 1872 train, dev and test sentences respectively. We sampled parallel corpora from the top 1M sentences for fine-tuning. Out of the remaining 2M sentences, we used the En side of the first 1M and the Ja side of the next 1M sentences as monolingual data for language modeling for data selection. We used Commo"
2020.acl-srw.37,P18-4003,0,0.115289,"er (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Method: Using Assisting Languages We propose a novel monolingual pre-training meth"
2020.acl-srw.37,D18-2012,0,0.0403888,"t languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-training. When mixing datasets of different sizes, we always oversampled the smaller datasets to match the size of the largest. For all pre-training models, we saved checkpoints every 1000 steps and for all fine-tuning models, we saved checkpoints every 200 steps. We used earl"
2020.acl-srw.37,N18-2084,0,0.0301046,"ever, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S2S specific pre-training task for NMT and obtained new state-of-the-art results in low-resource settings. MASS assumes that a large amount of monolingual data is available for the languages involved but some language pairs may lack both parallel and monolingual corpora and are “truly low-resource” and challenging. Fortu"
2020.acl-srw.37,P16-1009,0,0.233853,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,P16-1162,0,0.411738,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,D18-2010,1,0.834981,"selection. We used Common Crawl3 monolingual corpora for pre-training. To train LMs for data-selection of the assisting languages corpora, we used news commentary datasets 4 . While this data selection step for the assisting languages won’t minimize the domain difference from the parallel corpus, it can help in filtering noisy sentences. In this paper we consider the ASPEC and news commentary data as in-domain and the rest of the pre-training data as out-of-domain. 4.2 Data Pre-processing 1. Normalization and Initial Filtering: We applied NFKC normalization to data of all languages. Juman++ (Tolmachev et al., 2018) for Ja tokenization, jieba5 for Zh tokenization and NLTK6 tokenization for other languages. We filtered out all sentences from the pre-training data that contain fewer than 3 and equal or more than 80 tokens. For Chinese data, we filtered out sentences containing fewer than 30 percent Chinese words or more than 30 percent English words. 2. Script Mapping: Chinese is the only assisting language that can be mapped to Japanese reliably. We converted Chinese to Japanese script to make them more similar by using the mapping table from (Chu et al., 2012) and the mapping approaches mentioned in the"
2020.acl-srw.37,W18-1819,0,0.0277583,"om (Chu et al., 2012) and the mapping approaches mentioned in the previous section. French and English are written using the Roman alphabet and do not need any script mapping. We did not perform script mapping for Arabic and Russian to show the impact of using distant languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-train"
2020.acl-srw.37,P19-1583,0,0.0136871,"ine ∈ TargetFile do 6 TargetD[len(Line)]+ = 1 ; 9 10 ically the monolingual corpus). When selecting monolingual data of languages of interest, we can first calculate the length distribution of parallel data as target distribution (the ratio of all lengths in T argetF ile) and we fill the length distribution by selecting sentences from monolingual data of same language. As a result, the monolingual data and parallel data have similar length distribution. Data Selection Often, the pre-training monolingual data and the fine-tuning parallel data belong to different domains. (Axelrod et al., 2011; Wang and Neubig, 2019) have shown that proper data selection can reduce the differences between the natures of data between different training domains and phases. In this paper we experiment with (a) Scoring monolingual sentences using a language model (LM) and selecting the highest scoring ones and (b) Selecting monolingual sentences to match the sentence length distribution of the development set sentences in the parallel corpus. 1. LM based data selection: We use a language model trained on corpora belonging to the domain that the fine-tuning data belongs to. We use this sort monolingual sentences according to L"
2020.acl-srw.37,D16-1163,0,0.199846,"resource language translation. Pre-training has enjoyed great success in other NLP tasks with the development of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Targ"
2020.acl-srw.37,D19-1632,0,\N,Missing
2020.acl-srw.37,D19-1146,1,\N,Missing
2020.coling-main.374,D18-1549,0,0.0786934,"e the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists"
2020.coling-main.374,P18-1163,0,0.0194499,"put sentences, for example, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word nois"
2020.coling-main.374,P19-1425,0,0.025815,"xample, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noi"
2020.coling-main.374,P18-2006,0,0.0213336,"ask of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced the translation performance. Belinkov and Bisk (2018), Ebrahimi et al. (2018), and Karpukhin et al. (2019) designed character-level noise, which affects the spelling of a single word, to improve the model robustness. Meanwhile, both textual and phonetic embeddings were used to improve the robustness of SNMT to homophone noises (Liu et al., 2019). Adversarial examples, generated by gradient-based method, attacked the translation model to improve the robustness of SNMT (Cheng et al., 2019). In contrast with this work, we applied adversarial perturbation to the denoising training of UNMT , instead of translation training, to enhance the learning ability of UNMT model. 424"
2020.coling-main.374,D17-1215,0,0.08645,"Missing"
2020.coling-main.374,D19-5506,0,0.0949022,"er misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noise. Then we empirically ∗"
2020.coling-main.374,P07-2045,0,0.00911683,"nal embedding (Position AT), and the combination of word and positional adversarial training (Both AT), all of which enrich robust information via adversarial perturbation. 5 Experiments 5.1 Datasets We considered two language pairs to do simulated experiments on the Fr↔En and German(De)↔En translation tasks. We used 50 million sentences from WMT monolingual news crawl datasets for each language. To make our experiments comparable with previous work (Conneau and Lample, 2019), we reported results on newstest2014 for Fr↔En and newstest2016 for De↔En. For preprocessing, we used Moses tokenizer (Koehn et al., 2007)1 for all languages. For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 tokens. For BPE (Sennrich et al., 2016b), we used a shared vocabulary for every language pair with 60K subword tokens based on BPE. 5.2 UNMT Settings We used a transformer-based XLM toolkit2 and followed settings of Conneau and Lample (2019) for UNMT: 6 layers for the encoder and the decoder. The dimension of hidden layers was set to 1024. The Adam optimizer (Kingma and Ba, 2015) was used to optimize the model parameters. The initial learning r"
2020.coling-main.374,P19-1291,0,0.0385075,"Missing"
2020.coling-main.374,W19-5330,1,0.743883,"noisy input. 6 Related Work Recently, UNMT (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019) that relies solely on monolingual corpora in each language via bilingual word embedding initialization, denoising auto-encoder, back-translation and sharing latent representations. More recently, Conneau and Lample (2019) and Song et al. (2019) introduced the pretrained cross-lingual language model to achieve state-of-the-art UNMT performance. Sun et al. (2020a) extended UNMT to the multilingual UNMT training on a large scale of European languages. Marie et al. (2019) won the first place in the unsupervised translation task of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced t"
2020.coling-main.374,D18-1050,0,0.0202074,"ut with different level of noise was 22.76 BLEU scores more in average for the word noise scenario, and 24.09 BLEU scores more in average for the word order noise scenario, compared with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tunin"
2020.coling-main.374,W18-6319,0,0.0183283,"e impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previous work(Michel and Neubig, 2018; Zhou et al., 2019) by approximately 10 BLEU scores. The performance of our proposed sys"
2020.coling-main.374,P16-1009,0,0.166098,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P16-1162,0,0.368756,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P19-1119,1,0.906137,"s of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character missp"
2020.coling-main.374,2020.acl-main.324,1,0.900456,"ining sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character misspelling, replacemen"
2020.coling-main.374,P18-1005,0,0.100191,"First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sent"
2020.coling-main.374,W19-5368,0,0.0186829,"with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previo"
2020.coling-main.374,W18-6401,0,\N,Missing
2020.coling-main.374,N19-1120,0,\N,Missing
2020.coling-main.374,W19-5301,0,\N,Missing
2020.coling-main.376,D17-1209,0,0.0287748,"Missing"
2020.coling-main.376,2020.acl-main.147,0,0.0221429,"ng strategies using RNN namely parallel, hierarchical and mixed, are tried to integrate the dependency information. Wu et al. (2018) used dependency information of both the source and the target languages. As their model needs multiple encoders and decoders, so it is not worthy for use under low-resource condition. In the work of Zhang et al. (2019), the authors employed a supervised encoder-decoder dependency parser and used the outputs from the encoder as a syntax-aware representations of words, which in turn, are concatenated to the input embeddings of the translation model. Most recently, Bugliarello and Okazaki (2020) proposed dependency-aware self-attention in the Transformer that needs no extra parameter. For a pivot word, its self-attention scores with other words are weighted considering their distances from the dependency parent of the pivot word. Apart from the above works, there are some studies which use the factors in the target side (Burlot et al., 2017; Garc´ıa-Mart´ınez et al., 2016a; Garc´ıa-Mart´ınez et al., 2016b). In general, their approach is to predict the roots and other morphological tags of the target words instead of producing the surface forms. Additionally, a morphological analyzer"
2020.coling-main.376,W17-4703,0,0.042239,"Missing"
2020.coling-main.376,P17-1177,0,0.0183396,"anguage Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and fou"
2020.coling-main.376,P16-1078,0,0.020133,"s having diverse morphological variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a"
2020.coling-main.376,J82-2005,0,0.664999,"Missing"
2020.coling-main.376,D17-1012,0,0.0119381,"iza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers r"
2020.coling-main.376,U16-1001,0,0.0175246,"ords due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN architecture to model so"
2020.coling-main.376,P17-4012,0,0.0292268,"p 15 512 15 15 15 Table 2: Embedding dimensions of the components. Baseline models Proposed models Base Concat Add Linear Self-rel Word-rel en-bg 4.97 5.56 4.66 4.89 6.10 6.25 en-fi 25.59 23.75 22.02 24.26 26.26 26.01 en-hi 18.54 20.69 15.45 20.65 21.27 21.63 en-id 27.93 27.99 24.78 27.17 30.41 26.53 en-khm 22.88 23.53 21.65 23.42 24.76 25.13 en-ms 32.40 32.92 30.45 32.64 34.71 33.20 en-my 13.93 14.92 11.86 13.79 16.53 15.62 en-vi 24.99 26.50 22.78 25.36 27.74 27.66 Table 3: BLEU scores of the models for all reference language pairs. Hyperparameters: We use the OpenNMT PyTorch implementation (Klein et al., 2017) to build our models and mostly follow the Transformer-base hyperparameter setting mentioned there4 . There are 6 layers in each of the encoder and the decoder stacks. The number of multi-heads used is 8. The dimension of the fully-connected-feed-forward network is 2, 048. Total number of training steps is set to 200, 000 and after each 10, 000 steps validation checking is performed. We use the early-stopping strategy in training. If the validation accuracy does not improve for 5 consecutive validation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dime"
2020.coling-main.376,P17-1064,0,0.0608027,"sis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent differen"
2020.coling-main.376,D15-1166,0,0.23826,") word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017; Kitaev et al., 2020) is known to give state-of-the-art translation quality for language pairs having abundance of parallel corpora. In case of resource poor scenarios, additional translation knowledge is acquired either through transfer learning in the form of pre-trained model parameters or by supplying external monolingual corpora. However, exploiting linguistic information effectively in low-resource conditions is still an under-researched field. Annotating the source side with various syntactic features e.g. part-of-speech (POS), lemma, dependency labels etc. can he"
2020.coling-main.376,P14-5010,0,0.00251552,"ymbols. Linguistic Features Used: We use three linguistic features of the source language in our experiments. They are - (i) lemma, (ii) POS tag and (iii) dependency label. For morphologically rich languages where roots have multiple variants, there tagging the raw text with these three features helps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the sourc"
2020.coling-main.376,P02-1040,0,0.110315,"ation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dimension of the final embedding which is fed to the Transformer, comparable across the models without and with using features so that the number of model parameters does not influence the performance. In Table 2 we list the embedding dimensions of the subword and its features for all experimental settings. Inference is done keeping beam size equal to 5. We carry out our experiments using single GPU with the specification of 32 GB Tesla V100-SXM2. 4.1 Results BLEU Scores: We present the BLEU scores5 (Papineni et al., 2002) of our proposed methods and the baselines in Table 3. The scores are computed after undoing the BPE segmentation of the translations. For en-fi, en-id, en-ms, en-my and en-vi, the self relevance checking strategy yields the best results (26.26, 30.41, 34.71, 16.53 and 27.74 respectively). For en-bg, en-hi and en-khm, the wordbased relevance method outperforms others (6.25, 21.63 and 25.13 respectively). Compared to the base configuration, maximum improvement is obtained for en-hi (18.54 → 21.63) and minimum for en-fi (25.59 → 26.26). Compared to (Sennrich and Haddow, 2016) i.e. the concat com"
2020.coling-main.376,W16-2209,0,0.275768,"shared by different root words due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN arch"
2020.coling-main.376,P16-1162,0,0.0584049,"elps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the source side in addition to the three linguistic features stated above. Every subword is annotated with one of the four markers - B (beginning), I (inside), E (end), S (single). The annotation is done with the help of the script provided in the corresponding url3 . Table 1 depicts the structure of a samp"
2020.coling-main.376,D16-1159,0,0.0269826,"logical variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of trai"
2020.coling-main.376,W18-6459,0,0.0831158,"y validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax"
2020.coling-main.376,N19-1118,0,0.0608803,"ng the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax. This work gives the"
2020.coling-main.378,P18-2049,0,0.0215536,"stically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performa"
2020.coling-main.378,W18-1810,0,0.0176152,"stically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performa"
2020.coling-main.378,D18-1461,0,0.0739323,"re, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT training process and does not need a large number of epochs. BPE-dropout (Provilkov et al., 2020) is a method that extends BPE to use subword regularization. In this method, multiple subword candidates are obtained by probabilistically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Atama"
2020.coling-main.378,W17-4706,0,0.0615828,"rs. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performance for morphologically-ri"
2020.coling-main.378,W04-3250,0,0.578892,"Missing"
2020.coling-main.378,D18-2012,0,0.0472157,"ntil they exceed the given vocabulary size. BPE is widely used in many NMT systems; however, since BPE is a greedy and deterministic algorithm, obtaining multiple subword candidates is not possible. The unigram language model is a likelihood-based subword segmentation algorithm. Each subword occurrence probability is estimated by the EM algorithm. The unigram language model has a more complicated algorithm than BPE, but it has the advantages that it can obtain multiple subword candidates based on likelihood and that it can be learned from raw sentences without pre-tokenization. SentencePiece (Kudo and Richardson, 2018) is an implementation of the unigram language model we used. Subword regularization (Kudo, 2018) is an NMT training method that uses multiple subword candidates obtained by the unigram language model and maximizes the marginal likelihood of sampled multiple subword candidates. This method requires on-the-ﬂy subword sampling in training; therefore, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT traini"
2020.coling-main.378,P18-1007,0,0.166217,"er Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU). 1 Introduction Subword units have recently been widely used in neural machine translation (NMT) to solve open vocabulary problems. Byte Pair Encoding (BPE) (Sennrich et al., 2016) is a dominant subword segmentation method for NMT, but it is designed for segmented languages in which words are divided by spaces. Kudo (2018) has proposed a subword segmentation method based on a unigram language model, that can be applied to non-segmented languages such as Chinese and Japanese. Both BPE and the unigram language model tokenize sentences by minimizing the number of segments under a limitation on subword vocabulary size, which relies on a data compression principle. In these existing segmentations, a sentence is segmented without considering its translation, and therefore the segmented sentence might not be optimal for NMT. This paper proposes a new subword segmentation method for NMT, “Bilingual Subword Segmentation"
2020.coling-main.378,P02-1040,0,0.108274,"as set to 0.1, and the batch size was set to 256 sentences. In subword regularization, we used 1-best decoding, which translates a segment sequence with the highest score of the unigram language model for a fair comparison with our proposed method because an NMT model with our segmentation method translates one segmented sequence. 4.2 Results Table 1 shows our experimental results: “Unigram LM,” “Subword Regularization,” and “BiSW” indicate NMT models using the unigram language model, subword regularization, and our proposed method, respectively. Translation performance was evaluated by BLEU (Papineni et al., 2002). We followed WAT Automatic Evaluation Systems5 . The statistical signiﬁcance test was performed by paired bootstrap re2 https://github.com/google/sentencepiece http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/baseline/dataPreparationJE.html 5 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html#automatic_evaluation_ systems.html 3 4291 Ja En Precision 97.05 98.82 Recall 97.44 99.22 F-measure 97.24 99.02 BiSW Oracle Ja-En 29.39 29.49 En-Ja 43.29 43.49 (a) Segmentation performance of the character-based BiLSTM (b) Comparison with the translation using go"
2020.coling-main.378,2020.acl-main.170,0,0.0139625,"regularization (Kudo, 2018) is an NMT training method that uses multiple subword candidates obtained by the unigram language model and maximizes the marginal likelihood of sampled multiple subword candidates. This method requires on-the-ﬂy subword sampling in training; therefore, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT training process and does not need a large number of epochs. BPE-dropout (Provilkov et al., 2020) is a method that extends BPE to use subword regularization. In this method, multiple subword candidates are obtained by probabilistically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages an"
2020.coling-main.378,P16-1162,0,0.0818193,"by using subword units induced from bilingual sentences; this method could be more favorable to machine translation. Evaluations on WAT Asian Scientiﬁc Paper Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU). 1 Introduction Subword units have recently been widely used in neural machine translation (NMT) to solve open vocabulary problems. Byte Pair Encoding (BPE) (Sennrich et al., 2016) is a dominant subword segmentation method for NMT, but it is designed for segmented languages in which words are divided by spaces. Kudo (2018) has proposed a subword segmentation method based on a unigram language model, that can be applied to non-segmented languages such as Chinese and Japanese. Both BPE and the unigram language model tokenize sentences by minimizing the number of segments under a limitation on subword vocabulary size, which relies on a data compression principle. In these existing segmentations, a sentence is segmented without considering its translation, and therefore the"
2020.coling-main.385,N19-1423,0,0.172588,"references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training and fine-tuning, an intermediate training step has been proposed (Phang et al., 2018), using large scale labeled data relevant to the target task. This approach is nonetheless limited by it"
2020.coling-main.385,2020.acl-main.740,0,0.034613,"Missing"
2020.coling-main.385,W19-5406,0,0.203061,"Missing"
2020.coling-main.385,W17-4763,0,0.115819,"nslation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training a"
2020.coling-main.385,W19-5407,0,0.196873,"aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning and direct fine-tuning of the pre-trained language model. However, fine-tuning pre-trained models is highly unstable when the dataset is small (Devlin et al., 2019; Zhang et al., 2020), which is the case in QE for MT as annotated datasets are scarse. To provide a smooth transition between pre-training and fine-tuning, an intermediate training step has been proposed (Phang et al., 2018), using l"
2020.coling-main.385,2020.acl-main.703,0,0.0323107,"ediate training task and will explore this research direction in future work. Our proposed approach can be seen as an extension of the work done by (Gururangan et al., 2020), where the authors evaluated the impact of continued pre-training LMs on domain and task relevant data, but keeping the training objective similar to the pre-training step. Our approach differs from this work by adding a training component relevant to the final QE task, i.e. fake masking. We would like to explore masking variants in future work, for instance by allowing variable masking spans such as the ones proposed in (Lewis et al., 2020). Finally, an extension of our work inspired by (Rubino, 2020) is to explore the pre-training masking hyper-parameters, namely the number of tokens masked and randomly replaced by other tokens sampled from the vocabulary. Because detecting mistranslations in MT output is crucial for QE, increasing the number of replaced tokens would force the model to learn more accurately which tokens are mistranslated or not. This hyper-parameter search could be part of the intermediate training procedure to avoid increasing the computational costs of large LMs pre-training. Acknowledgements A part of this w"
2020.coling-main.385,L18-1004,0,0.0147155,"f 320 sentence pairs. For QE fine-tuning, two learning rates were used, one for the pre-trained LM layers and one for the randomly initialized QE layers. We optimized these learning rates through hyper-parameter search and kept the best values based on the performance reached on the validation set. The batch size was set to 32 sentence pairs with gradient accumulation over 8 batches. 3.4 Datasets The intermediate training data was composed of the supplementary parallel corpus released for the WMT’19 QE shared task for the English–German pair and the Escape corpus for the English–Russian pair (Negri et al., 2018) filtered with the QE training and validation vocabularies. The sentence and word-level annotated datasets use for training, validation and testing our QE models were the official 1 2 More details about the task are available at http://www.statmt.org/wmt19/qe-task.html Model called xlm-mlm-tlm-xnli15-1024 and available at https://github.com/huggingface/transformers 4357 Sentence-level Domain- Fineadaptation tuning r↑ ρ↑ X X 0.321 0.440 0.481 0.510 X X 0.263 0.402 0.495 0.528 X X X X Source F1 ↑ MCC ↑ Word-level MT words MT gaps F1 ↑ MCC ↑ F1 ↑ MCC ↑ F1 ↑ MT all MCC ↑ MAE ↓ RMSE ↓ 0.396 0.478 0"
2020.coling-main.385,P02-1040,0,0.106674,"o-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straigh"
2020.coling-main.385,quirk-2004-training,0,0.0787766,"task. The proposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained"
2020.coling-main.385,2020.wmt-1.121,1,0.817429,"ure work. Our proposed approach can be seen as an extension of the work done by (Gururangan et al., 2020), where the authors evaluated the impact of continued pre-training LMs on domain and task relevant data, but keeping the training objective similar to the pre-training step. Our approach differs from this work by adding a training component relevant to the final QE task, i.e. fake masking. We would like to explore masking variants in future work, for instance by allowing variable masking spans such as the ones proposed in (Lewis et al., 2020). Finally, an extension of our work inspired by (Rubino, 2020) is to explore the pre-training masking hyper-parameters, namely the number of tokens masked and randomly replaced by other tokens sampled from the vocabulary. Because detecting mistranslations in MT output is crucial for QE, increasing the number of replaced tokens would force the model to learn more accurately which tokens are mistranslated or not. This hyper-parameter search could be part of the intermediate training procedure to avoid increasing the computational costs of large LMs pre-training. Acknowledgements A part of this work was conducted under the commissioned research program “Res"
2020.coling-main.385,2006.amta-papers.25,0,0.139252,"s show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings (Kim et al., 2019). While both approaches make use of sentence encoder models, such as BERT (Devlin et al., 2019) or XLM (Conneau and Lample, 2019), only the second approach allows for straightforward end-to-end learning"
2020.coling-main.385,2009.eamt-1.5,0,0.0503891,"oposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders. 1 Introduction Machine translation (MT) quality estimation (QE) (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009) aims at evaluating the quality of translation system outputs without relying on translation references, which are required by automatic evaluation metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006). Current state-of-the-art (SotA) QE approaches have switched from hand-crafted features to large data-driven neural-based models (Bojar et al., 2016). Best performing QE methods from the latest WMT QE shared task (Fonseca et al., 2019) are based on two approaches: predictor–estimator (Kim et al., 2017) and QE-specific output layers on top of pre-trained contextual embeddings"
2020.coling-main.385,W18-5446,0,0.0453026,"/s> + + + + + + + + + + + + + 0 1 2 3 4 5 0 1 2 3 4 5 6 + + + + + + + + + + + + + en en en en en en de de de de de de de Figure 1: Intermediate self-supervised learning task based on the translation language model training objective of XLM with the addition of NULL tokens associated with randomly inserted MASK tokens. 2 Intermediate Self-supervised Task for QE Pre-trained language model (LM) fine-tuning has shown to improve the results of many natural language processing tasks such as grammatical sentence classification, paraphrases detection or textual entailment to name a few popular tasks (Wang et al., 2018). Some of the prevailing fine-tuned pre-trained models studied in the literature are BERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), among others. At the core of these approaches are similar LM techniques, using the sequentiality of languages Q to learn probabilities over sequences (X) of words (xi , i ∈ [0; n]) as in p(X) = ni=1 p(xn |x1 , ..., xn ) (causal LM) or randomly masking some input tokens and learning to retrieve them based on both left and right contexts (masked LM). The masked LM approach introduced in BERT was extended in XLM to learn relations between translated s"
2020.findings-emnlp.371,abdelali-etal-2014-amara,0,0.0286749,"erence agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese"
2020.findings-emnlp.371,N19-1388,0,0.0312615,"2005)). PBSMT + NMT: (Lample et al., 2018b), XLM: (Conneau and Lample, 2019), MASS: (Song et al., 2019). In the form x[y], x and y respectively indicate results on in-domain and out-of-domain sets. Note, the BLEU used in ro→zh is based on Chinese words segmented by the jieba toolkit. en-ro fr-ro zh-ro en-fr-ro en-zh-ro en fr ro zh 6.5 / 64.3 6.9 / 60.1 7.4 / 53.8 4.1 / 68.7 4.2 / 68.4 - 4.9 / 68.3 4.9 / 68.5 5.3 / 65.8 5.0 / 68.1 5.5 / 64.9 11.5 / 52.9 11.4 / 53.4 Table 2: Perplexity / Accuracy for masked language modeling in different languages joint pre-training. UNMT Lample et al. (2018a); Aharoni et al. (2019); Song et al. (2019) have demonstrated the importance of pre-training, which is a key ingredient of UNMT. Conneau and Lample (2019) used masked language modeling (MLM) to pretrain the full model for the initialization step before applying a denoising autoencoder and BT training step. Therefore, we take the XLM architecture proposed by Conneau and Lample (2019) as our backbone baseline model. MUNMT Our method studies the impact of adding a reference language to the existing UNMT language pair, which makes our model essentially multilingual. Therefore, MUNMT is the baseline for comparison. We ad"
2020.findings-emnlp.371,N19-1121,0,0.059622,"irs, including unseen language pairs, transfer learning should be considered when low-resource languages are trained together with rich-resource ones. As discussed by Arivazhagan et al. (2019), MUNMT usually performs worse than pivot-based supervised NMT; however, the pivot-based method easily experiences a computationally expensive quadratic growth in the number of source languages and suffers from the error propagation problem. Arivazhagan et al. (2019) addressed the zeroshot generalization problem that some translation directions have not been optimized well due to a lack of parallel data. Al-Shedivat and Parikh (2019) introduced a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in zero-shot translation, which share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingua"
2020.findings-emnlp.371,P17-1042,0,0.0856606,"Missing"
2020.findings-emnlp.371,1981.tc-1.7,0,0.709837,"Missing"
2020.findings-emnlp.371,C18-1233,1,0.900017,"Missing"
2020.findings-emnlp.371,P05-1066,0,0.0305915,"Missing"
2020.findings-emnlp.371,2020.findings-emnlp.283,0,0.202912,"ize is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performance is slightly lower. This is because M"
2020.findings-emnlp.371,P18-1192,1,0.906335,"Missing"
2020.findings-emnlp.371,D19-1080,0,0.0441941,"Missing"
2020.findings-emnlp.371,W17-3204,0,0.0440239,"and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/cor"
2020.findings-emnlp.371,J82-2005,0,0.623357,"Missing"
2020.findings-emnlp.371,P19-1017,0,0.0906893,"ich share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingual supervised NMT. As for the agreement in UNMT, (Sun et al., 2019) investigate the enhancement of unsupervised bilingual word embedding agreement in the UNMT training. Leng et al. (2019) propose a multi-hop UNMT that automatically selects a good translation path for a distant language pair during UNMT. Baijun et al. (2019) proposed a cross-lingual pre-training approach that makes use of the source–pivot data to pre-train the language model. As for the multilingualism, Liu et al. (2020) proposes a multilingual denoising pre-training technique to improve machine translation tasks. Bai et al. (2020) and Garcia et al. (2020) both studied the agreement across language pairs. Their method is much the same as one of our proposed approaches, XBT, which relies on the supervision signa"
2020.findings-emnlp.371,D18-1262,1,0.884281,"Missing"
2020.findings-emnlp.371,L16-1147,0,0.020699,"language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennr"
2020.findings-emnlp.371,2020.tacl-1.47,0,0.167936,"the byte pair encoding (BPE) code size is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performa"
2020.findings-emnlp.371,N09-2056,1,0.895514,"Missing"
2020.findings-emnlp.371,D18-1549,0,0.061576,"benchmarks has achieved great success (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) because of advances in deep learning and the availability of large-scale parallel corpora; however, the applicability of MT systems is limited because of their reliance on large parallel corpora for the majority of language pairs. In real-world situations, the majority of language pairs have very little parallel data, although large volumes of monolingual data are available for each language. UNMT removes the dependence on parallel corpora, relying only on monolingual corpora in each language (Reddi et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Li et al., 2019b). UNMT uses translation symmetry for dual learning in each language direction. Existing UNMT models are mainly built on the encoder– decoder schema. The essence of UNMT is to 4151 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4151–4162 c November 16 - 20, 2020. 2020 Association for Computational Linguistics learn unsupervised cross-lingual word alignment and/or sentence alignment. For unsupervised word alignment, the most popular methods are word embedding mapping (Conneau et al., 2017; Lample e"
2020.findings-emnlp.371,W16-2323,0,0.058197,"2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/corpus/global-voices.html http://paracrawl.eu 4 https://github.com/fxsjy/jieba 5 https://github.com/BYVoid/OpenCC 3 en-fr-ro en-zh-ro en→ro ro→en fr→ro ro→fr"
2020.findings-emnlp.371,P19-1119,1,0.807626,"Missing"
2020.findings-emnlp.371,2020.acl-main.324,1,0.653027,"ation models (as the generation direction is the same as the training direction), but also train the BT models, i.e., T → S and T → R. This gives the RABT training approach shown in Figure 2(c). The learning objective of RABT can be described as: LRABT (S, T , R) = L(θT →S ) + L(θT →R ). 3.4 (8) Cross-lingual Back-translation The traditional BT analyzed in Section 2 and illustrated in Figure 2(a) allows us to train a T → S model with the help of an S → T model, and vice versa; however, this mutually beneficial training is performed entirely within one language pair. Multilingual UNMT (MUNMT) (Sun et al., 2020) is a special case of UNMT that is capable of translating between multiple source and target languages. Although multiple language pairs are trained jointly in MUNMT, there is an obvious shortcoming for BT: translating between language pairs that do not occur together during training, i.e., lack of optimization across language pairs. Joint training across language pairs can be performed through forced high-order BT in UNMT, which takes the form L1 → L2 → ... → LO+1 → L1 , where O is the translation order indicating the number of bridge languages in BT. This approach may fail because decoding t"
2020.findings-emnlp.371,N07-1061,1,0.653675,"Missing"
2020.findings-emnlp.371,P07-1108,0,0.24056,"Missing"
2020.findings-emnlp.371,P19-1230,1,0.787194,"Missing"
2020.findings-emnlp.371,L16-1561,0,0.0615968,"olingual sentences as those extracted from the WMT News Crawl datasets for the period 2007–2017 by Conneau and Lample (2019) for a fair comparison and limited the maximum number of sentences in each language to 50 million(M), which results in 50M, 50M, and 14M sentences, respectively. For Chinese, we combined all of the sentences available in the WMT News Crawl datasets with the source sentences from the WMT’17 Chinese–English translation task, leading to 26M sentences. For the parallel data of en-fr and en-zh introduced by the two experimental settings, we only use those provided by MultiUN (Ziemski et al., 2016). Finally, the size of the resulting language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we"
2020.findings-emnlp.371,tiedemann-2012-parallel,0,\N,Missing
2020.lrec-1.364,D14-1179,0,0.0409666,"Missing"
2020.lrec-1.364,W16-2711,1,0.828506,"on the character/grapheme level rather than the word/phrase level, with no (or few) reordering operations. The technical background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration of 591 languages into English. Their conclusion was that a PBSMT system outperformed other systems including NN-based approaches. Regarding the case of Myanmar processing, there are few previous works. Ding et al. (2017) first attempted a Myanmar name Romanization task, where NN-based approaches did not outperform the traditional approaches of the conditional random field and support vector machine. It"
2020.lrec-1.364,P17-4012,0,0.0161047,"n this study, we focus on transliteration between Myanmar (Burmese) and English. To facilitate the application of data-driven approaches, we manually collected a dictionary containing more than eighty thousand MyanmarEnglish transliteration instances. The data have been released under a CC BY-NC-SA license for research purposes. 1 Based on the dictionary, we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar scr"
2020.lrec-1.364,N03-1017,0,0.0396493,"ded and future work is presented. 2. Related Work Many Asian languages apply special writing systems, and efforts have been made on transliteration processing for major languages such as Chinese, Japanese, and Korean (Merhav and Ash, 2018). However, studies are required on understudied languages with limited resources. Generally, the transliteration task can be modeled as a simplified translation task on the character/grapheme level rather than the word/phrase level, with no (or few) reordering operations. The technical background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration o"
2020.lrec-1.364,P07-2045,0,0.0859893,"he data have been released under a CC BY-NC-SA license for research purposes. 1 Based on the dictionary, we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar script was also investigated. To the best of our knowledge, this study is the first systematic work on the topic of Myanmar-English transliteration driven by a relatively large-scale dataset. * Corresponding author http://www2.nict.go.jp/astrec-att/member/"
2020.lrec-1.364,J03-1002,0,0.0365005,"uly.” Dictionary To collect the transliteration instances, we began with two English-Myanmar parallel corpora, and then moved to resources on the Internet to enlarge the scale of the data. Specifically, we used the ALT corpus (Riza et al., 2016; Ding et al., 2018; Ding et al., 2019; Ding et al., 2020)5 and UCSY corpus (Sin et al., 2018). 6 The ALT corpus consists of twenty thousand parallel sentences from news articles and the UCSY corpus contains two hundred thousand parallel sentences collected from different domains, including local news articles and textbooks. We used the GIZA++ toolkit7 (Och and Ney, 2003) to obtain the raw alignments between the source and target language, based on which the transliteration instances were filtered. From the Internet, we further collected instances of places, organizations, and person names. The dictionary was encoded in Unicode. For experimental investigation, we divided it into three parts by taking the first one thousand lines as the test data and the final one thousand lines as the development data. Statistics for the data are listed in Table 1, where #Char. denotes Unicode characters, #Syl. denotes syllables, and #Sub-Syl. denotes the sub-syllable units pr"
2020.lrec-1.364,P03-1021,0,0.0592599,",884,108 13,604 14,253 Table 1: Statistics for the data. Char. Sub-Syl. Syl. <ဆ><ွွ><စ><ွ><ဇ><ွာ><လ><န><ွ> <ဆ><ွွစ><ဇ><ွာ><လ><န> <ဆွစ><ဇာ><လန> Table 2: Different units in Myanmar processing. 4. Experiment The Myanmar data were segmented into different units, as shown in Table 2, and all English data were lowercased. Moses was used to train the baseline PBSMT system. Character alignment was generated using GIZA++ and symmetrized using the grow-diag-final-and heuristics. A 5-gram language model on character was trained using the SRILM toolkit (Stolcke, 2002).8 Minimum error rate training (Och, 2003) was used to tune the weight of the features. All other parameters were adapted using the default setting of Moses. Regarding the JANUS toolkit, which implements a bidirectional agreement LSTM-RNN model, the hyperparameters followed the original paper (Liu et 5 http://www2.nict.go.jp/astrecatt/member/mutiyama/ALT/index.html 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/my-en-data/ 7 http://www.statmt.org/moses/giza/GIZA++.html 8 http://www.speech.sri.com/projects/srilm/ 2981 al., 2016). Specifically, the settings were 500 for embedding, 500 for the hidden unit dimensions, and 16 for the batch size. Ad"
2020.lrec-1.364,P02-1040,0,0.107849,"we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar script was also investigated. To the best of our knowledge, this study is the first systematic work on the topic of Myanmar-English transliteration driven by a relatively large-scale dataset. * Corresponding author http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/my-e n-transliteration.zip 2 http://opennmt.net/OpenNMT-py/ 3 https://github.com/lemaoliu/Agta"
2020.lrec-1.364,L18-1150,0,0.0146687,"background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration of 591 languages into English. Their conclusion was that a PBSMT system outperformed other systems including NN-based approaches. Regarding the case of Myanmar processing, there are few previous works. Ding et al. (2017) first attempted a Myanmar name Romanization task, where NN-based approaches did not outperform the traditional approaches of the conditional random field and support vector machine. It can be considered that the transliteration task may be sensitive to the quality and quantity of training data, in addition t"
2020.wat-1.3,P11-2031,0,0.127481,"Missing"
2020.wat-1.3,N19-1423,0,0.0206702,"aster than the self-attention mechanism because they depend only on the last state. Similar to our method, which is described in this paper, Zhou et al. (2019) proposed a model that simultaneously decodes two tokens from the head and tail of a 2.2 Non-autoregressive Decoding Non-autoregressive decoding generates all tokens simultaneously, utilizing the parallelism of Transformer (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). For example, the Mask-Predict method (Ghazvininejad et al., 2019) recovers masked tokens ([mask]) using the left and right contexts, like BERT encoders (Devlin et al., 2019). The initial tokens are all masks. Because of the parallel generation, the generation lengths must be determined in advance. The Mask-Predict method predicts these lengths from the encoder output. Although non-autoregressive decoding performs fast generation, the translation quality in one step is relatively low. We can improve the quality by iteratively applying the parallel decoding. However, iterative decoding causes the following problems. • The decoding speed reduces as the number of iterations increases. This is a trade-off between quality and speed. • Iterative non-autoregressive decod"
2020.wat-1.3,D19-1633,0,0.16977,"n systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoregressive decoding. We then explain the proposed"
2020.wat-1.3,W17-4123,0,0.10563,"tion Most neural machine translation systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoreg"
2020.wat-1.3,Q19-1042,0,0.0301868,"Missing"
2020.wat-1.3,W18-2716,0,0.0339688,"Missing"
2020.wat-1.3,P18-1166,0,0.0718059,"o compute the current state hℓt . However, the previous states have already been computed while predicting the previous tokens. Therefore, only the state hℓ−1 needs to be computed if t we preserve the previous states. We call this inner state preservation in this paper. When training, all tokens can be learned in parallel using a mask of the triangular matrix, which restricts the tokens for the self-attention mechanism (Figure 1(a)). Another strategy for speeding up autoregressive decoding is to substitute the self-attention mechanism with the other units. The Average Attention Network (AAN) (Zhang et al., 2018; JunczysDowmunt et al., 2018) and Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019) are faster than the self-attention mechanism because they depend only on the last state. Similar to our method, which is described in this paper, Zhou et al. (2019) proposed a model that simultaneously decodes two tokens from the head and tail of a 2.2 Non-autoregressive Decoding Non-autoregressive decoding generates all tokens simultaneously, utilizing the parallelism of Transformer (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). For example, the Mask-Predict method (Ghazvininejad et al"
2020.wat-1.3,D18-1149,0,0.0691345,"machine translation systems are based on an encoder–decoder architecture. Although there are some frameworks, such as recurrent neural network-based translation (Sutskever et al., 2014; Bahdanau et al., 2014) and Transformerbased translation (Vaswani et al., 2017), they employ autoregressive decoding for high-quality translation. However, autoregressive decoding requires a decoding time that depends on sentence length because it generates a single token in each step. To solve this problem, non-autoregressive decoding, which generates all tokens in one step, has been proposed (Gu et al., 2017; Lee et al., 2018; Ghazvininejad et al., 2019). However, the translation quality of non-autoregressive decoding has not yet matched the quality of autoregressive decoding. To improve the quality, some methods, such as Mask-Predict (Ghazvininejad et al., 2019), apply non-autoregressive decoding iteratively. This is a trade-off between quality and – All tokens can be learned in parallel in the training phase. – In contrast to non-autoregressive decoders, our method does not determine generation lengths in advance. In the following sections, we first briefly review autoregressive and non-autoregressive decoding."
2020.wat-1.3,N19-4009,0,0.0242604,"used as the validation sets. All corpora were segmented into subwords (Sennrich et al., 2016). We used 37K shared vocabulary in WMT-14 and 16K vocabularies for the source and target languages in ASPEC. 1. Divide the sequence of tokens into left and right halves, reverse the right half, and alternately fold the halves. 2. Supply one or two EOD tokens, to make the total number of tokens even. The self-attention mask for training is a triangular matrix in which the unit is a pair of tokens (Figure 1(b)). 4 Experiments 4.1 Experimental Settings Systems: We modified the fairseq translation system (Ott et al., 2019)1 for the proposed method. For comparison, we considered the following three system types. Models and Hyperparameters: We used two model types: the Transformer base model (six layers, eight heads, 512 model dimensions, and 2,048 FFN dimensions) and the Transformer big model (six layers, 16 heads, 1,024 model dimensions, and 4,096 FFN dimensions). Table 1 shows the details of the hyperparameters. All models were trained using almost the same settings, except for the learning rates and stopping criteria. In the test phase, we used a beam width of 10 for autoregressive decoding. For Mask-Predict,"
2020.wat-1.3,P02-1040,0,0.106409,"Missing"
2020.wat-1.3,P16-1162,0,0.0524334,"ds.3 Corpora: We used two corpora: the English– German (en-de) corpus of WMT-14 (4.5M sentences) (Bojar et al., 2014) and the Japanese– English (ja-en) corpus of ASPEC (3M sentences) (Nakazawa et al., 2016). To make the evaluation stable, we concatenated all test sets in the corpora, except for validation sets. That is, we used 19,666 sentences (newstest2010-2016) for the WMT-14 corpus and 3,596 sentences (devtest and test) for the ASPEC corpus, as the test sets. The newstest2009 set in WMT-14 and the dev set in ASPEC were used as the validation sets. All corpora were segmented into subwords (Sennrich et al., 2016). We used 37K shared vocabulary in WMT-14 and 16K vocabularies for the source and target languages in ASPEC. 1. Divide the sequence of tokens into left and right halves, reverse the right half, and alternately fold the halves. 2. Supply one or two EOD tokens, to make the total number of tokens even. The self-attention mask for training is a triangular matrix in which the unit is a pair of tokens (Figure 1(b)). 4 Experiments 4.1 Experimental Settings Systems: We modified the fairseq translation system (Ott et al., 2019)1 for the proposed method. For comparison, we considered the following three"
2020.wmt-1.22,D18-1399,0,0.0372296,"Missing"
2020.wmt-1.22,N18-1118,0,0.0256328,"al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or fu"
2020.wmt-1.22,P18-1192,1,0.83718,"019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT mo"
2020.wmt-1.22,D19-5603,1,0.814693,"rmance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an XLM model, a variant of BERT, pre-trained from scratch on the monolingual data provided by the official to enhance our NMT model, is a good choice to keep the system constrained. Moreover, the XLM model has the advantages of simple traini"
2020.wmt-1.22,P19-4007,0,0.0555909,"Missing"
2020.wmt-1.22,J82-2005,0,0.698509,"Missing"
2020.wmt-1.22,P19-1285,0,0.0196835,"ting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentences, etc., rather than the truly whole document information. Recently, with the increase in computational power available to us and the well-designed neural network structures (Dai et al., 2019; Kitaev et al., 2019; Beltagy et al., 2020) for long sequence encoding, we are finally in a position to employ the whole document information for enhancing sentence-level NMT. In addition, we argue that since long sequences encoding is easier than decoding, truly whole document-level translation is still a long way off, since the bidirectional context is available in the encoder, but only the past is visible by the decoder. Longformer To make the long documents processed with Transformer (Vaswani et al., 2017) architecture feasible or easier, a modified Transformer architecture named Longform"
2020.wmt-1.22,P19-1120,0,0.0384069,"Missing"
2020.wmt-1.22,N19-1423,0,0.0103748,"the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an"
2020.wmt-1.22,J93-1004,0,0.207938,"ull model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the second setting base data + paracrawl, we converted the paragraph-level alignment data in Paracrawl to sentence-level alignment and incorporated it with the base data. In the conversion process, we adopted the method and program proposed by (Gale and Church, 1993) for aligning sentences based on a simple statistical model of character lengths, which uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter 224 Systems Transformer big +D2GPo XLM-enhanced Document-enhanced Ensemble ++TF-IDF finetune ++Re-ranking 19test Test BLEU BLEU chrF 37.2 37.7 38.9 39.2 40.0 40.2 40.5 48.6 48.8 49.1 0.418 0.422 0.427 (NSP) classification model provided by Google for document interval prediction to recover the documents. DE-HSB In RUNMT on EN-DE-"
2020.wmt-1.22,P07-2045,0,0.0121171,"s the adjustable parameters. ∀t ∈ Dterms ], (12) where Dterms indicates the all terms set in corpus D. We calculate the cosine similarity as final scores between the query and every source sentence in corpus, and ranked on the scores to get the topK pairs (K=1000 in our experiments) as the subtraining set for finetuning. 3 Data Preprocessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtain"
2020.wmt-1.22,C18-1271,1,0.818947,"In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from"
2020.wmt-1.22,N19-4009,0,0.0819852,"(U1836222 and 61733011), Huawei-SJTU Long Term AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. (DE) ↔ Upper Sorbian (HSB) both directions are focused. Our baseline system in supervised track is based on the Transformer big architecture proposed by Vaswani et al. (2017), in which its opensource implementation version Fairseq (Ott et al., 2019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He e"
2020.wmt-1.22,N09-2056,1,0.713764,"eference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we would ideally have P(·|s; θS→T ) = P(·|r; θR→T )"
2020.wmt-1.22,N18-1202,0,0.00847445,"model training is finished, the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submi"
2020.wmt-1.22,2020.findings-emnlp.371,1,0.64243,"Missing"
2020.wmt-1.22,2021.ccl-1.108,0,0.100832,"Missing"
2020.wmt-1.22,P12-3005,0,0.0427849,"gned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. For the Polish pre-trained XLM language model, we used all NewsCrawl monolingual data and some CommonCrawl monolingual data. Since the CommonCrawl data is very large and noisy and can potentially decrease the performance of LM if it is used in its raw form. We apply language identification filtering (langid; Lui and Baldwin (2012)), keeping sentences with correct languages. In order to filter out the sentences shorter than 5 words or longer than 150 words more precisely, we re-split sentences using Spacy (Honnibal and Montani, 2017) toolkit. EN-ZH In EN-ZH, the pre-training of Longformer as a document encoder is unique. As described in (Beltagy et al., 2020), the Longformer needs a large number of gradient updates to learn the local context first; before learning to utilize longer context. In the first phase of the staged training procedure, an initial RoBERTa (Liu et al., 2019) model implemented in Fairseq (Ott et al."
2020.wmt-1.22,2020.acl-main.571,1,0.800756,", 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a bidirectional translation model of EN-"
2020.wmt-1.22,W18-6319,0,0.0144898,"uent phase, we trained the model on the paragraph text, doubled the window size and the sequence length, and halve the learning rate. For the paragraph text, the Wikidumps and NewsCommentary v15 have document intervals and can be used directly, while UN v1.0 has no document intervals but the sentence order is not interrupted. Therefore, we use the BERT Next Sentence Prediction Results and Analysis Results and ablations for PL→EN2 are shown in Table 1, EN→ZH in Table 2, unsupervised DE↔HSB in Table 3 and low-resource DE↔HSB in Table 4. We report case-sensitive SacreBLEU scores using SacreBLEU (Post, 2018) for EN-PL, DE-HSB, and BLEU based on characters for ENZH. In the results, “+” means addition based on baseline, and “++” means cumulative addition based on the previous one. In PL→EN, the introduction of ParaCrawl data improves the baseline performance on the dev dataset by about 4.2 BLEU. +D2GPo, XLM-enhanced NMT, Bidirectional NMT, and ensembling outperforms our strong baseline by 2 BLEU point. Finally, finetuning and reranking further gives another 0.5 BLEU. For EN→ZH, as with PL→EN, we see similar improvements with +D2GPo, XLM-enhanced NMT, ensembling and reranking. We also observe that t"
2020.wmt-1.22,D19-6506,0,0.0216909,"irections, the differences are also very obvious. To further expose the model to the direction difference and improve the effect of unidirectional translation, we further finetune the 220 bidirectional pre-trained model on the bilingual data. Take S2T translation as an example; the model is optimized as follows: L(θS→T ) = N X log p(y (n) |x(n) ), (5) n=1 where θS→T is the parameters of child model which is initialized with θparent . Similarly, the T2S child model can also be obtained. Due to the introduction of bidirectional translation in one model, follow the practice of Conneau and Lample (2019), shared subword vocabulary and shared encoder-decoder (source and target) embedding were employed to improves the alignment of embedding spaces across languages. In addition, since the encoder and decoder need to be able to handle two languages simultaneously, a language embedding was used to indicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky"
2020.wmt-1.22,P16-1162,0,0.0703717,"cessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the secon"
2020.wmt-1.22,2021.naacl-main.311,1,0.626638,"Missing"
2020.wmt-1.22,P95-1026,0,0.836605,"e unlabeled dataset U = {x(j) }L j=1 is used for the synthesis of pseudo-parallel corpora. While in UNMT, since the model is trained with backtranslation on unpaired monolingual data, the pseudo-parallel corpora is synthesized by the monolingual data, i.e., U = {x(m) }M m=1 . Considering the translation quality can’t effectively be evaluated across languages in machine translation with only the monolingual data, therefore the selection of the subset Q, is one of the key factors for self-training. It is usually selected based on some confidence scores (e.g. log probability or perplexity, PPL) (Yarowsky, 1995), but it is also possible for S to be the whole pseudo parallel data (Zhu and Goldberg, 2009). In the backward translation based on the pseudo-parallel data, the DAE method widely used in UNMT can alleviate the impact of the noise resulted from the synthesized sentences on model training, since the synthesized sentences are only used as input. However, in the forward translation training, the quality of noisy targets will directly affect the success of the model training. Therefore, the selection of synthetic parallel corpus becomes particularly critical. fθT →S 5: Calculate BT-BLEU B for two"
2020.wmt-1.22,N07-1061,1,0.7001,"Missing"
2020.wmt-1.22,P18-1117,0,0.0215559,"-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentenc"
2020.wmt-1.22,P07-1108,0,0.0458195,"truction training of UNMT through a proposed reference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we w"
2020.wmt-1.22,P19-1298,1,0.820418,"successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020)."
2020.wmt-1.22,D13-1163,0,0.0303897,"dicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and"
2020.wmt-1.22,C18-1317,1,0.818967,"ed and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorpo"
2020.wmt-1.22,P19-1230,1,0.815888,"ce of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a"
2020.wmt-1.22,D16-1163,0,0.0162403,"t , where [0, pnet 2 ) is the probability of attending to L, the final sum for the first attn in HEL and HD pnet pnet [ 2 , 1 − 2 ) is the probability for the whole HEL L equation, [1 − pnet , 1] is the probability and HD 2 L. for the second attn in in HEL and HD 2.2 Bidirectional NMT Machine translation, in general, is unidirectional, that is, from the source language to the target language. The encoder-decoder framework for NMT has been shown effective in large data scenarios, and the more high-quality bilingual training data, the better performance the model tends to achieve. Recent works (Zoph et al., 2016; Kim et al., 2019) on translation transfer learning (Torrey and Shavlik, 2010; Pan and Yang, 2009) from rich-resource language pairs to low-resource language pairs demonstrate that translation has some universal nature in essence between different language pairs. As the sourceto-target (S2T) forward translation and target-tosource (T2S) backward translation can be seen as two special language pairs in bilingual translation, it can make use of the translation universal nature to improve each other, i.e., dual learning (He et al., 2016). Based on this motivation, we developed a bidirectional NM"
2021.emnlp-demo.1,D18-1045,0,0.01542,"ation of the translation y is always k tokens behind reading x; that is, at the t-th decoding step, we generate token yt based on x ≤ t − k + 1. We thus adopt a Transformerbased NMT model with the wait-k strategy, aiming for balance between translation performance and efficiency. 2.3 translation’ of a translated sequence back into its original language – is a potential method of generating reference sentences for comparison that utilizes the duality of direction in translation (He et al., 2016). Back-translation is currently mainly used as a data-enhancement method for supervised NMT systems (Edunov et al., 2018) and as a crucial training method for unsupervised NMT systems (Conneau and Lample, 2019), though it has been more controversial as a method of assessing translations. According to (Behr, 2017)’s conclusion, while back-translation can give some evaluation of the translation, it often raises issues not noted by human assessors, and more importantly, is less reliable in general, as many problems remain hidden. These shortcomings are mainly are a result of commonly used automatic evaluation methods (like BLEU) using only surface-level similarity; they do not strictly measure , Semantic Equivalenc"
2021.emnlp-demo.1,W19-4822,0,0.0146487,"tics as Google and Baidu have introduced simultaneous translation feature, due to the integration of simultaneous translation and whole-sentence translation, users cannot easily control whether the system uses simultaneous translation or whole-sentence translation, and the automated control of commercial systems sometimes does not follow the user’s wishes. Since user input errors are unavoidable for any human-computer interaction system, the quality of NMT system also has been shown to significantly degrade when confronted with source-side noise (Heigold et al., 2018; Belinkov and Bisk, 2018; Anastasopoulos, 2019). The previous grammatical error detection and correction work focused on computer-aided writing systems. Some existing computer-aided writing systems (Grammarly2 and Pigai3 , Write&Improve4 , and LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited appl"
2021.emnlp-demo.1,D19-1435,0,0.0143281,"ain better translation sequences. We wanted to emphasize efficient inference, so we adopted a Transformer (Base) setting with fewer parameters. The training data used was the same as that in the multi-style NMT model. We formulated the GEC task as a sequence labeling problem and thus adopted a neural sequence tagging model to handle the task. We followed (Omelianchuk et al., 2020)’s model architecture, which was an encoder consisting of a pre-trained BERT-like transformer stacked with two linear layers with softmax layers on the top - one for error detection and one for error labeling. As in (Awasthi et al., 2019), the architecture uses an iterative correction strategy in which predicted transformations are applied to the input sequence successively. After errors are detected and predicted, a modified Levenshtein distance guides the generation of a corrected sentence. We limit the maximum number of inference iterations to 4 to speed up the overall correction process while still maintaining good correction accuracy. The training data we used for GEC is shown in Table 3. We trained our English GEC model at the word level and our Chinese and Japanese models at the character level. We used pre-trained lang"
2021.emnlp-demo.1,W18-1807,0,0.0687644,"Missing"
2021.emnlp-demo.1,P18-4024,1,0.847809,"apan charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide suff"
2021.emnlp-demo.1,N12-1067,0,0.0355332,"016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dahlmeier and Ng, 2012). The results of our models for standard NMT and simultaneous NMT are shown in Table 4. First, for the evaluation results of standard NMT, we found that the joint training of multiple styles of data does not bring performance improvement compared to separate training, especially when the corpora sizes of the two styles are similar. The translation performance gap between different styles demonstrates that the level difficulty of translation in different styles is different. Since style essentially refers to deviation from standard textual norms, the greater the deviation, the greater the trans"
2021.emnlp-demo.1,P19-1289,0,0.0633091,"Missing"
2021.emnlp-demo.1,2016.amta-researchers.9,0,0.0252712,"I SS, users can get real-time translations while writing, flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 B"
2021.emnlp-demo.1,W17-3204,0,0.0152215,"nd LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited applications because different machine translation users want to generate translations that can be used in the scenario. On the one hand, general purpose translation systems usually perform poorly (Koehn and Knowles, 2017). On the other hand, appropriate translation is also a very important goal to pursue. There are two typical methods to achieve this goal. One is to use the domain adaptation method to obtain a domainspecific model from the existing general machine translation model through transfer learning. The other is to adopt an conditional translation decoder to integrate various domains into the same model and generate translations according to different input conditions (Keskar et al., 2019). At present, the commercial machine translation system mainly adopts the former one, but it also brings the addit"
2021.emnlp-demo.1,D18-1512,0,0.0469907,"Missing"
2021.emnlp-demo.1,C18-1271,1,0.743867,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,2020.bea-1.16,0,0.389675,"NMT model, making this decoder also conditioned on a variety of control codes (Pfaff, 1979; Poplack, 2000). We call our system CTRL-NMT. Formally speaking, the target distribution of CTRL-NMT can be decomposed using the chain rule of probability and trained with a loss that takes the control code into account: Feature #3: Grammatical Error Correction Detecting potential grammatical errors and offering corrective suggestions for them sentence is also a very important feature in M I SS. We chose the tag-based modeling approach for this feature based on the fresearch field’s latest achievements (Omelianchuk et al., 2020) and our recent work (Parnow et al., 2020, 2021) in the Grammatical Error Correction (GEC). Specifically, the g-transformations developed by (Omelianchuk et al., 2020) were included in our system in the hopes of providing learners more specific suggestions (i.e., the edit type of an error) to revise the users’ input. Predicting edits rather than tokens also increases the generalization of our GEC model. G-transformations are based on several basic transformations: $KEEP (keep the current token unchanged), $DELETE (delete current token), $APPEND_t1 (append new token t1 next to the current token"
2021.emnlp-demo.1,D18-1262,1,0.848954,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,N19-4009,0,0.0214697,"models. As (Zhang et al., 2019, 2020) observed that fine-tuning the pre-trained conSE X X X Table 1: User operations used for our crowdsourcing data collection in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model,"
2021.emnlp-demo.1,2021.findings-acl.290,1,0.806502,"Missing"
2021.emnlp-demo.1,2020.findings-emnlp.371,1,0.876743,"Missing"
2021.emnlp-demo.1,P16-1112,0,0.0293237,"ous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8 3.1M EN EN ZH JA PrLM Dict P R F0.5 − +XLNet − − +BERT − − +BERT Word Word Word Char Char Word Char Char 53.46 76.92 38.72 45.06 50.34 36.83 45.68 46.56 37.45 41.03 15.07 19.55 33.46 20.52 16.49 27.34 54.22 65.47 29.47 35.73 45.72 31.78 33.74 40.82 Table 5: The performance of our GEC models Table 3: The GEC training data Models For the GEC component, we followed common practice in the GEC task (Rei and Yannakoudakis, 2016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dah"
2021.emnlp-demo.1,D19-3018,0,0.0188754,"flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 Basis: Transformer-based NMT Transformer (Vaswani et a"
2021.emnlp-demo.1,W19-5341,0,0.017823,"em. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8"
2021.emnlp-demo.1,P19-1176,0,0.0137142,"in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8"
2021.emnlp-main.261,D18-1024,0,0.0183798,"erformance even declines compared to baseline. C ONST BTLM can adapt to larger γ and higher phrase utilization proportions, it achieves better results. Method Cosine sim. L2 dist. Pearson cor. Concat Fasttext MUSE 0.36 0.38 4.89 5.13 0.52 0.65 XLM + C ONST BTLM‡ 0.55 0.60 2.64 2.55 0.69 0.71 Table 4: Unsupervised cross-lingual alignment evaluation with word embedding Cosine similarity (Cosine sim.), L2 distance (L2 dist.), and Pearson correlation (Pearson cor.) between source words and their translations. We adopted the same vocabulary size for Concat Fasttext (Bojanowski et al., 2016), MUSE (Alaux et al., 2018), and XLM baselines, and our best EnDe UNMT model and extracted the embeddings for comparison. The results are shown in Table 4. As the results show, our method is not only better than pure embedding training methods, Concat Fasttest and MUSE, on the three evaluation metrics, but also surpasses our strong XLM baseline, which demonstrates that the alignment of the UGUNMT model is indeed improved with the weak alignment information from syntactic categories. 4.4 Universal Constituent Labels To illustrate the universal nature of the phrase grammar, we calculate statistics on the labels of the con"
2021.emnlp-main.261,P17-1042,0,0.0443604,"Missing"
2021.emnlp-main.261,D18-1399,0,0.0182256,"in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019;"
2021.emnlp-main.261,J82-2005,0,0.644622,"Missing"
2021.emnlp-main.261,D17-1209,0,0.0602426,"Missing"
2021.emnlp-main.261,S17-2002,0,0.0139469,"on label UNMT model is obtained using UG and our pro- proportions, indicating the UD annotation’s uniposed training approaches, we conducted an ex- versality and the effectiveness of our conversion perimental exploration of embedding alignment in preserving grammatical features. This does not according to the experimental settings of (Conneau explain more complicated issues such as language and Lample, 2019) and evaluated models on the similarity or commonality but rather indicates the SemEval’17 En-De cross-lingual semantic word overlap of grammatical phenomena and universal similarity task (Camacho-Collados et al., 2017). features in the annotations and parser predictions. 3256 4.5 Effects of SpanBERT, LIMIT-BERT, and C ONST BTLM for UNMT From the main experiments, the UNMT performance is improved, especially for the small-scale data setting. To find out that if the improvements are caused by C ONST MLM/C ONST BTLM and the syntactic information is really necessary, we compare our approaches with LIMIT-BERT which apply a linguistically guided span based MLM objective during UNMT training, and SpanBERT which is with a non-syntax based span masking strategy. Compared with SpanBERT and LIMIT-BERT in our UNMT fram"
2021.emnlp-main.261,D19-1538,1,0.900451,"Missing"
2021.emnlp-main.261,P18-1192,1,0.892196,"Missing"
2021.emnlp-main.261,2020.acl-main.703,0,0.0326422,"Missing"
2021.emnlp-main.261,D18-1262,1,0.8931,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.371,1,0.856979,"Missing"
2021.emnlp-main.261,H94-1020,0,0.851239,"Missing"
2021.emnlp-main.261,J93-2004,0,0.0748203,"rged IWSLT’14 En-Fr and En-De tasks. † means the to evaluate the En-Fr translation model and encoder-only version is adopted, and ‡ means the dev2010, dev2012, tst2010, tst2011, and tst2012 in encoder-decoder version is adopted. IWSLT14.TED to evaluate the En-De model. To acquire constituent parse trees for monolingual sentences, we adopted the current state-of-the- 3.2 Results and Analysis art Berkeley Neural Parser (Kitaev and Klein, 2018) The results of the UNMT experiment are mainly as our parsing model and trained an En parser us- shown in Table 1. When a large-scale monolingual ing PTB (Marcus et al., 1993), Fr and De parsers corpus is used, our baseline model outperforms using the SPMRL14 multilingual constituent tree- XLM’s reported results. This may be due to the use bank (Seddah et al., 2014), and a Zh Parser using of the larger epoch size, which makes for more adeCTB (Xue et al., 2005). Since a constituent tree- quate training. Based on our strong baseline model, bank is not available in Ro and for the consistency the four implementations of our C ONST MLM and 3254 In the small-scale monolingual training data scenario, the performance of the baseline model has a large decline compared with"
2021.emnlp-main.261,W17-4707,0,0.0590694,"Missing"
2021.emnlp-main.261,W08-2119,0,0.078955,"Missing"
2021.emnlp-main.261,P11-1002,0,0.0926002,"Missing"
2021.emnlp-main.261,P16-1009,0,0.23002,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P16-1162,0,0.135289,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P19-1119,1,0.827825,"ne Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of the 2021 Conferen"
2021.emnlp-main.261,2020.acl-main.324,1,0.803643,"Missing"
2021.emnlp-main.261,P15-1150,0,0.0327086,"Missing"
2021.emnlp-main.261,J10-2004,0,0.0915598,"Missing"
2021.emnlp-main.261,W18-6459,0,0.0630501,"Missing"
2021.emnlp-main.261,P17-1179,0,0.0596818,"Missing"
2021.emnlp-main.261,P08-1064,0,0.109236,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.398,1,0.838011,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,2020.findings-emnlp.399,1,0.781794,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,L16-1561,0,0.020456,"f constituent trees from English Penn Treebank (PTB) and German dataset of SPMRL14 shared task. The dotted box indicates the constituents that can be masked for prediction. Introduction Recently, Neural Machine Translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has been greatly developed and become the dominant paradigm in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel informa"
2021.emnlp-main.261,P18-1005,0,0.0165051,"rvised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of"
2021.emnlp-main.261,N19-1118,0,0.0470832,"Missing"
2021.emnlp-main.299,L16-1432,0,0.0159602,"f possible edges derived from the discourse parsing, namely, default-in, default-out, reverse-in, reverse-out, self, and global. Furthermore, to build the relationship with the background user scenario, we add an extra global vertex of the user scenario that connects all the other vertices. As a result, there are three types of vertices, including the rule conditions, discourse relations, and the global scenario vertex. For rule condition and user scenario vertices, 2 This discourse parser gives a state-of-the-art performance on STAC so far. There are 16 discourse relations according to STAC (Asher et al., 2016), including comment, clarificationquestion, elaboration, acknowledgment, continuation, explanation, conditional, question-answer, alternation, questionelaboration, result, background, narration, correction, parallel, and contrast. we fetch the contextualized representation of the special tokens [RULE] and [CLS] before the corresponding sequences, respectively. For relation vertices, they are initialized as the conventional embedding layer, whose representations are obtained through a lookup table. For each rule document that is composed of multiple rule conditions, i.e., EDUs, let hp denote th"
2021.emnlp-main.299,N19-1124,0,0.0608009,"Missing"
2021.emnlp-main.299,D18-1241,0,0.0170919,"framework where the dialogue states for decision making are employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves r"
2021.emnlp-main.299,2020.acl-main.130,0,0.0166896,"employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves rule documents, scenarios, asking clarifi1 Our source codes"
2021.emnlp-main.299,N19-1423,0,0.00556007,"are packed from Hc . Then a gate control λ is computed ˆ + Uλ H e ) to get the final as sigmoid(Wλ H ˆ representation H = H e + λH. (7) The overall loss for decision making is: Ld = Ldecision + λLentail . (8) Question Generation If the decision is made to be Inquire, the machine needs to ask a follow-up question to further clarify. Question generation in this part is mainly based on the uncovered information in the rule document, and then that information will be rephrased into a question. We predict the position of an under-specified span within a rule document in a supervised way. Following Devlin et al. (2019), our model learns a start vector ws ∈ Rd and end vector we ∈ Rd to indicate the start and end positions of the desired span: span = arg min(wsT tk,i + weT tk,j ), H is then passed to the BART decoder to generate the follow-up question. At the i-th time-step, H is used to generate the target token yi by P (yi |y&lt;i , x; θ) ∝ exp(Wd tanh(Ww H)), (10) where θ denotes all the trainable parameters. Wd and Ww are projection matrices. The training objective is computed by Lg = arg max (9) I X log P (yi |y&lt;i , x; θ). (11) i=1 i,j,k where tk,i denote the i-th token in the k-th rule sen- is tence. The g"
2021.emnlp-main.299,2020.acl-main.88,0,0.257863,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2020.emnlp-main.191,0,0.35027,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2021.acl-long.285,0,0.0126044,"ate-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et"
2021.emnlp-main.299,D19-1001,0,0.255904,"actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant e"
2021.emnlp-main.299,2020.emnlp-main.120,0,0.0111371,"eak the rule text into clauselike units called elementary discourse units (EDUs) to extract the in-line rule conditions from the rule texts. Embedding We employ pre-trained language model (PrLM) model as the backbone of the encoder. As shown in the figure, the input of our model includes rule document which has already be parsed into EDUs with explicit discourse relation tagging, user initial question, user scenario and the dialog history. Instead of inserting a [CLS] token before each rule condition to get a sentence-level representation, we use [RULE] which is proved to enhance performance (Lee et al., 2020). Formally, the sequence is organized as: {[RULE] EDU0 [RULE] EDU1 [RULE] EDUk [CLS] Question [CLS] Scenario [CLS] History [SEP]}. Then we feed the sequence to the PrLM to obtain the contextualized representation. Interaction To explicitly model the discourse structure among the rule conditions, we first annotate the discourse relationships between the rule conditions and employ a relational graph convolutional network following Ouyang et al. (2021) by regarding the rule conditions as the vertices. The graph is formed as a Levi graph (Levi, 1942) that regards the relation edges as additional v"
2021.emnlp-main.299,2021.findings-acl.279,1,0.782265,"e encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF m"
2021.emnlp-main.299,H90-1020,0,0.0999002,"Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018):"
2021.emnlp-main.299,2020.emnlp-main.550,0,0.10908,"R. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF method and the latest dense passage retrieval model (Karpukhin et al., 2020). and a generator G(·, ·) on {R, Us , Uq , C} for question generation. 4 Model Our model is composed of three main modules: retriever, encoder, and decoder. The retriever is employed to retrieve the related rule texts for the given user scenario and question. The encoder takes the tuple {R, Us , Uq , C} as the input, encodes the elements into vectors and captures the contextualized representations. The decoder makes a decision or generates a question once the decision is “inquiry”. Figure 1 overviews the model architecture, we will elaborate the details in the following part. 4.1 Retrieval To"
2021.emnlp-main.299,P18-2124,0,0.0621738,"Missing"
2021.emnlp-main.299,D16-1264,0,0.0496147,"Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according"
2021.emnlp-main.299,2020.emnlp-main.589,0,0.0933312,"ques. BART-base *relations obtained by tagging model &lt;rule&gt; user scenario e1 Document k ... e2 e2 e3 ... e4 e3 ... ...... Rule Conditions Tagging Double-channel Decoder relations e1 e3 e1 Comment us GCN layer e 4 Document 1 ...... r3 e4 &lt;rule&gt; ... Rule e5 &lt;/s&gt; ... Span e6 &lt;/s&gt; ... BART-base &lt;rule&gt; EDU0 &lt;rule&gt; EDU1 &lt;rule&gt; EDU... &lt;s&gt; Scen. &lt;/s&gt; Ques. &lt;/s&gt; QA... &lt;/s&gt; Figure 2: The overall structure of our model O SCAR. The left part introduces the retrieval and tagging process for rule documents, which is then fed into the encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, t"
2021.emnlp-main.299,Q19-1016,0,0.115503,"hallenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al., 2018; Qin et al., 2"
2021.emnlp-main.299,D18-1299,0,0.0339546,"Missing"
2021.emnlp-main.299,D18-1233,0,0.202927,"tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing"
2021.emnlp-main.299,Q19-1014,0,0.0881508,"logue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al.,"
2021.emnlp-main.299,2020.acl-demos.30,0,0.237205,"u.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue con"
2021.emnlp-main.299,C18-1317,1,0.893958,"Missing"
2021.emnlp-main.299,P19-1223,0,0.0225589,"Missing"
2021.emnlp-main.299,C18-2024,1,0.833381,"eriments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an"
2021.naacl-main.311,D18-1549,0,0.0207702,"adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-re"
2021.naacl-main.311,P07-2045,0,0.0126746,"gual WMT news crawl datasets3 for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr–En, newstest2016 for Ro–En, and newstest2018 for Et–En. Language En Fr Ro Et Sentences Words 50.00M 50.00M 8.92M 3.00M 1.15B 1.19B 207.07M 51.39M Table 2: Statistics of the monolingual corpora. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). To clean the data, we only applied the Moses script clean-corpus-n.perl to remove lines from the monolingual data containing more than 50 words. We used a shared vocabulary for all language pairs, with 60,000 subword tokens based on BPE (Sennrich et al., 2016b). learning rate was 0.0001, β1 = 0.9, and β2 = 0.98. We trained a specific cross-lingual language model for each different training dataset. The language model was used to initialize the full parameters of the UNMT system. Eight V100 GPUs were used to train all UNMT models. We used the casesensitive 4-gram BLEU score computed by the mu"
2021.naacl-main.311,P16-1009,0,0.302263,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P16-1162,0,0.856645,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P95-1026,0,0.892071,"∗ (Y |X) logPM U (X|Y ) logPM U (Y |X), U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancin"
2021.naacl-main.311,D16-1160,0,0.0127253,", U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancing the performance of the translation, even 3"
2021.naacl-main.311,P19-1119,1,0.628784,"raining data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline)"
2021.naacl-main.311,2020.acl-main.324,1,0.817815,"improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Ta"
2021.naacl-main.311,N19-1120,0,0.040071,"Missing"
2021.naacl-main.438,W15-4319,0,0.048961,"Missing"
2021.naacl-main.438,2020.lrec-1.773,0,0.0611393,"Missing"
2021.naacl-main.438,P11-1038,0,0.0383248,"al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated catego"
2021.naacl-main.438,W15-4313,0,0.0214366,"ion of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et a"
2021.naacl-main.438,D14-1011,0,0.0999202,"emmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has th"
2021.naacl-main.438,W04-3230,0,0.164412,"Missing"
2021.naacl-main.438,D08-1108,0,0.0604526,"al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English raw tweets using noise insertion operations to achieve comparable for “I don’t know”), repetition (e.g., “soooo” for “so”), and phonetic transformation (e.g., “hackd” performance without manually annotated data to an existing LN system. for “hacked”). For Chinese, Li and Yarowsky (2008) classified informal words in Chinese webpages into four 19 types: homophone (informal words with similar Pinyin pronunciation is shown in “hi”. 5539 7 Conclusion We presented a publicly available Japanese UGT corpus annotated with morphological and normalization information. Our corpus enables the performance comparison of existing and future systems and identifies the main remaining issues of MA and LN of UGT. Experiments on our corpus demonstrated the limited performance of the existing systems for non-general words and non-standard forms mainly caused by two types of difficult examples: co"
2021.naacl-main.438,P11-2093,0,0.0259792,"entence IDs and annotation information, including word boundaries, POS, lemmas, standard forms of non-standard word tokens, and word categories. We will release the annotation information that enables BCCWJ applicants to replicate the full BQNC data from the original BCCWJ data.3 Using the BQNC, we evaluated two existing Japanese morphological analysis (MA) is a fundamental and important task that involves word segmentation, part-of-speech (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available a"
2021.naacl-main.438,D15-1211,0,0.0186263,"classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English ra"
2021.naacl-main.438,I17-1094,0,0.0705833,"o¯ ‘oh’ and サラサラ〜 was normalized to サラサラ sarasara ‘smoothly’). However, we assessed these as errors based on our criterion that interjections have no (non-)standard forms and the BCCWJ guidelines that regards onomatopoeia with and without long sound insertion as different lemmas. malization errors into two types: complicated variant forms and unknown words of specific vocabulary types such as emoticons and neologisms/slang. The effective use of linguistic resources may be required to build more accurate systems, for example, discovering variant form candidates from large raw text similar to (Saito et al., 2017), and constructing/using term dictionaries of specific vocabulary types. 6 Related Work UGT Corpus for MA and LN Hashimoto et al. (2011) developed a Japanese blog corpus with morphological, grammatical, and sentiment information, but it contains only 38 non-standard forms and 102 misspellings as UGT-specific examples. Osaki et al. (2017) constructed a Japanese Twitter corpus annotated with morphological information and standard word forms. Although they published tweet URLs along with annotation information, we could only restore parts of sentences because of the deletion of the original tweet"
2021.naacl-main.438,P19-3032,0,0.0256965,"Missing"
2021.naacl-main.438,L18-1109,0,0.0396716,"Missing"
2021.naacl-main.438,I13-1015,0,0.0257161,"irs of formal and informal words on Twitter. variants. Ikeda et al. (2010)’s classification of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “befo"
2021.naacl-main.438,D13-1007,0,0.0741565,"Missing"
2021.naacl-main.438,C14-1167,0,0.0805651,"anese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has the disadvantage that"
2021.naacl-main.438,I13-1019,0,0.0948969,"h (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to di"
2021.wat-1.4,D17-1098,0,0.0211247,"(Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To allev"
2021.wat-1.4,N19-1090,0,0.0512166,"Missing"
2021.wat-1.4,D16-1162,0,0.0114004,"of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the"
2021.wat-1.4,C18-1111,0,0.0241363,"ong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017"
2021.wat-1.4,J10-4005,0,0.0298736,"ese sampled positions to achieve the goal of restrict translation with soft constraints on the model: Convk (H) = DepthConvk (HW V )W out DepthConvk (H) = k X Softmax( d X Q Wj,c Hi,c ) c=1 j=1  · Hi+j−d k+1 e,c , 2 Conv(H) = K X i=1 LSCC = − exp (αi ) Convki (X) n P exp (αj ) (1 + γ 1(yi ∈ C)) i=1  logP (yi |X; C; y<i ; θ) , j=1 where 1(·) is the indicator function and γ is the penalty factor. in which DepthConv(·) is the depth convolution structure proposed in Wu et al. (2019). And Pointwise(·) refers to a position-wise feed-forward network: 2.4 Lexically Constrained Decoding Beam search (Koehn, 2010) is a common approximate search algorithm for sequence generation task. Lexically constrained decoding is a modification to the beam search algorithm, which is proposed to enforce hard constraints that force a given constrained sequence to appear in the generated sequence. Specifically, beam search maintains a beam Bt on time step t, which contains only the b most likely partial sequences, where b is known as the beam size. The beam Bt is updated by retaining the b most likely sequences in the candidate set Et generated by considering all possible next word predictions: Pointwise(H) = max(0, H"
2021.wat-1.4,N03-1017,0,0.112871,"Missing"
2021.wat-1.4,W19-6721,0,0.0563198,"Missing"
2021.wat-1.4,D18-1262,1,0.831361,"ECT En→Ja test sets. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCra"
2021.wat-1.4,D19-1538,1,0.835069,"ts. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et"
2021.wat-1.4,D18-1458,0,0.0396717,"Missing"
2021.wat-1.4,2020.wmt-1.22,1,0.828689,"Missing"
2021.wat-1.4,2020.findings-emnlp.371,1,0.890157,"Missing"
2021.wat-1.4,K19-2004,1,0.848833,"hat the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and W"
2021.wat-1.4,N18-1119,0,0.0866255,"usion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To alleviate the shortcomings in constrained decoding, Post and Vilar (2018) proposed a new constrained decoding algorithm with a claimed complexity of O(1) in the number of constraints - dynamic beam allocation which allocates the slots in a fixed-size beam. However, their approach still processes sentence constraints sequentially rather than batch processing, limiting the GPU’s parallel processing capabilities. Based on Post and Vilar (2018), a vectorized dynamic beam allocation approach was proposed in Hu et al. (2019), which which vectorThis paper describes our system (Team ID: nictrb) for participating in the WAT’21 restricted machine translation task. In our sub"
2021.wat-1.4,2020.findings-emnlp.398,1,0.72695,"evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and Wiki Titles v2 datas"
2021.wat-1.8,2020.acl-main.703,0,0.0355193,"ally pretrained the model on the ﬁve languages in the NICT-SAP task. The corpus for additional pretraining was extracted from Wikipedia dump ﬁles as follows. Unlike the XLM models (Lample and Conneau, 2019), which were also pretrained using Wikipedia corpora, we divided each article into sentences in our corpus, to train the sentence permutation task. Additionally, we applied sentence ﬁltering to clean each language. In this section, we brieﬂy review the pretrained mBART model (Liu et al., 2020). The mBART model is a multilingual model of bidirectional and auto-regressive Transformers (BART; (Lewis et al., 2020)). The model is based on the encoder-decoder Transformer (Vaswani et al., 2017), in which the decoder uses an autoregressive method (Figure 1). Two tasks of BART are trained in the mBART model. One is the token masking task, which restores masked tokens in input sentences. The other is the sentence permutation task, which predicts the original order of permuted sentences. Both tasks learn using monolingual corpora. To build multilingual models based on BART, mBART supplies language tags (as special tokens) at the tail of the encoder input and head of the decoder input. Using these language tag"
2021.wat-1.8,W19-6601,1,0.847731,"Missing"
2021.wat-1.8,2020.tacl-1.47,0,0.262755,"roduction In this paper, we present the NICT system (NICT2) that we submitted to the NICT-SAP shared task at the 8th Workshop on Asian Translation (WAT2021) (Nakazawa et al., 2021). Because the NICTSAP task expects to perform translations with little parallel data, we developed a system to improve translation quality by applying the following models and techniques. Pretrained model: An encoder-decoder model pretrained using huge monolingual corpora was used. We used a multilingual bidirectional auto-regressive Transformer (mBART) (i.e., multilingual sequence-to-sequence denoising autoencoder (Liu et al., 2020)) model, which supports 25 languages. Because it includes English and Hindi, but does not include Indonesian, Malay, and Thai, we expanded it to include the unsupported languages and additionally pretrained it on these ﬁve languages.1 2 NICT-SAP Shared Task The NICT-SAP shared task was to translate text between English and four languages, that is, Hindi (Hi), Indonesian (Id), Malay (Ms), and Thai (Th), for which the amount of data in parallel corpora is relatively low. The task contained two domains. The data in the Asian Language Translation (ALT) domain (Thu et al., 2016) consisted of transl"
2021.wat-1.8,N19-4009,0,0.0546528,"Missing"
2021.wat-1.8,P02-1040,0,0.11082,"Therefore, we created as many domain models as the number of domains. Other Options 5 Experiments We ﬁne-tuned the pretrained model using the NICT-SAP parallel corpora shown in Table 1. We also used Transformer base models (six layers, the model dimension of 512 on 8 heads) for comparison without the pretrained model. In addition to the effect of the pretrained models, we investigated the effects of multilingual models and domain adaptation. 4.2.1 The models and methods described above were ﬁne-tuned and tested using the hyperparameters in Table 4. Tables 5 and 6 show the ofﬁcial BLEU scores (Papineni et al., 2002) for the test set in the ALT and IT domains, respectively. Similar results were obtained on the development sets, but they were omitted in this paper. We submitted the results using the pretrained mBART model, which were good on the development sets, on average. The results are summarized as follows; Multilingual Models Similar to the multilingual training of mBART, the multilingual model translated all the language pairs using one model by supplying source and target language tags to parallel sentences. By contrast, bilingual models were trained using the corpora of each language pair. When w"
2021.wat-1.8,P16-1162,0,0.0180532,"ger than the target/source sentences if they had over 20 tokens. 3 #Sentences 7,000,000 (*1) 1,968,984 6,997,907 2,723,230 2,233,566 (*2) model can learn multiple languages. The published pretrained mBART model2 consists of a 12-layer encoder and decoder with a model dimension of 1,024 on 16 heads. This model was trained on 25 languages in the Common Crawl corpus (Wenzek et al., 2019). Of the languages for the NICT-SAP task, English and Hindi are supported by the published mBART model, but Indonesian, Malay, and Thai are not supported. The tokenizer for the mBART model uses bytepair encoding (Sennrich et al., 2016) of the SentencePiece model (Kudo and Richardson, 2018)3 . The vocabulary size is 250K subwords. 4 Our System 4.1 Language Expansion/Additional Pretraining of mBART mBART Model As described above, the published mBART model does not support Indonesian, Malay, and Thai. We expanded the mBART model to support these three languages, and additionally pretrained the model on the ﬁve languages in the NICT-SAP task. The corpus for additional pretraining was extracted from Wikipedia dump ﬁles as follows. Unlike the XLM models (Lample and Conneau, 2019), which were also pretrained using Wikipedia corpor"
2021.wat-1.8,L16-1249,1,0.829096,"oising autoencoder (Liu et al., 2020)) model, which supports 25 languages. Because it includes English and Hindi, but does not include Indonesian, Malay, and Thai, we expanded it to include the unsupported languages and additionally pretrained it on these ﬁve languages.1 2 NICT-SAP Shared Task The NICT-SAP shared task was to translate text between English and four languages, that is, Hindi (Hi), Indonesian (Id), Malay (Ms), and Thai (Th), for which the amount of data in parallel corpora is relatively low. The task contained two domains. The data in the Asian Language Translation (ALT) domain (Thu et al., 2016) consisted of translations obtained from WikiNews. The ALT is not supported by either the mBART model or mBART-50 models. Therefore, we applied additional pretraining to the mBART model. 1 The mBART-50 model (Tang et al., 2020) supports 50 languages including Indonesian and Thai. However, Malay 90 Proceedings of the 8th Workshop on Asian Translation, pages 90–95 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics Domain ALT IT Set Train Dev Test Train Dev Test En-Hi 18,088 252,715 2,016 2,073 En-Id En-Ms 18,087 18,088 1,000 1,018 158,200 504,856 2,023"
2021.wat-1.8,tiedemann-2012-parallel,0,0.0561721,"aximum number of the other languages. (*2) Sentences in Thai were detected using an in-house sentence splitter. Table 1: Data sizes for the NICT-SAP task after ﬁltering. data is a multilingual parallel corpus, that is, it contains the same sentences in all languages. The training, development, and test sets were provided from the WAT organizers. The data in the IT domain consisted of translations of software documents. The WAT organizers provided the development and test sets (Buschbeck and Exel, 2020). For the training set, we obtained GNOME, KDE, and Ubuntu sub-corpora from the OPUS corpus (Tiedemann, 2012). Therefore, the domains for the training and dev/test sets were not identical. The data sizes are shown in Table 1. There were fewer than 20K training sentences in the ALT domain. Between 73K and 504K training sentences were in the IT domain. Note that there were inadequate sentences in the training sets. We ﬁltered out translations that were longer than 512 tokens, or where source/target sentences were three times longer than the target/source sentences if they had over 20 tokens. 3 #Sentences 7,000,000 (*1) 1,968,984 6,997,907 2,723,230 2,233,566 (*2) model can learn multiple languages. The"
2021.wnut-1.9,2021.findings-acl.84,0,0.0123695,"ons that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sentence pairs by hand-crafted rules to convert standard forms to nonstandard forms. Zhang et al. (2017) synthesized training data for Chinese informal word detection by random substitution of formal words in segmented sentences by informal words in a dictionary of formal-informal word pairs. To train statistical and neural MT models for Turkish text normalization, Çolako˘glu et al. (2019) generated a pseudo-parallel corpus where nonstandard words in original twee"
2021.wnut-1.9,2021.naacl-main.438,1,0.897268,"ers. For this reason, the problem of Japanese lexical normalization has been solved by predicting word boundaries, part-of-speech (POS) tags, and normalized word forms simultaneously (Sasano et al., 2013; Saito et al., 2014). Similarly to previous work, we tackle the joint task comprising Japanese word Segmentation, POS tagging, and lexical Normalization (SPN). A critical problem in lexical normalization is the lack of labeled data. Manual annotation of normalized forms is a time-consuming task; therefore, the size of the available annotated corpora is quite small (Kaji and Kitsuregawa, 2014; Higashiyama et al., 2021). A prospective solution to this problem 2 Task Definition As shown in Table 2, a training instance for the SPN task is defined as a pair, comprising a sentence x = (x1 , . . . , xn ) and its label sequence t = {(fj , lj , pj , Sj )}m j=1 , where n and m (≤ n) are the numbers of characters and words in x, fj and lj are the indexes of the first and last character in j-th word wj , and pj is the POS tag of wj . The set of standard forms Sj is equal to the empty set ∅ when wj is a standard form, whereas Sj consists of one or more standard forms when wj is a nonstandard form. A system is required"
2021.wnut-1.9,2020.findings-emnlp.111,0,0.0245681,"(2008) extracted formal-informal word pairs using websearched sentences defining informal words and a conditional log-linear ranking model. Wang and Text Editing. Text editing methods have also been applied to English lexical normalization. Chrupała (2014) used character embeddings based on a recurrent neural network LM and trained CRFs to predict character-level edit operations. Min and Mott (2015) proposed an LSTM-based model to perform word-level edit operations that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sente"
2021.wnut-1.9,P82-1020,0,0.785309,"Missing"
A00-1006,1997.tmi-1.21,0,0.057569,"Missing"
A00-1006,P98-2185,0,0.0676193,"Missing"
A00-1006,1999.mtsummit-1.34,1,0.898011,"Missing"
A00-1006,W97-0400,0,0.313823,"Missing"
A00-1006,C96-1070,0,0.0632717,"Missing"
A00-1006,W97-0403,0,0.0435074,"Missing"
A00-1006,C98-2180,0,\N,Missing
A00-1006,C98-2126,0,\N,Missing
A00-1006,1993.mtsummit-1.8,0,\N,Missing
A94-1017,C92-2097,1,0.889257,"Missing"
A94-1017,P07-2007,1,0.876536,"Missing"
A94-1017,1991.mtsummit-papers.18,0,0.0607863,"nts of spoken language translation. 2 TDMT and its Cost Analysis 2.1 O u t l i n e o f T D M T In TDMT, transfer knowledge is the primary knowledge, which is described by an example-based framework (Nagao, 1984). A piece of transfer knowledge describes the correspondence between source language expressions (SEs) and target language expressions (TEs) as follows, to preserve the translational equivalence: SE 1 Introduction Research on speech translation that began in the mid-1980s has been challenging. Such research has resulted in several prototype systems (Morimoto et al., 1993; Kitano, 1991; Waibel et al., 1991). Speech translation consists of a sequence of processes, i.e., speech recognition, spoken language translation and speech synthesis. Each process must be accelerated in order to achieve real-time response. This paper focuses on the second process, spoken language translation, which requires (1) an accurate translation and (2) a real-time response. We have already proposed a model that utilizes examples and translates a sentence by combining pieces of transfer knowledge, i.e., target language expressions that correspond to source language expressions that cover the sentence jointly. The model"
abekawa-etal-2010-community,abekawa-kageura-2008-constructing,1,\N,Missing
abekawa-etal-2010-community,P01-1008,0,\N,Missing
abekawa-etal-2010-community,P07-2002,1,\N,Missing
abekawa-etal-2010-community,I08-1032,1,\N,Missing
abekawa-etal-2010-community,2007.mtsummit-papers.60,1,\N,Missing
akiba-etal-2004-incremental,W02-0715,1,\N,Missing
akiba-etal-2004-incremental,takezawa-etal-2002-toward,1,\N,Missing
C02-1017,2001.mtsummit-papers.53,0,\N,Missing
C02-1017,C00-2135,0,\N,Missing
C02-1017,C00-1014,0,\N,Missing
C02-1017,C96-1023,0,\N,Missing
C02-1017,W01-1401,1,\N,Missing
C02-1017,C92-2101,0,\N,Missing
C02-1017,C94-1091,0,\N,Missing
C02-1017,P91-1024,1,\N,Missing
C02-1017,P93-1004,0,\N,Missing
C02-1017,C96-1078,0,\N,Missing
C02-1017,takezawa-etal-2002-toward,1,\N,Missing
C02-1050,J93-2003,0,0.0164084,"English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation. 1 Introduction The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text. According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned transl"
C02-1050,2001.mtsummit-papers.22,0,0.0203283,"the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. This paper p"
C02-1050,P01-1030,0,0.286203,"of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. This paper presents two decoding methods, one is the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of a sentence. The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one. The experimental results of Japanese and En"
C02-1050,J99-4005,0,0.0380221,"ated from the cept ( j0 ). • NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f . For details, refer to Brown et al. (1993). 2.2 Search Problem The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(f|e) = P a P(f, a|e) and P(e). For the space of a is extremely large, |a|l+1 , where the l is the output length, an approximation of P(f|e) &apos; P(f, a|e) is used when exploring the possible candidates of translation. This problem is known to be NP-Complete (Knight, 1999), for the re-ordering property in the model further complicates the search. One of the solution is the left-to-right generation of output by consuming input words in any-order. Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al. (2001), though they all based on almost linearly Translation Model Lexical Q Model t( f j |ei ) Head Non-Head Fertility Q Model n(φi |ei ) Distortion Model Q d ( j − cρi"
C02-1050,W01-1408,0,0.341697,"According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffe"
C02-1050,1999.mtsummit-1.34,1,0.903591,"Missing"
C02-1050,takezawa-etal-2002-toward,1,0.793701,"he quality of translation. The similar statement can hold for postfix languages, such as Japanese, where emphasis is placed around the end of a sentence. For such languages, right-to-left decoding will be suitable but left-toright decoding will degrade the quality of translation. The bidirectional decoding is expected to take the benefits of both of the directions, and will show the best results in any kind of languages. 4 Experimental Results The corpus for this experiment consists of 172,481 bilingual sentences of English and Japanese extracted from a large-scale travel conversation corpus (Takezawa et al., 2002). The statistics of the corpus are shown in Table 1. The database was split into three parts: a training set of 152,183 sentence pairs, a validation set of 10,148, and a test set of 10,150. The translation models, both for the Japanese-toEnglish (J-E) and English-to-Japanese (E-J) translation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity. In modeling IBM Model 4, POSs were used as word classes. From the viterbi alignments of the training corpus, A list of possible insertion of zero fertility words w"
C02-1050,C00-2123,0,0.408326,"el model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the"
C02-1050,P97-1047,0,0.122269,"Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar probl"
C02-1050,2002.tmi-papers.20,1,0.77024,"exity. In modeling IBM Model 4, POSs were used as word classes. From the viterbi alignments of the training corpus, A list of possible insertion of zero fertility words were extracted with frequency more than 10, around 1,300 sequences of words for both of the JE and E-J translations. The test set consists of 150 Japanese sentences varying by the sentence length of 6, 8 and 10. The translation was carried out by three decoding methods:left-to-right, right-toleft and bidirectional one. The translation results were evaluated by worderror-rate (WER) and position independent worderror-rate (PER) (Watanabe et al., 2002; Och et al., 2001). The WER is the measure by penalizing insertion/deletion/replacement by 1. The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection. It has been also evaluated by subjective evaluation (SE) with the criteria ranging from A(perfect) to D(nonTable 1: Statistics on a travel conversation corpus Japanese English # of sentences 172,481 # of words 1,186,620 1,005,080 vocabulary size 22,801 15,768 6.88 5.83 avg. sentence length 26.16 36.92 3-gram perplexity Table 3: Comparison of"
C02-1076,J93-2003,0,0.0139183,"ection system is much better than that of each component MT system. Conventional approaches to the selection problem include methods (Callison-Burch and Flournoy, 2001; Kaki et al., 1999) that automatically select the output assigned the highest probability P(t) (hereafter, LM-score), according to a language model (LM) for the translation target language. As a preliminary experiment, the authors applied this LM-score to selecting the best among the outputs from the three J-E MT systems. In order to make a comparison, the authors also used a score based on a translation model (TM) called IBM4 (Brown et al., 1993) (hereafter, TM-score) and a score based on the product of the TM-score and the LM-score (hereafter, TM3LM-score) to select the best output. Table 1 shows the results of this preliminary experiment. The oating number indicates the di erence between the performance for Rank A of each selection system and that of D3 (the best MT system, i.e., with the highest performance for Rank A). The LMscore and TM-score based selections did not Ideal Selection System TDMT HPAT 80 70 60 50 40 30 20 I-subj computer-of system engineer am I&apos;m a computer engineer.&quot; (2) I&apos;m a computer systems engineer. (TRN1) (3"
C02-1076,2001.mtsummit-papers.12,0,0.419313,"ntences ranked as A or B to the total number of sentences translated by each MT system (hereafter, performance for Rank A+B). The right-hand group of bars indicates the ratio of the number of sentences ranked as A, B, or C to the total number of sentences translated by each MT system (hereafter, performance for Rank A+B+C). The black bars indicate the performance of the ideal selection system. As Figures 2 and 3 show, the performance of the ideal J-E and E-J selection system is much better than that of each component MT system. Conventional approaches to the selection problem include methods (Callison-Burch and Flournoy, 2001; Kaki et al., 1999) that automatically select the output assigned the highest probability P(t) (hereafter, LM-score), according to a language model (LM) for the translation target language. As a preliminary experiment, the authors applied this LM-score to selecting the best among the outputs from the three J-E MT systems. In order to make a comparison, the authors also used a score based on a translation model (TM) called IBM4 (Brown et al., 1993) (hereafter, TM-score) and a score based on the product of the TM-score and the LM-score (hereafter, TM3LM-score) to select the best output. Table 1"
C02-1076,C96-1070,0,0.0120845,"m of automatically selecting the best among outputs from multiple machine translation (MT) systems (Figure 1). In combinations of multiple MT systems, some component MT systems can translate a source sentence well while others cannot well. In such a case, correct selection of the best can obviously boost performance. ATR has been developing such multiple MT systems, including three Japanese-to-English (J-E) MT systems: TDMT (Furuse and Iida, Input MTa MTb MTc Outputa Outputb Outputc 1996), D3 (Sumita, 2001), and SMT (Watanabe et al., 2002), and three English-to-Japanese (EJ) MT systems: TDMT (Furuse and Iida, 1996), HPAT (Imamura, 2002), and SMT (Watanabe et al., 2002). In order to evaluate each MT system, the MT outputs were manually assigned one of four ranks1 , A, B, C, and D, by native speakers of the target language. The ideal selection for J-E MT systems is the highestranked outputs from the three J-E MT systems: TDMT, D3, and SMT. The ideal selection for E-J MT systems is the highest-ranked outputs from the three E-J MT systems: TDMT, HPAT, and SMT. Figure 2 shows the individual performances of the three J-E MT systems and the ideal selection system derived from their combination. Figure 3 shows"
C02-1076,2002.tmi-papers.9,0,0.0817741,"he best among outputs from multiple machine translation (MT) systems (Figure 1). In combinations of multiple MT systems, some component MT systems can translate a source sentence well while others cannot well. In such a case, correct selection of the best can obviously boost performance. ATR has been developing such multiple MT systems, including three Japanese-to-English (J-E) MT systems: TDMT (Furuse and Iida, Input MTa MTb MTc Outputa Outputb Outputc 1996), D3 (Sumita, 2001), and SMT (Watanabe et al., 2002), and three English-to-Japanese (EJ) MT systems: TDMT (Furuse and Iida, 1996), HPAT (Imamura, 2002), and SMT (Watanabe et al., 2002). In order to evaluate each MT system, the MT outputs were manually assigned one of four ranks1 , A, B, C, and D, by native speakers of the target language. The ideal selection for J-E MT systems is the highestranked outputs from the three J-E MT systems: TDMT, D3, and SMT. The ideal selection for E-J MT systems is the highest-ranked outputs from the three E-J MT systems: TDMT, HPAT, and SMT. Figure 2 shows the individual performances of the three J-E MT systems and the ideal selection system derived from their combination. Figure 3 shows the individual perform"
C02-1076,P00-1056,0,0.0630269,"xpression (BE) corpus (Takezawa et al., 2002), which is split into three parts: a training set of 125,537 sentence pairs, a veri cation set of 9,872 pairs, and a test set of 10,023 pairs. The full corpus C in training translation target language model and translation model is the training set. Ten subsets of the full corpus were Preparing multiple RTs for each component MT system enables the second method to be extended so as to select the best output according to the multiple comparison. 5 used for the rst proposed method. The translation model and language model are learned by using GIZA++ (Och and Ney, 2000) and the CMU-Cambridge Toolkit (Clarkson and Rosenfeld, 1997), respectively. The translation model is learned from IBM 1 to 4, including the HMM model, as suggested by Och and Ney (2000), and its training loop was terminated when the perplexity for the validation set indicated the lowest scores. The word classes used in TM learning are the part-of-speech (POS) classes in TDMT. The P-value used for the multiple comparison test is 0.05. Four sets of about ve hundred pairs of English and Japanese sentences were randomly selected from the test set. The English sentences in the four sets were trans"
C02-1076,W01-1401,1,0.785248,"Missing"
C02-1076,takezawa-etal-2002-toward,1,0.814887,"i 3 Experimental Comparison 3.1 Experimental Method The authors evaluated the proposed methods in order to answer the following question: Which selection system improves performance best in comparison with that of the best MT system i.e. the MT systems with the highest performance as shown in Figures 2 and 3? In order to answer the above question, the authors used a set of three J-E component MT systems (TDMT, D3, and SMT) and a set of three E-J component MT systems (TDMT, HPAT, and SMT). Bilingual English and Japanese data were from ATR broad-coverage bilingual basic expression (BE) corpus (Takezawa et al., 2002), which is split into three parts: a training set of 125,537 sentence pairs, a veri cation set of 9,872 pairs, and a test set of 10,023 pairs. The full corpus C in training translation target language model and translation model is the training set. Ten subsets of the full corpus were Preparing multiple RTs for each component MT system enables the second method to be extended so as to select the best output according to the multiple comparison. 5 used for the rst proposed method. The translation model and language model are learned by using GIZA++ (Och and Ney, 2000) and the CMU-Cambridge Tool"
C02-1076,2002.tmi-papers.20,1,0.903029,"Missing"
C04-1015,C02-1076,1,0.882262,"own et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (2002) achieved selection of the best translation among those output by multiple MT engines. This paper presents an example-based MT method based on syntactic transfer, which selects the best translation by using models of statistical MT. This method is roughly structured using two modules (Figure 2). One is an example-based syntactic transfer module. This module constructs Transfer Rules Example-based Syntactic Transfer Statistical Generation Thesaurus Translation Dictionary Preprocessing Postprocessing Input Sentence Output Sentence Translation Model Language Model Figure 2: Structure of Proposed"
C04-1015,J93-2003,0,0.00358261,"g case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (200"
C04-1015,P01-1030,0,0.0451743,"shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is li"
C04-1015,P03-1057,1,0.874225,"ds involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is limited by the example-based transfer, so we have to improve this stage in order to further improve MT quality. For instance, example-based MT can be improved by applying an optimization algorithm that uses an automatic evaluation of MT quality (Imamura et al., 2003). 6 Conclusions This paper demonstrated that example-based MT can be improved by incorporating it in models of statistical MT. The example-based MT used in this paper is based on syntactic transfer, so word reordering is achieved in the transfer module. Using this feature, the best translation was selected by using only a lexicon model and an n-gram language model. In addition, bottom-up generation achieved faster translation speed by using the tree structure of the target sentence. Acknowledgements The authors would like to thank Kadokawa Publishers, who permitted us to use the hierarchy of R"
C04-1015,2002.tmi-papers.9,1,0.88153,"because the example-based transfer generates syntactically correct candidates for the most appropriate translation. The rest of this paper is organized as follows: Section 2 describes the example-based syntactic transfer, Section 3 describes the statistical generation, Section 4 evaluates an experimental system that uses this method, and Section 5 compares other hybrid methods of example-based and statistical MT. 2 Example-based Syntactic Transfer The example-based syntactic transfer used in this paper is a revised version of the Hierarchical Phrase Alignment-based Translator (HPAT, refer to (Imamura, 2002)). This section gives an overview with an example of Japanese-to-English machine translation. 2.1 Transfer Rules Transfer rules are automatically acquired from bilingual corpora by using hierarchical phrase alignment (HPA; (Imamura, 2001)). HPA parses bilingual sentences and acquires corresponding syntactic nodes of the source and target sentences. The transfer rules are created from their node correspondences. Figure 3 shows an example of the transfer rules. Variables, such as X and Y in Figure 3, denote non-terminal symbols that correspond between source and target grammar. The set of transf"
C04-1015,P01-1050,0,0.0174084,"essary for improving MT quality. Finally, focusing on translation speed, the worst time for Bottom-up generation was dramatically faster than that for All Search. Bottom-up generation effectively uses shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT"
C04-1015,J03-1002,0,0.00490061,"sic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). Compared Methods We compared the following four methods. • Baseline (Example-based Transfer only) The best translation that had the same semantic distance was randomly selected from the S &lt;s&gt; bus will leave at 11 o’clock TM: -7.13 LM: -14.30 bus will start at 11 o’clock TM: -8.03 LM: -13.84 &lt;/s&gt; the bus will leave at 11 o’clock TM: -7.13 LM: -13.54 XNP bus TM: -0.07 LM: -0.0 the bus TM: -0.07 LM: -1.94 a bus TM: -0.07 LM: -2.11 n-best YVP leaves at 11 o’clock TM: -"
C04-1015,P02-1040,0,0.0702122,"Missing"
C04-1015,P91-1024,1,0.748447,"ock NP -&gt; X6 11 11 Figure 4: Example of Syntactic Transfer Process (Bold frames are syntactic nodes mentioned in text) that do not correspond between the source and target sentences (e.g., the determiner ‘a’ or ‘the’) are automatically inserted or eliminated by the target grammar (cf. NP node represented by a bold frame). Namely, transfer rules work in a manner similar to the functions of distortion, fertility, and NULL in IBM models. 2.3 Usage of Source Examples Example-based transfer utilizes the source examples for disambiguation of mapping and parsing. Specifically, the semantic distance (Sumita and Iida, 1991) is calculated between the source examples and the headwords of the input sentence, and the transfer rules that contain the nearest example are used to construct the target tree structure. The semantic distance between words is defined as the distance from the leaf node to the most specific common abstraction (MSCA) in a thesaurus (Ohno and Hamanishi, 1984). For example, if the input phrase “ie (home) ni kaeru (return)” is given, Rules 1 to 3 in Figure 3 are used for the syntactic transfer, and three target nodes are generated without any disambiguation. However, when we compare the source exa"
C04-1015,takezawa-etal-2002-toward,1,0.73883,"sentence and end-of-sentence, and the n-best list is re-sorted. As a result, the translation “The bus will leave at 11 o’clock” is obtained from the tree of Figure 4. Bottom-up generation calculates the probabilities of shared nodes only once, so it effectively uses tree information. 4 Evaluation In order to evaluate the effect when models of statistical MT are integrated into example-based MT, we compared various methods that changed the statistical generation module. 4.1 Experimental Setting Bilingual Corpus The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram mode"
C04-1015,2003.mtsummit-papers.53,0,0.0163217,"is determined from the product of the translation model and the language model in the same manner as statistical MT. In other words, when F and E denote the channel target and channel source sequence, ˆ that satrespectively, the output word sequence E isfies the following equation is searched for. Set Name Training Test ˆ = argmax P (E|F ) E Item # of Sentences # of Words # of Sentences # of Words English Japanese 152,170 886,708 1,007,484 510 2,973 3,340 E = argmax P (E)P (F |E). E (1) We only utilize the lexicon model as the translation model in this paper, similar to the models proposed by Vogel et al. (2003). Namely, when f and e denote the channel target and channel source word, respectively, the translation probability is computed by the following equation. P (F |E) =  j t(fj |ei ). (2) i The IBM models include other models, such as fertility, NULL, and distortion models. As we described in Section 2.2, the quality of machine translation is maintained using only the lexicon model because syntactical correctness is already preserved by example-based transfer. For the language model, we utilize a standard word n-gram model. 3.2 Bottom-up Generation We can construct word graphs by serializing th"
C04-1015,2003.mtsummit-papers.54,1,0.938164,"ngual corpus as a database and retrieves examples that are similar to an input sentence. Then, a translation is generated by modifying the target part of the examples while referring to translation dictionaries. Most example-based MT systems employ phrases or sentences as the unit for examples, so they can translate while considering case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solut"
C04-1017,J96-1002,0,0.00283964,"ions can provide an adequate translation of the complete input is relatively high. For example, the input sentence, ”This is a medium size jacket I think it’s a good size for you try it on please” 1 can be split into three portions, ”This is a medium size jacket”, ”I think it’s a good size for you” and ”try it on please”. In this case, translating the three portions and arranging the results in the same order give us the translation of the input sentence. In previous research on splitting sentences, many methods have been based on word-sequence characteristics like N-gram (Lavie et al., 1996; Berger et al., 1996; Nakajima and Yamamoto, 2001; Gupta et al., 2002). Some research efforts have achieved high performance in recall and precision against correct splitting positions. Despite such a high performance, from the view point of translation, MT systems are not always able to translate the split sentences well. In order to supplement sentence splitting based on word-sequence characteristics, this paper introduces another measure of sentence similarity. In our splitting method, we generate candidates for splitting positions based on N-grams, and select the best combination of positions by measuring sen"
C04-1017,W03-0318,1,0.830007,"milarity. We conducted experiments using two EBMT systems, one of which uses a phrase and the other of which uses a sentence as a translation unit. The translation results on various conditions were evaluated by objective measures and a subjective measure. The experimental results show that the proposed method is valuable for both systems. 1 Introduction We are exploring methods to boost the translation quality of corpus-based Machine Translation (MT) systems for speech translation. Among them, the technique of splitting an input sentence and translating the split sentences appears promising (Doi and Sumita, 2003). An MT system sometimes fails to translate an input correctly. Such a failure occurs particularly when an input is long. In such a case, by splitting the input, translation may be successfully performed for each portion. Particularly in a dialogue, sentences tend not to have complicated nested structures, and many long sentences can be split into mutually independent portions. Therefore, if the splitting positions and the translations of the split portions are adequate, the possibility that the arrangement of the translations can provide an adequate translation of the complete input is relati"
C04-1017,P98-1070,0,0.245239,"Conditions We evaluated the splitting method through experiments, whose conditions are as follows. 3.1 MT Systems We investigated the splitting method using MT systems in English-to-Japanese translation, to determine what effect the method had on translation. We used two different EBMT systems as test beds. One of the systems was Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002), whose unit of translation expression is a phrase. HPAT translates an input sentence by combining phrases. The HPAT system is equipped with another sentence splitting method based on parsing trees (Furuse et al., 1998). The other system was DP-match Driven transDucer (D3 ) (Sumita, 2001), whose unit of expression is a sentence. For both systems, translation knowledge is automatically acquired from a parallel corpus. 3.2 Linguistic Resources We used Japanese-and-English parallel corpora, i.e., a Basic Travel Expression Corpus (BTEC) and a bilingual travel conversation corpus of Spoken Language (SLDB) for training, and English sentences in Machine-Translation-Aided bilingual Dialogues (MAD) for a test set (Takezawa and Kikui, 2003). BTEC is a collection of Japanese sentences and their English translations usu"
C04-1017,W02-1035,0,0.0609683,"Missing"
C04-1017,2002.tmi-papers.9,0,0.0216269,"size |for you try it on please 4. This is a medium size jacket |I think it’s a good size |for you |try it on please 5. This is a medium size jacket I think it’s a good size for you try it on please 3 Experimental Conditions We evaluated the splitting method through experiments, whose conditions are as follows. 3.1 MT Systems We investigated the splitting method using MT systems in English-to-Japanese translation, to determine what effect the method had on translation. We used two different EBMT systems as test beds. One of the systems was Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002), whose unit of translation expression is a phrase. HPAT translates an input sentence by combining phrases. The HPAT system is equipped with another sentence splitting method based on parsing trees (Furuse et al., 1998). The other system was DP-match Driven transDucer (D3 ) (Sumita, 2001), whose unit of expression is a sentence. For both systems, translation knowledge is automatically acquired from a parallel corpus. 3.2 Linguistic Resources We used Japanese-and-English parallel corpora, i.e., a Basic Travel Expression Corpus (BTEC) and a bilingual travel conversation corpus of Spoken Language"
C04-1017,2001.mtsummit-papers.68,0,0.0328378,"98 7.08 8.31 14,548 21,686 27.58 27.37 Table 1: Statistics of the training corpus 3.3 Instantiation of the Method For the splitting method, the NLM was the word trigram model using Good-Turing discounting. The number of split portions was limited to 4 per sentence. The weight of Sim, λ in equation (5) was assigned one of 5 values: 0, 1/2, 2/3, 3/4 or 1. 3.4 Evaluation We compared translation quality under the conditions of with or without splitting. To evaluate translation quality, we used objective measures and a subjective measure as follows. The objective measures used were the BLEU score (Papineni et al., 2001), the NIST score (Doddington, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al., 2002). They were calculated with the test set. Both BLEU and NIST compare the system output translation with a set of reference translations of the same source text by finding sequences of words in the reference translations that match those in the system output translation. Therefore, achieving higher scores by these measures means that the translation results can be regarded as being more adequate translations. mWER indicates the error rate based on the edit-distance between the system output and"
C04-1017,P91-1024,1,0.788506,"n is considered as the semantic distance position, we compare the probabilities of the between two substituted words, described as Sem, sentence-splittings before and after splitting. which is defined using a thesaurus and ranges from When calculating the probability of a sentence 0 to 1. Sem is the division of K (the level of the including a sub-sentence, we put pseudo words at least common abstraction in the thesaurus of two the head and tail of the sentence to evaluate the words) by N (the height of the thesaurus) accordprobabilities of the head word and the tail word. ing to equation (3) (Sumita and Iida, 1991). For example, the probability of the sentence,  ”This is a medium size jacket” based on a trigram I + D + 2 Sem language model is calculated as follows. Here, (2) Sim0 (s1 , s2 ) = 1 − Ls 1 + Ls 2 p(z |x y) indicates the probability that z occurs after the sequence x y, and SOS and EOS indicate K the pseudo words. (3) Sem = N Using Sim0 , the similarity of a sentenceP (this is a medium size jacket) = splitting to a corpus is defined as Sim in equap(this |SOS SOS) × tion (4). In this equation, S is a sentence-splitting p(is |SOS this) × and C is a given corpus that is a set of sentences. p(a"
C04-1017,W01-1401,1,0.863825,"tions are as follows. 3.1 MT Systems We investigated the splitting method using MT systems in English-to-Japanese translation, to determine what effect the method had on translation. We used two different EBMT systems as test beds. One of the systems was Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002), whose unit of translation expression is a phrase. HPAT translates an input sentence by combining phrases. The HPAT system is equipped with another sentence splitting method based on parsing trees (Furuse et al., 1998). The other system was DP-match Driven transDucer (D3 ) (Sumita, 2001), whose unit of expression is a sentence. For both systems, translation knowledge is automatically acquired from a parallel corpus. 3.2 Linguistic Resources We used Japanese-and-English parallel corpora, i.e., a Basic Travel Expression Corpus (BTEC) and a bilingual travel conversation corpus of Spoken Language (SLDB) for training, and English sentences in Machine-Translation-Aided bilingual Dialogues (MAD) for a test set (Takezawa and Kikui, 2003). BTEC is a collection of Japanese sentences and their English translations usually found in phrase-books for foreign tourists. The contents of SLDB"
C04-1017,W02-1021,0,0.0255513,"the Method For the splitting method, the NLM was the word trigram model using Good-Turing discounting. The number of split portions was limited to 4 per sentence. The weight of Sim, λ in equation (5) was assigned one of 5 values: 0, 1/2, 2/3, 3/4 or 1. 3.4 Evaluation We compared translation quality under the conditions of with or without splitting. To evaluate translation quality, we used objective measures and a subjective measure as follows. The objective measures used were the BLEU score (Papineni et al., 2001), the NIST score (Doddington, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al., 2002). They were calculated with the test set. Both BLEU and NIST compare the system output translation with a set of reference translations of the same source text by finding sequences of words in the reference translations that match those in the system output translation. Therefore, achieving higher scores by these measures means that the translation results can be regarded as being more adequate translations. mWER indicates the error rate based on the edit-distance between the system output and the reference translations. Therefore, achieving a lower score by mWER means that the translation res"
C04-1017,P02-1040,0,\N,Missing
C04-1017,C98-1067,0,\N,Missing
C04-1030,J90-2002,0,0.427447,"significant improvements compared to the unconstrained search. 1 Introduction In statistical machine translation, we are given a source language (‘French’) sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language (‘English’) sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability: © ª eˆI1 = argmax P r(eI1 |f1J ) eI1 = argmax eI1 © ª P r(eI1 ) · P r(f1J |eI1 ) This decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of target language model P r(eI1 ) and translation model P r(f1J |eI1 ). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. An alternative to the classical sourcechannel approach is the direct"
C04-1030,J99-4005,0,0.599269,"ls or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). In this paper, we will investigate the reordering problem for phrase-based translation approaches. As the word order in source and target language may differ, the search algorithm has to allow certain reorderings. If arbitrary reorderings are allowed, the search problem is NP-hard (Knight, 1999). To obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm. Note that in the latter case we cannot guarantee to find an optimal solution. The remaining part of this work is structured as follows: in the next section, we will review the baseline translation system, namely the alignment template approach. Afterward, we will describe different reordering constraints. We will begin with the IBM constraints for phrase-based translation. Then, we will describe constraints based on inversion transduction grammars (ITG). In t"
C04-1030,N03-1017,0,0.32893,"Missing"
C04-1030,W02-1018,0,0.0915666,"Missing"
C04-1030,P02-1038,1,0.858868,"lation model P r(f1J |eI1 ). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. An alternative to the classical sourcechannel approach is the direct modeling of the posterior probability P r(eI1 |f1J ). Using a loglinear model (Och and Ney, 2002), we obtain: Ã P r(eI1 |f1J ) = exp ! M X λm hm (eI1 , f1J ) · Z(f1J ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant. As a decision rule, we obtain: ( eˆI1 = argmax eI1 M X ) λm hm (eI1 , f1J ) m=1 This approach is a generalization of the source-channel approach. It has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measu"
C04-1030,W99-0604,1,0.401442,"nslation system, namely the alignment template approach. Afterward, we will describe different reordering constraints. We will begin with the IBM constraints for phrase-based translation. Then, we will describe constraints based on inversion transduction grammars (ITG). In the following, we will call these the ITG constraints. In Section 4, we will present results for two Japanese–English translation tasks. 2 Alignment Template Approach In this section, we give a brief description of the translation system, namely the alignment template approach. The key elements of this translation approach (Och et al., 1999) are the alignment templates. These are pairs of source and target language phrases with an alignment within the phrases. The alignment templates are build at the level of word classes. This improves the generalization capability of the alignment templates. We use maximum entropy to train the model scaling factors (Och and Ney, 2002). As feature functions we use a phrase translation model as well as a word translation model. Additionally, we use two language model feature functions: a word-based trigram model and a class-based five-gram model. Furthermore, we use two heuristics, namely the wor"
C04-1030,P03-1021,0,0.209602,"= exp ! M X λm hm (eI1 , f1J ) · Z(f1J ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant. As a decision rule, we obtain: ( eˆI1 = argmax eI1 M X ) λm hm (eI1 , f1J ) m=1 This approach is a generalization of the source-channel approach. It has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). In this paper, we will investigate the reordering problem for phrase-based translation approaches. As the word order in source and target language may differ, the search algorithm has to allow certain reorderings. If arbitrary reorderings are allowed, the search problem is NP-hard (Knight, 1999). To obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm. Note that in the latter case we cannot guarantee to find an optimal solution. The remaining part of this work is structured as follows: in the next section, we will"
C04-1030,P02-1040,0,0.113983,"eletion operations that have to be performed to convert the generated sentence into the reference sentence. PER (position-independent word error rate). A shortcoming of the WER is that it requires a perfect word order. The word order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading. The PER compares the words in the two sentences ignoring the word order. BLEU. This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences (Papineni et al., 2002). The BLEU score measures accuracy, i.e. large BLEU scores are better. NIST. This score is similar to BLEU. It is a weighted n-gram precision in combination with a penalty for too short sentences (Doddington, 2002). The NIST score measures accuracy, i.e. large NIST scores are better. Note that for each source sentence, we have as many as 16 references available. We compute all the preceding criteria with respect to multiple references. 4.3 System Comparison In Table 3 and Table 4, we show the translation results for the BTEC task. First, we observe that the overall quality is rather high on th"
C04-1030,takezawa-etal-2002-toward,1,0.296878,"to the left of the current position jc , e.g. positions (a) and (d). Somewhere in between there has to be an covered position j whose successor position j + 1 is uncovered, e.g. (b) and (c). Therefore, any reordering that violates Equation 1 generates the pattern on the left-hand side in Figure 4, thus it violates the ITG constraints. 4 Results 4.1 Corpus Statistics To investigate the effect of reordering constraints, we have chosen two Japanese–English tasks, because the word order in Japanese and English is rather different. The first task is the Basic Travel Expression Corpus (BTEC) task (Takezawa et al., 2002). The corpus statistics are shown in Table 1. This corpus consists of phrasebook entries. The second task is the Spoken Language DataBase (SLDB) task (Morimoto et al., 1994). This task consists of transcription of spoken dialogs in the domain of hotel reservation. Here, we use domain-specific training data in addition to the BTEC corpus. The corpus statistics of this additional corpus are shown in Table 2. The development corpus is the same for both tasks. 4.2 Evaluation Criteria WER (word error rate). The WER is computed as the minimum number of substitution, insertion and deletion operations"
C04-1030,J03-1005,1,0.779472,"a sequence of word classes as used in the alignment templates. 3.1 IBM Constraints In this section, we describe restrictions on the phrase reordering in spirit of the IBM constraints (Berger et al., 1996). First, we briefly review the IBM constraints at the word level. The target sentence is produced word by word. We keep a coverage vector to mark the already translated (covered) source positions. The next target word has to be the translation of one of the first k uncovered, i.e. not translated, source positions. The IBM constraints are illustrated in Figure 1. For further details see e.g. (Tillmann and Ney, 2003). For the phrase-based translation approach, we use the same idea. The target sentence is produced phrase by phrase. Now, we allow skipping of up to k phrases. If we set k = 0, we obtain a search that is monotone at the phrase level as a special case. Q(1, ∅, $) = 1  with inversion target positions without inversion target positions The search problem can be solved using dynamic programming. We define a auxiliary function Q(j, S, e). Here, the source position j is the first unprocessed source position; with unprocessed, we mean this source position is neither translated nor skipped. We use th"
C04-1030,J97-3002,0,0.799973,"the language model history. The symbol $ is used to mark the sentence start and the sentence end. The extension to higher-order n-gram language models is straightforward. We use M to denote the maximum phrase length in the source language. We obtain the following dynamic programming equations: source positions source positions Figure 2: Illustration of monotone and inverted concatenation of two consecutive blocks. setting k = 0 results in a search algorithm that is monotone at the phrase level. 3.2 ITG Constraints Q(j, S, e) = max In this section, we describe the ITG conn straints (Wu, 1995; Wu, 1997). Here, we interj−1 max max Q(j 0 , S, e0 ) · p(fj 0 |˜ e) · p(˜ e|e0 ), pret the input sentence as a sequence of blocks. e0 ,˜ e j−M ≤j 0 <j o 0 0 , e0 ) · p(f j +l−1 |˜ 0 ) , In the beginning, each alignment template is a max Q(j, S e ) · p(˜ e |e j0 block of its own. Then, the reordering process (j 0 ,l)∈S 0 S=S 0 {(j 0 ,l)}  can be interpreted as follows: we select two consecutive blocks and merge them to a single max0 Q(j 0 , S 0 , e) j−M ≤j <j block by choosing between two options: either S 0 :S=S 0 ∪{(j 0 ,j−j 0 )}∧|S 0 |<k keep the target phrases in monotone order or Q(J + 2, ∅, $) ="
C04-1030,P03-1019,1,0.776962,"entence word by word or phrase the position to be translated next jn . Then, by phrase. The idea is to start with the beam it is not allowed to move from an uncovered search decoder for unconstrained search and position to a covered one. modify it in such a way that it will produce Now, we sketch the proof that these cononly reorderings that do not violate the ITG straints are equivalent to the ITG constraints. constraints. Now, we describe one way to obIt is easy to see that the constraint in Equatain such a decoder. It has been pointed out tion 1 avoids the pattern on the left-hand side in (Zens and Ney, 2003) that the ITG conin Figure 4. To be precise: after placing the straints can be characterized as follows: a refirst two phrases at (b,1) and (d,2), it avoids ordering violates the ITG constraints if and the placement of the third phrase at (a,3). only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as Similarly, the constraint in Equation 2 avoid a subsequence. This means, if we select four the pattern on the right-hand side in Figcolumns and the corresponding rows from the ure 4. Therefore, if we enforce the constraints alignment matrix and we obtain one of the two in Equation 1 and Equation 2, we"
C04-1047,C02-1076,1,0.897431,"on the goodness/density of MT outputs in the Nbest list from the system. However, the system’s N-best list does not always give a good approximation of the total summation of the probability of all candidate translations given the source sentence/utterance. The N-best list is expected to approximate the total summation as closely as possible. This paper proposes a method that eliminates unsatisfactory top output by using an alternative RSCM based on a mixture of N-best lists from multiple MT systems (Figure 2). The elimination system is intended to be used in the selector architecture, as in (Akiba et al., 2002). The total translation quality of the selector architecture proved to be better than the translation quality of each element MT system. The final output from the selection system is the best among the satisfactory top 2 outputs from the elimination system. In the case of Figure 2, the selection system can receive zero to three top MT outputs. When the selection system receive fewer than two top MT outputs, the selection system merely passes a null output or the one top MT output. The proposed RSCM differs from the existing RSCM in its N-best list. The proposed RSCM re2 To distinguish the best"
C04-1047,J93-2003,0,0.0058823,"s in the order of the average product of the scores of a language model and a translation model (Akiba et al., 2002). This sorted mixture is alternatively used instead of the system’s N-best list in the existing RSCM. That is, the proposed RSCM checks whether it accepts/rejects each top MT output in the original M-best lists by using the sorted mixture; on the other hand, the existing RSCM checks whether it accepts/rejects the top MT output in the system’s N-best list by using the system’s N-best. For scoring MT outputs, the proposed RSCM uses a score based on a translation model called IBM4 (Brown et al., 1993) (TM-score) and a score based on a language model for the translation target language (LM-score). As Akiba et al. (2002) reported, the products of TM-scores and LM-scores are statistical variables. Even in the case where the translation model (TM) and the language model for the translation target language (LM) are trained on a sub-corpus of the same size, changing the training corpus also changes the TM-score, the LM-score, and their product. Each pair of TM-score and LMscore differently order the MT outputs. For robust scoring, the authors adopt the multiple scoring technique presented in (Ak"
C04-1047,W03-0318,1,0.91793,"er is not. 0 0.2 0.4 0.6 0.8 Correct acceptance rate: x 1 Figure 1: Performance of the existing RSCM on three different types of Japanese-to-English (J2E) MT systems: D3 , HPAT, and SAT. The existing RSCM tried to accept perfect MT outputs (grade A in Section 4) and to reject imperfect MT outputs (grades B, C, and D in Section 4). on types of MT systems other than SMT systems. Figure 1 shows the differences among the performances, indicated by the Receiver Operating Characteristics (ROC) curve (Section 4.1), of the existing RSCM on each of three MT systems (Section 4.2.1): D3 , HPAT, and SAT (Doi and Sumita, 2003; Imamura et al., 2003; Watanabe et al., 2003). Only SAT is an SMT system; the others are not. The ideal ROC curve is a square (0,1), (1,1), (1,0); thus, the closer the curve is to a square, the better the performance of the RSCM is. The performances of the existing RSCM on the non-SMT systems, D3 and HPAT, are much worse than that on the SMT system, SAT. The performance of the existing RSCM depends on the goodness/density of MT outputs in the Nbest list from the system. However, the system’s N-best list does not always give a good approximation of the total summation of the probability of all"
C04-1047,P03-1057,1,0.890736,"Missing"
C04-1047,niessen-etal-2000-evaluation,0,0.0430757,"nfidence values of all words in the MT output are larger than a fixed threshold, the MT output is judged as correct/perfect. Otherwise, the output is judged as incorrect/imperfect. The existing RSCM does not always work well Performance of existing method (A|BCD) 1 0.8 Correct rejection rate: y Abstract 0.6 0.4 0.2 J2E SAT + Existing method J2E HPAT + Existing method J2E D3 + Existing method 0 ∗ This research was supported in part by the Ministry of Public Management, Home Affairs, Posts and Telecommunications, Japan. 1 These confidence measures are a kind of automatic evaluator such as mWER (Niessen et al., 2000) and BLEU (Papineni et al., 2001). While mWER and BLEU cannot be used online, these confidence measures can. This is because the former are based on reference translations, while the latter is not. 0 0.2 0.4 0.6 0.8 Correct acceptance rate: x 1 Figure 1: Performance of the existing RSCM on three different types of Japanese-to-English (J2E) MT systems: D3 , HPAT, and SAT. The existing RSCM tried to accept perfect MT outputs (grade A in Section 4) and to reject imperfect MT outputs (grades B, C, and D in Section 4). on types of MT systems other than SMT systems. Figure 1 shows the differences am"
C04-1047,P00-1056,0,0.0363548,"ogue Corpus No. 1 (Kikui et al., 2003) Japanese English # of sentences 449,357 3,471,996 2,978,517 . # of words Vocabulary size 43,812 28,217 Ave. sent. length 7.7 6.6 were randomly selected from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). BTEC contains a variety of expressions used in a number of situations related to overseas travel. 4.2.3 Training TMs and LMs The corpora used for training TMs and LMs described in Section 3.2 were merged corpora (Table 3). The number of trained TMs/LMs was three. The translation models and language models were learned by using GIZA++ (Och and Ney, 2000) and the CMU-Cambridge Toolkit (Clarkson and Rosenfeld, 1997), respectively. 4.3 Experimental results and discussion 4.3.1 ROC Curve In order to plot the ROC Curve, the authors conducted the same experiment as shown in Figure 1. That is, in the case where the grade of satisfactory translations is only grade A, each of the proposed and existing RSCMs tried to accept grade A MT outputs and to reject grade B, C, or D MT outputs. Figures 5 to 7 show the ROC Curves for each of the three J2E MT systems (D3, HPAT, and SAT). The curves with diamond marks, cross marks, triangle marks, and circle marks"
C04-1047,2001.mtsummit-papers.68,0,0.0300245,"the MT output are larger than a fixed threshold, the MT output is judged as correct/perfect. Otherwise, the output is judged as incorrect/imperfect. The existing RSCM does not always work well Performance of existing method (A|BCD) 1 0.8 Correct rejection rate: y Abstract 0.6 0.4 0.2 J2E SAT + Existing method J2E HPAT + Existing method J2E D3 + Existing method 0 ∗ This research was supported in part by the Ministry of Public Management, Home Affairs, Posts and Telecommunications, Japan. 1 These confidence measures are a kind of automatic evaluator such as mWER (Niessen et al., 2000) and BLEU (Papineni et al., 2001). While mWER and BLEU cannot be used online, these confidence measures can. This is because the former are based on reference translations, while the latter is not. 0 0.2 0.4 0.6 0.8 Correct acceptance rate: x 1 Figure 1: Performance of the existing RSCM on three different types of Japanese-to-English (J2E) MT systems: D3 , HPAT, and SAT. The existing RSCM tried to accept perfect MT outputs (grade A in Section 4) and to reject imperfect MT outputs (grades B, C, and D in Section 4). on types of MT systems other than SMT systems. Figure 1 shows the differences among the performances, indicated b"
C04-1047,1999.mtsummit-1.34,1,0.834092,"from an MT system is satisfactory, that is, whether the translation quality of the top MT output is better than or as good as that which the user can permit. In this experiment, the translation quality of MT outputs was assigned one of four grades: A, B, C, or D as follows: (A) Perfect: no problems in either information or grammar; (B) Fair: easy-tounderstand, with either some unimportant information missing or flawed grammar; (C) Acceptable: broken, but understandable with effort; (D) Nonsense: important information has been translated incorrectly. This evaluation standard was introduced by Sumita et al. (1999) to evaluate S2SMT systems. In advance, each top MT output was evaluated by nine native speakers of the target language, who were also familiar with the source language, and then assigned the median grade of the nine grades. To conduct a fair comparison, the number of MT outputs in the system’s N-best list and the number of MT outputs in the mixture are expected to be the same. Thus, the authors used either a threebest list from each of three MT systems or a fivebest list from each of two non-SMT MT systems for the proposed RSCM and a ten-best list for the existing RSCM. Naturally, this settin"
C04-1047,takezawa-etal-2002-toward,1,0.917102,"imination System Satisfactory top outputs Selection System The best output Figure 2: Image of our eliminator ceives an M-best list from each element MT system. Next, it sorts the mixture of the MT outputs in all M-best lists in the order of the average product (Section 3.2) of the scores of a language model and a translation model (Akiba et al., 2002). This sorted mixture is used instead of the system’s N-best list in the existing RSCM. To experimentally evaluate the proposed RSCM, the authors applied the proposed RSCM and the existing RSCM to a test set of the Basic Travel Expression Corpus (Takezawa et al., 2002). The proposed RSCM proved to work better than the existing RSCM on the non-SMT systems and to work as well as the existing RSCM on the SMT system. The next section outlines the existing RSCM. Section 3 proposes our RSCM. Experimental results are shown and discussed in Section 4. Finally, our conclusions are presented in Section 5. 2 The Existing RSCM The existing confidence measures include the ranksum-based confidence measure (RSCM) for SMT systems (Ueffing et al., 2003). The basic idea of this RSCM is to roughly calculate the word posterior probability by using ranks of MT outputs in the N-"
C04-1047,2003.mtsummit-papers.52,0,0.394172,"her users permit even translations with flawed grammar. Unsatisfactory MT outputs are those whose translation quality is worse than the level the user can permit. In this paper, the authors intend to eliminate unsatisfactory outputs by using confidence measures for MT outputs. The confidence measures 1 indicate how perfect/satisfactory the MT outputs are. In the discipline of MT, confidence measures for MT outputs have rarely been investigated. The few existing confidence measures include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems, Crank in (Ueffing et al., 2003). The basic idea of this confidence measure is to roughly calculate the word posterior probability by using ranks of MT outputs in an N-best list from an SMT system. In the discipline of non-parametric statistical test, ranks of numerical values are commonly used instead of the numerical values themselves for statistical tests. In the case of the existing RSCM, the ranks of probabilities of MT outputs in the N-best list were used instead of the probabilities of the outputs themselves. The existing RSCM scores each word in an MT output by summing the complemented ranks of candidates in the N-be"
C04-1047,P03-1039,1,0.881025,"nce rate: x 1 Figure 1: Performance of the existing RSCM on three different types of Japanese-to-English (J2E) MT systems: D3 , HPAT, and SAT. The existing RSCM tried to accept perfect MT outputs (grade A in Section 4) and to reject imperfect MT outputs (grades B, C, and D in Section 4). on types of MT systems other than SMT systems. Figure 1 shows the differences among the performances, indicated by the Receiver Operating Characteristics (ROC) curve (Section 4.1), of the existing RSCM on each of three MT systems (Section 4.2.1): D3 , HPAT, and SAT (Doi and Sumita, 2003; Imamura et al., 2003; Watanabe et al., 2003). Only SAT is an SMT system; the others are not. The ideal ROC curve is a square (0,1), (1,1), (1,0); thus, the closer the curve is to a square, the better the performance of the RSCM is. The performances of the existing RSCM on the non-SMT systems, D3 and HPAT, are much worse than that on the SMT system, SAT. The performance of the existing RSCM depends on the goodness/density of MT outputs in the Nbest list from the system. However, the system’s N-best list does not always give a good approximation of the total summation of the probability of all candidate translations given the source sente"
C04-1047,P02-1040,0,\N,Missing
C08-3006,W05-0909,0,0.0284034,"70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Figure 4 that also illustrates the graphi"
C08-3006,2007.iwslt-1.15,1,0.895955,"Missing"
C08-3006,P02-1040,0,0.0823612,"65.83 69.61 75.39 72.17 72.82 69.00 70.70 – 89.73 67.33 72.19 69.14 68.32 64.55 62.91 70.81 77.62 languages like Danish, German, English, Spanish, etc. does not differ much. Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more difficult to translate. This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best). Besides Korean (single references only), all languages were evaluated using 16 reference translations. The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality. 3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via ‘http://www.atr-trek.co.jp/contents html’ or using the QR code in Fig"
C08-3006,2008.iwslt-evaluation.11,1,\N,Missing
C16-1291,D16-1162,0,0.0517274,"NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine trans"
C16-1291,J16-2001,0,0.0929956,"ved by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://crea"
C16-1291,J93-2003,0,0.0944848,"igures out those source words will be translated next, even though the next target word yt is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows: exp(F (x, y, α)) 0 α0 exp(F (x, y, α )) p(α |x, y) = P where F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α |x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer αt , alignment models can readily use the entire y, of course including yt as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation. 3 Supervised Attention In this section, we introduce supervised attention to improve"
C16-1291,2016.amta-researchers.10,0,0.330705,"vel reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is worse than conventiona"
C16-1291,P14-1129,0,0.0162995,"e original paper, αt is not explicitly dependent on the yt−1 in Eq.(2), but this dependency was explicitly retained in our direct baseline NMT2. 5 Although the alignment is loosely related to the downstream translation (Liu and Sun, 2015), substantial improvements in alignment usually leads to the improvements in translation as observed in our experiments. 3095 Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens ‘eol’ added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as α ˆ. 3.2 Jointly Supervising Translation and Attention We propose a soft constraint method to jointly supervise the translation and attention as follows: − X log p(yi |xi ; θ) + λ × ∆(αi , α ˆ i ; θ) (4) i where αi is as defined in Eq. (1), ∆ is a loss function that"
C16-1291,N13-1073,0,0.108639,"allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervise"
C16-1291,P07-2045,0,0.0608892,"the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 Inter"
C16-1291,N06-1014,0,0.0172418,"because the supervision of α is more close to Ex than y as in Figure 1(b). In order to quantify the disagreement between αi and α ˆ i , three different methods are investigated in our experiments: • Mean Squared Error (MSE) ∆(αi , α ˆ i ; θ) = XX 1 m 2 n i α(θ)im,n − α ˆ m,n 2 MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly i encourages α(θ)im,n to be equal to α ˆ m,n . • Multiplication (MUL) ∆(αi , α ˆ i ; θ) = − log XX m n i α(θ)im,n × α ˆ m,n  MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016). Note that different from those in (Cheng et al., 2016), α ˆ is not a parametrized variable but a constant in this paper. • Cross Entropy (CE) ∆(αi , α ˆ i ; θ) = − XX m n i α ˆ m,n × log α(θ)im,n Since for each t, α(θ)t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004). 4 Experiments We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel do"
C16-1291,2015.iwslt-evaluation.11,0,0.28045,"1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets. Methods GIZA++ NMT2 SA-NMT AER 30.6∗ 50.6 43.3∗ Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). ‘*’ denotes that the corresponding result is significanly better than NMT2 with p &lt; 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following (Luong and Manning, 2015), we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised count"
C16-1291,D15-1166,0,0.100348,"ver the standard attention based NMT. 1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper,1 is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015). One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more"
C16-1291,D16-1249,0,0.218565,"network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is"
C16-1291,W15-5003,0,0.0260557,"Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn"
C16-1291,P00-1056,0,0.102849,"mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering mo"
C16-1291,P14-1138,1,0.532331,"). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out set"
C16-1291,P16-1008,0,0.0286294,"over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itse"
C16-1291,P06-1066,0,0.0297014,"p in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons A"
C16-1291,P13-1017,0,0.0104401,"d counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 an"
C16-1291,C14-1179,0,\N,Missing
C16-1295,D11-1033,0,0.575583,"corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al."
C16-1295,2011.iwslt-evaluation.18,0,0.251266,"ion probabilities of connecting phrases calculated by NN can also be used to enhance SMT, and the experimental results will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, r"
C16-1295,P16-1039,1,0.833935,"Missing"
C16-1295,P13-1141,0,0.0645321,"Missing"
C16-1295,N13-1114,0,0.0207534,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P13-1126,0,0.0188404,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P08-1010,0,0.178554,"focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and mono"
C16-1295,P13-2119,0,0.636704,"elated-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et"
C16-1295,2015.mtsummit-papers.10,0,0.834828,"ild lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technolog"
C16-1295,D10-1044,0,0.0735489,"et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the abov"
C16-1295,D14-1062,0,0.238959,"Missing"
C16-1295,C14-1182,0,0.301219,"Missing"
C16-1295,D15-1147,0,0.484005,"selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shangha"
C16-1295,W07-0733,0,0.496687,"ed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods). 1 Introduction Large corpora are important for Statistical Machine Translation (SMT) training. However only the relevant additional corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it i"
C16-1295,W04-3250,0,0.163106,"methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K f"
C16-1295,W07-0734,0,0.0163639,"d to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora"
C16-1295,D12-1088,0,0.0610705,"Missing"
C16-1295,N13-1074,0,0.0171997,"ee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT p"
C16-1295,C14-1105,0,0.0122047,"problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially"
C16-1295,P10-2041,0,0.0842928,"evel selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwh"
C16-1295,2012.iwslt-papers.3,0,0.0240338,"e used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) FR-EN parallel corpora5 . 2) NIST 2006 Chinese (CN) to English corpus6 is used as in-domain corpus, which follows the setting of (Wang et al., 2014b) and mainly consists of news and blog texts. Chinese to English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data are used as out-of-domain bilingual (Bil) parallel corpora. The English patent data in NTCIR-8 (Fujii et al., 2010) is also used as additional out-of-dom"
C16-1295,P03-1021,0,0.0187056,"ts will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nati"
C16-1295,P02-1040,0,0.0955457,"ics on data sets (‘B’ for billions). 3 NN based methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. Tha"
C16-1295,C12-2104,0,0.173123,"and Pout (E|F ) by N N T Mout should be lower. This hypothesis is partially motivated by (Axelrod et al., 2011), which use bilingual cross-entropy difference to distinguish in-domain and out-of-domain data. The translation probability of a phrase-pair is estimated as, P (E|F ) = P (e1 , ..., eq |f1 , ..., fp ), (1) where fs (s ∈ [1, p]) and et (t ∈ [1, q]) are source and target words, respectively. Originally, P (e1 , ..., eq |f1 , ..., fp ) = q ∏ P (ek |e1 , ..., ek−1 , f1 , ...fp ). (2) k=1 The structure of NN based translation model is similar to Continuous Space Translation Model (CSTM) (Schwenk, 2012). For the purpose of adaptation, the dependence between target words is dropped2 and the probabilities of different length target phrase are normalized. For an incomplete source phrase, i.e. with less than seven words, we set the projections of the missing words to zero. The normalized translation probability Q(E|F ) can be approximately computed by the following equation, v u q u∏ q P (ek |f1 , ...fp ). Q(E|F ) ≈ t (3) k=1 Finally, the minus Dminus (E|F ) is used to rank connecting phrase pairs from mix-domain PT, Dminus (E|F ) = Qin (E|F ) − Qin (E|F ). (4) where Qin (E|F ) and Qin (E|F ) ar"
C16-1295,P13-1082,0,0.203536,"). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B."
C16-1295,E12-1055,0,0.0662449,"input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora are huge, part of them are resampled (resample coefficient 0.01 for IWSLT and NIST). Several related existing methods are selected as baselines7 : Koehn and Schroeder (2007)’s method for using two (in and out-of-domain) TMs and LMs together, entropy based method for TM (Ling et al., 2012) and LM (Stolcke, 1998) adaptation (pruning), (Duh et al., 2013) for NNLM based sentence adaptation, (Sennrich, 2012) for TM weights combination, and (Bisazza et al., 2011) for TM fill-up. In Table Tables 2 and 3, ‘in-domain’, ‘out-of-domain’ and ‘mix-domain’ indicate training all models using corresponding corpora, ‘in+NN’ indicates applying NN based adaptation directly for all phrases, and ‘in+connect’ indicates adding all connecting phrases and n-grams to in-domain PT and LM, respectively. For tuning methods, ‘in+connect+OP/NN’ indicates tuning connecting phrase pairs and n-grams using Occurring Probability (OP) and NN, respectively. Only the best preforming systems (for both the baselines and proposed me"
C16-1295,D14-1023,1,0.921449,"Missing"
C16-1295,P14-2122,1,0.886787,"ails: http://creativecommons.org/licenses/by/4.0/ 3135 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3135–3145, Osaka, Japan, December 11-17 2016. Instead of focusing on sentence selection or single model adaptation, we propose a phrase adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram selection. It is based on a linguistic observation that the translation hypotheses of a phrase-based SMT system are concatenations of phrases from Phrase Table (PT), which has been applied to LM growing (Wang et al., 2014a; Wang et al., 2015). As a straightforward linear method, it is much efficient in comparison with NN based non-linear methods. The remainder of this paper is organized as follows. Section 2 will introduce the connecting phrase based adaptation method. The size of adapted connecting phrase will be tuned in Section 3. Empirical results will be shown in Section 4. We will discuss the methods and conduct extension experiments in Section 5. The last section will conclude this paper. 2 Connecting Phrase based Adaptation Suppose that two phrases ‘would like to learn’ and ‘Chinese as second language’"
C16-1295,P16-1131,1,0.811994,"Missing"
C16-1295,W14-3348,0,\N,Missing
C16-1295,C12-1010,0,\N,Missing
C16-2007,2014.iwslt-papers.8,1,0.731983,"s can be used to monitor the running of the system: the speech recognition log and the machine translation log (Figure 2). The speech recognition log shows the recognized words from the speakers. The machine translation log shows the recognized sentences and their translations. The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED ta"
C16-2007,2005.iwslt-1.19,0,0.0456124,"The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED talk from English to Japanese by the system. The talk is ”Your elusive creative genius “ given by Elizabeth Gilbert in 20097 . The oracle transcript is, I am a writer. Writing books is my profession but it’s more than that, of course. It is also my great lifelong love and fascinati"
C16-2007,2006.iwslt-papers.1,0,0.0264925,"mage is caused. The Online Sentence Segmenter converts the stream of words into sentences. The implementation is based on the method proposed in (Wang et al., 2016a). The implementation uses a linear combination of a language model, a length model and a prosodic model to calculate the confidence of segmentation boundaries, and uses a threshold-latency-based heuristic to make decisions. The Punctuation Predictor converts an un-punctuated sentence into a punctuated sentence. The implementation is based on the findings in (Wang et al., 2016b). It uses a hidden N-gram model (Stolcke et al., 1998; Matusov et al., 2006), which is available in the toolkit of SRILM (Stolcke, 2002), to insert punctuation. The Machine Translation Engine translates a source-language sentence into a target-language sentence. The implementation is our in-house pre-ordering translation system, called the General Purpose Machine Translation (GPMT) engine. The system is publicly accessible through a Web API 5 The Speech Synthesizer converts sentences into speech. The implementation is based on the HTS open-source toolkit (Tokuda et al., 2013)6 4 https://github.com/kaldi-asr/kaldi https://mt-auto-minhon-mlt.ucri.jgn-x.jp/ 6 http://hts."
C16-2007,N16-3017,0,0.0615103,"Missing"
C16-2007,W16-4613,1,0.864082,"Missing"
C16-2008,W02-2117,1,0.549993,"Missing"
C16-2008,J14-1005,0,0.0222758,"TUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system. 1 Introduction For improved machine translatability, a wide variety of controlled language (CL) rule sets have been proposed (Kittredge, 2003; Kuhn, 2014). Evidence of reduced post-editing costs when a CL is employed is provided (Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007), and several controlled authoring support tools, such as Acrolinx1 and MAXIT2 , have been developed. The fundamental limitation of the CLs proposed hitherto is, however, that they are defined at the level of the sentence rather than at the level of the document (Hartley and Paris, 2001). In fact, the notion of functional document element (see Section 2.1) does figure in some CL rule sets. ASD Simplified Technical English (ASD, 2013), for example, specifies writing p"
C16-2008,2003.eamt-1.10,0,0.0917482,"pic template is the core interface for authoring self-contained topics in a structured manner. The left pane in Figure 3 provides the basic DITA Task topic structure for composing municipal procedural documents. • CL authoring assistant analyses each sentence in the text box and highlights any segment that violates a local CL rule or controlled terminology, together with diagnostic comments and suggestions for rewriting (shown at bottom centre in Figure 3) (Miyata et al., 2016). In addition, we have implemented a preliminary rewriting support function with several of the features advocated by Mitamura et al. (2003). For a particular CL-noncompliant segment, the function offers alternative expressions; clicking one of the suggestions automatically replaces the offending segment in the text box above. • Pre-translation processing automatically modifies source segments in the background following transformation rules defined for each functional element, and then MT produces the translation and back-translation at the same time. 3 We used a Japanese morphological analyser MeCab. http://taku910.github.io/mecab/ 37 MT and back translation DITA task topic CL authoring assistant Figure 3: Task topic template fo"
C16-2008,2015.mtsummit-papers.8,1,0.857596,"Missing"
C96-2191,C96-1070,1,0.876491,"s not necessarily converge a correct interpretation. 5. S u f f i c i e n t speed to avoid to b r e a k c o m munication As an interpreter intervenes between speakers, real-time response is required to keep smooth turn taking. 1074 3 3.1 Meeting the seven requirements Incremental processing This is an essential technology if one is to build an incremental translation system like a simultaneous interpreter, and the proper way to grasp a chunk of a translation unit corresponding to some chunk in a target language is to extend 'constituent boundary parsing' to b o t t o m - u p - t y p e parsing [Furuse96]. 3.2 Recovering from e r r o r s A certain recovery method is now under consideration: a re-entrizing model for phoneme candidates by means of searching the correct phonemes using modification depending on recognition error characteristics in an example-based framewbrk [Wakita95]. This approach provides a recovery effect in handling phoneme or syllable sequences, and the effect depends on the particular speakers because of individual error characteristics. 3.3 Requirements covered b y EBMT/TDMT The remaining requirements are handled effectively by an example-based approach as explained here."
C96-2191,1995.tmi-1.20,1,0.808663,"Missing"
C96-2191,1993.mtsummit-1.11,0,0.0645573,"Missing"
C96-2191,C94-1015,1,\N,Missing
C96-2191,H93-1041,0,\N,Missing
C96-2191,1993.tmi-1.16,0,\N,Missing
C98-2228,P95-1017,0,0.0696926,"Missing"
C98-2228,C96-2137,0,0.0867604,"cue words (Walker and Moore, 1997). This section describes a method to apply a decision-tree learning approach, which is one of the machinelearning approaches, to ellipsis resolution. As described above, our major application for ellipsis resolution is in machine translation. In an MT process, there can be several approaches about the timing of ellipsis resolution: when analyzing the source language, when generating the target language, or at the same time as translating process. Among these candidates, most of the previous works with Japanese chose the source-language approach. For instance, Nakaiwa and Shirai (1996) attempted to resolve Japanese ellipsis in the source language analysis of J-to-E MT, despite utilizing targetdependent resolution candidates. We originally thought that ellipsis resolution in the MT was a generation problem, namely a target-driven problem which utilizes some help, if necessary, of source-language information. This is because the problem is outputdependent and it relies on demands fl'om a target language, in the J-to-Korean or J-toChinese MT, all or most of the ellipses that must be resolved in J-to-E are not necessary to resolve. ttowever, we adopted source-language policy in"
C98-2228,C94-2116,0,0.0298411,"ed the C~.5 algorithm by Quinlan (1993), which is a well-known automatic classifier that produces a binary decision tree. Although it may be necessary to prune decision trees, no pruning is performed throughout this experiment, since we want to concentrate the discussion on the feasibility of machine learning. As shown in the experiment by Aone and BenSince a huge text corlms has become widely available, the machine-learning approach has been utilized for some problems in natural language processing. The most popular touchstone in this field is the verbal case fl'ame or the translation rules (Tanaka, 1994). Machine-learning algorithm has also been attempted to solve some 1429 3.1 Ellipsis Tagging Learning Method Table 2: Number of training attributes Attributes Content words ( p r e d i c a t e ) Content words (case frame) Func. words (case particle) Func. words (conj. particle) Func. words (auxiliary verb) Func. words (other) Exophoric information Total Num. 100 100 9 21 132 4 1 367 nett (1995), which a t t e m p t e d to discuss pruning effects on the decision tree, no more conclusions are expected other than a trade-off between recall and precision. We leave the details of decision-tree lear"
C98-2228,J97-1001,0,\N,Missing
D07-1054,A94-1010,0,0.431893,". The following formula embodies the search for the optimal domain. argmax P (e|D)P (f |D) (9) D This formula ensures that the search for the domain maximizes the domain specific probabilities of both e and f simultaneously. 4.2 Clustering of the bilingual corpus As mentioned above, we maximize the domain specific probabilities of e and f to ascertain the domain. We define our domains as sub-corpora of the bilingual corpus, and these sub-corpora are formed by clustering bilingually by entropy reduction. For this clustering, the following extension of monolingual corpus clustering is employed (Carter 1994). (7) 1. The total number of clusters (domains) is given by the user. Therefore, we can confirm reasonability of this assumption by calculating P (f |D)P (D) all domains (P (f ) is constant). 2. Each bilingual sentence pair is randomly assigned to a cluster. = P (f |D)P (D)/P (f ) 4.1 Domain Definition When the domain is known in advance, it is usually expressible, for example it could be a topic that matches a human-defined category like “sport”. On the other hand, when the domain is delimited in an unsupervised manner, it is used only as a probabilistic variable and does not need to be expre"
D07-1054,2005.eamt-1.17,0,0.144531,"Missing"
D07-1054,takezawa-etal-2002-toward,1,0.807035,"eters P (D|f ) are estimated separately for each sentence. • In the sentence mixture model, the probabilities of all cluster dependent language models are summed. In the proposed model, only the cluster that gives the highest probability is considered as approximation. • In the proposed method, a domain specific translation model is also used. 6 Experiments 6.1 Japanese to English translation 6.1.1 Experimental corpus To evaluate the proposed model, we conducted experiments based on a travel conversation task corpus. The experimental corpus was the travel arrangements task of the BTEC corpus (Takezawa et al., 2002),(Kikui et al., 2003) and the language pair was Japanese and English. The training, development, and evaluation corpora are shown in Table 1. The development and evaluation corpora each had sixteen reference translations for each sentence. This training corpus was also used for the IWSLT06 Evaluation Campaign on Spoken Language Translation (Paul 2006) J-E open track, and the evaluation corpus was used as the IWSLT05 evaluation set. 6.1.2 Experimental conditions For bilingual corpus clustering, the sentence entropy must be calculated. Unigram language models were used for this calculation. The"
D07-1054,P02-1040,0,0.0748381,"es dynamically. For these cases, not only the translation target sentence but also the domain must be predicted. This paper focuses on the domain prediction problem for statistical machine translation. In the proposed method, a bilingual training corpus, is automatically clustered into sub-corpora. Each sub-corpus is deemed to be a domain. The domain of a source sentence is predicted by using its similarity to the sub-corpora. The predicted domain (sub-corpus) specific language and translation models are then used for the translation decoding. This approach gave an improvement of 2.7 in BLEU (Papineni et al., 2002) score on the IWSLT05 Japanese to English evaluation corpus (improving the score from 52.4 to 55.1). This is a substantial gain and indicates the validity of the proposed bilingual cluster based models. 514 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 514–523, Prague, June 2007. 2007 Association for Computational Linguistics 7 offers a summary and some concluding remarks. 3 Outline of the Proposed Method 2 Domain Specific Models in SMT Our method can be analysed into two processes: an off-line pr"
D07-1054,koen-2004-pharaoh,0,\N,Missing
D07-1054,J03-1002,0,\N,Missing
D07-1054,2006.iwslt-evaluation.1,0,\N,Missing
D07-1054,2006.iwslt-evaluation.15,0,\N,Missing
D09-1117,W05-0909,0,0.0346751,"sented in this paper are given in terms of the BLEU score (Papineni et al., 2001). This metric measures the geometric mean of ngram precision of n-grams drawn from the output translation and a set of reference translations for that translation. There are large number of proposed methods for carrying out machine translation evaluation. Methods differ in their focus of characteristics of the translation (for example fluency or adequacy), and moreover anomolous results can occur if a single metric is relied on. Therefore, we also carried out evaluations using the NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), WER (Hunt, 1989), PER (Tillmann et al., 1997) and TER (Snover et al., 2005) machine translation evaluation techniques. 4 Results The results of the experiments in terms of the BLEU score are given in Tables ??, 5, 3 and 3. These results show the performance of the reverse and bidirectional decoding strategies relative to the usual forward decoding strategy. The cells in the tables that represent experiments in which 1128 ar da de en es fr id it ja ko ms nl pt ru th vi zh ar 58.3 53.8 63.6 57.6 57.8 54.7 54.1 38.2 34.4 54.5 55.1 56.8 51.4 53.8 53.6 32.0 da 47.8 55.5 65.8 58.2 58.3 52.8 53.4 3"
D09-1117,C02-1050,1,0.614722,"case that this is most effective strategy for all language pairs. In this paper we investigate the effect of direction in phrase-based SMT decoding. For the purposes of this paper, we will refer to target word sequence generation that follows the same order as human language production as forward generation, and generation in the opposite direction to human language production as reverse generation. These are often referred ”left-toright” and ”right-to-left” respectively in the literature, but we avoid this notation as many languages are naturally written from right-to-left. In earlier work (Watanabe and Sumita, 2002), it was hypothesized that the optimal direction for decoding was dependent on the characteristics of the target language. Their results show that for Japanese to English translation a reverse decoding strategy was the most effective, whereas for English to Japanese translation, a forward decoding strategy proved superior. In addition they implemented a bidirectional decoder, but their results were mixed. For English to Japanese translation, decoding bidirectionally gives higher performance, but for Japanese to English translation they were unable to improve performance by decoding bidirection"
D09-1117,P07-2045,0,0.0722139,"e effect of direction in decoding is likely to be strongly language dependent. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation of source phrases into t"
D09-1117,koen-2004-pharaoh,0,0.101661,"y suggests that the effect of direction in decoding is likely to be strongly language dependent. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation"
D09-1117,P02-1038,0,0.157398,"nt. The next section briefly describes the mechanisms underlying phrase-based decoding. Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments. Section 3 presents the experiments we performed. Section 4 gives the results and some analysis. Finally in Section 5, we conclude and offer possible directions for future research. 2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002). One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model. For the decoders used in the experiments in this paper, we included the following feature functions: • An n-gram language model over the target word sequence - Ensures the target word sequence is a likely sequence of words in the target language • A phrase translation model - Effects the segmentation of the source word sequence, and is also responsible for the transformation of source phrases into target phrases. • A target word sequence length model - Controls the length"
D09-1117,P03-1021,0,0.029344,"corpus for all experiments, and the corpus used for evaluation consisted of 5000 sentences with a single reference for each sentence. 3.2 Training Each instance of the decoder is a standard phrasebased machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al., 2007) SMT decoders. In these experiments 5-gram language models built with Witten-Bell smoothing were used along with a lexicalized distortion model. The system was trained in a standard manner, using a minimum error-rate training (MERT) procedure (Och, 2003) with respect to the BLEU score (Papineni et al., 2001) on held-out development data to optimize the loglinear model weights. For simplicity, the MERT procedure was performed on independently on the forward and reverse decoders for the bidirectional system, rather them attempting to tune the parameters for the full system. 3.3 3.3.1 Translation Engines Forward The forward decoding translation systems used in these experiments represent the baseline of our experiments. They consist of phrase-based, multistack, beam search decoders commonly used in the field. 3.3.2 Reverse The reverse decoding t"
D09-1117,2001.mtsummit-papers.68,0,0.246121,"used for evaluation consisted of 5000 sentences with a single reference for each sentence. 3.2 Training Each instance of the decoder is a standard phrasebased machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al., 2007) SMT decoders. In these experiments 5-gram language models built with Witten-Bell smoothing were used along with a lexicalized distortion model. The system was trained in a standard manner, using a minimum error-rate training (MERT) procedure (Och, 2003) with respect to the BLEU score (Papineni et al., 2001) on held-out development data to optimize the loglinear model weights. For simplicity, the MERT procedure was performed on independently on the forward and reverse decoders for the bidirectional system, rather them attempting to tune the parameters for the full system. 3.3 3.3.1 Translation Engines Forward The forward decoding translation systems used in these experiments represent the baseline of our experiments. They consist of phrase-based, multistack, beam search decoders commonly used in the field. 3.3.2 Reverse The reverse decoding translation systems used in these experiments were exact"
D09-1117,J93-2003,0,\N,Missing
D09-1117,P02-1040,0,\N,Missing
D09-1117,2006.iwslt-evaluation.1,0,\N,Missing
D09-1117,2002.tmi-tutorials.2,0,\N,Missing
D12-1003,N07-1026,0,0.0604188,"Missing"
D12-1003,C10-1003,0,0.604363,"ered when computing bilingual relations, which have been neglected in previous methods. In addition to the co-occurrence-based graph construction, we propose a similarity graph, which also takes into account context similarities between words. The main contributions of this paper are as follows: • We propose a bilingual lexicon extraction method that captures co-occurrence relations with all the seeds, including indirect relations, using graph-based label propagation. In our experiments, we confirm that the proposed method outperforms conventional context-similarity-based methods (Rapp, 1999; Andrade et al., 2010), and works well even if the coverage of a seed lexicon is low. • We propose a similarity graph which represents context similarities between words. In our experiments, we confirm that a similarity graph is more effective than a co-occurrence-based graph. 2 Context-Similarity-based Extraction Method The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of e"
D12-1003,W11-1203,0,0.623374,"ertain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different languages are mapped into the same space. Previous methods generally use a seed bilingual lexicon for this mapping. After that, similarities are calculated b"
D12-1003,C02-2020,0,0.0683871,"ニア (piranha)”. There are three context words for the query. However, the information on co-occurrence with “淡水 (freshwater)” disappears after the context vector is mapped, because the seed lexicon does not include “淡水 (freshwater)”. The same thing happens with the English word “piranha”. As a result, the pair of “ピラニア (piranha)” and “anaconda” could be wrongly identified as a translation pair. Some previous work focused on the problem of seed lexicon limitation. Morin and Prochasson (2011) complemented the seed lexicon with bilingual lexicon extracted from parallel sentences. Koehn and Knight (2002) used identically-spelled words in two languages as a seed lexicon. However, the method is not applicable for language pairs with different types of characters such as English and Japanese. Hazem et al. (2011) exploited k-nearest words for a query, which is very sensitive to the parameter k. Some previous work did not require any seed lexicon. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. Ismail and Manandhar (2010) introduced a similarity measure between two words in different languages"
D12-1003,I05-1062,0,0.0123874,"d captures relations with all the seeds including indirect relations by propagating seed information. Moreover, we proposed using similarity graphs in propagation process in addition to cooccurrence graphs. Our experiments showed that the proposed method outperforms conventional contextsimilarity-based methods (Rapp, 1999; Andrade et al., 2010), and the similarity graphs improve the performance by clustering synonyms into the same translation. We are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (Daille and Morin, 2005; Morin et al., 2007). In addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora (Kok and Brockett, 2010). We will utilize their random walk approach or other graph-based techniques such as modified adsorption (Talukdar and Crammer, 2009) for generating seed distributions. We are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an MT system. Acknowledgments We thank anonymous reviewers of EMNLP-CoNLL 2012 for helpful suggestions and comments on a first version of this"
D12-1003,P11-1061,0,0.0421256,"i (or vj ). Then, in edge pruning, we preserve the edges with top 100 weight for each vertex. 3.2 Seed Propagation LP is a graph-based technique which transfers the labels from labeled data to unlabeled data in order to infer labels for unlabeled data. This is primarily used when there is scarce labeled data but abundant unlabeled data. LP has been successfully applied in common natural language processing tasks such as word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), multi-class lexicon acquisition (Alexandrescu and Kirchhoff, 2007), and part-of-speech tagging (Das and Petrov, 2011). LP iteratively propagates label information from any vertex to nearby vertices through weighted edges, and then a label distribution for each vertex is generated where the weights of all labels add up to 1. We adopt LP to obtain relations with all bilingual seeds including indirect relations by treating each seed as a label. First, each translated seed is assigned to a label, and then the labels are propagated in the graph described in Section 3.1. The seed distribution for each word is initialized as follows:  if vi ∈ Vs and z = vi  1 0 0 if vi ∈ Vs and z = vi , qi (z) =  u(z) otherwise"
D12-1003,P11-2071,0,0.0913553,"Missing"
D12-1003,C02-1166,0,0.042563,"ニア (piranha)”. There are three context words for the query. However, the information on co-occurrence with “淡水 (freshwater)” disappears after the context vector is mapped, because the seed lexicon does not include “淡水 (freshwater)”. The same thing happens with the English word “piranha”. As a result, the pair of “ピラニア (piranha)” and “anaconda” could be wrongly identified as a translation pair. Some previous work focused on the problem of seed lexicon limitation. Morin and Prochasson (2011) complemented the seed lexicon with bilingual lexicon extracted from parallel sentences. Koehn and Knight (2002) used identically-spelled words in two languages as a seed lexicon. However, the method is not applicable for language pairs with different types of characters such as English and Japanese. Hazem et al. (2011) exploited k-nearest words for a query, which is very sensitive to the parameter k. Some previous work did not require any seed lexicon. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. Ismail and Manandhar (2010) introduced a similarity measure between two words in different languages"
D12-1003,W11-1204,0,0.0705797,"Missing"
D12-1003,C94-2178,0,0.0448485,"onservatism” in English. The proposed methods merge different senses by propagating seeds through these polysemous words in only one language side. This is why translation pairs could have wrong seed distributions and then the proposed methods could not identify correct translation pairs. We will leave this word sense disambiguation problem for future work. 6 Related Work Besides the comparable corpora approach discussed in Section 2, many alternatives have been proposed for bilingual lexicon extraction. The first is a method that finds translation pairs in parallel corpora (Wu and Xia, 1994; Fung and Church, 1994; Och and Ney, 2003). However, large parallel corpora are only available for a few language pairs and for limited domains. Moreover, even the large parallel corpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from the mixed-language text"
D12-1003,W97-0119,0,0.0676358,"The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneit"
D12-1003,P98-1069,0,0.941339,"generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in context"
D12-1003,W95-0114,0,0.749213,"rities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Na"
D12-1003,W09-1117,0,0.116105,"eps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a contex"
D12-1003,P04-1067,0,0.821213,"Missing"
D12-1003,P08-1088,0,0.774795,"sually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learni"
D12-1003,W11-1206,0,0.10853,"arity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference o"
D12-1003,H05-1061,0,0.00755926,"rpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from the mixed-language texts utilizing various clues: Zhang and Vines (2004) used cooccurrence statistics, Cheng et al. (2004) used cooccurrences and context similarity information, and Huang et al. (2005) used phonetic, semantic and frequency-distance features. Lin et al. (2008) proposed a method for extracting parenthetically translated terms, where a word alignment algorithm is used for establishing the correspondences between in-parenthesis and pre-parenthesis words. However, those methods cannot find translation pairs when they are not connected with each other through link structures, or when they do not co-occur in the same text. Transliteration is a completely different way for bilingual lexicon acquisition, in which a word in one language is converted into another language using phonet"
D12-1003,C10-2055,0,0.242532,"seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 24–36, Jeju Island"
D12-1003,W02-0902,0,0.880473,"act word translation pairs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of tex"
D12-1003,N10-1017,0,0.0168808,"Missing"
D12-1003,C10-1070,0,0.0987065,"2 Context-Similarity-based Extraction Method The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (An"
D12-1003,C10-2070,0,0.563014,"decessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different languages are mapped into the same space. Previous methods generally use a"
D12-1003,P08-1113,0,0.0531508,"Missing"
D12-1003,W11-1205,0,0.171411,"rs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 20"
D12-1003,P07-1084,0,0.0601746,"h all the seeds including indirect relations by propagating seed information. Moreover, we proposed using similarity graphs in propagation process in addition to cooccurrence graphs. Our experiments showed that the proposed method outperforms conventional contextsimilarity-based methods (Rapp, 1999; Andrade et al., 2010), and the similarity graphs improve the performance by clustering synonyms into the same translation. We are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (Daille and Morin, 2005; Morin et al., 2007). In addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora (Kok and Brockett, 2010). We will utilize their random walk approach or other graph-based techniques such as modified adsorption (Talukdar and Crammer, 2009) for generating seed distributions. We are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an MT system. Acknowledgments We thank anonymous reviewers of EMNLP-CoNLL 2012 for helpful suggestions and comments on a first version of this paper. We also thank"
D12-1003,P05-1049,0,0.0557205,"Missing"
D12-1003,J03-1002,0,0.0104573,"ocuments of all the domains. The Japanese texts were segmented and part-ofspeech tagged by ChaSen7 , and the English texts were tokenized and part-of-speech tagged by TreeTagger (Schmid, 1994). Next, function words were removed since function words with little semantic information spuriously co-occurred with many words. As a result, the number of distinct words in Japanese corpus and English corpus amounted to 1,111,302 and 4,099,8258 , respectively. We employed seed lexicons from two sources: (1) EDR bilingual dictionary (EDR, 1990), (2) automatic word alignments generated by running GIZA++ (Och and Ney, 2003) with the NTCIR parallel data consisting of 3,190,654 parallel sentences. From each source, we extracted pairs of nouns appearing in our corpus. From (2), we excluded word pairs where the average of 2-way translation proba6 SECTION G of IPC code indicates the physics domain. http://chasen-legacy.sourceforge.jp/ 8 The English words contain words in tables or mathematical formula but the Japanese words do not because the data format differs between English and Japanese. This is why the number of English words is larger than that of Japanese words, even though the number of English documents is s"
D12-1003,P11-1133,0,0.285386,"tors in accordance with the notion that importance varies by context positions. Gaussier et al. (2004) mapped context vectors via latent classes to capture synonymy and polysemy in a seed lexicon. Fiˇser et al. (2011) and Kaji (2005) calculated 2-way similarities. Step 3. Finding translation pairs: A pair of words is treated as a translation pair when their context similarity is high. Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (D´ejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004). 2.1 Problems from Previous Works Most of previous methods used a seed bilingual lexicon for mapping modeled contexts in two different languages into the same space. The mapping heavily relies on the entries in a given bilingual lexicon. Therefore, if the coverage of the seed lexicon is low, 26 the context vectors become sparser and its discriminative capability becomes lower, leading to extraction of incorrect translation equivalents. Consider the example in Figure 1, where a context-similarity-based method and our proposed method find tra"
D12-1003,P95-1050,0,0.887097,"ntext similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Comp"
D12-1003,P99-1067,0,0.751917,", the automatic building of bilingual lexicons from corpora is one of the issues that have attracted many researchers. As a solution, a number of previous works proposed extracting bilingual lexicons from comparable corpora, in which documents were not direct translations but shared a topic or domain1 . The use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains. Most of the previous methods are based on assumption (I), that a word and its translation tend to appear in similar contexts across languages (Rapp, 1999). Based on this assumption, many methods calculate word similarity using context and then extract word translation pairs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon."
D12-1003,C04-1089,0,0.494708,"dow (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different language"
D12-1003,P11-2084,0,0.397648,"Missing"
D12-1003,1994.amta-1.26,0,0.191355,"ans “right” and “conservatism” in English. The proposed methods merge different senses by propagating seeds through these polysemous words in only one language side. This is why translation pairs could have wrong seed distributions and then the proposed methods could not identify correct translation pairs. We will leave this word sense disambiguation problem for future work. 6 Related Work Besides the comparable corpora approach discussed in Section 2, many alternatives have been proposed for bilingual lexicon extraction. The first is a method that finds translation pairs in parallel corpora (Wu and Xia, 1994; Fung and Church, 1994; Och and Ney, 2003). However, large parallel corpora are only available for a few language pairs and for limited domains. Moreover, even the large parallel corpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from"
D12-1003,C98-1066,0,\N,Missing
D12-1003,J98-4003,0,\N,Missing
D14-1019,J92-4003,0,0.555798,"do move label X tentatively to cluster K compute F (C) for this exchange move label X to cluster with maximum F (C) do until the cluster mapping does not change 3.1 Optimization Criterion The generative probability in each rule of the form of Equation (6) can be approximated by clustering nonterminal symbols as follows: p(Y, Z|X) ≈ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y ), c(Z)|c(X)) (7) Table 1: Outline of syntax-label clustering method where we map a syntax label X to its equivalence cluster c(X). This can be regarded as the clustering criterion usually used in a class-based n-gram language model (Brown et al., 1992). If each label on the right-hand side of a synchronous rule (4) is independent of each other, we can factor the joint model as follows: where C denotes all clusters and N denotes all syntax labels. For Equation (11), the last summation is equivalent to the sum of the occurrences of all syntax labels, and canceled out by the first summation. K in the third summation considers clusters in a synchronous rule whose left-hand side label is X, and we let ch(X) denote a set of those clusters. The second summation equals ∑ K∈C N (K) · log N (K). As a result, Equation (11) simplifies to p(Y, Z|X) ≈ p("
D14-1019,P07-2045,0,0.00610793,"and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number for We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4 . We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3 LDC2003E14 Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in the Moses 4 168 each cluster number when using the IWSLT07 dataset. Since the +clustering maximizes the likelihood of synchronous rules, it can introduce appropriate rules adapted to training data given a fixed number of clusters. For e"
D14-1019,N12-1047,0,0.015901,"n zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number for We did experiments with the SAM"
D14-1019,J07-2003,0,0.806338,"on (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules. Each synchronous rule in R takes the form X → ⟨α, β, ∼⟩ 165 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165–171, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics The clustering of Hanneman and Lavie proved successful in decreasing"
D14-1019,P06-1077,0,0.0160689,"rectly maximizing the likelihood of synchronous rules, whereas previous work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonte"
D14-1019,P10-1146,0,0.0858067,"distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules."
D14-1019,P11-1065,0,0.0387772,"Missing"
D14-1019,P06-1121,0,0.0544737,"evious work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targe"
D14-1019,P11-2093,0,0.0346993,"tuning set has seven English references and the test set has six English references. For zh-en data we prepared two kind of data. The one is extracted from FBIS3 , which is a collection of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). Label Rule F(C) SD 70 5,460 80 80 2.1 M 60 M 32 M 38 M -1.5 e+10 -7.9 e+09 526 154 70 7,328 80 12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The lang"
D14-1019,W08-0509,0,0.0184457,"M 32 M 38 M -1.5 e+10 -7.9 e+09 526 154 70 7,328 80 12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation tim"
D14-1019,P02-1040,0,0.088965,"12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU scor"
D14-1019,N13-1029,0,0.643228,"ho, Soraku-gun, Kyoto, JAPAN {hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jp Abstract which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended syntactical information may improve the coverage of rules and syntactic correctness in translation, the increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the likelihood of synchronous rules instead of the similarity of probabilistic distributions of syntax labels. Second, we use exchange clustering (Uszkoreit and Brants, 2008), which is faster than the agglomerativeclustering algorithm used in the previous work. We tested our"
D14-1019,N06-1031,0,0.0258994,"d only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is"
D14-1019,N03-1017,0,0.0178391,"Missing"
D14-1019,P08-1086,0,0.0580322,"he increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the likelihood of synchronous rules instead of the similarity of probabilistic distributions of syntax labels. Second, we use exchange clustering (Uszkoreit and Brants, 2008), which is faster than the agglomerativeclustering algorithm used in the previous work. We tested our proposed method on JapaneseEnglish and Chinese-English translation tasks and observed gains comparable to those of previous work with similar reductions in grammar size. Recently, syntactic information has helped significantly to improve statistical machine translation. However, the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules, especially when syntax labels are projected from a parser in syntax-augmented machine transl"
D14-1019,W06-3119,0,0.604595,"se-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules. Each synchronous rule in R takes the form X → ⟨α, β, ∼⟩ 165 Proceedings of the 2014 Conference on Em"
D14-1019,N07-1051,0,\N,Missing
D14-1022,C10-2044,0,0.0352255,"translation spans, respectively. A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span. However, a source span whose first and last words are both boundaries is not always a translation span. In Figure 1, “I” is a beginning boundary since it is the first word of translation span “I will” and “experiment” is an ending boundary since it is the last word of translation span “finish this experiment” , but “I will finish this experiment” is not a translation span. This happens because the translation spans are nested or hierarchical. Note that (He et al., 2010) also learned phrase boundaries to constrain decoding, but their approach identified boundaries only for monotone translation. In this paper, taking fully into account that translation spans being nested, we propose an approach to learn hierarchical translation spans directly from an aligned parallel corpus that makes more accurate identification over translation spans. The rest of the paper is structured as follows: In Section 2, we briefly review the HPB translation model. Section 3 describes our approach. We describe experiments in Section 4 and conclude in Section 5. 2 w (X → hγ, α, ∼i) ="
D14-1022,N03-1017,0,0.0383474,"118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning"
D14-1022,P07-2045,0,0.0296419,"Missing"
D14-1022,W04-3250,0,0.0165102,"word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the span length growing, the advantage of NN became more significant. Since the class"
D14-1022,P10-1052,0,0.0701371,"Missing"
D14-1022,2011.mtsummit-papers.28,0,0.029711,"ine system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English"
D14-1022,P08-1114,0,0.0223738,"t require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using"
D14-1022,J96-1002,0,0.0295998,"ta for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and trigram of the phrase [fi−3 , ..., fj+3 ]. As the second method, these classification tasks are learned in the continuous space using feedforward neural networks (NNs). Each Ck has the similar structure with the NN language model (Vaswani et al., 2013). The inputs to the NN are indices of the words: [fi−3 , ..., fj+3 ]. Each source word is projected into an N dimensional vecto"
D14-1022,P05-1033,0,0.124086,"wever, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “experiment by”, then the Chinese translation can not be correctly obtained, because the target span projected from “exIntroduction The hierarchical phrase-based (HPB) translation model (Chiang, 2005) has been widely adopted in statistical machine translation (SMT) tasks. The HPB translation rules based on the synchronous context free grammar (SCFG) are simple and powerful. One drawback of the HPB model is the applications of translation rules to the input sentence are highly ambiguous. For example, a rule whose English side is “X1 by X2” can be applied to any word sequence that has “by” in them. In Figure 1, this rule can be applied to the whole sentence as well as to “experiment by tomorrow”. In order to tackle rule application ambiguities, a few previous works used syntax trees. Chiang"
D14-1022,P11-1065,0,0.036752,"Missing"
D14-1022,P10-2002,0,0.0140531,"experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “expe"
D14-1022,P02-1038,0,0.10792,"0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the unbalanced rate of positive and negative instances in the training corpus. For example, if there are more positive traini"
D14-1022,J03-1002,0,0.0125398,"rds #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classi"
D14-1022,P03-1021,0,0.0214277,".08 0.73 0.52 0.36 0.26 0.20 0.16 0.13 0.10 0.08 JE ME P N 0.85 0.79 0.69 0.84 0.56 0.89 0.48 0.93 0.30 0.96 0.25 0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the"
D14-1022,P02-1040,0,0.0924408,"erence at the p < 0.01 level and - represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our"
D14-1022,2006.amta-papers.25,0,0.020084,"- represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the s"
D14-1022,D13-1140,0,0.074174,"ab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and t"
D14-1022,W06-0127,1,0.879402,"i to n do 3: if ∃eqp , 1 ≤ p ≤ q ≤ m & ∃ (k, t) ∈ A, i ≤ k ≤ j, p ≤ t ≤ q & ∀ (k, t) ∈ A, i ≤ k ≤ j ↔ p ≤ t ≤ q then 4: fij is a positive instance for Cj−i+1 5: else 6: fij is a negative instance for Cj−i+1 7: end if 8: end for 9: end for 3.3 Experiment CE JE #Sents #Words #Vocab #Sents #Words #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical"
D14-1022,N10-1016,0,\N,Missing
D14-1023,P14-1142,1,0.292552,"i ∈ V0 Pb (wi |hi ) otherwise 0 (1) where V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calc"
D14-1023,W07-0733,0,0.109437,"Missing"
D14-1023,P07-2045,0,0.00563399,"ased and the BLEU scores trended to increase. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences,"
D14-1023,D13-1106,0,0.0135132,"accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the trainSince larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual"
D14-1023,W04-3250,0,0.0727048,"est lists of SMT. Our previous converted LM, Arsoy’s grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM5 . To reduce the randomness of MERT, we used two methods for tuning the weights of different SMT features, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indicated that the MERT was run independently by three times and the average BLEU scores were taken. We also performed the paired bootstrap resampling test (Koehn, 2004)6 . Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy’s grown LMs with the same IDs for SMT (“++/−−”: significantly better/worse at α = 0.01, “+/−”: α = 0.05, no mark: not significantly better/worse at α = 0.05). From the results shown in Table 1, we can get the following observations: (1) Nearly all the bilingual grown LMs outperformed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LMs is increased, the PPL always decreased and the BLEU"
D14-1023,2012.eamt-1.60,0,0.0244855,"200240, China 2 Multilingual Translation Laboratory, MASTAR Project, National Institute of Information and Communications Technology, 3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan 3 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, whic"
D14-1023,P14-1140,0,0.0260099,"Missing"
D14-1023,P14-1129,0,0.0325731,"Missing"
D14-1023,2012.iwslt-papers.3,0,0.044911,"s very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translati"
D14-1023,J03-1002,0,0.00542098,"e. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002;"
D14-1023,I13-1170,1,0.794008,"ed by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the"
D14-1023,P03-1021,0,0.0161605,"can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained"
D14-1023,P95-1030,0,0.0389552,"rpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, ∗ Part of this work was done as Rui Wang visited in NICT. 189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ing corpus can be generated by using so"
D14-1023,D13-1140,0,0.0268942,"Missing"
D14-1023,P06-2093,0,0.0965172,", blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram"
D14-1023,D13-1082,1,0.598465,"Missing"
D14-1023,W12-2702,0,0.0279048,".go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to i"
D14-1023,I13-1069,1,0.740408,"Missing"
D14-1023,C12-2131,1,0.530178,"ist,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach."
D14-1023,C12-3067,1,0.224044,"re V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our"
D14-1023,D10-1076,0,0.35607,"{mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are seve"
D14-1023,N12-1005,0,0.0314071,"In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural ne"
D14-1173,li-etal-2010-enriching,0,0.0242748,"rst briefly describes the GALE WA corpus, then presents an analysis of the WS arising from a CTB-standard word segmenter with reference to the segmentation of the atomic blocks in the GALE WA corpus, finally the impact of the findings on SMT is discussed. 2.1 GALE WA corpus The GALE WA corpus was developed by the LDC, and was used as training data in the DARPA GALE global autonomous language exploitation program 5 . The corpus incorporates linguistic knowledge into word aligned text to help improve automatic WA and translation quality. It employs two annotation schemes: alignment and tagging (Li et al., 2010). Alignment identifies minimum translation units and translation relations; tagging adds contextual, syntactic and languagespecific features to the alignment annotation. For example, the sample shown in Figure 1 carries tags on both alignment edges and tokens. The GALE WA corpus contains 18,057 manually word aligned Chinese and English parallel sentences which are extracted from newswire and web blogs. Table 1 presents the statistics on the corpus. One third of the sentences are approximately newswire text, and the remainder consists of web blogs. 2.2 Analysis of WS In order to produce a Chine"
D14-1173,W08-0336,0,0.0775486,"s. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently"
D14-1173,D09-1075,0,0.0804234,"the boundaries in the CTB WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictio"
D14-1173,P07-1003,0,0.0245675,"re often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods"
D14-1173,P05-1045,0,0.00487485,"es were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase"
D14-1173,P09-1104,0,0.0131808,"T. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro"
D14-1173,P07-2045,0,0.00855791,"ord named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner and the proposed lexical splitter are presented in Table 6. The proposed method achieves the highest scores on all the criterion of F1 , precision and recall. The length tuner outperforms the CTB segmenter in terms of recall, but with lower precision. 8 http://nlp.stanford.edu/software/ CRF-NER.shtml 9 http://www.statmt.org/moses/giza/ GIZA++.html 10"
D14-1173,N06-1014,0,0.651297,"t the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Confere"
D14-1173,E09-1063,0,0.0167738,"There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes both analyzing WS and improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the C"
D14-1173,W03-0301,0,0.0319011,"orpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012"
D14-1173,C10-1092,0,0.107692,"B WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictionary of segmented wor"
D14-1173,C00-2163,0,0.802782,"Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,"
D14-1173,J03-1002,0,0.00860593,"posed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner a"
D14-1173,P03-1021,0,0.039167,"nce samples which contain one Chinese sentence and four English reference sentences. The experimental corpus for unsupervised WA was the union set of the NIST OpenMT training set and the 2000 test sentence pairs from GALE WA corpus. We removed the United Nations corpus from the NIST OpenMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The"
D14-1173,P02-1040,0,0.0990795,"aluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The characte"
D14-1173,C04-1081,0,0.0484789,"roach This paper employs a condition random field (CRF) to solve this sequence labeling task (Lafferty et al., 2001). A linear-chain CRF defines the conditional probability of y given x as, T 1 XX λk fk (yt−1 , yt , x, t)), ( Zx t=1 k (2) where Λ = {λ1 , . . .} are parameters, Zx is a perinput normalization that makes the probability of all state sequences sum to one; fk (yt−1 , yt , x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), PΛ (y|x) = X X λ2 k Λ∗ = argmax( logPΛ (yi |xi ) − 2 ), 2δ Λ k i k (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk . When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y ∗ that maximizes the conditional probability, y∗ = argmax PΛ (y|x). y (4) In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The fol"
D14-1173,W03-1719,0,0.02299,"nMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the"
D14-1173,I05-3027,0,0.0191438,"d by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid sear"
D14-1173,C08-1128,0,0.0713041,"Missing"
D14-1173,C02-1145,0,0.0532368,"on those languages that have no explicit space between words, such as Arabic, Chinese and Japanese. As the first processing step, WS affects all successive steps, thus it has a large potential impact on the final performance. For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually"
D14-1173,2002.tmi-tutorials.2,0,0.0201863,"ET (determiner) are tags of tokens. Genre Newswire Web blog Total # Files 2,175 2,560 4,735 # Sentences† 6,218 11,839 18,057 # CN tokens 246,371 373,818 620,189 # EN tokens 205,281 312,856 518,137 # Alignment edges 164,033 257,730 421,763 Table 1: GALE WA corpus. † Sentences rejected by the annotators are excluded. four atomic blocks; the CTB segmenter produces five words which all locate within the blocks, so they are all small enough. • Alignment inconsistent: the word aligns to more than one atomic block, but the target expression is contiguous, allowing for correct phrase pair extraction (Zens et al., 2002). For example, in Figure 2(b), the characters in the word “shuang fang”, which is produced by the CTB segmenter, contains two atomic blocks, but the span of the target “to both side” is continuous, therefore the phrase pair • Alignment inconsistent and extraction hindered: the word aligned to more than one atomic block, and the target expression is not contiguous, which hinders correct phrase pair extractions. For example, in Figure 2(c), the word “zeng chan” has to be split in order to match the target language. Table 2 shows the statistics of the three categories of CTB WS on the GALE WA cor"
D14-1173,W03-1726,0,\N,Missing
D15-1119,N15-1105,0,0.0443516,"Missing"
D15-1119,P10-4002,0,0.060743,"., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned, the word alignment is noisy and our intention is a robust and rough process; therefore, we restricted seg rev to two applications and did not consider the difference in sentence lengths or different languages during training. Within each iteration, fast alig"
D15-1119,N13-1073,0,0.208935,"), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimation algorithm is devised. It has been reported that the fast align approach is more than 10 times faster than baseline GIZA++, with comparable results in end-to-end French-, Chinese-, and Arabic-to-English translation experiments. Howe"
D15-1119,P15-2023,0,0.0125535,"ing time becomes 2–4 times that of a baseline fast align, which is still at least 2 – 4 times faster than the training time required by baseline GIZA++. Results for German-, French-, and Chinese-English translations are also reported. 2 Segmenting-Reversing Reordering The seg rev is inspired by the “REV preorder” (Katz-Brown and Collins, 2008), which is a simple pre-reordering approach originally designed for the Japanese-to-English translation task. More efficient pre-reordering approaches usually require trained parsers and sophisticated machine learning frameworks (de Gispert et al., 2015; Hoshino et al., 2015). We adopt the REV method in KatzBrown and Collins (2008) considering it is the simplest and lightest pre-reordering approach (to our knowledge), which may bring a minimal effect on the efficiency of fast align. An example seg rev process, where the word alignment is generated by fast align, is illustrated in Fig. 1. The example we selected has relatively correct word alignment and seg rev performs well. In general cases, the alignment has significant noise and the reordering is rougher . Algorithm 1 describes the repeated (δ times) application of the seg rev process, and Algorithm 2 describes"
D15-1119,N03-1017,0,0.065222,"oblem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (D"
D15-1119,P07-2045,0,0.0216472,"tion 4 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et"
D15-1119,W04-3250,0,0.0329281,"T data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned,"
D15-1119,2005.mtsummit-papers.11,0,0.0460474,"ligible. Note that seg rev processes are accelerated easily by parallel processing. 3 GIZA++ FAλini =4.0 FAλini =0.1 iteration 2 iteration 3 iteration 4 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast a"
D15-1119,P06-1077,0,0.0458507,"t the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-o"
D15-1119,J06-4004,0,0.0845468,"Missing"
D15-1119,P11-1064,1,0.905317,"Missing"
D15-1119,J03-1002,0,0.0215403,"align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantag"
D15-1119,P03-1021,0,0.0453797,"he approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach"
D15-1119,P02-1040,0,0.0977428,"ly different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value an"
D15-1119,D11-1046,0,0.117983,"task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimati"
D15-1119,J07-2003,0,\N,Missing
D15-1128,P07-2045,0,0.0216808,"lish-Spanish) and (English-French), and also language pairs that required a greater amount of word re-ordering for 1 http://www.ted.com example (English-Chinese). The Chinese corpus was segmented using the Stanford Chinese word segmenter (Tseng et al., 2005) according to the Chinese Penn Treebank standard. 3.2 Experimental Methodology Our stream decoder was implemented within the framework of the AUGUSTUS decoder, a hierarchical statistical machine translation decoder (Chiang, 2007) that operates in a similar manner to the moses-chart decoder provided in the Moses machine translation toolkit (Koehn et al., 2007). The training procedure was quite typical: 5-gram language models were used, trained with modified 1092 English input stream: ... we want to encourage a world of creators of inventors of contributors because this world that we live in this interactive world is ours ... Sequence of translated segments: Segment 1: Segment 2: Segment 3: Segment 4: Segment 5: Segment 6: Segment 7: Segment 8: queremos animar a un mundo de creadores de inventores de colaboradores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of"
D15-1128,2008.iwslt-papers.5,0,0.161611,"sing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper (Kolss et al., 2008a) for a complete description of the stream decoding process; in this section we provide a brief summary. Figure 1 depicts a stream decoding process, and the figure applies"
D15-1128,N13-1023,0,0.132338,"segmentation. This approach has the advantage that it can be implemented without the need to modify the machine translation decoding software. In the second type of strategy, which we will call incremental decoding, the segmentation process is performed during the decoding of the input stream. In this approach the segmentation process is able to exploit segmentation cues arising from the decoding process itself. That is to say, the order in which the decoder would prefer to generate the target sequence is taken into account. A number of diverse strategies for presegmentation were studied in (Sridhar et al., 2013). They studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigatio"
D15-1128,I05-3027,0,0.0491287,"Missing"
D15-1128,P06-1098,0,0.456205,"rching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper ("
D15-1128,P03-1021,0,0.146861,"Missing"
D15-1128,P14-2090,0,0.161343,"studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigation. In (Oda et al., 2014) a strategy for segmentation prior to decoding based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter"
D15-1128,2001.mtsummit-papers.68,0,0.0256782,"radores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of collaborators] [because this world] [in which we live] [this interactive world] [is ours] Figure 4: Example translation segmentation from the English-Spanish task (Lmax = 8 and Lmin = 4). Kneser-Ney smoothing; MERT (Och, 2003) was used to train the log-linear weights of the models; the decoding was performed with a distortion limit of 20 words. To allow the results to be directly comparable to those in (Finch et al., 2014), the talk level BLEU score (Papineni et al., 2001) was used to evaluate the machine translation quality in all experiments. 3.3 Results The results for decoding with various values of the latency parameters are shown in Figure 3 for English-French, English-Spanish, English-Arabic, English-Hebrew, English-Russian and EnglishChinese. Overall the behavior of the system was quite similar in character to the published results for phrase-based stream decoding for EnglishSpanish (Kolss et al., 2008b; Finch et al., 2014). The hierarchical system seemed to be more sensitive to small values of minimum latency, and less sensitive to larger values. The r"
D15-1128,P02-1040,0,\N,Missing
D15-1128,J07-2003,0,\N,Missing
D15-1128,D08-1076,0,\N,Missing
D15-1143,N10-1028,0,0.0896146,"nsduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to p"
D15-1143,P09-1088,0,0.107802,"a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model o"
D15-1143,W10-1703,0,0.0126229,"ions, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) w"
D15-1143,J14-1007,0,0.0220988,"Missing"
D15-1143,P11-2031,0,0.0259328,"here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that the gains were achieved with the comparable grammar size. When comparing German-English and Japanese-English language pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+future) achieved further gains over the slice sampling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-Eng"
D15-1143,C10-2021,0,0.0174855,"ing et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the"
D15-1143,P13-1077,0,0.0150821,"lating the detailed balance, its time complexity of O(|f |3 |e|3 ) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let Σ denote a set of terminal symbols in the source langua"
D15-1143,W06-3105,0,0.425911,"ecreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A"
D15-1143,D08-1033,0,0.045527,"Missing"
D15-1143,P10-4002,0,0.0242682,"or those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to s"
D15-1143,D11-1125,0,0.0169852,"65.5k fr-en 1.54M 1.83M 55.6M 65.5k 72.5k 61.9k 70.5k ja-en 1.80M 2.03M 27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better t"
D15-1143,D07-1103,0,0.194885,"basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu,"
D15-1143,N03-1017,0,0.199013,"ilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to score every phrase pair created during decoding with ba"
D15-1143,P07-2045,0,0.0145853,"Missing"
D15-1143,D12-1021,0,0.0497587,"Missing"
D15-1143,D12-1088,0,0.038562,"Missing"
D15-1143,W02-1018,0,0.0414167,"ntly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom"
D15-1143,P11-1064,1,0.480702,"s sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sam"
D15-1143,J03-1002,0,0.0330693,"Missing"
D15-1143,P03-1021,0,0.0510498,"heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at least once in the sample. During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior (f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4 . The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probabil"
D15-1143,C12-1176,0,0.200354,"granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Euro"
D15-1143,D12-1089,0,0.0958848,"a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg"
D15-1143,N13-1038,0,0.0306641,"tion. If the Score(rspi ) is less than usp , we prune the rspi from cube. Similar to Blunsom and Cohn (2010), if the span sp is not in the current derivation, the rules with low probability are pruned acd denotes a rule in cording to Equation (14). Let rsp d with span sp, P (d|u) is calculated by: sp∈d d ) P (rsp ∑ rj ∈rsp P (rj )I(usp &lt; Score(rj )) |φ | p| ∏ [θp ]dpp ∏ |φ (14) P (usp |d) = Beta(usp ; a, 1.0), ∏ pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample ⟨dp , θp"
D15-1143,P02-1040,0,0.0940086,"27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that"
D15-1143,D14-1180,0,0.0144786,"ion. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f |3 |e|3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xia"
D15-1143,P06-1124,0,0.367232,"2: Derivation tree generated from the hierarchical back-off model we reach phrase pairs which are generated without any back-offs. Let a discount parameter be dp , a strength parameter be θp , and a base measure be Gp0 . More formally, the generative process is represented as follows: GX ∼ Prule (dr , θr , Gphrase ), Gphrase ∼ Pphrase (dp , θp , GX ), X → ⟨s/t⟩ ∼ Gphrase , X → ⟨α/β⟩ ∼ GX , (4) where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. Pphrase is composed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation (2): ck − dp · |φpk | , θ p + np (5) where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: θp + dp · |φp | cback + γb · Gb · θp + np cback + cbase + γb ·Prule (dr , θr , Gphrase ) ∏ · Pphrase (dp , θp , GX ), X∈⟨α/β⟩ base: As an alternative to the back-off state, we may rea"
D15-1143,J97-3002,0,0.677402,"007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) m"
D15-1143,J07-2003,0,\N,Missing
D15-1209,W13-2201,0,0.0199167,"ing the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment. The Chinese word “HE ZHANG,” denoted wr , which means river custodian, only occurs once in the whole corpus. We performe"
D15-1209,H93-1039,0,0.474031,"Missing"
D15-1209,J93-2003,0,0.167506,"and BLEU scores of end-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects"
D15-1209,2013.iwslt-evaluation.1,0,0.0234888,"Missing"
D15-1209,P07-1003,0,0.0292356,"essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted"
D15-1209,D08-1033,0,0.0481387,"Missing"
D15-1209,N13-1073,0,0.0288713,"st statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish"
D15-1209,P08-1112,0,0.0529177,"Missing"
D15-1209,P07-2045,0,0.00687099,"nd converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Wat"
D15-1209,2005.mtsummit-papers.11,0,0.0582682,"GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings"
D15-1209,N06-1014,0,0.168812,"e pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a rea"
D15-1209,P11-1064,1,0.897734,"Missing"
D15-1209,C00-2163,0,0.833427,"nd-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a"
D15-1209,J03-1002,0,0.26461,"d word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Gra"
D15-1209,P03-1021,0,0.015174,"d was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2). Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines. 4.1 Experimental Settings The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server i"
D15-1209,C02-1145,0,0.0241296,"nMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imp"
D15-1209,P14-1072,0,0.0607884,"ing phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)"
D15-1209,P13-1083,1,0.86032,"el 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003). T"
D15-1209,J10-3007,0,0.035402,"Missing"
D15-1209,P12-1033,0,0.0623327,"r et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)"
D15-1209,P14-2122,1,0.835056,"ly based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)                    (c) Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed). 2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014). 3 Methodology This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in Table 1. 3.1 Standard EM for IBM Models 1, 2 and HMM Model To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney,"
D15-1209,P11-1125,1,0.848372,"settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision"
D15-1209,N12-1026,1,0.860338,"07). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1"
D15-1209,P10-1049,0,0.0224644,"e propose a leave-one-out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of proba"
D15-1209,W12-3158,0,0.0196466,"out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al.,"
D15-1250,N15-1027,0,0.0153685,"sh training data from noise by maximize the conditional likelihood, L = log P (v = 1|C, ti ) + k P j=1 log P (v = 0|C, tik ). The normalization cost can be avoided by using p (ti |C) as an approximation of P (ti |C).2 1 If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2 The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source cona +(m−1)/2 text words saii −(m−1)/2 and target history words i−1 ti−n+1 ,   a +(m−1)/2 P v|saii −(m−1)/2 , ti−1 i−n+1 , ti . The BNNJM is learned by a feedforward neural network o with m + n inputs n ai +(m−1)/2 i−1 sai −(m−1)/2 , ti−n+1 ,"
D15-1250,D13-1106,0,0.0273384,"classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost dur"
D15-1250,P14-1129,0,0.0837573,"Missing"
D15-1250,E14-1003,0,0.0168384,"native to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to"
D15-1250,P07-2045,0,0.00974293,"9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples"
D15-1250,P02-1040,0,0.0928294,"tion task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words. First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) Ti contains J individual words, To (Wij ) is how many times Wij occurs in Ti and Ro (Wij ) is how many times Wij occurs in Ri . The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as, I P J P Pg = i=1 j=1 min(To (Wij ),Ro (Wij )) I P J P To (Wij ) i=1 j=1 This general 1-gram translation accuracy does not distinguish word frequency. We use a modified 1-gram translation accuracy that weights infrequent words more heavily, I P J P Pc = i=1 j=1 min(To (Wij ),Ro (Wij ))· I P J P i=1 j=1 1 Occur Wij ( ) To (Wij ) where Occur (Wij ) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word t"
D15-1250,C12-2104,0,0.0869208,"kes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. H"
D15-1250,D14-1003,0,0.0152779,"JM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive nor"
D15-1250,D13-1140,0,0.562022,"t of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE). To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise. This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word"
D15-1250,D11-1104,0,0.0257476,"g 68.2 68.4 0.29 JE Pc 4.15 4.30 3.6 Pg 61.2 61.7 0.81 FE Pc 6.70 6.86 2.4 Table 5: 1-gram precisions and improvements. grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Wij ∈ W ords (Ti ) Pg 70.3 70.9 0.85 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses rea"
D15-1250,W06-0127,0,0.144311,"P align(sai ,ti 0 ) ti 00 ∈U (sai ) align(sai ,ti Experiments We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The wo"
D15-1250,W04-3250,0,0.0992848,"(E) and time (T) in minutes per epoch for each task. Base NNJM BNNJM UPD TPD UPD TPD CE 32.95 34.36+ 34.60+ 32.89 35.05+* JE 30.13 31.30+ 31.50+ 30.04 31.42+ FE 24.56 24.68 24.80 24.50 25.84+* Table 3: Translation examples. Here, S: source; R: reference; T1 uses NNJM; T2 uses BNNJM. 该− &gt;the 移动− &gt;mobile 持续− &gt;continues 到− &gt;to SUM 该− &gt;this 移动− &gt;movement null− &gt;is 持续− &gt;continued 到− &gt;until SUM Table 2: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling (Koehn, 2004). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. 5.2 Results and Discussion Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2. We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the t"
D15-1250,D09-1022,0,0.0791321,"Missing"
D15-1250,P03-1021,0,0.105131,"Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate 3 00 ) where align (sai , ti 0 ) is how many times ti 0 is aligned to sai in the parallel corpus. Note that ti could be unaligned, in"
D16-1210,C00-1004,0,0.0964455,"ulating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since there exists no distinction for sure-possible alignments in the KFTT and BTEC data sets, we treat all alignments of them as sure alignments. 7 We tried values from 0.1 to 1.0 at an interval of 0.1. 2001 Table 2 shows the results of word alignment evaluations8 , where none denotes that the model has no constraint. In KFTT and BTEC Corpus, itg achieved significant improvement against sym and none on IBM Model 4 (p ≤ 0.05)9 . However, in the Hansard Corpus, itg shows no improvement against sym. This indicates that capturing structural coherence by itg yields a significant benefit to wor"
D16-1210,J93-2003,0,0.0759293,"thods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on"
D16-1210,N12-1047,0,0.0145612,"statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is comparable to sym in KFTT, while itg significantly outperforms sym and is comparable to none in IWSLT 2007. 4 Discussion 4.1 Effects of ITG Constraints on Word Alignment and Translation We discuss the effect of our ITG constraint on word alignment and machine translation. As described in Section 2, the ITG constraint is imposed in the E-step of the EM a"
D16-1210,W07-0403,0,0.0603047,"Missing"
D16-1210,J07-2003,0,0.0575014,"6; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 199"
D16-1210,P11-2031,0,0.0294483,"straint are directly reflected in the machine translation results because the phrase tables are extracted from the posterior probabilities calculated in training steps. Therefore, our ITG constraint has a potential to achieve a large improvement of machine translation performance relative to an improvement of alignment performance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction"
D16-1210,P07-1003,0,0.0288247,"effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that use"
D16-1210,J07-3002,0,0.0327245,"used the first 10K sentence pairs in the training data for the IWSLT 2007 translation task, which were manually annotated with word alignment (Chooi-Ling et al., 2010), as the BTEC Corpus. In translation evaluations, we used the KFTT and Ja-En IWSLT 2007 translation tasks5 . Table 1 shows each corpus size. In each training data set, all words were lowercased and sentences with over 80 words on either side were removed. 3.1 Word Alignment Evaluation We measured the performance of word alignment with AER and F-measure (Och and Ney, 2003). We used only sure alignments for calculating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since ther"
D16-1210,P08-1112,0,0.0536238,"Missing"
D16-1210,N03-1017,0,0.0205063,"onal models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical"
D16-1210,P07-2045,0,0.00624361,"e relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1"
D16-1210,W04-3250,0,0.119987,"e models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five differen"
D16-1210,W13-2263,0,0.0170773,"alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to e"
D16-1210,N06-1014,0,0.0348688,"ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (C"
D16-1210,W13-3523,0,0.058296,"Missing"
D16-1210,C04-1032,0,0.391898,"ro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly"
D16-1210,W03-0301,0,0.16238,"Missing"
D16-1210,2007.iwslt-1.23,0,0.0267202,"sed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is c"
D16-1210,J03-1002,0,0.0550643,".titech.ac.jp Eiichiro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the"
D16-1210,P02-1040,0,0.113232,"cally different language pair such as Ja-En. For example, some function words appear more than once in both a source and target sentence, and they are not symmetrically aligned with each other, especially in regards to the Ja-En language pair. Although the baseline methods tend to be unable to align such long-distance word pairs, the proposed method can correctly catch them because itg can determine the relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap"
D16-1210,C12-1142,0,0.018121,"d for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 both to P (A → ϵ/ej ) and P (A → fi /ϵ). To reduce computational cost, the probabilities of phrasal rules P (A → ⟨Y /Z⟩) and P (A → [Y /Z]) are not trained, which are set to 0.5 following Saers et al. (2012). In addition to the probability of each ITG rule, we must provide a probability to an one-to-many alignment because the two step parsing approach must pre-compute probabilities for all one-to-many alignments in the first step. An one-to-many alignment 2 3 We set n to 30 in our experiments. We set pnull to 10−5 . can be decomposed to a rule A → fi /ej and some A → ϵ/ej rules under the ITG form. We select a set of rules with the highest probability for an one-tomany alignment using Viterbi algorithm, which has a complexity of O(|e|). 2.3 Previous Agreement Constraint This section provides an ov"
D16-1210,takezawa-etal-2002-toward,1,0.572641,"ed as a soft constraint in the posterior regularization framework (Ganchev et al., 2010). In addition, our ITG constraint works also on word alignments that are not covered by ITG parse trees, as a standard symmetric constraint. Hence, the proposed method is robust to ITG parse errors compared to an alignment method that uses an ITG directly in model training (e.g., Zhang and Gildea (2004, 2005)). Word alignment evaluations show that the proposed method achieves significant gains in Fmeasure and alignment error rate (AER) on the KFTT (Neubig, 2011) and the BTEC JapaneseEnglish (Ja-En) corpus (Takezawa et al., 2002). Machine translation evaluations show that our constraint significantly outperforms or is comparable to the baseline symmetric constraint (Ganchev et al., 2010) in BLEU on the KFTT Ja-En and IWSLT 2007 Ja-En corpus (Fordyce, 2007). 2 ITG Constraint in the Posterior Regularization Framework 2.1 Overview The proposed method introduces an ITG constraint into the posterior regularization framework (Ganchev et al., 2010) in model training. The proposed model is trained as follows, where agreement constraints are imposed in the E-step of the EM algorithm1 : E-step: 1. Calculate a source-to-target p"
D16-1210,C96-2141,0,0.571603,"onal models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et"
D16-1210,J97-3002,0,0.668646,"ty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on"
D16-1210,C12-1176,0,0.0172759,"n, we present our ITG parsing method, which uses bracketing ITG (Wu, 1997). The rules of the bracketing ITG are as follows: A → ⟨Y /Z⟩, A → [Y /Z], A → fi /ej , A → fi /ϵ, and A → ϵ/ej , where A, Y , and Z are non-terminal symbols, fi and ej are terminal strings, ϵ is a null symbol, ⟨⟩ denotes the inversion of two phrase positions, and [] denotes the reversion of two phrase positions. In general, a bracketing ITG has O(|f |3 |e|3 ) time complexity for parsing a sentence pair {f , e}, where |f |and |e |are the lengths of f and e. For efficient ITG parsing, we use the two-step parsing approach (Xiao et al., 2012), which has been proposed to induce Synchronous Context Free Grammar (SCFG) using n-best pruning2 with time complexity O(|f |3 ). Because ITG is a kind of SCFG, this method can be adopted for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 bo"
D16-1210,P13-1017,0,0.0173604,"ance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction, because excessively long phrase pairs are extracted by bridging the gaps in wrong alignments or simply no phrase pairs are extracted from wrong gappy alignments. Consequently, the phrase table generated by IBM Model 4+sym tend to be sparse and contain longer phrase pairs than the one generated by IBM Model 4+itg. 5 Conclusi"
D16-1210,C04-1060,0,0.0378121,"y different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regu"
D16-1210,P05-1059,0,0.0437533,"rs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regularization framework (Gan"
D16-1210,2007.iwslt-1.1,0,\N,Missing
D17-1155,D11-1033,0,0.732683,"two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as"
D17-1155,2015.iwslt-evaluation.1,0,0.0573695,"Missing"
D17-1155,2016.amta-researchers.8,0,0.201897,"it some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rous"
D17-1155,P17-2061,0,0.26518,"anslation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain control (Kobus et al., 2016), which applied word-level domain features to word embedding layer. This approach provides natural baselines for comparison. To the best of our knowledge, there is no existing work concerning instance weighting in 1482 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1482–1488 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics NMT. The main challenge is that NMT is not a liner model or combin"
D17-1155,P13-2119,0,0.0568442,"corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT"
D17-1155,C16-1299,0,0.0285091,"Missing"
D17-1155,2015.mtsummit-papers.10,0,0.103141,"een shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foste"
D17-1155,D10-1044,0,0.0589565,"2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain mod"
D17-1155,D14-1062,0,0.0330635,"Missing"
D17-1155,C14-1182,0,0.0385476,"Missing"
D17-1155,P07-1034,0,0.35209,"by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directl"
D17-1155,kobus-etal-2017-domain,0,0.0736135,"Missing"
D17-1155,W04-3250,0,0.163481,"Missing"
D17-1155,P07-2045,0,0.024644,"Missing"
D17-1155,2015.iwslt-evaluation.11,0,0.461478,"technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D15-1166,0,0.185744,"ies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D09-1074,0,0.0172604,"Missing"
D17-1155,P10-2041,0,0.135114,"hine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity"
D17-1155,P02-1040,0,0.116309,"Missing"
D17-1155,2011.iwslt-evaluation.10,0,0.120719,"Missing"
D17-1155,E12-1055,0,0.0316873,"domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural M"
D17-1155,E17-3017,0,0.0358134,"Missing"
D17-1155,P13-1082,0,0.0411855,"uages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT"
D17-1155,W10-1759,0,0.0888214,",b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model"
D17-1155,P17-2089,1,0.366616,"Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain contr"
D17-1155,C16-1295,1,0.845483,"ral PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further t"
D17-1304,W09-2307,0,0.0166967,"annotation vectors H and dependency annotation vectors D. The current context vector csi and cdi are compute by eq.(4), respectively: J X … CNN yi-1 … Figure 2: SDRNMT-1 for the i-th time step. csi = … … x7 CNN ci … x6 VU2 ? i,1 ? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We als"
D17-1304,P05-1066,0,0.206266,"Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al.,"
D17-1304,P14-1129,0,0.0116576,"or VUj for Uj . In our experiment, the output of the output layer is 1 × d-dimension vector. It should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017). In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder. Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation. 4 NMT with SDR In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT. 4.1 greatly tackle the sparsity issues associated with large dependency units. Motivated by (Sennrich and Haddow, 2016), we"
D17-1304,P16-1078,0,0.0619476,"h translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). I"
D17-1304,P17-2012,0,0.0146372,"ovements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (S"
D17-1304,N16-1101,0,0.0209728,"cially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; H"
D17-1304,D14-1176,0,0.0464241,"Missing"
D17-1304,P17-1177,0,0.0492407,"achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent pr"
D17-1304,D13-1176,0,0.0524365,"successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency"
D17-1304,W15-4906,0,0.0209798,"6; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vect"
D17-1304,P07-2045,0,0.0114868,"data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017). For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80. The word embedding dimension is 620,5 and the hidden l"
D17-1304,P17-1064,0,0.0668486,"that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has"
D17-1304,P02-1040,0,0.11802,"? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-"
D17-1304,E17-3017,0,0.043072,"Missing"
D17-1304,W16-2209,0,0.149106,"), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information. To this end, we create a de"
D17-1304,N16-1004,0,0.0215619,"Missing"
D17-1304,W16-2301,0,\N,Missing
D18-2023,D17-1151,0,0.0576179,"ern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5 https://github.com/nyu-dl/dl4mt-cdec 6 https://github.com/EdinburghNLP/nematus 7 https://github.com/OpenNMT/OpenNMT-py 8 https://github.com/google/seq2seq 2 9 https://github.com/paarthneekhara/byteNet-tensorflow (unofficial) and others. 10 https://github.com/facebookresearch/fairseq 11 https://github.com/tensorflow/tensor2tensor 12 https://github.com/marian-nmt/marian 133 Proceedings of the 2018 Conference on Empirical Methods"
D18-2023,E17-3017,0,0.0368643,"Missing"
D18-2023,P15-1001,0,0.0387061,"Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://g"
D18-2023,P18-4020,0,0.10183,"Missing"
D18-2023,P17-4012,0,0.0268699,"et al., 2014; Bahdanau et al., 2014; Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://githu"
D18-2023,P07-2045,0,0.0193419,"ftware. However, there is a common issue – they are all written in script languages with dependencies on third-party GPU platforms (see Table 1) except Marian, which is developed simultaneously with our toolkit. Using script languages and third-party GPU platforms is a two-edged sword. On one hand, it greatly reduces the workload of coding neural networks. On the other hand, it also causes two problems as follows, Introduction Neural Machine Translation (NMT) has made remarkable progress over the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3"
D18-2023,1983.tc-1.13,0,0.384524,"Missing"
D18-2023,D15-1166,0,0.231977,"s (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5"
D18-2023,Q16-1027,0,0.0394418,"Missing"
D18-2023,P15-1002,0,0.167553,"s (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5"
D18-2023,P16-1160,0,\N,Missing
D18-2023,W17-4717,0,\N,Missing
D19-1139,P19-1174,1,0.410366,"er than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead (or +MPRHead), with"
D19-1139,P18-1008,0,0.205951,"the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and"
D19-1139,D18-1317,0,0.0146913,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,C18-1271,0,0.0316368,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,P19-1352,0,0.0214787,"significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead"
D19-1139,D14-1179,0,0.090697,"Missing"
D19-1139,W19-5330,1,0.891821,"Missing"
D19-1139,P05-1066,0,0.659276,"Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates that the model was significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the Op"
D19-1139,W18-6419,1,0.850701,"t and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14"
D19-1139,D18-1457,0,0.0176619,"Vaswani et al., 2017), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue"
D19-1139,P16-1162,0,0.0431505,"r architecture. 5 5.1 Experiments Experimental Setup The proposed methods were evaluated on the WMT’14 English to German (EN-DE) and NIST Chinese-to-English (ZH-EN) translation tasks. The ZH-EN training set includes 1.28 million bilingual sentence pairs from the LDC corpora, where the NIST06 and the NIST02/NIST03/NIST04 data sets were used as the development and test sets, respectively. The EN-DE training set includes 4.43 million bilingual sentence pairs of the WMT’14 corpora, where the newstest2013 and newstest2014 data sets were used as the development and test sets, respectively. The BPE (Sennrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018),"
D19-1139,N19-1122,0,0.0571523,"ion networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and original input repres"
D19-1139,N18-2074,0,0.243236,"nrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018), and directional SAN (DiSAN) (Shen 1363 System Vaswani et al. (2017) Chen et al. (2018) Hao et al. (2019) This work Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates th"
D19-1139,P19-1119,1,0.863476,"Missing"
D19-1139,P18-1166,0,0.130583,"Missing"
D19-1139,C18-1153,0,0.027204,"17), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported impro"
D19-5206,P07-2045,0,0.00846795,"Missing"
D19-5206,P18-1073,0,0.151243,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,J82-2005,0,0.725497,"Missing"
D19-5206,D18-1399,0,0.253018,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,W18-1811,1,0.810257,"oduction This paper describes neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC"
D19-5206,Y18-3007,1,0.640277,"slation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC corpus, collected by National Institute of Posts, Telecoms & ICT (NI"
D19-5206,N12-1047,0,0.0345309,"ur MT systems. 8 We used a Myanmar dictionary that contains a list of unique 41,343 Myanmar words from https: //github.com/chanmratekoko/Awesome-Myanmar. 70 right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, on the Common Crawl corpus for Khmer, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 10000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4"
D19-5206,W19-5330,1,0.784097,"contrasted when ensembling 7 NMT models during decoding (#4). While we observe an improvement of 3.3 BLEU points for (my→en), the improvements for the other directions were limited to 1.0 BLEU points or less. Considering the cost of independently training 7 NMT models and the cost of decoding with 7 models, ensembling does not seem to offer a cost-effective solution. Finally, combining SMT and NMT (#5) provides the best results with improvements over #4 ranging from 0.9 (en→km) to 2.4 BLEU points (my→en). Our results for unsupervised SMT (#6) follow the same trend as the results presented by Marie et al. (2019) for English-Gujarati and English-Kazakh at WMT19: while unsupervised MT has shown promising results for European languages, it is far from being useful for real-world applications, i.e., truly low-resource distant language pairs. We assume that training useful bilingual weaklysupervised/unsupervised bilingual word embeddings for initializing the system remains one of the main challenges. Results Table 6 presents the results for different versions of our SMT and NMT systems. We can observe that NMT (#2) is significantly better than SMT (#1) for my-en while we can observe the reverse for km-en."
D19-5206,P13-2121,0,0.0393413,"Missing"
D19-5206,P02-1040,0,0.117309,"Missing"
D19-5206,P16-1009,0,0.0322957,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P16-1162,0,0.048767,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P18-1072,0,0.0254507,"highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hypothesis in these merged n-best lists than the one-best hypothesis originated by the individual systems. We chose kb-mira as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are descri"
D19-5207,D14-1179,0,0.011769,"Missing"
D19-5207,W19-6613,1,0.88935,"pora for L1, L2 and L3 into all other languages. (b) a. Fine-tune the previous model using all in-domain parallel and pseudoparallel corpora. This stage-wise division of training ensures that the model focuses on a specific domain per training step and relies on multilingualism to address the scarcity of data. The resultant model used for back-translation leads to an inflation in good quality in-domain data which should substantially increase translation performance. In our experiments, L1 is Russian, L2 is Japanese and L3 is English. Multilingual Multi-stage Training with Back-translation In Imankulova et al. (2019), we proposed leveraging multilingualism via multiple training stages. Although we explain the idea in detail below, we urge the readers to read this paper for minute details regarding implementation and datapreprocessing. Assume that our language pair of interest is L1– L2 for which we have very little data. We have the following types of helping data: large L1–L3 and L2–L3 out-of-domain parallel corpora, small L1–L3 and L2–L3 in-domain parallel corpora and in-domain monolingual corpora that are slightly larger than the in-domain parallel corpora. In order to train robust NMT models we do the"
D19-5207,D16-1163,0,0.0336808,"be the Transformer which is the state-of-the-art NMT model we used for our experiments. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2019, we participated as team “NICT5” and worked on Russian–Japanese and English– Tamil translation. The techniques we focused on for each translation task can be summarized as below: 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensi"
D19-5207,W18-6325,0,0.019379,"imilar L3 and L2 allow for better transfer learning. As such, we used Hindi as the helping language, L3 for which L2 is Tamil because both are Indian languages. In theory, Tamil should benefit more from Dravidian languages but there is no large helping corpus involving a Dravidian language. It is reasonable to expect improvements in translation by fine-tuning a L3→L1 model on L2→L1 data because of the additional target language monolingual data that helps improve the decoderside language model. However, previous research has shown that this works even if the translation direction is reversed (Kocmi and Bojar, 2018). As such, we also experiment with fine-tuning a L1→L3 model on L1→L2 data with the expectation that the encoder representations will be improved. 2.3 4. Use robust multilingual model for backtranslation and final model training: (a) Use the previous model to backtranslate all in-domain monolingual corpora for L1, L2 and L3 into all other languages. (b) a. Train a multilingual model for L1↔L2, L1↔L3 and L2↔L3 using all in-domain parallel and pseudo-parallel corpora. 5. Repeat N 1 times: (a) Use the previous model to backtranslate all in-domain monolingual corpora for L1, L2 and L3 into all oth"
D19-5207,P07-2045,0,0.00456996,"to the submissions of other WAT participants, kindly refer to the overview paper (Nakazawa et al., 2019). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2019, we participated as team “NICT5” and worked on Russian–Japanese and English– Tamil translation. The techniques we focused on for each translation task can be summarized as below: 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural m"
D19-5207,W12-5611,0,0.048395,"Missing"
D19-5209,D18-1549,0,0.0213276,"train from scratch a new source-to-target NMT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these m"
D19-5209,W19-5330,1,0.708168,"r for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Data Preprocessing As parallel data to train our systems, we used all the provided parallel data for all our targeted ∗ Rui and Haipeng have equal contribution to this paper. This work was conductd when Haipeng visited NICT as an internship student. 1 This system is based on our WMT-2019 system (Marie et al., 2019). 2 http://data.statmt.org/news-crawl/ 90 Proceedings of the 6th Workshop on Asian Translation, pages 90–93 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 3 MT Systems --lgs ’en-my’ --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --batch size 32 --bptt 256 --optimizer adam inverse sqrt,beta1=0.9, beta2=0.98,lr=0.0001 --eval bleu true To build competitive NMT systems, we chose to rely on the Transformer architecture (Vaswani et al., 2017) since it has been show"
D19-5209,P02-1040,0,0.110492,"oolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT model for 50000 iterations. The En-My UNMT system was trained on 8 GPUs, with the parameters listed in Table 6. Table 3: Parameters for training TLM. 3.2 Back-translation NMT We trained a Transformer-based NMT model with the pre-trained TLM using XLM toolkit. Our NMT system was consistently trained on 8 GPUs, with the following parameters listed in Table 4. We performed NMT decoding with a single model according to the best BLEU (Papineni et al., 2002) and the perplexity scores. 3 https://github.com/facebookresearch/ XLM 4 NVIDIA @ Tesla @ V100 32Gb. 91 Systems UNMT NMT NMT NMT+TLM NMT+TLM NMT+TLM+back-translation ALT UCSY X X X X X X X X X MONO My-En En-My X 0.81 8.06 14.97 18.42 21.33 29.89 0.31 10.50 14.15 16.12 19.73 19.01 X X Table 5: Results (BLEU-cased) of our MT systems on the test set. ALT denotes that ALT training data was used in this system; UCSY denotes that UCSY training data was used in this system; MONO denotes monolingual training data was used in this system. +TLM denotes that language model pretraining was used in this sy"
D19-5209,P16-1009,0,0.035489,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P16-1162,0,0.0522179,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P19-1119,1,0.8583,"Missing"
D19-5209,P18-1005,0,0.0212656,"MT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT mode"
D19-5217,W17-5711,1,0.866816,"Missing"
D19-5217,W04-3230,0,0.142547,"Missing"
D19-5217,W18-2713,1,0.848795,"n is that the pseudo-parallel sentences become less varied than those created manually, because of machine translation. This characteristic makes it difficult for the back-translation method to enhance the encoder, in contrast to the decoder. To solve this problem, Imamura et al. (2018) proposed a method that combines the following two methods. 4.3 Static and Dynamic Self-Training There are two types of self-training based on backtranslation, depending on the pseudo-parallel sentences and the structure of mini-batches: the static self-training (Imamura et al., 2018) and dynamic self-training (Imamura and Sumita, 2018). Steps 3 and 4 of Section 4.2 are different for each type. Static self-training constructs a training set by combining Kstatic pseudo-parallel sentences with each of the original sentences. In this paper, we set Kstatic = 4. During training, the training set is fixed. In static self-training, the number of pseudoparallel sentences is Kstatic times larger than the number of original sentences. If we simply mix these sentences, the ratio of pseudo-parallel sentences to original sentences would be too high. To avoid this problem, we oversample the original sentences by a factor of Kstatic , inst"
D19-5217,P07-2045,0,0.0188997,"ing; the back-translation of additional monolingual corpora has different features. Results of ASPEC and TDDC Tasks The results of ASPEC and TDDC are shown in Tables 3 and 4, respectively. Both tables show the translation quality (BLEU) and the perplexity of the development set (Dev. PPL), depending on the model type and training method. The effect of the long warm-up has already been shown in Section 3. 5.1 Notes of Experimental Settings The BLEU scores (Papineni et al., 2002) in the tables were computed based on the tokenizers MeCab (for Japanese (Kudo et al., 2004)) and Moses (for English (Koehn et al., 2007)). We trained four models with different random seeds. The single model rows of the tables show the average score of four models, and the ensemble rows show the score of the ensemble of four models. The length penalty for testing was set to maximize the BLEU score of the development set. However, in the TDDC task, we used different penalties for the items and texts sets, and independently optimized according to the set. Finally, we submitted the ensemble models for which the BLEU scores of the development set (in the single model cases) were the highest. 6 Conclusions This paper explained the"
D19-5217,P02-1040,0,0.105856,"nvestigating the conditions that influence translation quality is our future work. Note that this phenomenon is only observed for self-training; the back-translation of additional monolingual corpora has different features. Results of ASPEC and TDDC Tasks The results of ASPEC and TDDC are shown in Tables 3 and 4, respectively. Both tables show the translation quality (BLEU) and the perplexity of the development set (Dev. PPL), depending on the model type and training method. The effect of the long warm-up has already been shown in Section 3. 5.1 Notes of Experimental Settings The BLEU scores (Papineni et al., 2002) in the tables were computed based on the tokenizers MeCab (for Japanese (Kudo et al., 2004)) and Moses (for English (Koehn et al., 2007)). We trained four models with different random seeds. The single model rows of the tables show the average score of four models, and the ensemble rows show the score of the ensemble of four models. The length penalty for testing was set to maximize the BLEU score of the development set. However, in the TDDC task, we used different penalties for the items and texts sets, and independently optimized according to the set. Finally, we submitted the ensemble mode"
D19-5217,P16-1009,0,0.169362,"of ASPEC (abbreviated to ASPEC.en-ja and ASPEC.ja-en, respectively), and Japanese-to-English of the TDDC (TDDC.ja-en). The corpus used in the ASPEC tasks is Asian Scientific Paper Excerpt Corpus (Nakazawa et al., 2016), which is a collection of scientific papers. In the TDDC task, the Timely Disclosure Documents Corpus (TDDC) was used. The development and test sets of TDDC are divided into items and texts sets, which are collections of titles and body texts, respectively. The sizes of the corpora are shown in Table 1. All corpora were divided into sub-words using the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the training sets of each corpus. The rules were independently acquired from the source and target languages, to give a vocabulary size around 16K. • We investigated the relationship between the learning rate, warm-up, and model perplexity, and found that a long warm-up allows high learning rates, and consequently the translation quality improves. According to this finding, we applied the long warm-up. • We applied the self-training strategy, which uses multiple back-translations generated by sampling (Imamura et al., 2018) to increase the robustness of the encoder and improve"
D19-5217,P16-1162,0,0.47311,"of ASPEC (abbreviated to ASPEC.en-ja and ASPEC.ja-en, respectively), and Japanese-to-English of the TDDC (TDDC.ja-en). The corpus used in the ASPEC tasks is Asian Scientific Paper Excerpt Corpus (Nakazawa et al., 2016), which is a collection of scientific papers. In the TDDC task, the Timely Disclosure Documents Corpus (TDDC) was used. The development and test sets of TDDC are divided into items and texts sets, which are collections of titles and body texts, respectively. The sizes of the corpora are shown in Table 1. All corpora were divided into sub-words using the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the training sets of each corpus. The rules were independently acquired from the source and target languages, to give a vocabulary size around 16K. • We investigated the relationship between the learning rate, warm-up, and model perplexity, and found that a long warm-up allows high learning rates, and consequently the translation quality improves. According to this finding, we applied the long warm-up. • We applied the self-training strategy, which uses multiple back-translations generated by sampling (Imamura et al., 2018) to increase the robustness of the encoder and improve"
D19-5603,D18-1045,0,0.0207437,"sizes. BERT encoders themselves do not need parallel corpora for training. They can be applied to low-resource language pairs for which large parallel corpora are difficult to obtain. However, huge monolingual corpora are necessary to train BERT encoders. Therefore, they are suitable for translation from resource-rich languages (e.g., English) to low-resource languages. By contrast, back-translation requires a certain size of parallel corpora to translate back from the target to the source languages. This is because back-translated results are not confident if the parallel corpora are small (Edunov et al., 2018). Therefore, back-translation is suitable for translating middle-resource language pairs. Note that unsupervised machine translation can be realized using the XLM described in Section 2.2 by connecting two autoencoders as an encoderdecoder. Those autoencoders are trained to encode source-target-source and target-source-target using monolingual corpora. Therefore, this approach can be regarded as including back-translation. Because back-translation was originally developed to enhance decoders, it is reasonable to incorporate it into pre-training. simultaneously trains a translation model and tw"
D19-5603,P02-1040,0,0.103539,"Missing"
D19-5603,P18-2124,0,0.0854278,"Missing"
D19-5603,P16-1009,0,0.104341,"Missing"
D19-5603,P16-1162,0,0.19246,"Missing"
D19-5603,P16-1185,0,0.0269904,"Missing"
D19-5603,P11-2031,0,0.121927,"Missing"
D19-5603,N19-1423,0,0.227764,"ostage optimization, which first trains only the unlearned parameters by freezing the BERT model, and then fine-tunes all the sub-models. In our experiments, stable two-stage optimization was achieved, in contrast the BLEU scores of direct fine-tuning were extremely low. Consequently, the BLEU scores of the proposed method were better than those of the Transformer base model and the same model without pre-training. Additionally, we confirmed that NMT with the BERT encoder is more effective in low-resource settings. 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a language representation model trained in advance on a very large monolingual dataset. We adapt this model to our own tasks after fine-tuning (Freitag and Al-Onaizan, 2016; Servan et al., 2016) using task-specific data. Systems using BERT have achieved high accuracy in various tasks, such as the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and the reading comprehension benchmark using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2018). However, most tasks using BERT are monolingual because it was originally developed for natural lang"
D19-5603,D16-1160,0,0.0537589,"Missing"
E03-1029,2002.tmi-papers.9,1,0.900479,"rce sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic a"
E03-1029,J00-2004,0,0.0227829,"k up words in the dictionary by target words. Tt denotes the number of target words found in the definition parts of the dictionary. 3. If there is an entry that includes both the source and target word, the word pair is regarded as the word link L denotes the number of word links. 4.1 Literalness Measure A literal translation means that source words are translated one by one to target words. Therefore, a bilingual sentence that has many word correspondences is literal. The word correspondences can be acquired by referring to translation dictionaries or using statistical word aligners (e.g., (Melamed, 2000)). However, not all source words always have an exact corresponding target word. For example, in 158 4. Calculate the literalness with the following equation, which we call the Translation Correspondence Rate (TCR) in this paper. TCR = 2L Ts + Tt (1) The TCR denotes the portion of the directly translated words among the words that should be translated. This definition is bi-directional, Ts,Tt TCR Word Links and Words in the Dictionary Target 1 (English) Source (Japanese: 4111M va Target 2 (English) wo is Cihfferet from what dei muse 0 ordere Figure 2: Example of Measuring Literalness Using Tra"
E03-1029,W01-1406,0,0.0360482,"Missing"
E03-1029,C00-1078,0,0.0154717,"ould you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appr"
E03-1029,1995.tmi-1.12,0,0.0357637,"lly, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-dependent translations should"
E03-1029,1991.mtsummit-papers.9,0,0.022503,"are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-de"
E03-1029,J93-2003,0,0.00415245,"rce sentence agrees substantially with that of a target sentence. This measure ensures that the cost of word order adjustment is small. • Word Translation Stability: A source word is better translated into the same target word through the corpus. For example, the Japanese adjectival verb `hitstiyoo-da&apos; can be translated into the English adjective &apos;necessary,&apos; the verb &apos;need,&apos; or the verb &apos;require.&apos; It is better for an MT system to always translate this word into &apos;necessary,&apos; if possible. Effective measures of controlled translation depend on MT methods. For example, word-level statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rule"
E03-1029,P02-1040,0,0.0821353,"Missing"
E03-1029,C94-1015,0,0.114985,"l statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rules of HPAT are automatically acquired from a parallel corpus, but those of TDMT were constructed manually. The procedure of HPAT is briefly described as follows (Figure 1). First, phrasal correspondences are hierarchically extracted from a parallel corpus using Hierarchical Phrase Alignment (Imamura, 2001). Next, the hierarchical correspondences are transferred into patterns, and transfer rules are generated. At the time of translation, the input sentence is parsed by using source patterns in the transfer rules. The MT result is generated by mapping the source patterns to the tar"
E03-1029,takezawa-etal-2002-toward,1,0.802206,"r situation. 2.2 Multiple Translations Generally speaking, a single source expression can be translated into multiple target expressions. Therefore, a corpus contains multiple translations even though they are translated from the same source sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed fo"
E03-1029,2001.mtsummit-ebmt.4,0,\N,Missing
E03-1048,P02-1040,0,0.0780651,"ere an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs 5 (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC 4 . A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about Average is calculated: A, B, C, and D are assigned values of 4"
E03-1048,2001.mtsummit-papers.3,1,0.832851,"e and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the"
E03-1048,shimohata-sumita-2002-automatic,1,0.89347,"Missing"
E03-1048,C02-1076,1,0.819515,"ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3 : paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for"
E03-1048,1995.tmi-1.17,0,0.00827994,"w the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3 , HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the"
E03-1048,2001.mtsummit-papers.12,0,0.0767102,"Missing"
E03-1048,W02-1611,1,0.895885,"Missing"
E03-1048,suyaga-etal-2002-proposal,0,0.0182927,"or example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun"
E03-1048,2002.tmi-tutorials.2,0,0.0456358,"corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation"
E03-1048,W01-1401,1,0.84948,"stical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra"
E03-1048,2002.tmi-papers.9,1,0.841258,"slation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f"
E03-1048,1983.tc-1.13,0,0.350529,"Missing"
E03-1048,E03-1029,1,0.834211,"g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3 , places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment"
E03-1048,1999.mtsummit-1.34,1,0.836105,"ezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Ver"
E03-1048,W01-1405,0,0.048248,"g a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3 . Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual tree"
E03-1048,C02-1050,1,0.897796,"Missing"
E03-1048,J90-2002,0,\N,Missing
E03-1048,takezawa-etal-2002-toward,1,\N,Missing
finch-etal-2004-automatic,niessen-etal-2000-evaluation,0,\N,Missing
finch-etal-2004-automatic,H93-1040,0,\N,Missing
finch-etal-2004-automatic,1993.mtsummit-1.24,0,\N,Missing
finch-etal-2004-automatic,N03-2021,0,\N,Missing
finch-etal-2004-automatic,P02-1040,0,\N,Missing
finch-etal-2004-automatic,takezawa-etal-2002-toward,1,\N,Missing
finch-etal-2004-automatic,suyaga-etal-2002-proposal,0,\N,Missing
finch-etal-2004-automatic,P00-1056,0,\N,Missing
I05-1021,P01-1008,0,0.124508,"roach involve not only synonyms but also many nearsynonyms, hypernyms, and antonyms. It is diﬃcult to distinguish synonyms from other related words [8]. Second, words to be clustered need to have high frequencies to determine similarity, therefore, words appearing only a few times are outside the scope of this approach. These shortcomings are greatly reduced with synonym acquisition from MCT owing to its characteristics. Lexical Paraphrase Extraction from MCT Here, we draw comparisons with works sharing the same conditions for acquiring synonyms (lexical paraphrases) from MCT. Barzilay et al. [1] shared the same conditions in that their extraction relies on local context. The diﬀerence is that 2 We call texts that yield synonyms as “source texts.” Acquiring Synonyms from Monolingual Comparable Texts 235 their method introduces a reﬁnement of contextual conditions for additional improvement, while our method introduces two non-contextual conditions. Pang et al. [10] built word lattices from MCT, where diﬀerent word paths that share the same start nodes and end nodes represent paraphrases. Lattices are formed by top-down merging based on structural information. Their method has a remark"
I05-1021,A00-2018,0,0.022219,"their method is not applicable to structurally diﬀerent MCTs. Shimohata et al. [13] extracted lexical paraphrases based on the substitution operation of edit operations. Text pairs having more than three edit distances are excluded from extraction. Therefore, their method considers sentential word ordering. Our ﬁndings, however, suggest that local contextual information is reliable enough for extracting synonyms. 3 Synonym Acquisition Synonym extraction relies on word pairs that satisfy the following three constraints: (1) agreement of context words; (2) prevention of outside appearance; and (3) POS agreement. Details of these constraints are described in the following sections. Then, we describe reﬁnement of the extracted noun synonyms in Sect. 3.4. 3.1 Agreement of Context Words Synonyms in MCTs are considered to have the same context since they generally share the same role. Therefore, agreement of surrounding context is a key feature for synonym extraction. We deﬁne contextual information as surrounding one word on each side of the target words. This minimum contextual constraint permits extraction from MCT having diﬀerent sentence structures. Figure 1 shows two texts that have d"
I05-1021,P90-1034,0,0.128211,"synonyms by frequency, synonyms that appear only once can be captured. In this paper, we describe related work in Sect. 2. Then, we present our acquisition method in Sect. 3 and describe its evaluation in Sect. 4. In the experiment, we provide a detailed analysis of our method using monolingual parallel texts. Following that, we explain an experiment on automatically constructed MCT data of news articles, and conclude in Sect. 5 2 Related Work Word Clustering from Non-comparable Text There have been many studies on computing similarities between words based on their distributional similarity [6,11,7]. The basic idea of the technique is that words sharing a similar characteristic with other entities form a single cluster [9,7]. A characteristic can be determined from relations with other entities, such as document frequency, co-occurrence with other words, and adjectives depending on target nouns. However, this approach has shortcomings in obtaining synonyms. First, words clustered by this approach involve not only synonyms but also many nearsynonyms, hypernyms, and antonyms. It is diﬃcult to distinguish synonyms from other related words [8]. Second, words to be clustered need to have high"
I05-1021,P98-2127,0,0.158683,"synonyms by frequency, synonyms that appear only once can be captured. In this paper, we describe related work in Sect. 2. Then, we present our acquisition method in Sect. 3 and describe its evaluation in Sect. 4. In the experiment, we provide a detailed analysis of our method using monolingual parallel texts. Following that, we explain an experiment on automatically constructed MCT data of news articles, and conclude in Sect. 5 2 Related Work Word Clustering from Non-comparable Text There have been many studies on computing similarities between words based on their distributional similarity [6,11,7]. The basic idea of the technique is that words sharing a similar characteristic with other entities form a single cluster [9,7]. A characteristic can be determined from relations with other entities, such as document frequency, co-occurrence with other words, and adjectives depending on target nouns. However, this approach has shortcomings in obtaining synonyms. First, words clustered by this approach involve not only synonyms but also many nearsynonyms, hypernyms, and antonyms. It is diﬃcult to distinguish synonyms from other related words [8]. Second, words to be clustered need to have high"
I05-1021,N03-1024,0,0.0858628,"cquisition from MCT owing to its characteristics. Lexical Paraphrase Extraction from MCT Here, we draw comparisons with works sharing the same conditions for acquiring synonyms (lexical paraphrases) from MCT. Barzilay et al. [1] shared the same conditions in that their extraction relies on local context. The diﬀerence is that 2 We call texts that yield synonyms as “source texts.” Acquiring Synonyms from Monolingual Comparable Texts 235 their method introduces a reﬁnement of contextual conditions for additional improvement, while our method introduces two non-contextual conditions. Pang et al. [10] built word lattices from MCT, where diﬀerent word paths that share the same start nodes and end nodes represent paraphrases. Lattices are formed by top-down merging based on structural information. Their method has a remarkable advantage in that synonyms do not need to be surrounded with the same words. On the other hand, their method is not applicable to structurally diﬀerent MCTs. Shimohata et al. [13] extracted lexical paraphrases based on the substitution operation of edit operations. Text pairs having more than three edit distances are excluded from extraction. Therefore, their method co"
I05-1021,P93-1024,0,0.247264,"synonyms by frequency, synonyms that appear only once can be captured. In this paper, we describe related work in Sect. 2. Then, we present our acquisition method in Sect. 3 and describe its evaluation in Sect. 4. In the experiment, we provide a detailed analysis of our method using monolingual parallel texts. Following that, we explain an experiment on automatically constructed MCT data of news articles, and conclude in Sect. 5 2 Related Work Word Clustering from Non-comparable Text There have been many studies on computing similarities between words based on their distributional similarity [6,11,7]. The basic idea of the technique is that words sharing a similar characteristic with other entities form a single cluster [9,7]. A characteristic can be determined from relations with other entities, such as document frequency, co-occurrence with other words, and adjectives depending on target nouns. However, this approach has shortcomings in obtaining synonyms. First, words clustered by this approach involve not only synonyms but also many nearsynonyms, hypernyms, and antonyms. It is diﬃcult to distinguish synonyms from other related words [8]. Second, words to be clustered need to have high"
I05-1021,W02-1611,1,0.85982,"onolingual Comparable Texts 235 their method introduces a reﬁnement of contextual conditions for additional improvement, while our method introduces two non-contextual conditions. Pang et al. [10] built word lattices from MCT, where diﬀerent word paths that share the same start nodes and end nodes represent paraphrases. Lattices are formed by top-down merging based on structural information. Their method has a remarkable advantage in that synonyms do not need to be surrounded with the same words. On the other hand, their method is not applicable to structurally diﬀerent MCTs. Shimohata et al. [13] extracted lexical paraphrases based on the substitution operation of edit operations. Text pairs having more than three edit distances are excluded from extraction. Therefore, their method considers sentential word ordering. Our ﬁndings, however, suggest that local contextual information is reliable enough for extracting synonyms. 3 Synonym Acquisition Synonym extraction relies on word pairs that satisfy the following three constraints: (1) agreement of context words; (2) prevention of outside appearance; and (3) POS agreement. Details of these constraints are described in the following secti"
I05-1021,C98-2122,0,\N,Missing
I05-5003,C04-1046,0,0.00665543,"ed, that is MT output might not consist of grammatically correct sentences. Moreover, MT evaluation scoring need not necessarily be computed on a sentence-by-sentence basis, but can be based on statistics derived at the corpus level. Finally, the process of MT evaluation is asymmetrical. That is, there is a distinction between the references and the candidate machine translations. Fortunately, the automatic MT evaluation techniques commonly in use do not make any explicit attempt to score grammaticality, and (except BLEU) decompose naturally into their component scores at the sentence level. (Blatz et al., 2004) used a variant of the WER score and the NIST score at the sentence level to assign correct17 ness to translation candidates, by scoring them with respect to a reference set. These correctness labels were used as the ‘ground truth’ for classifiers for the correctness of translation candidates for candidate sentence confidence estimation. We too adopt sentence level versions of these scores and use them to classify paraphrase candidates. The motivation for these experiments is twofold: firstly to determine how useful the features used by these MT evaluation techniques to semantic equivalence cl"
I05-5003,O97-1002,0,0.043188,"Missing"
I05-5003,2001.mtsummit-papers.68,0,0.0180454,"t operations required to transform one sentence into another, defined as: W ER(si , ri ) = I(si , ri ) + D(si , ri ) + S(si , ri ) |ri | where I(si , ri ), D(si , ri ) and S(si , ri ) are the number of insertions, deletions and substitutions respectively. 2.2 PER Position-independent word error rate (PER) (Tillmann et al., 1997) is similar to WER except that word order is not taken into account, both sentences are treated as bags of words: P ER(si , ri ) = max[dif f (si , ri ), dif f (ri , si )] |ri | where dif f (si , ri ) is the number of words observed only in si . 2.3 BLEU The BLEU score (Papineni et al., 2001) is based on the geometric mean of n-gram precision. The score is given by: BLEU = BP × exp &quot; N X 1 n=1 N × log(pn ) # where N is the maximum n-gram size. The n-gram precision pn is given by: P P count(ngram) i=1..I ngram∈si pn = P P countsys (ngram) i=1..I ngram∈si where count(ngram) is the count of ngram found in both si and ri and countsys (ngram) is the count of ngram in si . The brevity penalty BP penalizes MT output for being shorter than the corresponding references and is given by: &quot; &quot; ## Lref BP = exp min 1 − ,1 Lsys where Lsys is the number of words in the MT output sentences and Lre"
I05-5003,C92-2067,0,0.00985787,"the job” of an MT evaluator. Our second motivation is the conjecture that successful techniques and strategies will be transferable between the two tasks. 2 MT Evaluation Methods MT evaluation schemes score a set of MT system output segments (sentences in our case) S = {s1 , s2 , ..., sI } with respect to a set of references R corresponding to correct translations for their respective segments. Since we classify sentence pairs, we only consider the case of using a single reference for evaluation. Thus the set of references is given by: R = {r1 , r2 , ..., rI }. 2.1 WER Word error rate (WER) (Su et al., 1992) is a measure of the number of edit operations required to transform one sentence into another, defined as: W ER(si , ri ) = I(si , ri ) + D(si , ri ) + S(si , ri ) |ri | where I(si , ri ), D(si , ri ) and S(si , ri ) are the number of insertions, deletions and substitutions respectively. 2.2 PER Position-independent word error rate (PER) (Tillmann et al., 1997) is similar to WER except that word order is not taken into account, both sentences are treated as bags of words: P ER(si , ri ) = max[dif f (si , ri ), dif f (ri , si )] |ri | where dif f (si , ri ) is the number of words observed only"
I05-5003,J93-2004,0,\N,Missing
I05-5003,P02-1040,0,\N,Missing
I08-1030,N06-1003,0,0.0206914,"word translation, they can be used for UNK translations indirectly. Most existing work focuses on named entity translation (Carpuat et al., 2006) because named entities are the large proportion of unknown words. We also used similar methods for translating named entities in this work. Some used stem and morphological analysis for UNKs such as (Goldwater and McClosky, 2005). Morphological analysis is effective for inflective languages but not for Chinese. Using unknown word modeling such as backoff models was proposed by (Yang and Kirchhoff, 2006). Other proposed methods include paraphrasing (Callison-Burch et al., 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. However, This approach does not work if no phonetic relationship is found. Splitting compound words into translatable subwords as we did in this work have been used by (Nießlen and Ney, 2000) and (Koehn and Knight, 2003) for languages other than Chinese where detailed splitting methods are proposed. We used forward maximum match method to split unknown words. This splitting method is relatively simple but works well for Chinese. The splitting for Chinese is not as complicated as those languages with al"
I08-1030,2006.iwslt-evaluation.5,0,0.0207798,"We can do more experiments repeatedly to find this value. We found the size of 10,000 subwords achieved the best results for our experiments. 8 Related work Unknown word translation is an important problem for SMT. As we showed in the experiments, appropriate handling of this problem results in a significant improvement of translation quality. As we have known, there exists some methods for solving this problem. While these approaches were not proposed in aim to unknown word translation, they can be used for UNK translations indirectly. Most existing work focuses on named entity translation (Carpuat et al., 2006) because named entities are the large proportion of unknown words. We also used similar methods for translating named entities in this work. Some used stem and morphological analysis for UNKs such as (Goldwater and McClosky, 2005). Morphological analysis is effective for inflective languages but not for Chinese. Using unknown word modeling such as backoff models was proposed by (Yang and Kirchhoff, 2006). Other proposed methods include paraphrasing (Callison-Burch et al., 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. However, This approach do"
I08-1030,H05-1085,0,0.112934,"al with translating unknown words is to remove them from the target sentence without translation on assumption of fewer UNKs in the test data. Of course, this simple way produces a lower quality of translations if there are a lot of UNKs in the test data, especially for using a Chinese word segmenter that produces many UNKs. The translation of UNKs need to be solved by a special method. The translation of Chinese unknown words seems more difficult than other languages because Chinese language is a non-inflected language. Unlike other languages (Yang and Kirchhoff, 2006; Nießlen and Ney, 2000; Goldwater and McClosky, 2005), Chinese UNK translation cannot use information from stem and inflection analysis. Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997). But this approach is effective for translating phonetically related unknown words, not for other types. No unified approach for translating Chinese unknown words has been proposed. In this paper we propose a novel statistics-based approach for unknown word translation. This approach uses the properties of Chinese word composition rules – Chinese words are composed of one or more Chinese characters. We can split longer u"
I08-1030,E03-1076,0,0.0306753,"l analysis for UNKs such as (Goldwater and McClosky, 2005). Morphological analysis is effective for inflective languages but not for Chinese. Using unknown word modeling such as backoff models was proposed by (Yang and Kirchhoff, 2006). Other proposed methods include paraphrasing (Callison-Burch et al., 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. However, This approach does not work if no phonetic relationship is found. Splitting compound words into translatable subwords as we did in this work have been used by (Nießlen and Ney, 2000) and (Koehn and Knight, 2003) for languages other than Chinese where detailed splitting methods are proposed. We used forward maximum match method to split unknown words. This splitting method is relatively simple but works well for Chinese. The splitting for Chinese is not as complicated as those languages with alphabet. 9 Discussion and conclusion We made use of the specific property of Chinese language and proposed a subword re-segmentation to solve the translation of unknown words. Our approach was tested under various conditions such as using named entity translation and varied subword lexicons. We found this approac"
I08-1030,C00-2162,0,0.214585,"ble. One strategy to deal with translating unknown words is to remove them from the target sentence without translation on assumption of fewer UNKs in the test data. Of course, this simple way produces a lower quality of translations if there are a lot of UNKs in the test data, especially for using a Chinese word segmenter that produces many UNKs. The translation of UNKs need to be solved by a special method. The translation of Chinese unknown words seems more difficult than other languages because Chinese language is a non-inflected language. Unlike other languages (Yang and Kirchhoff, 2006; Nießlen and Ney, 2000; Goldwater and McClosky, 2005), Chinese UNK translation cannot use information from stem and inflection analysis. Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997). But this approach is effective for translating phonetically related unknown words, not for other types. No unified approach for translating Chinese unknown words has been proposed. In this paper we propose a novel statistics-based approach for unknown word translation. This approach uses the properties of Chinese word composition rules – Chinese words are composed of one or more Chinese ch"
I08-1030,J03-1002,0,0.00222754,"antly improved translation quality on the test data of NIST MT04 and MT05. We also found that the translation quality was further improved if we applied named entity translation to translate parts of unknown words before using the subword-based translation. 1 Introduction The use of phrase-based translation has led to great progress in statistical machine translation (SMT). Basically, the mechanism of this approach is realized by two steps:training and decoding. In the training phase, bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as GIZA++ (Och and Ney, 2003). Phrase pairs are then extracted to be a phrase translation table. Probabilities of a few pre-defined features are computed and assigned to the phrase pairs. The final outcome of the training is a translation table consisting of source phrases, target phrases, and lists of probabilities of features. In the decoding phase, the translation of a test source sentence is made by reordering the target phrases corresponding to the source phrases, and searching for the best hypothesis that yields the highest scores defined by the search criterion. However, this mechanism cannot solve unknown word tra"
I08-1030,P03-1021,0,0.0198073,"|e, a): the sum of the source word probabilities for the given target words and alignment. • Target phrase length model #(p): the number of phrases included in the translation hypothesis. • Target word penalty model: the number of words included in the translation hypothesis. • Distance model #(w): the number of words between the tail word of one source phrase and the head word of the next source phrase. In general, the following steps are used to get the above features. 1. Data processing: segment Chinese words and tokenize the English. Phrase-based SMT uses a framework of log-linear models (Och, 2003) to integrate multiple features. For Chinese to English translation, source sentence C is translated into target sentence E using a probability model: 3. Lexical translation: probabilities. PM • Target language model: an N-gram language model is used. • Phrase translation model p(e |f ): gives the probability of the target phrases for each source phrase. calculate word lexical 4. Phrase extraction: extract source target bilingual pairs by means of union, intersection, et. al. i=1 λi fi (C, E)) Λ = {λ1M , } PM 0 0 exp( λ f (C, E )) i i E i=1 (1) where fi (C, E) is the logarithmic value of the i"
I08-1030,P02-1040,0,0.0748599,"these numbers helps to understand the distribution of unknown words. Hyperword Subwords Characters 650 680 18 23 2 2 7.2 Effect of the various CWS As described in section 3, we used three lexicon size for the dictionary-based CWS. Therefore, we had three CWS denoted as: Character, Subword and Hyperword. We used the three CWS in turn to do word segmentation to the training data, and then built the translation models respectively. We tested the performance of each of the translation models on the test data. The results are shown on Table 4. The translations are evaluated in terms of BLEU score (Papineni et al., 2002). This experiment was just testing the effect of the three CWS. Therefore, all the UNKs of the test data were not translated, simply removed from the results. We found the character-based CWS yielded the lowest BLEU scores, indicating the translation quality of this type is the worst. The Hyperword CWS achieved the best results. If we relate it to Table 6, we found while the Hyperword CWS produced many more UNKs than the Character and Subword CWS, its translation quality was improved instead. The fact proves the quality of translation models play a more important role than the amount of unknow"
I08-1030,I05-3027,0,0.09111,"Missing"
I08-1030,E06-1006,0,0.0739315,"ords in the translation table. One strategy to deal with translating unknown words is to remove them from the target sentence without translation on assumption of fewer UNKs in the test data. Of course, this simple way produces a lower quality of translations if there are a lot of UNKs in the test data, especially for using a Chinese word segmenter that produces many UNKs. The translation of UNKs need to be solved by a special method. The translation of Chinese unknown words seems more difficult than other languages because Chinese language is a non-inflected language. Unlike other languages (Yang and Kirchhoff, 2006; Nießlen and Ney, 2000; Goldwater and McClosky, 2005), Chinese UNK translation cannot use information from stem and inflection analysis. Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997). But this approach is effective for translating phonetically related unknown words, not for other types. No unified approach for translating Chinese unknown words has been proposed. In this paper we propose a novel statistics-based approach for unknown word translation. This approach uses the properties of Chinese word composition rules – Chinese words are composed of"
I08-1030,zhang-etal-2004-interpreting,0,0.0587962,"Missing"
I08-1030,N06-2049,1,0.921926,"S. In fact, we use the following steps for generating the lexicon. In the beginning, we use the Hyperword CWS to segment the training data. Then, we extract a list of unique tokens and calculate their counts from the results of segmentation. Next, we sort the list as the decreasing order of the counts, and choose N most frequent words from the top of the list. We restrict the length of subwords to three. We use the N words as the lexicon for the subword CWS. N can be changed. Section 7.4 shows its effect to translations. The subword CWS uses a trigram language model to disambiguate. Refer to (Zhang et al., 2006) for details about selecting the subwords. We applied Subword CWS to re-segment the training data. Finally, we can train a subword-based SMT translation model used for translating the unknown words. Training this subword translation model was done in the same way as for the Hyperword translation model that uses the main CWS, as described in the beginning of Section 2. 6 are correctly recognized as named entity and fit a translation pattern. For example, the same words with different named entities are translated differently in the context. The word, “九”, is translated into “nine” for measures"
I08-1030,P97-1017,0,\N,Missing
I08-2088,N03-1017,0,0.0216196,"Missing"
I08-2088,D07-1036,0,0.172361,"Missing"
I08-2088,ma-2006-champollion,0,0.0100422,"mall in-domain parallel corpus was from the IWSLT workshop. This corpus was part of the ATR Bilingual Travel Expression Corpus (ATR-BTEC) (Kikui et al., 2006). The large out-of-domain parallel corpus was from the LDC corpus (LDC, 2007). Details on the data are listed in Table 1. We used the test set of the IWSLT2006 workshop for the evaluation. This test set consisted of 500 Chinese sentences with eight English reference translations per Chinese sentence. For the statistical machine-translation experiments, we first aligned the bilingual sentences for preprocessing using the Champollion tool (Ma, 2006). We then segmented the Chinese words using Achilles (Zhang et al., 2006). After the segmentation, we removed all punctuation from both English and Chinese corpuses and decapitalized the English corpus. We used the preprocessed data to train the phrase-based translation model by using GIZA++ (Och and Ney, 2003) and the Pharaoh tool kit (Koehn et al., 2003). 3.2.2 Features We used eight features (Och and Ney, 2003; Koehn et al., 2003) and their weights for the translations. 1. Phrase translation probability from source language to target language (weight = 0.2) 2. Phrase translation probability"
I08-2088,J03-1002,0,0.00274402,"Missing"
I08-2088,P03-1021,0,0.0373948,"Missing"
I08-2088,P02-1040,0,0.0995973,"tended to perform better than the GoodTuring language model in this translation task. For training of the language model, only the English side of the in-domain corpus was used. We used the same language model for the entire translation experiment. 3.3 Experimental results 3.3.1 Translation performance Figure 2 and 3 plot the results of the experiments. The horizontal axis represents the weight for the outof-domain translation model, and the vertical axis 658 Figure 2: Results of data selection and linear interpolation (BLEU) represents the automatic metric of translation quality (BLEU score (Papineni et al., 2002) in Fig. 2, and NIST score (NIST, 2002) in Fig. 3). Thick straight broken lines in the figures indicate automatic scores of a baseline system. This base line system was trained on the in-domain and all of the outof-domain corpus (2.5M sentence pairs). These data were concatenated before training; then one model was trained without linear interpolation. The five symbols in the figures represent the sizes (# of sentence pairs) of the selected parallel corpus. Here, the selection was carried out by using Eq. 1. For automatic evaluation, we used the reference translation with a case unsensitive an"
I08-2088,N06-2049,1,0.362951,"s corpus was part of the ATR Bilingual Travel Expression Corpus (ATR-BTEC) (Kikui et al., 2006). The large out-of-domain parallel corpus was from the LDC corpus (LDC, 2007). Details on the data are listed in Table 1. We used the test set of the IWSLT2006 workshop for the evaluation. This test set consisted of 500 Chinese sentences with eight English reference translations per Chinese sentence. For the statistical machine-translation experiments, we first aligned the bilingual sentences for preprocessing using the Champollion tool (Ma, 2006). We then segmented the Chinese words using Achilles (Zhang et al., 2006). After the segmentation, we removed all punctuation from both English and Chinese corpuses and decapitalized the English corpus. We used the preprocessed data to train the phrase-based translation model by using GIZA++ (Och and Ney, 2003) and the Pharaoh tool kit (Koehn et al., 2003). 3.2.2 Features We used eight features (Och and Ney, 2003; Koehn et al., 2003) and their weights for the translations. 1. Phrase translation probability from source language to target language (weight = 0.2) 2. Phrase translation probability from target language to source language (weight = 0.2) 3. Lexical weight"
I08-2088,D08-1076,0,\N,Missing
I08-4033,I05-3017,0,0.0157912,"8, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as “Achilles” that consists of four modules mainly: Regular expression extractor, dictionary-based Ngram segmentation, CRF-based subword tagging (Zhang et al., 2006), and confidence-based segmentation. Of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently. We will give a detail description to this approach in the following sections. In the followings, we illustrate our word segmentation process in Section 2, where the subwordbased tagging is implemented by"
I08-4033,P04-1059,0,0.0354979,"Missing"
I08-4033,J94-2001,0,0.045634,"word w1 ; In addition to the ME based POS tagging approach, we also combined a N-gram based POS tagging. N-gram tagger is the most widely used tagger in part-of-speech tagging methods. The basic idea is to maximize a posterior probability p(T |W) given a word sequence in order to find its tag sequence. By using Bayes rule, this can be transformed as to maximize p(T ) ∗ p(W|T ). Prior probability p(T ) is a Ngram language model of tag sequence. p(W|T ) is thought as an unigram model. In this experiment we used trigram to model p(T ). Differing from the interpolation smoothing algorithm used in(Merialdo, 1994), both p(T ) and p(W|T ) were smoothed by back-off methods(Katz, 1987). Because a N-gram backoff model P(T ) is well-known, a backoff implementation of p(W|T ) was given here only. It is of the following equation. 181 P 0.931 0.933 0.943 0.933 0.929 F 0.935 0.938 0.942 0.932 0.930 R-oov 0.640 0.686 0.663 0.592 0.487 R-iv 0.966 0.965 0.961 0.950 0.971 Table 1: Post evaluation of word segmentation. (4) where Z(h) is a normalizing factor determined by requirement Σt p(t|h) = 1 over all t: X CKIP CITYU CTB NCC SXU p(w|t) ¯ if p(w|t) ¯ ,0 β(t) p(w) ¯ otherwise (6) where: - p(w|t) ¯ and p(w) ¯ are d"
I08-4033,C04-1081,0,0.156864,",2 1 National Institute of Information and Communications Technology 2 ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as “Achilles” that consists of four modules mainly: Regular expression extractor, dictionary-based Ngram segmentation, CRF-based subword tagging (Zhang et al., 2006), and confidence-based segmentation. Of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently. We will give a detail descriptio"
I08-4033,W03-1719,0,0.0364124,"cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as “Achilles” that consists of four modules mainly: Regular expression extractor, dictionary-based Ngram segmentation, CRF-based subword tagging (Zhang et al., 2006), and confidence-based segmentation. Of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently. We will give a detail description to this approach in the following sections. In the followings, we illustrate our word segmentation process in Section 2, where the subwordbased taggin"
I08-4033,I05-3027,0,0.0323525,"of a Kronecker delta function series, δ(u, v), equal to 1 if both arguments are the same and 0 otherwise. Equation 1 indicates the process of dictionarybased word segmentation. We looked up the lexicon to find all the IVs, and evaluated the word sequences with the LMs. 2.2 Subword-based IOB tagging using CRFs If dictionary-based module recognizes IVs successfully, the subword-based IOB tagging can recognize OOVs. Before the subword-based tagging, the character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ”全(whole) 北 京 市(Beijing city)” is labeled as ”全(whole)/O 北(north)/B 京(capital)/I 市(city)/I”. We proposed the subword-based tagging (Zhang et al., 2006) to improve the existing character-based tagging. The subword-based IOB tagging assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chine"
I08-4033,W03-1728,0,0.157707,"s: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff Ruiqiang Zhang1,2 and Eiichiro Sumita1,2 1 National Institute of Information and Communications Technology 2 ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as “Achilles” that consists of four modules mainly: Regular expression extractor, dictionary-based Ngram segmentation, CRF-based subword tagging (Zhang et al., 2006), and confidence-based segmentation. Of the four modules, the subword-based tagging, diff"
I08-4033,W03-1730,0,0.113987,"Missing"
I08-4033,N06-2049,1,0.822673,"nd recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as “Achilles” that consists of four modules mainly: Regular expression extractor, dictionary-based Ngram segmentation, CRF-based subword tagging (Zhang et al., 2006), and confidence-based segmentation. Of the four modules, the subword-based tagging, differing from the existing character-based tagging, was proposed in our work recently. We will give a detail description to this approach in the following sections. In the followings, we illustrate our word segmentation process in Section 2, where the subwordbased tagging is implemented by the CRFs method. Section 3 illustrates our feature-based part-ofspeech tagging approach. Section 4 presents our experimental results. Section 5 describes current stateof-the-art methods for Chinese word segmentation. Sectio"
I08-4033,N01-1025,0,\N,Missing
I08-4033,W03-1726,0,\N,Missing
I08-8003,N03-1017,0,0.0548831,"tion systems have already been developed on these principles (Lepage and Denoual, 2006). Our system also takes a character-based approach but restricts itself to the translation of short phrases. This is to our advantage because machine translation systems struggle in the translation of longer sequences. Moreover, the process of transliteration tends to be a monotone process, and this assists us further. We will give only a brief overview of the process of phrase-based machine translation, for a fuller account of statistical machine translation we refer the reader to (Brown et al., 1991) and (Koehn, Och, and Marcu, 2003). During the process of phrase-based SMT the source sequence is segmented into sub-sequences , each sub-sequence being translated using bilingual sequence pairs (called phrase pairs when the translation proceeds at the word-level). The target generation process (for English-to-Japanese) at the character level is illustrated in Figure 3. The example is a real system output from an unseen phrase. The source sequence is segmented by the system into three segments. The translations of each of these segments have been gleaned from alignments of these segments where they occur in the training corpu"
I08-8003,J93-2003,0,\N,Missing
I08-8003,P97-1017,0,\N,Missing
I11-1091,N09-2056,1,0.759659,"Missing"
I11-1091,N07-1061,0,0.262952,"oaches heavily depends on the amount and coverage of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Da"
I11-1091,2008.iwslt-papers.1,0,0.311528,"age of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Danish da German de English en Spanish es Fre"
I11-1091,P07-1108,0,0.147268,"he amount and coverage of bilingual language resources available to train the statistical models. There exist several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large text data sets are readily available. However, for less frequently used language pairs only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on SMT focuses on the usage of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008). Instead of a direct translation between two languages where only a 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 811 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 811–818, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP translation model entropy, reordering, monotonicity, engine performance) are investigated to determine the significance of each factor in predicting translation quality using linear regression analysis. 2 Table 1: Language Resources (European Languages) Language Danish da German de E"
I11-1091,D08-1078,0,0.0649721,"third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness towards either the source or the target language. For most recent research efforts, English is the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality for certain language pairs (Paul et al., 2009; Leusch et al., 2010). Concerning the contribution of aspects of different language pairs on the quality of machine translation, (Birch et al., 2008) identified three features (morphological complexity, amount of reordering, historical relatedness) for predicting success of MT in translations between the official languages of the European Union. Moreover, (Koehn et al., 2009) investigated an additional feature (translation model complexity) using the JRC-Aquis corpus covering not only Indo-European languages, but also one semitic and three Finno-Ugric languages. This paper differs from previous research in the following aspects: we focus on the framework of pivot translation, where a target language translation of a source language input i"
I11-1091,2009.mtsummit-papers.7,0,0.237539,"choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality for certain language pairs (Paul et al., 2009; Leusch et al., 2010). Concerning the contribution of aspects of different language pairs on the quality of machine translation, (Birch et al., 2008) identified three features (morphological complexity, amount of reordering, historical relatedness) for predicting success of MT in translations between the official languages of the European Union. Moreover, (Koehn et al., 2009) investigated an additional feature (translation model complexity) using the JRC-Aquis corpus covering not only Indo-European languages, but also one semitic and three Finno-Ugric languages. This paper differs from previous research in the following aspects: we focus on the framework of pivot translation, where a target language translation of a source language input is obtained through an intermediate (pivot) language, investigate what factors make a good pivot language and what impact these factors have on the overall translation quality of language pairs not only including Indo-Euopean lang"
I11-1091,2010.iwslt-papers.12,0,0.283095,"Missing"
I11-1091,J03-1002,0,0.00320969,"periments, single translation references were used. Table 2: Pivot Language Dependency (European Languages) ru 57.1 57.9 59.4 58.3 57.3 52.3 44.7 55.3 Table 2 summarizes the BLEU score ranges of all pivot translation experiments obtained for a given pivot language. The results show a large variation in BLEU scores for all pivot languages indicating that the best pivot choice largely depends on the respective source and target language. For European pivot languages, the best language combination scores are in general much higher than the ones obtained for Asian pivot languages. word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training ( MERT) was used to tune the decoder’s parameters, and was performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based deTable 3 lists the highest BLEU scores of the pivot translation experiments obtained for all language pair combinations. The pivot language achieving 813 Table 4: Changes in Pivot Selection for Non-English European and Asian Language Pairs (BLEU) TRG → ↓ SRC da de es fr hi it nl pl pt ptb ru da – de (Non-English Europe"
I11-1091,P02-1040,0,0.082109,"– 39.9 (en) 41.1 (en) 40.4 (es) 40.4 (en) 39.9 (en) 36.9 (en) 40.5 (en) 39.9 (en) 39.7 (en) 42.6 (ptb) 42.2 (pt) 37.0 (en) 38.7 (en) 41.5 (ms) 33.1 (ko) 32.9 (ja) 42.9 (id) 37.0 (en) 39.3 (en) 38.3 (en) 33.8 (ja) 35.0 – (ja) coder was used. For the evaluation of translation quality, we applied the standard automatic evaluation metric BLEU which calculates the geometric mean of n-gram precision by the system output with respect to reference translations multiplied by a brevity penalty to prevent very short candidates from receiving too high a score. Scores range between 0 (worst) and 1 (best) (Papineni et al., 2002). For our experiments, single translation references were used. Table 2: Pivot Language Dependency (European Languages) ru 57.1 57.9 59.4 58.3 57.3 52.3 44.7 55.3 Table 2 summarizes the BLEU score ranges of all pivot translation experiments obtained for a given pivot language. The results show a large variation in BLEU scores for all pivot languages indicating that the best pivot choice largely depends on the respective source and target language. For European pivot languages, the best language combination scores are in general much higher than the ones obtained for Asian pivot languages. word"
I13-1032,D08-1024,0,0.179727,"grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of"
I13-1032,P05-1033,0,0.0461075,"n is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our proposed tuning method is with the following setting by tuning on the dev-test set: λ1 ="
I13-1032,D11-1125,0,0.0685367,"pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so"
I13-1032,W04-3250,0,0.0161007,"on tasks, whose training data consists of about 30K bilingual sentence 3 The reason is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our propo"
I13-1032,C10-1075,0,0.259742,"er-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be shown in our experiments later. Therefore, enlarging a tuning set is not always a sufficient solution for robust tuning, since it would be impractical to create a large scale tuning set with these requirements. We propose a novel tuning method by grouping a large number of features to leverage the above pitfalls. Instead of directly taking the large number of atomic features into translation model, we firstly learn their group structure on the training"
I13-1032,P02-1038,0,0.1062,"a result, we face an over-fitting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done whi"
I13-1032,P03-1021,0,0.104864,"sis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, wh"
I13-1032,P02-1040,0,0.0863901,"atomic features into translation model, we firstly learn their group structure on the training data to alleviate their serious sparsity. Then, we tune the translation model consisting of grouped features on a multi-reference development set to ensure robust tuning. Unlike unsupervised clustering methods such as k-means (MacQueen, 1967) for feature clustering, we group the features with the OSCAR (Octagonal Shrinkage and Clustering Algorithm for Regression) method (Bondell and Reich, 2008), which directly relates the objective of feature grouping to translation evaluation metrics such as BLEU (Papineni et al., 2002) and thus grouped features are optimized with respect to BLEU. Due to the large number of features and large number of training examples, efficient grouping is not simple. We apply the online gradient projection method under the FOBOS (forwardbackward splitting) framework (Duchi and Singer, 2009) to accelerate feature grouping. We employ a large number of features by treating each translation rule in a synchronous-CFG as a single feature. Experiments on IWSLT Chineseto-English translation tasks show that, with the help of grouping these features, our method can overcome the above pitfalls and"
I13-1032,P12-1002,0,0.0603095,"pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be sh"
I13-1032,P09-1054,0,0.0310359,"t features setting. Its main reason, as presented in Section 1, may be that multiple references and closeness5 of tuning sets are much helpful for translation tasks. Further, the id features do not achieve improvements and even decreases 0.9 BLEU scores when tuned on the development set, due to its serious sparsity. However, after grouping id features, the groups learned by our method can alleviate the feature sparsity and thus significantly obtain gains of 0.7 BLEU scores over default feature setting. Further, we implement another tuning method6 for comparison, i.e. L1 regularization method (Tsuruoka et al., 2009) based on the ranking loss L(W ) defined in Eq.1. We tune the translation 4 Both of them are derived from the Moses toolkit: http://www.statmt.org/moses/. 5 If the tuning set and test set are close enough or identically distributed, it is possible to get gains by sparse discriminative features without using feature grouping(Chiang et al., 2009). 6 It is similar to dtrain implemented in the cdec toolkit: http://cdec-decoder.org/, except that it does not use the distributed learning framework. Methods Tuning set Feature set MERT PRO PRO PRO L1 L1 OSCAR dev dev train dev train dev – default defau"
I13-1032,D07-1080,1,0.803531,"method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in"
I13-1032,D11-1081,0,0.367302,"ll suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT transla"
I13-1032,N09-1025,0,\N,Missing
I17-1002,P05-1066,0,0.210454,"Missing"
I17-1002,P16-2058,0,0.0359599,"Missing"
I17-1002,W04-3208,0,0.0171983,"the OOV’s semantic information. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focu"
I17-1002,P15-1001,0,0.0317148,"n 5 reports the experimental results obtained in the Chineseto-English task. Finally, we conclude the contributions of the paper and discuss the further work in Section 6. 12 proposed method can smooth the representation of word and reduce the unk’s negative effect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the"
I17-1002,P02-1051,0,0.0320588,"a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly intro"
I17-1002,D16-1162,0,0.0215161,"ect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words"
I17-1002,D13-1176,0,0.372214,"representation of unk. To alleviate this problem, we propose a novel contextaware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1"
I17-1002,P97-1017,0,0.104382,"each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate u"
I17-1002,P07-2045,0,0.0134503,"Missing"
I17-1002,P09-1089,0,0.0223355,"effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to"
I17-1002,N03-1017,0,0.0655639,"cal context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1 v2 v3 v4 vu … vJ 1 2 3 5 … J 4 … They are beating each other for a dispute x1 x2 x3 v1 v2 v3 1 2 3 (a) … Attention α x4 x5 … v4 v5 … vJ 5 … J 4 xJ … Decoder Encoder Trg2 Encoder Src1 他们 想 通过 打"
I17-1002,P02-1040,0,0.100942,"Missing"
I17-1002,E17-3017,0,0.069622,"Missing"
I17-1002,P16-1100,0,0.0170641,"y, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss`a and Fonollosa (2016) presented a character-based NMT, in which character-level embeddings were in combination with convolutional and highway layers to replace the standard lookup-based word representations. These methods extended the vocabulary to a larger or unlimited voca"
I17-1002,P16-1162,0,0.370792,"presentation (CAR) for each word. 3.1 Feedforward Context-of-Words Model Inspired by the representation learning of word (Bengio et al., 2003), the proposed FCWM includes an input layer, a projection layer, and a non-linear output layer, as shown in Figure 2 (a). Specifically, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss"
I17-1002,P15-1002,0,0.0275471,"d the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly introduced into NMT, because of NMT’s soft-alignment mechanism (Bahdanau et al., 2015). To relieve the negative effect of unknown words for NMT, Luong et al. (2015) proposed a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence, and to translate every OOV in a post-processing step using a external bilingual dictionary. Although The remainder of the paper is organized as follows. Section 2 introduces the related work in the NMT. Section 3 presents two novel neural models to learn the CAR for each word. Section 4 integrates the CAR into the NMT by using smoothing strategies. Section 5 reports the experimental results obtained in the Chineseto-Engl"
I17-1002,C04-1089,0,0.0293475,"ion. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the corr"
I17-1002,D09-1040,0,0.0346227,"nd decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Co"
I17-1002,P16-2021,0,0.0221027,"oved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words or OOVs, one often inferences the meaning of these words depending on its context words. Context plays an important role in"
I17-2049,D14-1179,0,0.044685,"Missing"
I17-2049,D15-1166,0,0.28351,"h and h are concatenated into the hidden states h = (h1 , ..., hM ) ∈ RK×M as − → ← −⊤ ⊤ hi = We [ h ⊤ (3) i ; hi ] , where We ∈ RK×2K is a matrix for the affine transform. Each hidden state, represented as a single vector, can be seen a memory vector that includes not only the lexical information at its source position, but also information about the left and right contexts. Then, the decoder predicts the target sentence y using a conditional probability calculated as bellow: Method 3.1 NMT with Attention p(yj |y1,j−1 , x) = softmax(Wo ej + bo ), Our work is based on an attention-based NMT (Luong et al., 2015), which generates a target sentence y = (y1 , ..., yN ) ∈ RVt ×N from the source sentence x = (x1 , ..., xM ) ∈ RVs ×M . Vs and Vt denote the vocabulary size of the source and target side, respectively. The attention-based model comprises two components, an encoder and a decoder. The encoder embeds the source sentence x into vectors through an embedding matrix and produces the hidden states using a bidirectional RNN, which represents a forward and a backward sequence. Thus, we have − → − → h i = enc1 (Ws xi , h i−1 ), ← − ← − h i = enc2 (Ws xi , h i+1 ). ?"" (4) where Wo ∈ RVt ×K and bo ∈ RVt a"
I17-2049,D16-1147,0,0.416559,"calculated as the inner product of the source hidden-state and the target word hidden-state. Note that the source hiddenstate acts as the key to weight itself. It also acts as the value to predict the target word through the context vector. Daniluk et al. (2017) suppose that the dual use of a single vector makes training the model difficult and propose the use of a key-value paired structure, which is a generalized way of storing content in the vector. In this paper, we propose splitting the matrix of the source hidden-states into two parts, an approach suggested by Daniluk et al. (2017) and Miller et al. (2016). The first part refers to the key used to calculate the attention distribution or weights. The second part refers to the value for the source-side context representation. We empirically demonstrate that the separation of the source-side context vector into the key and value significantly improves the performance of an NMT using three different English-to-Japanese translation tasks. In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two type"
I17-2049,D15-1044,0,0.0448359,"ory cells with a recurrently self-connected linear unit ∗ This work was performed while the first author was affiliated with National Institute of Information and Communication Technology, Kyoto, Japan. 290 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 290–295, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP have been proposed. Attention-based neural networks with soft or hard attention over an input have shown successful results in a wide range of tasks including machine translation (Bahdanau et al., 2015), sentence summarization (Rush et al., 2015), and image captioning (Xu et al., 2015). These attention-based networks use an encoded memory for both as the key and value as described in the Introduction to calculate the output. In contrast to the dual use of a single memory vector, Miller et al. (2016) have proposed keyvalue memory networks with key- and valuememory vectors to solve question-answering tasks, which use a generalized approach to store content in the memory. The key-memory vectors are used to calculate the attention weights, which address relevant memories with respect to the question, whereas the value-memory vectors are u"
K19-2004,P18-2058,0,0.0637148,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,P13-1023,0,0.215914,"of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes and lexical units. In the representation of AMR framework, the graph nodes are obtained by composition, derivation, lexical decomposition, normalization towards verb senses and so on. The features of the AMR graphs built on these graph nodes is similar UCCA UCCA is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). UCCA aims to recognize the level of semantic granularity which abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion and does not need to rely on language-specific resources. In the representation of the UCCA framework, some nodes have a one-to-one correspondence with the span in the sentence, which is called 3 MRP-transformed UCCA graph differs from on the terminal nodes from the original UCCA graph. In the original UCCA graph representation, terminal nodes refer to words, and in the MRP-transformed UCCA graph, terminal nodes refer to the lowest lay"
K19-2004,D19-1538,1,0.812819,"2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task a"
K19-2004,W13-2322,0,0.120818,"Missing"
K19-2004,P18-1192,1,0.922788,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,W12-3602,0,0.323392,"BERT as the encoder. In the training phase, in order to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence."
K19-2004,C18-1233,1,0.869457,"Missing"
K19-2004,S19-2002,0,0.248133,"ersal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored ("
K19-2004,N19-1423,0,0.0142873,"edge target label. Overall, we use multi-tasking learning strategy, shared hidden representation, The top node uses the same mechanism as node scoring, using binary crossentropy as loss implementation. The node pos tag and node frame label use independent feed-forward classifier, using cross-entropy as loss implementation. The edge source label and edge target label use a biaffine scorer consistent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the"
K19-2004,C18-1271,1,0.840306,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,D18-1262,1,0.847064,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,flickinger-etal-2014-towards,0,0.0124799,"ection, we will introduce this shared task and our modeling approach. Our key idea is to use a graph-based approach rather than a transition-based one; therefore, all the modeling and optimization methods we have on these frameworks are graph-based. The CoNLL shared 46 terminal nodes3 . Other nodes do not have any corresponding relationship with the span, which is introduced as a notion of a semantic constituency that transcends the pure dependency graphs to represent the semantic granularity. The UCCA graph has the following features: (2006) which encode the English Resource Semantics (ERS) (Flickinger et al., 2014). The EDS conversion from under-specified logical forms of the full ERS to variable-free graphs discards partial semantic information which makes the graph abstractly. In the representation of the EDS framework, the graph nodes are independent of surface lexical units. For each graph node, there is an explicit, many-to-many anchoring onto sub-strings of the underlying sentence. The EDS graph has the following features: • There is a one-to-one correspondence between the terminal nodes and the spans in the sentence. • Graph nodes may have multiple parents, among which one is annotated as the pri"
K19-2004,D18-1198,0,0.0419396,"ch span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsing model is suitable"
K19-2004,K18-2006,1,0.849176,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,K18-2001,0,0.0750259,"Missing"
K19-2004,hajic-etal-2012-announcing,0,0.427424,"Missing"
K19-2004,P14-5010,0,0.00246285,"ne correspondence with its usage pattern string (like “ACT PAT”) in the case of word determination, and the usage pattern has duplicates among different words, the number is much smaller than all item ids size; thus it is more suitable as a learning goal. In the subsequent recovery process, we can use lemma and the usage pattern to restore to the item id. Tokenization, Lemmatization, and Anchor conversion Since the sentence in the training dataset is the original text and no tokenization is performed, and the subsequent processing requires the word root form, we use the Stanford NLP toolkit4 (Manning et al., 2014) to tokenize and lemmatize the original text. As the graph node anchor in the original data is defined at the character level, we need to convert the anchor to the word level. In this process, due to the difference in tokenization criteria and the existence of tokenizing errors, some graph nodes will be converted into the same one in the process of conversion to word-level anchor. Therefore, we performed some post-processing modifications on the tokenization results of the Stanford NLP toolkit to ensure that the graph nodes after the conversion to the word level anchor correspond to the previo"
K19-2004,P19-1298,1,0.811368,"ing representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task and uses an additional component to recover the remote edges, and (3) Completely unanchored (i.e., AMR): uses the Seq2seq model to generate the nodes and the"
K19-2004,J93-2004,0,0.0644555,"propbank/accept-v.xml :: accept.01 :: 7] ACT()[accept.01 :: 0 :: :: 3]{} PAT()[accept.01 :: 1 :: :: 6]{} ORIG()[accept.01 :: 2 :: :: 3, accept.01 :: 1 :: :: 2]{} ev-w21f1 ACT PAT [propbank/access-v.xml :: access.01 :: 2] ACT()[access.01 :: 0 :: :: 1]{} PAT()[access.01 :: 1 :: :: 2]{} acclaim ev-w22f1 ACT PAT [propbank/acclaim-v.xml :: acclaim.01 :: 1] ACT(sub)[]{} PAT(obj1, ving)[acclaim.01 :: 1 :: :: 1]{} Data and Preprocessing ev-w22f2 3.1 Data The CoNLL shared task provides a training dataset of 5 subtasks, of which DM, PSD, and EDS are from Wall Street Journal (WSJ) text of Penn Treebank (Marcus et al., 1993) and contain 35,656 sentences. The UCCA training data comes from the English Web Treebank’s reviews text (Bies et al., 2012) and the English Wikipedia celebrity articles, with a total data volume of 5,672 sentences. AMR annotation data are drawn from a variety of texts, including online discussion forums, newswires, folktales, novels, and Wikipedia articles, which contain a total of 56,240 sentences. 3.2 ACT PAT ?CAUS ACT(sub)[]{} PAT(obj1)[]{} CAUS(for[objpp, ving])[]{} Figure 1: Examples of the most frequent frame-toframeset mapping extracted from “rng pb links.txt”. 3.3 Frame Label Projecti"
K19-2004,S14-2056,0,0.515659,"r to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence. • Graph nodes can have multiple in-edges or out-e"
K19-2004,P19-1009,0,0.09286,"Missing"
K19-2004,P05-1013,0,0.103018,"transformation is carried out: the pseudo node has a one-to-one relationship with the span in the sentence. The edge between nodes in the graph is transformed into the edge of the pseudo node, and two attributes are added for the edge: the source node label and the target node label which are used to indicate the node label in the original EDS graph. In this way, the many-to-one relationship is converted into a one-to-one relationship. After conversion, we can model the problem using in the same way as DM and PSD as described in Subsection 2.1. 2.3 Based on the above features and inspired by Nivre and Nilsson (2005), we transform the tree composed of primary edges (and nodes) into a constituent syntax tree structure, which is modeled using the constituent syntax tree parsing schema. Use an additional classifier for the remote edges prediction and recovery. 2.4 AMR Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes a"
K19-2004,K19-2001,0,0.0683072,"Missing"
K19-2004,P19-1230,1,0.841784,"s of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expres"
K19-2004,E17-1035,0,0.0249758,"of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsin"
K19-2004,P17-1076,0,0.0518095,"ent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the scores directly to span existence which determines the tree structure, and the other one assigns scores to span labels which provides the labeled outputs. (1) The node unary scorer φnode (·) is implemented with feed-forward networks based on the candidate graph nodes representation h: φnode (·) = sigmoid(MLPnode (h)). Constituent Parsing Model (2) Neural Architecture In this model, we also buil"
K19-2004,P19-1119,1,0.795869,"Missing"
K19-2004,P15-1095,0,0.0223824,"spans Sd in the range of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-b"
L16-1249,N03-1017,0,0.0633206,"ASTREC, plans to coordinate the development of the ALT Corpus between 2014 to 2018. As a first step, the corpus is scheduled to cover: Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese languages by the end of this time span. In 2014, the project commenced development for the Japanese and Myanmar langauges. The domain is news and 1888 articles were randomly selected from English Wikinews (Wikinews, 2014). 20,000 sentences for building the corpus. Although preparing a parallel corpus may be sufficient for building a standard statistical phrase-based SMT system (Koehn et al., 2003), we also added manual alignment, POS tagging and constituency trees to facilitate further study on SMT and also for other NLP fundamental research. In order to create the corpus, we implemented a web-based tool. This tool will be used in collaboration with research institutions of several Asian countries. The data was represented in XML format for all development steps. The following is an example of the XML data for English sentence “Visitors at the hotel were evacuated to the exhibition hall at street level.”: &lt;source&gt; &lt;text&gt;&lt;![CDATA[Work began in 1900.]]&gt;&lt;/text&gt; &lt;words&gt; &lt;word&gt;&lt;![CDATA[Work"
L16-1249,petrov-etal-2012-universal,0,0.0113014,"ds to align to. For example, the alignment of “the last” in “the last cars to finish” would be aligned to ေနာကဆုံး and မှ in ေနာကဆုံး မှ ပနးဝငေသာ ကားများ and likewise, the alignment of “to finish” would align to ပနးဝငေသာ. All words on both sides were required to be aligned to words in the other language. Unaligned words (null alignments) were not allowed (see Figure 1). 3.4. POS Tagging For the ALT Project, we did not use existing POS tagsets for Myanmar such as (Phyu Hninn et al., 2011). Our POS tag set was intended to be simple and universal similar to proposal of (Slav et al., 2011) (Petrov et al., 2012). The main difference with the Universal POS Tagset is we added necessary language specific POS tags to a core tagset that will be shared with other languages. Myanmar parts of speech are different from English since the grammatical structure is subject, object, verb. Some English parts of speech like determiners, prepositions and auxiliary verbs are not used in Myanmar and some Myanmar parts of speech like post positional markers are not used in English. Although the Myanmar Thadda (a book on Myanmar grammar) 1575 (Commission, 2005) defined 10 parts of speech (adjectives, adverbs, conjunction"
L16-1350,W14-7001,1,0.377751,"her unit belonging to the same paper in the translated data are extracted. Therefore, there is no sentence pairs sharing the same paper across the training, development, development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acq"
L16-1350,W15-5001,1,0.860841,"development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acquired in the workshop. WAT is unique in the following points: As described in Section 2., ASPEC-JC includes only 8 scientific fields. The distribution of the fields is s"
L16-1350,2007.mtsummit-papers.63,1,0.812108,"Missing"
L18-1545,L16-1502,0,0.0153575,"structed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpus. The characteristics of the GCP Corpus are similar to those of the Europarl. However, the GCP Corpus has different applications because it includes Asian languages and pseudo-dialogues that are being developed for use in speech translation systems. The remind"
L18-1545,2005.mtsummit-papers.11,0,0.109897,"n, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpus. The characteristics of the GCP Corpus are similar to those of the Europarl. However, the GCP Corpus has different applications because it includes Asian languages and pseudo-dialogues that are being developed for use in speech translation systems. The reminder of this paper is organized as follows. Sections 2. and 3. summarize the GCP and discuss the current status of the GCP Corpus, respectively. In Section 4., we construct a neural machine translation system using the GCP Corpus and evaluate the"
L18-1545,P07-1092,0,0.187897,"eted by the GCP. Therefore, the corpus is sentence-aligned. The target domains are medical care, disaster prevention, shopping, and tourism. Although the GCP Corpus is being developed for use in speech translation systems, it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japane"
L18-1545,2016.amta-researchers.7,1,0.674954,", it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT qualities between Japanese and other languages. Furthermore, we compare the qualities of direct, pivot, and zero-shot translations. Europarl (Koehn, 2005), a collection of European Parliament proceedings, is a well-known multilingual parallel corpu"
L18-1545,Q17-1024,0,0.112679,"Missing"
L18-1545,P17-4012,0,0.0314013,"Missing"
L18-1545,W17-5706,0,0.134396,"Missing"
L18-1545,W17-5712,1,0.79518,"Missing"
L18-1545,P02-1040,0,0.101218,"nsiders translation length, the first term of the right side denotes the log-likelihood, W P denotes a word penalty (W P ≥ 0), and T denotes the word number of the translation. Equation 1 corrects a translation length using the word penalty because NMTs typically generate short translations. The word penalty is optimized using a development set to make the translation length and reference length nearly equal. By correcting the translation length, we can compute the BLEU scores regardless of the brevity penalty. 4.2. Translation Quality Table 3 shows the quality of the MTs as measured by BLEU (Papineni et al., 2002). 3 http://opennmt.net/ First, the BLEU scores are significantly different for each language, ranging from 22.05 to 52.87 for translation from Japanese and from 23.39 to 58.13 for translation to Japanese. However, the score tended to increase with an increasing number of training sentences. Next, by comparing translations from Japanese with translations to Japanese, it was found that the scores when translating to Japanese were higher than those when translating from Japanese for all language pairs. This phenomenon shows that translating from Japanese was more difficult than translating to Jap"
L18-1545,P16-1162,0,0.0108932,"tween Japanese and the other languages (Ja ↔ X; a total of 18 systems) due to resource limitations. Datasets The corpora (Table 2) were divided into training, development, and test sets. Initially, we set aside some sentences from each corpus (held-out data) and used the remaining sentences as the training set. From the held-out data, we uniformly selected two 2,000 sentence sets as development and test sets. MT System The training, development, and test sets were segmented into words using in-house word segmenters, and the words were further segmented into subwords using a byte-pair encoder (Sennrich et al., 2016). 3454 Language Japanese English Chinese Korean Thai Vietnamese Indonesian Myanmar Spanish French Abbr. Ja En Zh Ko Th Vi Id My Es Fr Total 2,029,111 (25.2 chars. / sent.) 2,029,111 (11.2 words / sent.) 2,026,608 2,026,608 1,150,070 1,150,070 1,150,070 1,150,070 337,654 340,499 No. of Sentences (Utterances) Medical Care Disaster Prevention 420,270 249,495 420,270 249,495 420,270 249,495 420,270 249,495 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 145,054 117,636 Shopping 355,429 355,429 355,429 355,429 180,843 180,843 180,843 180,843 9,512 9,867 Tourism 527,0"
L18-1545,N07-1061,0,0.166814,"s (including Japanese) targeted by the GCP. Therefore, the corpus is sentence-aligned. The target domains are medical care, disaster prevention, shopping, and tourism. Although the GCP Corpus is being developed for use in speech translation systems, it could also be used in various other research fields because it has the following characteristics. 1. It is a multilingual sentence-aligned corpus that covers ten languages, including Asian languages. Therefore, this allows 90 different MT systems to be constructed. Furthermore, it can also be applied in comparative studies of pivot translation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007) and zero-shot translation (Johnson et al., 2016). 2. It covers four domains, namely, medical care, disaster prevention, shopping, and tourism. It can be applied to domain adaptation studies (e.g., (Imamura and Sumita, 2016)). 3. It consists of pseudo-dialogues. Therefore, it can also be applied to discourse studies that consider longdistance contexts. Note that such contexts are simpler than those of real dialogues because the dialogue never breaks down (Higashinaka et al., 2016). In this paper, we focus on the first characteristic. We use the GCP Corpus to confirm MT"
N03-2006,takezawa-etal-2002-toward,1,0.811878,"the part of the set used for translation examples according to the numbers mentioned above. Translation Experiments Conditions In order to evaluate the adaptability of an EBMT with out-of-domain examples, we applied the methods described in Section 2 to the EBMT and evaluated the translation quality in Japanese-to-English translation. We used an EBMT, DP-match Driven transDucer (D3, Sumita, 2001) as a test bed. We used two Japanese-and-English bilingual corpora. In this experiment on adaptation, as an out-ofdomain corpus, we used Basic Travel Expression Corpus (BTEC, described as BE-corpus in Takezawa, 2002); as an in-domain corpus, we used a telephone conversation corpus (TEL). The statistics of the corpora are shown in Table 1. TEL is split into two parts: a test set of 1,653 sentence pairs and a training set of 9,918. Perplexities reveal the large difference between the indomain and out-of-domain corpora. Table 1. Corpus Statistics # of sentences # of words Vocabulary size Average sentence length Perplexity (word trigram) BTEC Japanese English 152,172 1,045,694 909,270 19,999 12,268 TEL Japanese English 11,571 103,860 92,749 5,242 4,086 6.87 5.98 8.98 8.02 24.19 28.85 37.22 40.04 TEL language"
N03-2006,J93-2003,0,0.00460692,"Missing"
N03-2006,W01-1401,1,\N,Missing
N03-2006,P02-1040,0,\N,Missing
N03-2013,2001.mtsummit-papers.3,1,0.836719,"ences are regarded as bilingual sentences, and simplified machine translation is carried out. Paraphrasing by our method has the following characteristics. • Not only lexical paraphrasing but also phrasal paraphrasing can be generated because our method is based on structural substitution. • Equivalent phrases extracted by HPA are not only semantically but also grammatically equivalent. Thus, our method rarely generates ungrammatical sentences by substitution. Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality (Papineni et al., 2002; Akiba et al., 2001), for example. These methods evaluate the quality of the translation by measuring the similarity between machine translation results and translations done by humans (called references). However, the accuracy increases when multiple references are applied because one source sentence can be translated into multiple target expressions. Our method generates multiple sentences that are suitable for this purpose. 2 Acquisition of Paraphrasing Rules: Hierarchical Phrase Alignment Hierarchical Phrase Alignment is based on the assumption that an “equivalent phrase pair has the same information and the"
N03-2013,P01-1008,0,0.0368033,"meaning (called an equivalent sentence set). This task is regarded as paraphrasing. The features of our method are: 1) The paraphrasing rules are dynamically acquired by Hierarchical Phrase Alignment from the equivalent sentence set, and 2) A large equivalent sentence set is generated by substituting source syntactic structures. Our experiments show that 561 sentences on average are correctly generated from 8.48 equivalent sentences. 1 Introduction Sentences can be represented by various expressions even though they have the same meaning. Paraphrasing that transfer from sentence to sentence (Barzilay and McKeown, 2001) is a technique that generates such various expressions. In this paper, we propose an automatic quantitative expansion method for a sentence set that contains sentences of the same meaning (called an equivalent sentence set), as a paraphrasing technique. Our method is roughly structured from the following two phases. 1. Extract phrasal correspondences that have the same meaning (called equivalent phrases) from the source equivalent sentence set (acquisition phase). 2. Based on the parse tree of the sentence selected from the source set, generate target sentences by recursively substituting the"
N03-2013,P02-1040,0,0.0978924,"ly, two equivalent sentences are regarded as bilingual sentences, and simplified machine translation is carried out. Paraphrasing by our method has the following characteristics. • Not only lexical paraphrasing but also phrasal paraphrasing can be generated because our method is based on structural substitution. • Equivalent phrases extracted by HPA are not only semantically but also grammatically equivalent. Thus, our method rarely generates ungrammatical sentences by substitution. Expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality (Papineni et al., 2002; Akiba et al., 2001), for example. These methods evaluate the quality of the translation by measuring the similarity between machine translation results and translations done by humans (called references). However, the accuracy increases when multiple references are applied because one source sentence can be translated into multiple target expressions. Our method generates multiple sentences that are suitable for this purpose. 2 Acquisition of Paraphrasing Rules: Hierarchical Phrase Alignment Hierarchical Phrase Alignment is based on the assumption that an “equivalent phrase pair has the same"
N04-4003,J93-2003,0,0.00503405,"ement in the overall system performance compared to translation selection methods based on statistical scores only. 1 Introduction The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM·LM = argmaxT p(S|T ) ∗ p(T ), Parallel Text Corpus (1) where p(S|T ) is called a translation model (T M ), representing the generation probability from T into S, p(T ) is called a language model (LM ) and represents the likelihood of the target language (Brown et al., 1993). The T M and LM probabilities are trained automatically from a parallel text corpus (parameter estimation). They represent the general translation knowledge used to map a sequence of words from the source language into the target language. During the translation process (decoding) a statistical score based on the probabilities of the translation and the language models is assigned to each translation candidate and the one with the highest TM·LM score is selected as the translation output. However, the system might not be able to find a good translation due to parameter estimation problems of"
N04-4003,P01-1030,0,0.0536799,"information retrieval framework by treating each translation example as a document. For each word of the input, its term frequency tfi,j is combined with its document frequency dfi into a single weight wi,j , which is used to select the most relevant ones out of N documents (= example targets). Another possibility for obtaining translation examples is simply to utilize available (off-the-shelf) MT systems by pairing the input sentence with the obtained MT output. However, the quality of those translation examples might be much lower than manually created translations. 3 Statistical Decoding (Germann et al., 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. An extension of this approach that can take advantage of translation examples provided for a given input sentence is proposed in (Watanabe and Sumita, 2003). Instead of decoding and generating an output string word-by-word as is done in the basic concept, this greedy approach slightly modifies the target part of the translation examples so that the pair becomes the actual translation. The advantage of the example-based approach is that the search for a good transl"
N04-4003,P02-1040,0,0.0833116,"s and only the statistical scores for the selection of the translation. For the MT-based retrieval method we used eight machine translation systems for Japanese-to-English. Three of them were in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-based). They were trained on the same corpus as the statistical decoder. The remaining five systems were (off-the-shelf) generalpurpose translation engines with quite different levels of performance (cf. Table 2). • BLEU: the geometric mean of n-gram precision for the translation results found in reference translations (Papineni et al., 2002) • Translation Accuracy (ACC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged blindly by a native speaker (Sumita et al., 1999) In contrast to WER, higher BLEU and ACC scores indicate better translations. For the automatic scoring measures we utilized up to 16 human reference translations. 5.1 Downgrading Effects During Decoding In order to get an idea about how much degradation is to be expected in the translation candidates modified by the statistical decoder, we conducted an experiment using the reference translations of the test"
N04-4003,C92-2067,0,0.318995,"(Takezawa et al., 2002). The Basic Travel Expression Corpus (BTEC) contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5, respectively. The corpus was split randomly into three parts for training (155K), parameter tuning (10K), and evaluation (10K) purposes. The experiments described below were carried out on 510 sentences selected randomly as the test set. For the evaluation, we used the following automatic scoring measures and human assessment. • Word Error Rate (WER), which penalizes the edit distance against reference translations (Su et al., 1992) In the second experiment, we used two types of retrieval methods (tf·idf-based, M T -based), as introduced in Section 2, and compared the results with the baseline system TM·LM, i.e., the example-based decoding approach of (Watanabe and Sumita, 2003) using the tf·idf criteria for the retrieval of translation examples and only the statistical scores for the selection of the translation. For the MT-based retrieval method we used eight machine translation systems for Japanese-to-English. Three of them were in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-base"
N04-4003,1999.mtsummit-1.34,1,0.839395,"in-house EBMT systems which differ in the translation unit (sentence-based vs. phrase-based). They were trained on the same corpus as the statistical decoder. The remaining five systems were (off-the-shelf) generalpurpose translation engines with quite different levels of performance (cf. Table 2). • BLEU: the geometric mean of n-gram precision for the translation results found in reference translations (Papineni et al., 2002) • Translation Accuracy (ACC): subjective evaluation ranks ranging from A to D (A: perfect, B: fair, C: acceptable and D: nonsense), judged blindly by a native speaker (Sumita et al., 1999) In contrast to WER, higher BLEU and ACC scores indicate better translations. For the automatic scoring measures we utilized up to 16 human reference translations. 5.1 Downgrading Effects During Decoding In order to get an idea about how much degradation is to be expected in the translation candidates modified by the statistical decoder, we conducted an experiment using the reference translations of the test set as the input of the example-based decoder. These seed sentences are already accurate translations, thus simulating the “optimal” translation example retrieval case resulting in an uppe"
N04-4003,takezawa-etal-2002-toward,1,0.81227,"cale ∗ ED(sd ,d) ) (2) The second rescoring function assigns a probability to each decoder output that combines the exponential of the sum of log probabilities of TM and LM and the scaled negative ED scores of all translation candidates T C as follows. TM·LM·EDP (d) = P exp(log TM(d)+log LM(d)−scale ∗ ED(sd ,d)) exp(log TM(tc)+log LM(tc)−scale ∗ ED(stc ,tc)) (stc ,tc)∈T C (3) 5 Evaluation 5.2 Baseline Comparison The evaluation of our approach is carried out using a collection of Japanese sentences and their English translations that are commonly found in phrasebooks for tourists going abroad (Takezawa et al., 2002). The Basic Travel Expression Corpus (BTEC) contains 157K sentence pairs and the average lengths in words of Japanese and English sentences are 7.7 and 5.5, respectively. The corpus was split randomly into three parts for training (155K), parameter tuning (10K), and evaluation (10K) purposes. The experiments described below were carried out on 510 sentences selected randomly as the test set. For the evaluation, we used the following automatic scoring measures and human assessment. • Word Error Rate (WER), which penalizes the edit distance against reference translations (Su et al., 1992) In the"
N04-4003,2003.mtsummit-papers.54,1,0.961415,"part of the translation examples enables us to identify translation candidates that might be close to the actual translation. A common approach to measure the distance between sequences of words is the edit distance criteria (Wagner, 1974). The distance is defined as the sum of the costs of insertion (INS), deletion (DEL), and substitution (SUB) operations required to map one word sequence into the other. The edit distance can be calculated by a standard dynamic programming technique. ED(s1 ,s2 ) = |INS |+ |DEL|+ |SUB| An extension of the edit-distance-based retrieval method is presented in (Watanabe and Sumita, 2003). It incorporates the tf·idf criteria as seen in the information retrieval framework by treating each translation example as a document. For each word of the input, its term frequency tfi,j is combined with its document frequency dfi into a single weight wi,j , which is used to select the most relevant ones out of N documents (= example targets). Another possibility for obtaining translation examples is simply to utilize available (off-the-shelf) MT systems by pairing the input sentence with the obtained MT output. However, the quality of those translation examples might be much lower than man"
N06-2029,P02-1040,0,0.0901909,"Missing"
N06-2029,C02-1076,1,0.886913,"Missing"
N06-2029,2005.iwslt-1.5,1,0.884543,"Missing"
N06-2041,J03-3001,0,0.0506922,"Missing"
N06-2041,N06-2042,1,0.639455,"Missing"
N06-2041,mihalcea-2002-bootstrapping,0,\N,Missing
N06-2041,P05-3014,0,\N,Missing
N06-2042,J98-4003,0,\N,Missing
N06-2049,I05-3017,0,0.0630146,"-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, “ (whole) (Beijing city)” is labeled as ” (whole)/O (Beijing)/B (city)/I” in the subword-based tagging, where ” (Beijing)/B” is labeled as one unit. We will give a detailed description of this approach in Section 2. ∗ Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness. By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Section 2.2. In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is impl"
N06-2049,C04-1081,0,0.577246,"agging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chine"
N06-2049,I05-3027,0,0.24431,"easure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If onl"
N06-2049,W03-1728,0,0.348609,"n: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in"
N09-2056,J03-1002,0,0.00229055,"tence pairs was used to train the source-to-pivot translation models (80k sp ) and the second subset of sentence pairs was used to train the pivot-to-target translation models (80k pt ). Table 1 summarizes the characteristics of the BTEC corpus data sets used for the training (train) of the SMT models, the tuning of model weights (dev), and the evaluation of translation quality (eval). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and MET"
N09-2056,P02-1040,0,0.0971744,"vocabulary (voc), the sentence length (len) is also given, as the average number of words per sentence. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters, and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, an in-house multi-stack phrase-based decoder comparable to MOSES was used. For the evaluation of translation quality, we applied standard automatic evaluation metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). For the experimental results in this paper, the given scores are calculated as the average of the respective BLEU and METEOR scores obtained for each system output and are listed as percent figures. 222 Table 1: Language Resources BTEC Corpus # of sen en voc len de voc len es voc len fr voc len hi voc len id voc len ja voc len ko voc len ms voc len th voc len vi voc len zh voc len train 80ksp 80kpt 80,000 80,000 12,264 11,047 7.8 7.2 19,593 17,324 7.4 6.8 16,317 14,807 7.6 7.1 15,319 13,663 7.8 7.3 26,096 19,906 8.1 7.6 14,585 13,224 7.0 6.5 13,868 12,51"
N09-2056,N07-1061,0,\N,Missing
N09-2056,W05-0909,0,\N,Missing
N09-2056,D08-1078,0,\N,Missing
N16-1046,P15-1033,0,0.0135531,"Missing"
N16-1046,D09-1117,1,0.942985,"Missing"
N16-1046,P15-1001,0,0.0309557,"Missing"
N16-1046,P07-2045,0,0.0575108,"Missing"
N16-1046,N06-1014,0,0.0285511,"Missing"
N16-1046,D14-1209,1,0.815238,"Missing"
N16-1046,P15-1002,0,0.0282201,"Missing"
N16-1046,P00-1056,0,0.0407959,"Missing"
N16-1046,C02-1050,1,0.742069,"Missing"
N16-1046,P15-1113,1,0.580203,"Missing"
N16-1046,N13-1002,0,0.091757,"Missing"
N16-1046,P13-1016,1,\N,Missing
N16-1046,2002.tmi-tutorials.2,0,\N,Missing
N16-1046,P14-1129,0,\N,Missing
N16-1124,P96-1041,0,0.189126,"tatistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked."
N16-1124,P10-4002,0,0.0183497,"of using interlocking phrases to during the decoding process in phrase-based statistical machine translation (PBSMT). The motivation for this is two-fold. Firstly, during the phrase-pair extraction process that occurs in the training of a typical PBSMT system, all possible alternative phrase-pairs are extracted that are consistent with a set of alignment points. As a consequence, the source and target sides of these extracted phrase pairs may over(Karimova et al., 2014) presented a method to extract overlapping phrases offline for hierarchical phrase based SMT. They used the CDEC SMT decoder (Dyer et al., 2010) that offers several learners for discriminative tuning of weights for the new phrases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their experiments on translating Arabic-English text from the news domain were encouraging. 1076 Proceedings of NAACL-HLT 2016, pages 1076–1081, c San Die"
N16-1124,D09-1107,0,0.0656633,"Missing"
N16-1124,2014.iwslt-papers.12,0,0.0271547,"improve the performance of phrase-based decoders. 2 Related Work 1 Introduction In this paper we examine the effect on machine translation quality of using interlocking phrases to during the decoding process in phrase-based statistical machine translation (PBSMT). The motivation for this is two-fold. Firstly, during the phrase-pair extraction process that occurs in the training of a typical PBSMT system, all possible alternative phrase-pairs are extracted that are consistent with a set of alignment points. As a consequence, the source and target sides of these extracted phrase pairs may over(Karimova et al., 2014) presented a method to extract overlapping phrases offline for hierarchical phrase based SMT. They used the CDEC SMT decoder (Dyer et al., 2010) that offers several learners for discriminative tuning of weights for the new phrases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their expe"
N16-1124,W09-0429,0,0.0181259,"i (hi), Chinese (zh), Japanese (ja), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target inte"
N16-1124,N03-1017,0,0.208159,"Missing"
N16-1124,2005.mtsummit-papers.11,0,0.146899,"zh ja ko my ar da de en es fr Target Language collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Danish (da), German (de), English (en), Spenish (es), French (fr), Italian (it), Dutch (nl), Portugese (pt), Russian (ru), Tagalog (tl), Indonesian (id), Malaysian (ms), Vietnamese (vi), Thai (th), Hindi (hi), Chinese (zh), Japanese (ja), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM t"
N16-1124,P00-1056,0,0.295376,"a), Korean (ko) and Myanmar (my). 155,121 sentences were used for training, 5,000 sentences for development and 2,000 sentences for evaluation. In addition, we ran experiments on two language pairs from the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlo"
N16-1124,P03-1021,0,0.0180612,"ogy BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked. This was similar to the method proposed by (Tribble and et al., 2003). 0 &lt; BLEU Differ"
N16-1124,2001.mtsummit-papers.68,0,0.0674947,"sed SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as defined in Sections 3.1 and 3.2 are permitted freely. In the second, the target is allowed to interlock if and only if the source is also interlocked. This was similar to the method proposed by (Tribble and et al., 2003). 0 &lt; BLEU Difference ≤ 0.3 4.3 Results In this section, we will first present the results of the experim"
N16-1124,2010.amta-papers.31,0,0.0204863,"ases. Their results showed improvements of 0.3 to 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation. (Tribble and et al., 2003) proposed a method to generate longer new phrases by merging existing phraselevel alignments that have overlaping words on both source and target sides. Their experiments on translating Arabic-English text from the news domain were encouraging. 1076 Proceedings of NAACL-HLT 2016, pages 1076–1081, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics (Roth and McCallum, 2010) proposed a conditional-random-field approach to discriminatively train phrase based machine translation in which training and decoding are both cast in a sampling framework. Different with traditional PBSMT decoding that infers both a Viterbi alignment and the target sentence, their approach produced a rich overlapping phrase alignment. Their approach leveraged arbitrary features of the entire source sentence, target sentence and alignment. (K¨aa¨ ri¨ainen, 2009) proposed a novel phrase-based conditional exponential family translation model for SMT. The model operates on a feature representat"
N16-1124,N04-4026,0,0.0655285,"om the Europarl corpus (Koehn, 2005). The language pairs were English-German, GermanEnglish, English-Spanish and Spanish-English. The corpus statistics are given in Table 2. it nl pt ru tl id ms vi th hi zh ja ko my 4.2 Experimental Methodology BLEU Difference ≤ 0 We used a modified version of our in-house phrase based SMT system which operates similarly to Moses (Koehn and Haddow, 2009). GIZA++ (Och and Ney, 2000) was used for word alignment, together with the grow-diag-final-and heuristics (Koehn et al., 2003). A lexicalized reordering model was trained with the msd-bidirectional-fe option (Tillmann, 2004). We used the SRILM toolkit to create 5-gram language models with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). The weights for the loglinear models were tuned using the MERT procedure (Och, 2003). The translation performance was evaluated using the BLEU score (Papineni et al., 2001). We ran three sets of experiments; (1) target interlocking, (2) source interlocking and (3) both source and target interlocking for all possible combinations of languages (i.e. 380 language pairs). We studied two methods for accomplishing (3). In the first, interlocking as d"
N16-1124,P02-1040,0,\N,Missing
N16-1124,2009.eamt-smart.4,0,\N,Missing
N16-1124,D08-1076,0,\N,Missing
N19-1205,P15-1033,0,0.0276095,"training Ja–En models, and (2) LDC,5 which contains about 1.2M sentence pairs, for training En–Ch and Ch–En models. To tackling the problem of memory consumption, sentences longer than 150 were filtered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention has 5 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07,"
N19-1205,P16-1078,0,0.0191314,", and +1.0 (Ch–En) BLEU). 1 Introduction In recent years, neural machine translation (NMT) has been developing rapidly and has become the de facto approach for machine translation. To improve the performance of the conventional NMT models (Sutskever et al., 2014; Bahdanau et al., 2014), one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model. Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and (2) those that use linear-structured NNs to represent linearized syntax structures (Li et al., 2017; Ma et al., 2017, 2018). For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in∗ Corresponding author efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore tr"
N19-1205,D16-1026,0,0.0365324,"Missing"
N19-1205,D18-1162,0,0.0255237,"Missing"
N19-1205,W16-2209,0,0.0925436,"sent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input sequences1 , where an NSD is regarded as a linguistic input feature (Sennrich and Haddow, 2016). • Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning (Firat et al., 2016). • Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer (Vaswani et al., 2017). 1 Throughout this paper, ”input” means the input of an encoder or a decoder rather than the input of the NMT model (i.e., only source sentences), and ”output” is similar. • Add a loss function for NSD to achieve distance-aware training (Shen et al., 2018). 2 S S’ VP Neural Syntactic Distance (NSD) VP The NSD was firstly prop"
N19-1205,D17-1012,0,0.0351546,"Missing"
N19-1205,J10-4005,0,0.0369885,"constituent parsing (Shen et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018). NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input sequences1 , where an NSD is regarded as a linguistic input feature (Sennrich and Haddow, 2016). • Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning (Firat et al., 2016). • Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer (Vaswani et al., 2017). 1 Throughout this paper, ”input” means the input of an encoder or a decoder rather than the input of the NMT model (i.e., only source sentences), and ”output” is similar. • Add a loss function for NSD to achie"
N19-1205,P18-1249,0,0.02509,"op 100K sentence pairs for training En–Ja models and top 1M sentence pairs for training Ja–En models, and (2) LDC,5 which contains about 1.2M sentence pairs, for training En–Ch and Ch–En models. To tackling the problem of memory consumption, sentences longer than 150 were filtered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention"
N19-1205,P17-4012,0,0.0361245,"tered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention has 5 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06. 6 https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 8 https://github.com"
N19-1205,P17-1064,0,0.0196163,"acto approach for machine translation. To improve the performance of the conventional NMT models (Sutskever et al., 2014; Bahdanau et al., 2014), one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model. Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and (2) those that use linear-structured NNs to represent linearized syntax structures (Li et al., 2017; Ma et al., 2017, 2018). For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in∗ Corresponding author efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem. Although using a shorter sequence may improve the efficiency, some syntactic information is lost."
N19-1205,I17-1003,0,0.0192217,"he depth of the LCA. 4 dS (wn ) and dG (wn ) are undefined in both of the original papers. We give the definitions here to enable the use of NSD in NMT later. NP d d d S G R NP PRP VBZ VBG NN . She enjoys playing tennis . 4 1 1 2 3 2 1 4 1 3 2 -2 5 0 -2 Figure 1: Example of different NSDs. This example is from Shen et al. (2018). # d D She enjoys playing tennis . -1 2 1 1 3 Figure 2: Example of dependency NSDs. “#” is the root. Dependency labels are omitted. 3 Strategies to improve NMT with NSD 3.1 Dependency NSD There are many previous studies on using dependency trees to improve NMT (Nguyen Le et al., 2017; Wu et al., 2017). Therefore, we extend NSD to dependency trees. Formally, the dependency NSD between two nodes is defined as follows: dD (wi ) = i − h(i), (4) where h(i) is the index of the head of wi , and we let the index of root be 0. Note that dD (wi ) can be either positive or negative, representing the directional information. Figure 2 gives an example. 3.2 NSDs as Input Sequences It is easy to see that for w = (w1 , . . . , wn ), the lengths of dS , dG , dR and dD are all n. Denoting the NSD sequence as d = (d1 , . . . , dn ), we can see that di ∈ Z, i ∈ [1, n], so we can obtain a seq"
N19-1205,P02-1040,0,0.103502,"0.1 (Srivastava et al., 2014). The number of training epochs was fixed to 50, and we used the model which performs the best on the development set for testing. As for optimization, we used the Adam optimizer (Kingma and Ba, 2014), with β1 = 0.9, β2 = 0.998, and  = 10−9 . Warmup and decay strategy for learning rate of Vaswani et al. (2017) are also used, with 8, 000 warmup steps. We also used the label smoothing strategy (Szegedy et al., 2016) with ls = 0.1. 4.2 Experimental Results Table 1 compares the effects of the strategies. We evaluate the proposed strategies using characterlevel BLEU (Papineni et al., 2002) for Chinese and Japanese, and case-insensitive BLEU for English. Comparison of different NSDs. The first five rows of Table 1 compare the results of using different NSDs. When NSD was used at the source side (En–Ja/En–Ch), all kinds of NSDs improved translation performance. This indicates that NSD can be regarded as a useful linguistic feature to improve NMT. In contrast, when NSD was used at the target side (Ja–En/Ch–En), dS and dG hurt the performance. This is because the values of dS and dG are volatile. A tiny change of syntactic structure often causes a big change of dS and dG . Since th"
N19-1205,P18-1108,0,0.300875,"or efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem. Although using a shorter sequence may improve the efficiency, some syntactic information is lost. We propose a method of using syntactic information in NMT that overcomes the disadvantages of both methods. The basis of our method is the neural syntactic distance (NSD), a recently proposed concept used for constituent parsing (Shen et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018). NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input"
N19-1205,P17-1065,0,0.0341685,"CA. 4 dS (wn ) and dG (wn ) are undefined in both of the original papers. We give the definitions here to enable the use of NSD in NMT later. NP d d d S G R NP PRP VBZ VBG NN . She enjoys playing tennis . 4 1 1 2 3 2 1 4 1 3 2 -2 5 0 -2 Figure 1: Example of different NSDs. This example is from Shen et al. (2018). # d D She enjoys playing tennis . -1 2 1 1 3 Figure 2: Example of dependency NSDs. “#” is the root. Dependency labels are omitted. 3 Strategies to improve NMT with NSD 3.1 Dependency NSD There are many previous studies on using dependency trees to improve NMT (Nguyen Le et al., 2017; Wu et al., 2017). Therefore, we extend NSD to dependency trees. Formally, the dependency NSD between two nodes is defined as follows: dD (wi ) = i − h(i), (4) where h(i) is the index of the head of wi , and we let the index of root be 0. Note that dD (wi ) can be either positive or negative, representing the directional information. Figure 2 gives an example. 3.2 NSDs as Input Sequences It is easy to see that for w = (w1 , . . . , wn ), the lengths of dS , dG , dR and dD are all n. Denoting the NSD sequence as d = (d1 , . . . , dn ), we can see that di ∈ Z, i ∈ [1, n], so we can obtain a sequence of embedding"
N19-1276,P16-1039,0,0.142523,"tion decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typi"
N19-1276,P17-2096,0,0.0592388,"experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict opti"
N19-1276,P15-1168,0,0.356081,"acter-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidate words for charact"
N19-1276,D15-1141,0,0.0715249,"Missing"
N19-1276,I05-3017,0,0.636732,"Missing"
N19-1276,P82-1020,0,0.817381,"Missing"
N19-1276,Y18-1033,0,0.0225075,"s using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word features, such as POS and lemma. Kitagawa and Komachi (2018) applied a pure neural model based on LSTM and achieved a better performance than a popular statistical Japanese segmenter (Neubig et al., 2011). Around the same time as our work, two other character-based models for word segmentation have been proposed. Ma et al. (2018) showed a standard BiLSTM model can achieve state-of-theart results when combined with deep learning best practices, including dropout to recurrent connections (Gal and Ghahramani, 2016) and pre-trained embeddings of character bigrams. These techniques can also be applied to and can further boost performance of our model. Yang"
N19-1276,W04-3230,0,0.355922,"Missing"
N19-1276,P16-1200,0,0.0313472,"and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), relation extraction (Lin et al., 2016) and natural language inference (Parikh et al., 2016). Rei et al. (2016) introduced a gate-like attention mechanism on their word-based sequence labeling model to determine the importance between the word itself and the internal characters for each word. 7 Conclusion and Future Work In this paper, we proposed a word segmentation model that integrates word-level information into a character-based framework, aiming to take the advantages of both character- and word-based models. The experimental results show that our model with an attention-based composition function outperforms the state-of-the"
N19-1276,D18-1529,0,0.375472,"Missing"
N19-1276,P16-1101,0,0.0514462,"Yang et al. (2018) proposed a lattice LSTM-based model with subsequence (word or subword) information. Their model also considers the importance of multiple words by integrating character and word information into an LSTM cell vector using a gatemechanism. However, their model might not fully exploit word information, since word information is given to only the first and last characters of the word. 2706 LSTM-CRF LSTM-CRF is a popular neural architecture, which has been applied to various tagging tasks, including word segmentation (Chen et al., 2015b), POS tagging and NER (Huang et al., 2015; Ma and Hovy, 2016; Rei et al., 2016). Ma and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015)"
N19-1276,I13-1181,0,0.541426,"g. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are thre"
N19-1276,D15-1276,0,0.0187771,"r not to segment two consecutive characters, using a deep CNN consisting of more than ten layers. Recent works utilized word information on a character-based framework. Zhou et al. (2017) pre-trained character embeddings using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word features, such as POS and lemma. Kitagawa and Komachi (2018) applied a pure neural model based on LSTM and achieved a better performance than a popular statistical Japanese segmenter (Neubig et al., 2011). Around the same time as our work, two other character-based models for word segmentation have been proposed. Ma et al. (2018) showed a standard BiLSTM model can achieve state-of-theart results when combined with deep learning be"
N19-1276,P11-2093,0,0.449114,"ur preliminary experiments. Hyperparameter Setting Table 1 gives the hyperparameters for the proposed model. The same dropout strategy as in Zaremba et al. (2015) was applied to non-recurrent connections of recurrent layers. We used word vector dropout, which ran5 We restored provided auto-segmented texts to the original raw sentences and used them as unlabeled texts. 6 https://catalog.ldc.upenn.edu/ ldc2011t13 Method BCCWJ Our WCON model◦ 98.9 (Kitagawa and Komachi, 2018) 98.4 (Zhao and Kit, 2008)? – – (Zhou et al., 2017)◦ (Wang and Xu, 2017)◦ – (Liu et al., 2016)◦ – (Zhang et al., 2016)◦ – (Neubig et al., 2011)? 98.2∗ – (Sun et al., 2017)◦• CTB6 96.4 – – 96.2 – 95.5 96.0 – 96.3 MSR 97.8 – 97.6 97.8 98.0 97.6 97.7 – 97.9 Table 3: Comparison with state-of-the-art characterbased (top) and word-based (middle) and other types of models (bottom) on the test sets. Models marked with a symbol indicate ones based on linear statistical algorithms (?), ones with additional unlabeled texts (◦) and ones replacing specific characters as preprocessing (•). The result with ∗ is from our run on their released implementation. domly replaces a word embedding ew to a zero vector when calculating a word summary vector i"
N19-1276,D16-1244,0,0.109068,"Missing"
N19-1276,P14-1028,0,0.367972,"ategorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidat"
N19-1276,C04-1081,0,0.404506,"model learned the incorrect weights likely due to the infrequent occurrence of the correct words; the single words hiru and yoru occur in the training set tens or hundreds of times while the compound word ch¯uya occurs only twice. We may reduce these errors due to no or infrequent occurrences of gold words by increasing word vocabulary size, e.g., using larger texts to pre-train word embeddings. 6 Related Work Word Segmentation For both Chinese and Japanese, word segmentation has been traditionally addressed by applying linear statistical algorithms, such as maximum entropy (Xue, 2003), CRF (Peng et al., 2004; Kudo et al., 2004; Zhao and Kit, 2008), and logistic regression (Neubig et al., 2011). Various neural network architectures have been explored for Chinese word segmentation to reduce the burden of manual feature engineering. Specifically, character-based neural models have been developed to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisticated architectures have"
N19-1276,C16-1030,0,0.0225395,"proposed a lattice LSTM-based model with subsequence (word or subword) information. Their model also considers the importance of multiple words by integrating character and word information into an LSTM cell vector using a gatemechanism. However, their model might not fully exploit word information, since word information is given to only the first and last characters of the word. 2706 LSTM-CRF LSTM-CRF is a popular neural architecture, which has been applied to various tagging tasks, including word segmentation (Chen et al., 2015b), POS tagging and NER (Huang et al., 2015; Ma and Hovy, 2016; Rei et al., 2016). Ma and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), relation extracti"
N19-1276,I17-1018,0,0.0329611,"characters. More sophisticated architectures have also been used as standard components of word segmentation models to derive effective features automatically. Chen et al. (2015a) proposed gated recursive neural networks to model complicated combinations of characters. Chen et al. (2015b) used LSTM to capture long distance dependencies. Xu and Sun (2016) combined LSTM and GRNN to capture long term information better by utilizing chain and tree structures. CNNs have been used to extract complex features such as character n-grams (Chen et al., 2017) and graphical features of Chinese characters (Shao et al., 2017). On the other hand, word-based neural models have also been proposed. Typical word-based models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) sequentially determine whether or not to segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation candidates by beam search decoding. Liu et al. (2016) combined neural architectures for segment (i.e., word) representations into a semi-CRF framework, which searches for an optimal segmentation sequence consisting of variable length segments. Sun et al. (2017) p"
N19-1276,I17-1017,0,0.660438,"odels, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidate words for characters x3 , x4 and x5 in a sentence x1:5 in Figure 1. A feasible solution to develop a model with both characteristics is to incorporate word information into a character-based framework. An example of such work is that of Wang and Xu (2017). They concatenated embeddings of a character and candidate words and used it in their convolutional neural network (CNN)-based model. They treated candidate words equivalently, although the plausibility of a candidate word differs in the context of a target character. In this paper, we propose a character-based word segmentation model that utilizes word information. Our model is based on a BiLSTM-CRF architecture that has been successfully applied to sequence labeling tasks (Huang et al., 2015; Chen et al., 2015b). Differing from the work of Wang and Xu (2017), our model learns and distinguis"
N19-1276,P16-2092,0,0.0319066,"to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisticated architectures have also been used as standard components of word segmentation models to derive effective features automatically. Chen et al. (2015a) proposed gated recursive neural networks to model complicated combinations of characters. Chen et al. (2015b) used LSTM to capture long distance dependencies. Xu and Sun (2016) combined LSTM and GRNN to capture long term information better by utilizing chain and tree structures. CNNs have been used to extract complex features such as character n-grams (Chen et al., 2017) and graphical features of Chinese characters (Shao et al., 2017). On the other hand, word-based neural models have also been proposed. Typical word-based models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) sequentially determine whether or not to segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation c"
N19-1276,P16-1040,0,0.349286,"e of it for segmentation decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labelin"
N19-1276,I08-4017,0,0.254677,"ndomly initialized all character embeddings, since pre-trained character embeddings did not improve performance in our preliminary experiments. Hyperparameter Setting Table 1 gives the hyperparameters for the proposed model. The same dropout strategy as in Zaremba et al. (2015) was applied to non-recurrent connections of recurrent layers. We used word vector dropout, which ran5 We restored provided auto-segmented texts to the original raw sentences and used them as unlabeled texts. 6 https://catalog.ldc.upenn.edu/ ldc2011t13 Method BCCWJ Our WCON model◦ 98.9 (Kitagawa and Komachi, 2018) 98.4 (Zhao and Kit, 2008)? – – (Zhou et al., 2017)◦ (Wang and Xu, 2017)◦ – (Liu et al., 2016)◦ – (Zhang et al., 2016)◦ – (Neubig et al., 2011)? 98.2∗ – (Sun et al., 2017)◦• CTB6 96.4 – – 96.2 – 95.5 96.0 – 96.3 MSR 97.8 – 97.6 97.8 98.0 97.6 97.7 – 97.9 Table 3: Comparison with state-of-the-art characterbased (top) and word-based (middle) and other types of models (bottom) on the test sets. Models marked with a symbol indicate ones based on linear statistical algorithms (?), ones with additional unlabeled texts (◦) and ones replacing specific characters as preprocessing (•). The result with ∗ is from our run on their"
N19-1276,D13-1061,0,0.394171,"Missing"
N19-1276,D17-1079,0,0.121156,"segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation candidates by beam search decoding. Liu et al. (2016) combined neural architectures for segment (i.e., word) representations into a semi-CRF framework, which searches for an optimal segmentation sequence consisting of variable length segments. Sun et al. (2017) proposed a gap-based model to predict whether or not to segment two consecutive characters, using a deep CNN consisting of more than ten layers. Recent works utilized word information on a character-based framework. Zhou et al. (2017) pre-trained character embeddings using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word f"
N19-1276,O03-4002,0,0.41713,"e vocabulary. The model learned the incorrect weights likely due to the infrequent occurrence of the correct words; the single words hiru and yoru occur in the training set tens or hundreds of times while the compound word ch¯uya occurs only twice. We may reduce these errors due to no or infrequent occurrences of gold words by increasing word vocabulary size, e.g., using larger texts to pre-train word embeddings. 6 Related Work Word Segmentation For both Chinese and Japanese, word segmentation has been traditionally addressed by applying linear statistical algorithms, such as maximum entropy (Xue, 2003), CRF (Peng et al., 2004; Kudo et al., 2004; Zhao and Kit, 2008), and logistic regression (Neubig et al., 2011). Various neural network architectures have been explored for Chinese word segmentation to reduce the burden of manual feature engineering. Specifically, character-based neural models have been developed to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisti"
N19-1276,P17-1078,0,0.250174,"ts show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences"
N19-1276,N19-1278,0,0.0907116,"Missing"
N19-1276,P12-1083,0,0.455803,"Missing"
O07-5005,2006.iwslt-plenaries.1,0,0.0390049,"Missing"
O07-5005,takezawa-etal-2002-toward,1,0.79505,"Missing"
O07-5005,takezawa-kikui-2004-comparative,1,0.893781,"Missing"
P00-1054,C92-2101,0,0.0755892,"Missing"
P00-1054,P98-2139,0,0.0357072,"Missing"
P00-1054,C90-3044,0,0.038584,"hood. The most influential elements in the environment are of course the other words in the source sentence surrounding the concerned source word. Suppose that we have translation examples including the concerned source word and we know in advance which target word corresponds to the source word. By measuring the similarity between (1) an unknown sentence that includes the concerned source word and (2) known sentences that include the concerned source word, we can select the target word which is included in the most similar sentence. This is the same idea as example-based machine translation (Sato and Nagao, 1990 and Furuse et. al., 1994). Group1: 辛口 (not sweet) source sentence 1: This beer is drier and full-bodied. target sentence 1: □□□□□□□□辛口 辛口□□□□□□□□ 辛口 source sentence 2: Would you like dry or sweet sherry? target sentence 2: 辛口□□□□□□□□□□□□□□□□□□□□□□□□ 辛口 source sentence 3: A dry red wine would go well with it. target sentence 3: □□□□辛口 辛口□□□□□□□□□□□□ 辛口 Group2: 乾燥 (not wet) source sentence 4: Your skin feels so dry. target sentence 4: □□□□□乾燥 乾燥□□□□□ 乾燥 source sentence 5: You might want to use some cream to protect your skin against the dry air. target sentence 5: 乾燥□□□□□□□□□□□□□□□□□□□□□□□□□□□□"
P00-1054,A00-1006,1,\N,Missing
P00-1054,C98-2134,0,\N,Missing
P03-1039,J93-2003,0,0.00826573,"Missing"
P03-1039,W02-1018,0,0.0296906,"e now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach is suitable for inputs with less deviation from a training corpus, while the"
P03-1039,W99-0604,0,0.0676192,"ys be march hello hello i ’d like to change my reservation on march nineteenth 二 三 分 待っ て下さい 今 電話 中 な ん です wait a couple of minutes i ’m telephoning now is this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other han"
P03-1039,P02-1040,0,0.0699314,"Missing"
P03-1039,takezawa-etal-2002-toward,1,0.761178,"this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach i"
P03-1039,C00-2123,0,0.0624808,"del 4 parameters were used as the initial parameters for training. We directly applied the Lexicon Model and Fertility Model to the chunk-based translation model but set other parameters as uniform. 3.4 Table 1: Basic Travel Expression Corpus # of sentences # of words vocabulary size # of singletons 3-gram perplexity + weight × Decoding 2. Generate hypothesized output by consuming input chunks in arbitrary order and combining possible output chunks in left-to-right order. The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000). In addition, an example-based method is also introduced, which generates candidate chunks by looking up the viterbi chunking and alignment from a training corpus. Since the combination of all possible chunks is computationally very expensive, we have introduced the following pruning and scoring strategies. beam pruning: Since the search space is enormous, we have set up a size threshold to maintain partial hypotheses for both of the above two stages. We also incorporated a threshold for scoring, which allows partial hypotheses with a certain score to be processed. example-based scoring: Inpu"
P03-1039,2002.tmi-papers.20,1,0.840222,"ephoning now is this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach i"
P03-1039,P01-1067,0,\N,Missing
P03-1057,2001.mtsummit-papers.3,1,0.436254,"02)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluate"
P03-1057,C94-1015,0,0.0544641,"en the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules. Next, a tree structure of the target language is generated by mapping the source patterns to the corresponding target patterns. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules (Furuse and Iida, 1994). For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation va"
P03-1057,2002.tmi-papers.9,1,0.934691,"corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Aki"
P03-1057,W01-1406,0,0.0360282,"or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda e"
P03-1057,C00-1078,0,0.0461382,"e for transferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confide"
P03-1057,P02-1040,0,0.124322,", (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedbac"
P03-1057,C92-2067,0,0.186427,"ality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluates the contribution of each rule to the MT results and removes inappropriate rules as a way to increase the evaluation scores. Since the automatic evaluation correlates with a subjective evaluation, MT quality will improve after cle"
P03-1057,takezawa-etal-2002-toward,1,0.054083,"rule set is a subset of the base rule set. 4. Apply the feedback cleaning algorithm to each of the N pairs and record the rule contributions even if the rules are removed. The purpose of this step is to obtain the rule contributions. 5. For each rule in the base rule set, sum up the rule contributions obtained from the rule subsets. If the sum is negative, remove the rule from the base rule set. The major difference of this method from crossvalidation is Step 5. In the case of cross-cleaning, Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into sub-corpora for training, evaluation, and test as shown in Table 1. The number of rules acquired from the training corpus (the base rule set size) was 105,588. Evaluation Methods of MT Quality We used the following two methods to evaluate MT quality. 1. Test Corpus BLEU Score The BLUE score was calculated with the test corpus. The number of references was one for each sentence, in the same way used for the feedback clean"
P03-1057,2001.mtsummit-papers.67,0,0.0955976,"on, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1)"
P03-1057,E03-1010,0,0.0111989,"output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods. The effects of feedback cleaning depend on the characteristics of objective measures. DP-based measures and BLEU have different characteristics (Yasuda et al., 2003). The exploration of several measures for feedback cleaning remains an interesting future work. 7.2 Domain Adaptation When applying corpus-based machine translation to a different domain, bilingual corpora of the new domain are necessary. However, the sizes of the new corpora are generally smaller than that of the original corpus because the collection of bilingual sentences requires a high cost. The feedback cleaning proposed in this paper can be interpreted as adapting the translation rules so that the MT results become similar to the evaluation corpus. Therefore, if we regard the bilingual"
P03-1057,2001.mtsummit-ebmt.4,0,\N,Missing
P06-2028,P02-1044,0,0.0248088,"view we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present"
P06-2028,C96-1020,1,0.598072,"to 6 allowable tags for each word. During testing, only if the predicted tag fails to match any of the allowed tags is it considered an error. Experimental Data The primary corpus used for the experiments presented in this paper is the ATR General English Treebank. This consists of 518,080 words (approximately 20 words per sentence, on average) of text annotated with a detailed semantic and syntactic tagset. To understand the nature of the task involved in the experiments presented in this paper, one needs some familiarity with the ATR General English Tagset. For detailed presentations, see (Black et al., 1996b; Black et al., 1996a; Black and Finch, 2001). An apercu can be gained, however, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food 4 Tagging Model 4.1 ME Model Our tagging framework is based on a maximum entropy model of the following form: p(t, c) = γ K Y k=0 where: 216 f (c,t) αkk p0 (1) (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT when_CSWHEN ordering_VVGINTER-ACT OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD Figure 1: Two ATR Treebank Sentences from a Take–Out Food Flier - t"
P06-2028,J94-2001,0,0.408361,"addition to their syntactic function, a broad semantic class that signifies the semantics of the word in the context of the sentence, but does not necessarily provide information that is sufficiently finegrained as to disambiguate its sense. This differs 2 Related Work Our work is a synthesis of POS tagging and WSD, and as such, research from both these fields is directly relevant here. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistic"
P06-2028,W98-0703,0,0.0163846,"l the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labe"
P06-2028,P98-1020,1,0.863874,"ver, from Figure 1, which shows two sample sentences from the ATR Treebank (and originally from a Chinese take–out food 4 Tagging Model 4.1 ME Model Our tagging framework is based on a maximum entropy model of the following form: p(t, c) = γ K Y k=0 where: 216 f (c,t) αkk p0 (1) (_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DD1 coupon_NN1DOCUMENT when_CSWHEN ordering_VVGINTER-ACT OR_CCOR ONE_MC1WORD FREE_JJMONEY FANTAIL_NN1ANIMAL SHRIMPS_NN1FOOD Figure 1: Two ATR Treebank Sentences from a Take–Out Food Flier - t is tag being predicted; syntax, then semantics given the syntax, whereas in (Black et al., 1998) both syntax and semantics were predicted together in one step. In using syntactic tags as features, we take a softer approach to the two-stage process. The tagger has access to accurate syntactic information; however, it is not necessarily constrained to accept this choice of syntax. Rather, it is able to decide both syntax and semantics while taking semantic context into account. In order to find the most probable sequence of tags, we tag in a left-to-right manner using a beam-search algorithm. - c is the context of t; - γ is a normalization coefficient that ensures: QK fk (c,t) ΣL p0 = 1; t"
P06-2028,P96-1025,0,0.0860888,"Missing"
P06-2028,1995.iwpt-1.15,0,0.0119554,"two-stage approach to prediction, first predicting 217 which provide little or no benefit to the model, thus speeding up the training. In some cases it even allows a model to be trained where it would not otherwise be possible to train one. For the purposes of our experiments, we use the top 50,000 predicates for each model to form the feature set. the model should be aware of the dependent object, and conversely when tagging that object, the model should have a feature imposing a constraint arising from the identity of the dependent verb. 5 We parsed our corpus using the parser detailed in (Grinberg et al., 1995). The dependencies output by this parser are labeled with the type of dependency (connector) involved. For example, subjects (connector type S) and direct objects of verbs (O) are explicitly marked by the process (a full list of connectors is provided in the paper). We used all of the dependencies output by the parser as features in the models. 5.1.1 External Knowledge Sources 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnapar"
P06-2028,C02-1115,0,0.0275414,"ications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and after; the preposition before and after, and the syntactic category before and after the word being labeled. The features used in both of these approaches resemble those present in the feature set of a standard n-gram tagger, such as the one used as the baseline for the experiments in this paper. The semantic ta"
P06-2028,C96-2212,0,0.0353091,"redicate representing membership of this fruit synset should, if true, favor the selection of the correct tag for fruit words: NN1FOOD. The predicate will be true for the word pomegranate which will thereby benefit from the model’s knowledge of how to tag the other words in its class. Even if this is not so at this level in the hierarchy, it is likely to be so at some level of granularity. Precisely which levels of detail are useful will be learned by the model during training. 5.2.1 Automatic Clustering of Text We used the automatic agglomerative mutualinformation-based clustering method of (Ushioda, 1996) to form hierarchical clusters from approximately 50 million words of tokenized, unannotated text drawn from similar domains as the treebank used to train the tagger. Figure 5.2 shows the position of the word apple within the hierarchy of clusters. This example highlights both the strengths and weaknesses of this approach. One strength is that the process of clustering proceeds in a purely objective fashion and associations between words that may not have been considered by a human annotator are present. Moreover, the clustering process considers all types that actually occur in the corpus, an"
P06-2028,W04-0833,0,0.0419465,"Missing"
P06-2028,H93-1052,0,0.0921069,"ause it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literature on WSD is extensive. For a good overview we direct the reader to (Nancy and Jean, 1998). Typically, the local context around the ∗ National Institute of Information and Communications Technology † ATR Spoken Language Communication Research Labs 215 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 215–222, c Sydney, July 2006. 2006 Association for Computational Linguistics word to be sense-tagged is used to disambiguate the sense (Yarowsky, 1993), and it is common for linguistic resources such as WordNet (Li et al., 1995; Mihalcea and Moldovan, 1998; Ramakrishnan and Prithviraj, 2004), or bilingual data (Li and Li, 2002) to be employed as well as more longrange context. An ME-system for WSD that operates on similar principles to our system (Suarez, 2002) was based on an array of local features that included the words/POS tags/lemmas occurring in a window of +/-3 words of the word being disambiguated. (Lamjiri et al., 2004) also developed an ME-based system that used a very simple set of features: the article before; the POS before and"
P06-2028,J93-2004,0,\N,Missing
P06-2028,W96-0213,0,\N,Missing
P06-2028,J04-1001,0,\N,Missing
P06-2028,C98-1020,1,\N,Missing
P06-2123,C04-1067,0,0.0221711,"and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly. We used confidence measure to determine a better balance between R-oov and R-iv. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more than that. By way of the confidence measure we combined results from the dictionary-based and the IOBtagging-based and as a result, we could achieve the optimal performance. Our main contribution is to extend the IOB tagging approach from being a character-based to a subwo"
P06-2123,C04-1081,0,0.160179,"ce measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used"
P06-2123,W03-1728,0,0.537024,"88, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”. Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging. For new test data, word boundaries are determined based on the results of tagging. We proposed a subword-based tagging for Chinese"
P06-2123,I05-3018,0,0.360556,"iro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”. Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging. For new test data, word boundaries are determined based on the results of tagging. We proposed a subword-based tagging for Chinese word segmentation to improve the"
P06-2123,I05-3017,0,0.717137,"d Tagging for Confidence-dependent Chinese Word Segmentation Ruiqiang Zhang1,2 and Genichiro Kikui∗ and Eiichiro Sumita1,2 1 National Institute of Information and Communications Technology 2 ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled"
P06-2123,P04-1059,0,0.0675735,"Missing"
P06-2123,W03-1730,0,0.0709253,"ts of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine ∗ Now the second author is affiliated with NTT. While the IOB tagging approach has been widely used in Chinese word segmentation, we found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters"
P06-2123,N06-2049,1,0.825375,"43/0.948/0.948 0.951/0.961/0.962 R-oov 0.674/0.641/0.606 0.705/0.597/0.667 0.672/0.662/0.660 0.671/0.674/0.631 R-iv 0.950/0.964/0.969 0.950/0.977/0.968 0.958/0.966/0.966 0.951/0.967/0.970 Table 5: Effects of combination using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This pro"
P06-2123,W03-1719,0,\N,Missing
P06-2123,I05-3027,0,\N,Missing
P06-2123,N01-1025,0,\N,Missing
P06-2123,W03-1726,0,\N,Missing
P07-2007,N06-2049,1,\N,Missing
P07-2007,takezawa-kikui-2004-comparative,0,\N,Missing
P07-2007,2006.iwslt-evaluation.12,1,\N,Missing
P07-2007,2006.iwslt-papers.8,1,\N,Missing
P07-2046,H05-1085,0,0.015913,"rithms, one of which is the wellknown tool, GIZA++ (Och and Ney, 2003), for training IBM models (1-4). Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into a new format. This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these. One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data. Some published work has shown that applying morphological analysis improved the quality of 181 SMT (Lee, 2004; Goldwater and McClosky, 2005). We found that all this earlier work involved experiments conducted on translations from highly inflected languages, such as Czech, Arabic, and Spanish, to English. These researchers also provided detailed descriptions of the effects of foreign language morpho-syntactic analysis but presented no specific results to show the effect of English morphological analysis. To the best of our knowledge, there have been no papers related to English morphological analysis for Chinese-to-English (CE) translations even though the CE translation has been the main track for many evaluation campaigns includi"
P07-2046,N03-1017,0,0.0108764,"Missing"
P07-2046,N04-4015,0,0.295074,"gnment algorithms, one of which is the wellknown tool, GIZA++ (Och and Ney, 2003), for training IBM models (1-4). Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into a new format. This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these. One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data. Some published work has shown that applying morphological analysis improved the quality of 181 SMT (Lee, 2004; Goldwater and McClosky, 2005). We found that all this earlier work involved experiments conducted on translations from highly inflected languages, such as Czech, Arabic, and Spanish, to English. These researchers also provided detailed descriptions of the effects of foreign language morpho-syntactic analysis but presented no specific results to show the effect of English morphological analysis. To the best of our knowledge, there have been no papers related to English morphological analysis for Chinese-to-English (CE) translations even though the CE translation has been the main track for ma"
P07-2046,J03-1002,0,0.00377775,"hat while English is a language with less inflection, using English lemmas in training can significantly improve the quality of word alignment that leads to yield better translation performance. We carried out comprehensive experiments on multiple training data of varied sizes to prove this. We also proposed a new effective linear interpolation method to integrate multiple homologous features of translation models. 1 Introduction Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms, one of which is the wellknown tool, GIZA++ (Och and Ney, 2003), for training IBM models (1-4). Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into a new format. This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these. One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data. Some published work has shown that applying morphological analysis improved the quality of 181 SMT (Lee, 2004; Goldwater and McClosky, 2005). We found that all this earlier work inv"
P07-2046,2006.iwslt-evaluation.1,0,\N,Missing
P07-2046,P03-1021,0,\N,Missing
P10-2001,D07-1103,0,0.0130933,"Missing"
P10-2001,P05-1074,0,0.128063,"is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a paraphrase lattice. Then, we give the paraphrase lattice to the lattice decoder. • is there a beauty salon ? • is there a beauty parlor ? • is there a salon ? 3.1 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases e1 , e2 in one language are aligned to the same phrase c in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: In the paraphrase lattice, each node consists of a token, the distance to the next node and features for lattice decoding. We use following four features for lattice decoding. • Paraphrase probability (p) A paraphrase probability p(e2 |e1 ) calculated when acquiring the paraphrase. hp = p(e2 |e1 ) 1. Build a phrase table. • Language mod"
P10-2001,2005.mtsummit-papers.11,0,0.00371744,"(p) and (p, L) in EJ translation and (L) and (p, l) in EC translation. Since features related to the source-side language model were chosen in each direction, using the source-side language model is useful for decoding paraphrase lattices. We also tried a combination of Proposed Method and CCB, which is a method of decoding paraphrase lattices with an augmented phrase table. However, the result showed no significant improvements. This is because the proposed method includes the effect of augmenting the phrase table. Moreover, we conducted German-English translation using the Europarl corpus (Koehn, 2005). We used the WMT08 dataset1 , which consists of 1M sentences for training and 2K sentences for development and testing. We acquired 5.3M pairs of German-German paraphrases from a 1M German-Spanish parallel corpus. We conducted experiments with various sizes of training corpus, using 10K, 20K, 40K, 80K, 160K and 1M. Figure 3 shows the proposed method consistently get higher score than Moses and CCB. 4.2 Proposed method In the proposed method, we conducted experiments with various settings for paraphrasing and lattice decoding. Then, we chose the best setting according to the result of the dev2"
P10-2001,2008.iwslt-papers.2,0,0.615261,"Missing"
P10-2001,D09-1040,0,0.161716,"Missing"
P10-2001,N06-1003,0,0.301119,"araphrase pair is applied. As these features can penalize paraphrases which are not appropriate to the context, appropriate paraphrases are chosen and appropriate translations are output in lattice decoding. The features related to the sentence length, such as (L) and (d), are added to penalize the language model score in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low. In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 4.1 Baseline As baselines, we used Moses and Callison-Burch et al. (2006)’s method (hereafter CCB). In Moses, we used default settings without paraphrases. In CCB, we paraphrased the phrase table using the automatically acquired paraphrase list. Then, we augmented the phrase table with paraphrased phrases which were not found in the original phrase table. Moreover, we used an additional feature whose value was the paraphrase probability (p) if the entry was generated by paraphrasing and Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source 3 EJ EC Moses (w/o Paraphrases) 38.98 25.11 CCB 39.24 (+0.26) 26.14 (+1"
P10-2001,N09-1046,0,0.00902598,"mentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets. 1 Introduction Lattice decoding in SMT is useful in speech translation and in the translation of German (Bertoldi et al., 2007; Dyer, 2009). In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition. Thus, the translation quality for lattice inputs is better than the quality for 1best inputs. In this paper, we show that lattice decoding is also useful for handling input variations. “Input variations” refers to the differences of input texts with the same meaning. For example, “Is there a beauty salon?” and “Is there a beauty parlor?” have the same meaning with variations in “beauty salon” and “be"
P10-2001,P03-1021,0,0.00279147,"""beauty"" , 0.250, 1.172, 1, 1) (""salon"" , 0.133, 0.537, 0.367, 3) 4 -- (""parlor"" , 1, 1, 1, 2) Paraphrase probability (p) 5 -- (""salon"" , 1, 1, 1, 1) 6 -- (""?"" Language model score (l) , 1, 1, 1, 1) Paraphrase length (d) Figure 2: An example of a paraphrase lattice, which contains three features of (p, l, d). • Normalized language model score (L) SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). A language model score where the language model probability is normalized by the sentence length. The sentence length is calculated as the number of tokens. hL = LM (para) LM (orig) , 1 4 Experiments where LM (sent) = lm(sent) length(sent) • Paraphrase length (d) In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for dev"
P10-2001,2007.iwslt-1.1,0,\N,Missing
P10-2001,P07-2045,0,\N,Missing
P10-2004,P08-1009,0,0.0167973,"Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using the constraints softly (Cherry, 2008; Marton and Resnik, 2008). In their methods, a candidate 1 We use English examples for the sake of readability. 17 Proceedings of the ACL 2010 Conference Short Papers, pages 17–21, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics • the node syntactic labels, the bracketed sentence “((This) ((is) ((a) (pen))))” is obtained. Such a bracketed sentence can be used to produce constraints. For example, for the source-side bracketed tree “((f1 f2) (f3 f4)) ”, eight target sequences [e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3], [e2, e1, e4, e3], [e3, e4, e1,"
P10-2004,2008.amta-srw.2,0,0.0394404,"use GIZA++ to align the sentences in both the Chinese-English and EnglishChinese directions. We combine the alignments using the “grow-diag-final-and” procedure provided with MOSES (Koehn, 2007). Because there are many errors in the alignment, we remove the links if the alignment count is less than three for the source or the target word. Additionally, we also remove notoriously bad links in All the words covered by the node can be translated separately. That is to say, these words do not share a translation with any word outside the coverage of the node. 18 {de, le} × {the, a, an} following Fossum and Knight (2008). Thirdly, given the parse trees and the alignment information, we label each node as a frontier node or an interior node according to the definition introduced in this section. Using the labeled nodes as training data, we can build a classifier. In theory, a broad class of machine learning tools can be used; however, due to the scale of the task (see section 4), we utilize the Pegasos 2 which is a very fast SVM solver (Shalev-Shwartz et al, 2007). 3.2 MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU sco"
P10-2004,W02-1039,0,0.0292241,"odes In IST-ITG and many other methods which use syntactic constraints, all of the nodes in the parse trees are utilized. Though many nodes in the parse trees are useful, we would argue that some nodes are not trustworthy. For example, if we constrain the translation of “f1 f2 f3 f4” with node N2 illustrated in Figure 1, then word “e1” will never be put in the middle the other three words. If we want to obtain the translation “e2 e1 e4 e3”, node N3 can offer a good constraint while node N2 should be filtered out. In real corpora, cases such as node N2 are frequent enough to be noticeable (see Fox (2002) or section 4.1 in this paper). Therefore, we use the definitions in Galley et al. (2004) to classify the nodes in parse trees into two types: frontier nodes and interior nodes. Though the definitions were originally made for target language parse trees, they can be straightforwardly applied to the source side. A node which satisfies both of the following two conditions is referred as a frontier node: • All the words covered by the node remain contiguous after translation. f1 f2 f3 f4 e2 e1 e4 e3 Figure 1: An example parse tree and alignments 3.1 Training Ideally, we would have a human-annotat"
P10-2004,N04-1035,0,0.105736,"cture. For example1, the parse tree “(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen”, which consists of four words. By removing In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the streng"
P10-2004,J99-4005,0,0.0698668,"with the following constraint: the target sentence is obtained by rotating any node of the source sentence tree structure. After parsing the source sentence, a bracketed sentence is obtained by removing the node syntactic labels; this bracketed sentence can then be directly expressed as a tree structure. For example1, the parse tree “(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen”, which consists of four words. By removing In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering c"
P10-2004,P06-1077,0,0.0215411,"the parse tree “(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen”, which consists of four words. By removing In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phra"
P10-2004,W06-1606,0,0.0186219,"1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen”, which consists of four words. By removing In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts"
P10-2004,P08-1114,0,0.0179626,"traints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using the constraints softly (Cherry, 2008; Marton and Resnik, 2008). In their methods, a candidate 1 We use English examples for the sake of readability. 17 Proceedings of the ACL 2010 Conference Short Papers, pages 17–21, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics • the node syntactic labels, the bracketed sentence “((This) ((is) ((a) (pen))))” is obtained. Such a bracketed sentence can be used to produce constraints. For example, for the source-side bracketed tree “((f1 f2) (f3 f4)) ”, eight target sequences [e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3], [e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1], [e4"
P10-2004,P03-1021,0,0.00385146,"node or an interior node according to the definition introduced in this section. Using the labeled nodes as training data, we can build a classifier. In theory, a broad class of machine learning tools can be used; however, due to the scale of the task (see section 4), we utilize the Pegasos 2 which is a very fast SVM solver (Shalev-Shwartz et al, 2007). 3.2 MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexical reordering model was used in our experiments. The translation model was created from the FBIS corpus. We used a 5-gram language model trained with modified Knesser-Ney smoothing. The language model was trained on the target side of FBIS corpus and the Xinhua news in GIGAWORD corpus. The development and test sets are from NIST MT08 evaluation campaign. Table 1 shows the statistics of the corpora used in our experiments. Features For each node in the parse trees, we use the following feature templates: • A context-free grammar rule which rewrites the current node (In this and all the"
P10-2004,W06-3119,0,0.0338557,"(VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen”, which consists of four words. By removing In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (Knight, 1999). Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality. The most widely used reordering constraints are IBM constraints (Berger et al., 1996), ITG constraints (Wu, 1995) and syntactic constraints (Yamada et al., 2000; Galley et al., 2004; Liu et al., 2006; Marcu et al., 2006; Zollmann and Venugopal 2006; and numerous others). Syntactic constraints can be imposed from the source side or target side. This work will focus on syntactic constraints from source parse trees. Linguistic parse trees can provide very useful reordering constraints for SMT. However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data. The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases. Recent efforts attack this problem by using"
P10-2004,W08-0334,1,\N,Missing
P10-2004,P09-1058,0,\N,Missing
P10-2004,P01-1067,0,\N,Missing
P10-2004,P07-2045,0,\N,Missing
P10-2004,W08-0401,1,\N,Missing
P11-1064,N10-1028,0,0.636111,"tributions, and thus the parameters for the Pitman-Yor process will be different for each distribution. Further, as ll and lr must be smaller than l, Pt,l no longer contains itself as a base measure, and is thus not deficient. An example of the actual discount values learned in one of the experiments described in Section 7 is shown in Figure 2. It can be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase co"
P11-1064,P09-1088,0,0.778629,"nts. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to genera"
P11-1064,J93-2003,0,0.0978707,"Missing"
P11-1064,W10-1703,0,0.0131603,"he proposed method on translation tasks from four languages, French, German, Spanish, and Japanese, into English. 638 TM (en) TM (other) LM (en) Tune (en ) Tune (other) Test (en) Test (other) de-en 1.80M 1.85M 52.7M 49.8k 47.2k 65.6k 62.7k es-en 1.62M 1.82M 52.7M 49.8k 52.6k 65.6k 68.1k fr-en 1.35M 1.56M 52.7M 49.8k 55.4k 65.6k 72.6k ja-en 2.38M 2.78M 44.7M 68.9k 80.4k 40.4k 48.7k Table 1: The number of words in each corpus for TM and LM training, tuning, and testing. 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. Details of both corpora can be found in Table 1. Corpora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of o"
P11-1064,W07-0403,0,0.436106,"able that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-"
P11-1064,J07-2003,0,0.350869,"a fraction of the size of most heuristic extraction methods. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus size"
P11-1064,N10-1081,0,0.293077,"rates from the symbol distribution Px , then from the phrase distribution Pt , while HIER generates directly from Pt , which falls back to divide-and-conquer based on Px when necessary. It can be seen that while Pt in FLAT only generates minimal phrases, Pt in HIER generates (and thus memorizes) phrases at all levels of granularity. 4.1 Length-based Parameter Tuning There are still two problems with HIER, one theoretical, and one practical. Theoretically, HIER contains itself as its base measure, and stochastic process models that include themselves as base measures are deficient, as noted in Cohen et al. (2010). Practically, while the Pitman-Yor process in HIER shares the parameters s and d over all phrase pairs in the model, long phrase pairs are much more sparse those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f |) ≤ 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. Figure 2: Learned discount va"
P11-1064,P05-1066,0,0.0803727,"Missing"
P11-1064,P08-2007,0,0.0326897,"Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x; θx ). x can take the values TERM, REG , or INV . 2. According to the x take the following actions. (a) If x = TERM, generate a phrase pair from the phrase table Pt (he, f i; θt ). (b) If x = REG, a regular ITG rule, generate phrase pairs he1 , f1 i and he2 , f2 i from"
P11-1064,P10-1147,0,0.261523,"the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one importan"
P11-1064,W06-3105,0,0.0185751,"lso for phrases that, while not directly included in the model, are composed of two high probability child phrases. It should be noted that while for FLAT and HIER Pt can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting Pt (he, f i) = Pt,l (he, f i)c(l)/ L ∑ c(˜l) ˜ l=1 for every phrase pair, where l = |e |+ |f |and c(l) is the number of phrases of length l in the sample. We call this model-based extraction method MOD. 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extraction tends to out-perform approaches that use syntax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because generative models choose only a single phrase segmentation, and thus throw away many good phrase pairs that are in conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the jo"
P11-1064,D08-1033,0,0.563293,"Missing"
P11-1064,P06-1121,0,0.17041,"ds. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus sizes, the results from all three methods are comparable, with insign"
P11-1064,D07-1103,0,0.0694819,"mentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor gramma"
P11-1064,P07-2045,0,0.00525629,"Missing"
P11-1064,N03-1017,0,0.562956,"not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieve"
P11-1064,2005.iwslt-1.8,0,0.00647738,"tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA ++ and heuristic phrase extraction. Decoding is performed using Moses (Koehn and others, 2007) using the phrase tables learned by each method under consideration, as well as standard bidirectional lexical reordering probabilities (Koehn et al., 2005). Maximum phrase length is limited to 7 in all models, and for the LM we use an interpolated Kneser-Ney 5-gram model. For GIZA ++, we use the standard training regimen up to Model 4, and combine alignments with grow-diag-final-and. For the proposed models, we train for 100 iterations, and use the final sample acquired at the end of the training process for our experiments using a single sample6 . In addition, 6 For most models, while likelihood continued to increase gradually for all 100 iterations, BLEU score gains plateaued after 5-10 iterations, likely due to the strong prior information Al"
P11-1064,N06-1014,0,0.0794575,"f |; λ) 1 M0 (he, f i) =(Pm1 (f |e)Puni (e)Pm1 (e|f )Puni (f )) 2 . Ppois is the Poisson distribution with the average length parameter λ. As long phrases lead to sparsity, we set λ to a relatively small value to allow us to bias against overly long phrases4 . Pm1 is the word-based Model 1 (Brown et al., 1993) probability of one phrase given the other, which incorporates word-based alignment information as prior knowledge in the phrase translation probability. We take the geometric mean5 of the Model 1 probabilities in both directions to encourage alignments that are supported by both models (Liang et al., 2006). It should be noted that while Model 1 probabilities are used, they are only soft constraints, compared with the hard constraint of choosing a single word alignment used in most previous phrase extraction approaches. For Pbu , if g is the non-null phrase in e and f , we calculate the probability as follows: Pbu (he, f i) = Puni (g)Ppois (|g|; λ)/2. Note that Pbu is divided by 2 as the probability is considering null alignments in both directions. 4 Hierarchical ITG Model While in FLAT only minimal phrases were memorized by the model, as DeNero et al. (2008) note We choose 10−2 , 10−3 , or 10−"
P11-1064,W02-1018,0,0.126279,"Missing"
P11-1064,W07-0715,0,0.0636315,"n conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, n"
P11-1064,W99-0604,0,0.0831392,"on which value gives the best performance on the development set. 5 The probabilities of the geometric mean do not add to one, but we found empirically that even when left unnormalized, this provided much better results than the using the arithmetic mean, which is more theoretically correct. 3 and we confirm in the experiments in Section 7, using only minimal phrases leads to inferior translation results for phrase-based SMT. Because of this, previous research has combined FLAT with heuristic phrase extraction, which exhaustively combines all adjacent phrases permitted by the word alignments (Och et al., 1999). We propose an alternative, fully statistical approach that directly models phrases at multiple granularities, which we will refer to as HIER. By doing so, we are able to do away with heuristic phrase extraction, creating a fully probabilistic model for phrase probabilities that still yields competitive results. Similarly to FLAT, HIER assigns a probability Phier (he, f i; θx , θt ) to phrase pairs, and is parameterized by a phrase table θt and a symbol distribution θx . The main difference from the generative story of the traditional ITG model is that symbols and phrase pairs are generated i"
P11-1064,W09-3804,0,0.435467,"be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase counts. As a phrase pair ta may have been generated from two smaller component phrases tb and tc , when a sample containing ta is removed from the distribution, it may also be necessary to decrement the counts of tb and tc as well. The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solu"
P11-1064,P06-1124,0,0.832206,"tribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution. We combine this model with the Bayesian nonparametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a 632 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table generated by heuri"
P11-1064,J97-3002,0,0.466135,"le values of the hidden parameters: ∫ P (e|f , hE, Fi) = P (e|f , θ)P (θ|hE, Fi). (1) θ If θ takes the form of a scored phrase table, we can use traditional methods for phrase-based SMT to find P (e|f , θ) and concentrate on creating a model for P (θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x;"
P11-1064,P08-1012,0,0.360881,"t with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and con"
P11-1125,P89-1018,0,0.60505,"in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules"
P11-1125,W10-1703,0,0.114529,"during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions, {Czech, French, German, Spanish}-toEnglish (Callison-Burch et al., 2010), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others. First, we will review the state-of-the-art method which is a system combination framework based on confusion networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. .*. I.. .saw . t.he . . f. orest . . . .I. w . alked . .the . b. l"
P11-1125,J07-2003,0,0.621752,".VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to the left-hand side and the right-hand s"
P11-1125,W07-0414,0,0.0168751,"which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Papineni et al., 2002). We vary the horizontal orders, h = 1, 2, ∞ with vertical orders of v = 3, 4"
P11-1125,P81-1022,0,0.659596,"ts the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to those found in the parse trees. Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computa"
P11-1125,A94-1016,0,0.111864,"st derivation d: dˆ = arg max w⊤ · h(d, F ) (1) d∈D where h(d, F ) is a set of feature functions scaled by weight vector w. We use cube-pruning (Chiang, 2007; Huang and Chiang, 2007) to approximately intersect with non-local features, such as n-gram language models. Then, k-best derivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even wi"
P11-1125,J99-4004,0,0.0212069,"roach for constructing a confusion forest: First, MT outputs are parsed. Second, 1251 [TOP → S•, 0] Figure 3: The deductive system for Earley’s generation algorithm a grammar is learned by treating each hyperedge as an instance of a CFG rule. Third, a forest is generated from the unique root symbol of the extracted grammar through non-terminal rewriting. 3.1 Forest Generation Given the extracted grammar, we apply a variant of Earley’s algorithm (Earley, 1970) which can generate strings in a left-to-right manner from the unique root symbol, TOP. Figure 3 presents the deductive inference rules (Goodman, 1999) for our generation algorithm. We use capital letters X ∈ N to denote non-terminals and x ∈ T for terminals. Lowercase Greek letters α, β and γ are strings of terminals and non-terminals (T ∪ N )∗ . u and v are weights associated with each item. The major difference compared to Earley’s parsing algorithm is that we ignore the terminal span information each non-terminal covers and keep track of the height of derivations by h. The scanning step will always succeed by moving the dot to the right. Combined with the prediction and completion steps, our algorithm may potentially generate a spuriousl"
P11-1125,D08-1011,0,0.235716,"ivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a"
P11-1125,W99-0623,0,0.0846419,"ute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The"
P11-1125,W05-1506,0,0.13737,"N.N . . .was . . . . .forest . . I.. . .the .D.T .VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to t"
P11-1125,P07-1019,0,0.0780672,"@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to the left-hand side and the right-hand side of an instance of a r"
P11-1125,P05-3026,0,0.0241321,"acted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table i"
P11-1125,W01-1812,0,0.0430929,"BD@ V . 2.1 .PRP . .D.T .N.N . . .was . . . . .forest . . I.. . .the .D.T .VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail"
P11-1125,P03-1054,0,0.0181285,"al symbols assigned to each parse tree before extracting rules. Here, we replace each non-terminal symbol by the state representation of Earley’s algorithm corresponding to the sequence of prediction steps starting from TOP. Figure 4(a) presents an example parse tree with each symbol replaced by the Earley’s state in Figure 4(b). For example, the label for VBD is replaced by •S + NP : •VP + •VBD : NP which corresponds to the prediction steps of TOP → •S, S → NP • VP and VP → •VBD NP. The context represented in the Earley’s state is further limited by the vertical and horizontal Markovization (Klein and Manning, 2003). We define the vertical order v in which the label is limited to memorize only v previous prediction steps. For instance, setting v = 1 yields NP : •VP + •VBD : NP in our example. Likewise, we introduce the horizontal order h which limits the number of sibling labels memorized on the left and the right of the dotted label. Limiting h = 1 implies that each deductive step is encoded with at most three symbols. No limits in the horizontal and vertical Markovization orders implies memorizing of all the deductions and yields a confusion forest representing the union of parse trees through the gram"
P11-1125,P09-1019,0,0.0211587,"escribed in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S → X, S → S X and X → x. k-best translations are extracted from the forest using the forest-based algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combi"
P11-1125,P98-1116,0,0.0104244,"translation table is constructed from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is const"
P11-1125,A00-2023,0,0.05299,"ucted from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing"
P11-1125,N09-2003,0,0.0142979,"th system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Papineni et al., 2002). We vary the horizontal orders, h = 1, 2, ∞ with vertical orders of v = 3, 4, ∞. Systems without statistically significant differences from the best result (p"
P11-1125,P09-1065,0,0.0382463,"r grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). 1253"
P11-1125,E09-1061,0,0.0154029,"034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing"
P11-1125,D07-1105,0,0.0143057,".2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additio"
P11-1125,E06-1005,0,0.343573,"reen . t.rees . .. . . .the . . . . . .forest . .was . .found . (a) Pairwise alignment using the first starred hypothesis as a skeleton. .saw .I . . .the . . .ϵ ϵ. . alked w .blue f. orest .trees f. ound . . . . .ϵ .ϵ .green .was ϵ. (b) Confusion network from (a) .saw .I . . .ϵ . .the . ϵ. . alked w .blue f. orest .was .found . . . . .green .trees .ϵ .ϵ (c) Incrementally constructed confusion network 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from t"
P11-1125,P08-1023,0,0.168136,"ch recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to th"
P11-1125,P04-1063,0,0.0242747,"ek ˆ for the best derivation d: dˆ = arg max w⊤ · h(d, F ) (1) d∈D where h(d, F ) is a set of feature functions scaled by weight vector w. We use cube-pruning (Chiang, 2007; Huang and Chiang, 2007) to approximately intersect with non-local features, such as n-gram language models. Then, k-best derivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influe"
P11-1125,J03-1002,0,0.00509528,"rred hypothesis as a skeleton. .saw .I . . .the . . .ϵ ϵ. . alked w .blue f. orest .trees f. ound . . . . .ϵ .ϵ .green .was ϵ. (b) Confusion network from (a) .saw .I . . .ϵ . .the . ϵ. . alked w .blue f. orest .was .found . . . . .green .trees .ϵ .ϵ (c) Incrementally constructed confusion network 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The netwo"
P11-1125,P02-1040,0,0.0828041,"vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), w"
P11-1125,N07-1029,0,0.571826,"ork, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin"
P11-1125,P07-1040,0,0.766415,"ork, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin"
P11-1125,W08-0329,0,0.116632,"heses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The network consists of several arcs, each of which represents an alternative word at that position, including the empty symbol, ϵ. This pairwise alignment strategy is prone to spurious insertions and repetitions due to alignment errors such as in Figure 1(a) in which “green” in the third hypothesis is aligned with “forest” in the skeleton. Rosti et al. (2008) introduces an incremental method so that hypotheses are aligned incrementally to the growing confusion network, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produ"
P11-1125,2006.amta-papers.25,0,0.137982,"cations Technology 3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN {taro.watanabe,eiichiro.sumita}@nict.go.jp Abstract backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules t"
P11-1125,C98-1112,0,\N,Missing
P11-1125,N10-1141,0,\N,Missing
P11-1125,2005.eamt-1.20,0,\N,Missing
P11-2076,A00-2018,0,0.0172986,") in J-E/E-J translation. (w/ Context) and the method of specifying reordering constraints without a context document (w/o Context). In both methods, the feature weights used in decoding are the same value as those for the baseline (dl = −1). 4.2.1 Proposed method (w/ Context) In the proposed method, reordering constraints were defined with a context document. For J-E translation, we used the CaboCha parser (Kudo and Matsumoto, 2002) to analyze the context document. As coherent phrase candidates, we extracted all subtrees whose heads are noun. For E-J translation, we used the Charniak parser (Charniak, 2000) and extracted all noun phrases, labeled “NP”, as coherent phrase candidates. The parsers are used only when extracting coherent phrase candidates. When specifying zones for each source sentence, strings which match the coherent phrase candidates are defined to be zones. Therefore, the proposed method is robust against parsing errors. We tried various thresholds of the C-value and selected the value that yielded the highest BLEU score for the development set. 4.2.2 w/o Context In this method, reordering constraints were defined without a context document. For J-E translation, we converted the"
P11-2076,P08-1009,0,0.223081,"Missing"
P11-2076,C96-1009,0,0.772989,"g zones. We then give the zone-tagged sentence, an example is shown in Table 1, as a decoder input. 3.2.2 Ranking with C-value The candidates which have been extracted are nested and have different lengths. A naive method cannot rank these candidates properly. For example, ranking by frequency cannot pick up an important phrase which has a long length, yet, ranking by length may give a long but unimportant phrase a high rank. In order to select the appropriate coherent phrases, measurements which give high rank to phrases with high termhood are needed. As one such measurement, we use C-value (Frantzi and Ananiadou, 1996). C-value is a measurement of automatic term recognition and is suitable for extracting important phrases from nested candidates. The C-value of a phrase p is expressed in the following equation: In decoding, reorderings which violate zones, such as the baseline output in Table 1, are restricted and we get a more appropriate translation, such as the proposed output in Table 1. We use the Moses decoder (Koehn et al., 2007; Koehn and Haddow, 2009), which can specify reordering constraints using &lt;zone&gt; and &lt;/zone&gt; tags. Moses restricts reorderings which violate zones and translates zones as singl"
P11-2076,W10-1736,0,0.0543091,"Missing"
P11-2076,P03-1040,0,0.116281,"with similar word orders. However, it has problems with longdistance reordering when translating between languages with different word orders, such as JapaneseEnglish. These problems are especially crucial when translating long sentences, such as patent sentences, because many combinations of word orders cause high computational costs and low translation quality. In order to address these problems, various methods which use syntactic information have been proposed. These include methods where source sentences are divided into syntactic chunks or clauses and the translations are merged later (Koehn and Knight, 2003; Sudoh et al., 2010), methods where syntactic constraints or penalties for reordering are added to a decoder (Yamamoto et al., 2008; Cherry, 2008; Marton and Resnik, 2008; Xiong et al., 2010), and methods where source sentences are reordered into a similar word order as the target language in advance (Katz-Brown and Collins, 2008; Isozaki et al., 2010). However, these methods did not use document-level context to constrain reorderings. Document-level context is often available in real-life situations. We think it is a promising clue to improving translation quality. In this paper, we propose"
P11-2076,P07-2045,0,0.00562533,"rder to select the appropriate coherent phrases, measurements which give high rank to phrases with high termhood are needed. As one such measurement, we use C-value (Frantzi and Ananiadou, 1996). C-value is a measurement of automatic term recognition and is suitable for extracting important phrases from nested candidates. The C-value of a phrase p is expressed in the following equation: In decoding, reorderings which violate zones, such as the baseline output in Table 1, are restricted and we get a more appropriate translation, such as the proposed output in Table 1. We use the Moses decoder (Koehn et al., 2007; Koehn and Haddow, 2009), which can specify reordering constraints using &lt;zone&gt; and &lt;/zone&gt; tags. Moses restricts reorderings which violate zones and translates zones as single blocks. { C-value(p) = (l(p)−1) n(p) ( (l(p)−1) t(p) n(p)− c(p) ) (c(p) = 0) (c(p) &gt; 0) where l(p) is the length of a phrase p, n(p) is the frequency of p in a document, t(p) is the total frequency of phrases which contain p as a subphrase, c(p) is the number of those phrases. Since phrases which have a large C-value frequently occur in a context document, these phrases are considered to be a significant unit, i.e., a"
P11-2076,W04-3250,0,0.199744,"Missing"
P11-2076,W02-2016,0,0.130618,"Missing"
P11-2076,P08-1114,0,0.190549,"Missing"
P11-2076,P03-1021,0,0.00888357,"ains the patent specifications from which sentence pairs are extracted. We used these patent specifications as context documents. 4.1 Baseline We used Moses as a baseline system, with all the settings except distortion limit (dl) at the default. The distortion limit is a maximum distance of reordering. It is known that an appropriate distortion-limit can improve translation quality and decoding speed. Therefore, we examined the effect of a distortionlimit. In experiments, we compared dl = 6, 10, 20, 30, 40, and −1 (unlimited). The feature weights were optimized to maximize BLEU score by MERT (Och, 2003) using the development set. 4.2 Compared methods We compared two methods, the method of specifying reordering constraints with a context document w/o Context w/ Context in ( this case ) , ( the leading end ) 15f of ( the segment operating body ) ( ( 15 swings ) in ( a direction opposite ) ) to ( the a arrow direction ) . in ( this case ) , ( ( the leading end ) 15f ) of ( ( ( the segment ) operating body ) 15 ) swings in a direction opposite to ( the a arrow direction ) . Table 3: An example of the zone-tagged source sentence. &lt;zone&gt; and &lt;/zone&gt; are replaced by “(” and “)”. System dl 6 10 20 B"
P11-2076,P02-1040,0,0.0801919,"Missing"
P11-2076,W10-1762,0,0.0518035,"Missing"
P11-2076,W08-0401,1,0.853494,"Missing"
P11-2076,N10-1016,0,\N,Missing
P11-2076,W09-0429,0,\N,Missing
P12-2061,N09-1029,0,0.0274937,"Missing"
P12-2061,J07-2003,0,0.0775997,"rce language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh"
P12-2061,P05-1066,0,0.315048,"roblem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g.,"
P12-2061,D11-1018,0,0.0458521,"irst, we produce N-best HFE sentences using Japanese-to-HFE monotone phrase-based SMT. Next, we produce K-best parse trees for each HFE sentence by parsing, and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2 There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98"
P12-2061,P05-1067,0,0.024275,"yntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called hea"
P12-2061,N04-1035,0,0.0556164,"erring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The meth"
P12-2061,N10-1127,0,0.0611272,"ween languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This method fi"
P12-2061,C10-1051,0,0.0113551,"and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2 There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus a"
P12-2061,D10-1092,0,0.265016,"Missing"
P12-2061,W10-1736,0,0.581409,"ages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This method first changes the Englis"
P12-2061,N03-1017,0,0.0113125,"Missing"
P12-2061,P06-1077,0,0.0779107,"he syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, si"
P12-2061,P09-1063,0,0.0332901,"Missing"
P12-2061,J08-1002,0,0.0611705,"of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Japanese HFE Japanese: English HFE: monotone translation kare he wa _va0 kinou hon yesterday wo post-ordering katta _va2 books bought Parsing Figure 1: Post-ordering framework. S_ST VP_SW English words into Japanese. There are two key reasons why this pre-ordering method works for estimating Japanese word order. The first reason is that Japanese is a typical headfinal language. That is, a syntactic head word comes after nonhead (dependent) words. Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. Consequently, the parsed English input sentences can be pre-ordered into a Japaneselike word order using the head finalization rule. Pre-ordering using the head finalization rule naturally cannot be applied to Japanese-English translation, because English is not a head-final language. If we want to pre-order Japanese sentences into an English-like word order, we therefore have to build complex rules (Sudoh et al., 2011b). VP_SW NP_ST HFE: he _va0 NP_ST yesterday _va2 books bought Reordering S VP VP NP English: he _va0) ( NP bought books _va2) ( yesterday Figure"
P12-2061,N07-1051,0,0.21339,"d with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-p"
P12-2061,D09-1105,0,0.0460596,"g one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This"
P12-2061,2011.mtsummit-papers.34,0,0.0832434,"Missing"
P12-2061,J97-3002,0,0.424822,"e swapped nodes are annotated with “ SW”. When the two nodes are not swapped, they are annotated with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.2 HFE and Articles This section desc"
P12-2061,C04-1073,0,0.136648,"The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntacti"
P12-2061,P07-2045,0,\N,Missing
P13-1016,J96-1002,0,0.0215273,"i, and sj , which is the word specified at j, or both the context of i and the context of j simultaneously. Distance is considered using the distance class d. Distortion is represented by distance and orientation. The pair model considers distortion using six joint classes of d and o. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the NPC simultaneously to estimate the NP. This can be done by our distortion model definition and the learning strategy described in the previous section. In this work, we use the maximum entropy method (Berger et al., 1996) as a discriminative machine learning method. The reason for this is that a model based on the maximum entropy method can calculate probabilities. However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model. Let s be a source word and sn1 = s1 s2 ...sn be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as sn+1 0 (s0 = BOS, sn+1 = EOS). Our distortion"
P13-1016,J07-2003,0,0.673825,"for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155–165, c"
P13-1016,C10-2033,0,0.528996,"Missing"
P13-1016,D08-1089,0,0.0586676,"he NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their mod"
P13-1016,P06-1077,0,0.0741182,"nces appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider"
P13-1016,N04-1035,0,0.0561875,"n the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model,"
P13-1016,P09-2061,0,0.018513,"n if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context.5 Their model does not simultaneously con5 3.1 Distortion Model and Learning Strategy First, we define our distortion model. Let i be a CP, j be an NPC, S be a source sentence, and X be the random"
P13-1016,P03-1021,0,0.0383462,"ors, we removed articles {a, an, the} in English and particles {ga, wo, wa} in Japanese before performing word alignments because these function words do not correspond to any words in the other languages. After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5gram language models that were trained using the English side of each set of bilingual training data. We used an in-house standard phrase-based SMT system compatible with the Moses decoder (Koehn et al., 2007). The SMT weighting parameters were tuned by MERT (Och, 2003) using the development data. To stabilize the MERT results, we tuned three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different di"
P13-1016,N10-1129,0,0.272219,"at the CP but also the word at a NP candidate (NPC) should be considered simultaneously. In (c) and (d) in Figure 2, the word (kare) at the CP is the same and karita (borrowed) and katta (bought) are at the NPCs. Karita is the word at the NP and katta is not the word at the NP for (c), while katta is the word at the NP and karita is not the word at the NP for (d). From these examples, considering what the word is at the NP 3 NP is not always one position, because there may be multiple correct hypotheses. 4 This definition is slightly different from that of existing methods such as Moses and (Green et al., 2010). In existing methods, CP is the rightmost position of the last translated source phrase and NP is the leftmost position of the source phrase to be translated next. Note that existing methods do not consider word-level correspondences. 156 sider both the word specified at the CP and the word specified at an NPC. For example, the outbound model considers the word specified at the CP, but does not consider the word specified at an NPC. Their models also do not consider relative word order. In contrast, our distortion model solves the aforementioned problems. Our distortion models utilize the wor"
P13-1016,P02-1040,0,0.0864299,"010). The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model13 , i of the feature templates was changed to j. Features occurring four or more times in the training sentences were used. The maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters. The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. 4.4 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation score BLEU-4 (Papineni et al., 2002). We used distortion limits of 10, 20, 30, and unlimited (∞), which limited the number of words for word reordering to a maximum number. Table 3 presents our main results. The proposed SEQUENCE outperformed the baselines for both Japanese to English and Chinese to English translation. This demonstrates the effectiveness of the proposed SEQUENCE. The scores of the proposed SEQUENCE were higher than those 4.2 Training for the Proposed Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model"
P13-1016,N04-4026,0,0.287066,"Missing"
P13-1016,2005.iwslt-1.8,0,0.323302,"between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP"
P13-1016,J97-3002,0,0.0396293,"istance class feature used in the model was the same (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering dista"
P13-1016,P07-2045,0,0.0157849,"ummary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP"
P13-1016,C04-1073,0,0.12617,"9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association fo"
P13-1016,P12-1095,0,0.0139432,"models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider rich context. Experiments indicate that our models achieved better performance and the sequence model could learn the effect of distances appropriately. Since our models do not require a parser, they can be applied to many languages. Future work includes application to other language pairs, incorporation into ITG constraint methods and other reordering methods, and application to tree-based SMT methods. 5 Related Works We discuss related works other than discussed in Section 2. Xiong et al. (2012) proposed a model predicting the orientation of an argument with respect to its verb using a parser. Syntactic structures and predicate-argument structures are useful for reordering. However, orientations do not handle distances. Thus, our distortion model does not compete against the methods predicting orientations using a parser and would assist them if used References 16 Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association"
P13-1016,W04-3250,0,0.119941,"Missing"
P13-1016,W08-0401,1,0.819648,"e (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to u"
P13-1016,C04-1030,1,0.952204,"experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st"
P13-1016,P06-1067,0,\N,Missing
P13-1016,P01-1067,0,\N,Missing
P13-1078,1997.tmi-1.19,0,0.396779,"Missing"
P13-1078,D08-1024,0,0.0472501,"runing method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference"
P13-1078,P05-1033,0,0.522224,"ng; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into fea"
P13-1078,J07-2003,0,0.830393,", e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a linear component which captures nonlocal (or state dependent) features and a non-linear component (i.e., neural nework) which encodes loMost statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interprete"
P13-1078,P11-2031,0,0.0314142,") as 10 and 30, and M axIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively. Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except λ and M axIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al. (2011) for fairer comparisons. For AdNN, we report the averaged scores of five post-training runs, but both pre-training and training are performed only once. 5.2 NIST08 17.42+ 18.33+ 18.20+ 19.42 test4 23.68+ 23.66+ 24.03+ 24.45 Table 2: The BLEU comparisons between AdNNHiero-E and Log-linear translation models on the Chinese-to-English and Japanese-to-English tasks. + means the comparison is significant over AdNN-Hiero-E with p &lt; 0.05. these features are not dependent on the translation states, they are computed and saved to memory when loading the translation model. During decoding, we just look"
P13-1078,W09-0438,0,0.0124299,"computational times for the features in  the hidden units, i.e. σ M · h0 (r) + B . Since 5 For easier tuning, we tuned these two parameters on a given development test set without post-training in Algorithm 2. 797 Chinese-to-English NIST05 NIST06 L-Hiero 25.57+ 25.27+ 25.93 AdNN-Hiero-E 26.37 AdNN-Hiero-D 26.21 26.07 Japanese-to-English test2 test3 L-Hiero 24.38 25.55 AdNN-Hiero-E 25.14+ 26.32+ AdNN-Hiero-D 24.42 25.46 model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposab"
P13-1078,P08-2010,0,0.66897,", Taro Watanabe2 , Eiichiro Sumita2 , Tiejun Zhao1 1 School of Computer Science and Technology Harbin Institute of Technology (HIT), Harbin, China 2 National Institute of Information and Communication Technology (NICT) 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan {lmliu |tjzhao}@mtlab.hit.edu.cn {taro.watanabe |eiichiro.sumita}@nict.go.jp Abstract On the one hand, features are required to be linear with respect to the objective of the translation model (Nguyen et al., 2007), but it is not guaranteed that the potential features be linear with the model. This induces modeling inadequacy (Duh and Kirchhoff, 2008), in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing,"
P13-1078,D11-1125,0,0.0808631,"near model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference pair for f . Following PRO, w"
P13-1078,P02-1040,0,0.103298,"ts are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable Training Algorithm Algorithm 2 Training Algorithm Input: M axIter, a dev set, parameters (e.g. λ"
P13-1078,N03-1017,0,0.050145,"(news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phr"
P13-1078,P07-2045,0,0.00938653,"-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translati"
P13-1078,C12-2104,0,0.00594537,"model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking"
P13-1078,koen-2004-pharaoh,0,0.192436,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,W04-3250,0,0.540761,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,2012.amta-papers.17,0,0.553532,"presenting each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the m"
P13-1078,N12-1005,0,0.0332495,"erm is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of consid"
P13-1078,W07-0710,0,0.699181,"Missing"
P13-1078,P10-1040,0,0.0218209,"ine σ as a multilayer neural network. Again for the example shown in Figure 1, the model score defined in Eq. (5) for the pair he2 , d2 i can be represented as follows: because they empirically perform well in the loglinear model. For the local feature vector h0 in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X → hα, γi. Let VS be the vocabulary in the source language with size |VS |; Rn×|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS ; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − |α |words “N U LL” to the end of α to obtai"
P13-1078,P00-1056,0,0.0517744,"Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-samplin"
P13-1078,P02-1038,0,0.904636,"a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks. 1 Introduction Recently, great progress has been achieved in SMT, especially since Och and Ney (2002) proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model. Its most important advantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. 791 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801, c Sof"
P13-1078,D07-1080,1,0.791639,"rch strategy and cube pruning method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is c"
P13-1078,P10-1076,0,0.011635,"ural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. NIST08 18.33+ 19.42 19.54 test4 23.66 24.45+ 23.73 Table 3: The effect of different feature setting on AdNN model. + means the comparison is significant over AdNN-Hiero-D with p &lt; 0.05. both test sets. In addition, to investigate the effect of different feature settings on AdNN, we alternatively design another setting for h0 in Eq."
P13-1078,J93-2003,0,\N,Missing
P13-1078,P03-1021,0,\N,Missing
P13-1079,W07-0403,0,0.0221244,"n adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012)."
P13-1079,P05-1033,0,0.0914726,"ethod in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data"
P13-1079,P11-2031,0,0.0169713,"tences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus. Since pialign cannot handle large data, we did not experiment on the largest LDC data set. 5. Pialign-adaptive: Alignment and phrase pairs extraction are same to Pialign-batch, while translation probabilities are estimated"
P13-1079,P08-2007,0,0.01791,"ods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each IT"
P13-1079,W07-0717,0,0.0271745,"slation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified"
P13-1079,N10-1028,0,0.0143001,"explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds"
P13-1079,N03-1017,0,0.0141628,"phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using"
P13-1079,P07-2045,0,0.00605503,"h and Pialign-adaptive were not run on the largest data set. the feature values. Pialign is used with default parameters. The parameter ’samps’ is set to 5, which indicates 5 samples are generated for a sentence pair. The IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the featu"
P13-1079,P08-1024,0,0.0181494,"a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism whi"
P13-1079,D07-1090,0,0.0188054,"Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs"
P13-1079,D09-1079,0,0.025601,"‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based me"
P13-1079,N12-1047,0,0.0243501,"IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the"
P13-1079,N10-1062,0,0.0176112,"lassifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain ou"
P13-1079,I08-2089,0,0.0241935,"al phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned tog"
P13-1079,W11-2122,0,0.0170791,"(Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed."
P13-1079,P12-1048,0,0.0787144,"done while the first author visited NICT. 802 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802–810, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation pe"
P13-1079,P06-1124,0,0.0502351,"is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008).  More formally, P he, f i; θx , θt are the probability of phrase pairs he, f i, which is parameterized by a phrase pair distribution θt and a symbol distribution θx . θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), which is expressed as  θt ∼ P Y d, s, Pdac (1) where d is the discount parameter, s is the strength parameter, and , and Pdac is a prior probability which acts as a fallback probability when a phrase pair is not in the model. Under this model, the probability for a phrase pair found in a bilingual corpus hE, F i can be represented by the following equation using the Chinese restaurant process (Teh, 2006):  P hei , fi i; hE, F i = Figure 1: A word alignment (a), and its hierarchical derivation (b). c. If x = IN V , follow a similar process as b, but concatenate f1 and f2 in reverse order he"
P13-1079,D07-1036,0,0.0197915,"keeps growing. Consequently, how to effectively use those data and improve translation performance becomes a challenging issue. This joint work was done while the first author visited NICT. 802 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802–810, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent ba"
P13-1079,2012.amta-papers.18,0,0.0920973,"on a large data set, and it requires us to re-train every time new training data is available. Even if we can handle the large computation cost, improvement is not guaranteed every time we perform batch tuning on the newly updated training data obtained from divergent domains. Traditional domain adaption methods for SMT are also not adequate in this scenario. Most of them have been proposed in order to make translation systems perform better for resource-scarce domains when most training data comes from resourcerich domains, and ignore performance on a more generic domain without domain bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to p"
P13-1079,P11-1064,1,0.92446,"bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to perform phrase table extraction. Second, extracted phrase tables are combined as if they are drawn from a hierarchical Pitman-Yor process, in which the phrase tables represented as tables in the Chinese restaurant process (CRP) are hierarchically chained by treating each of the previously learned phrase tables as prior to the current one. Thus, we can easily update the chain of phrase tables by appending the newly extracted phrase table and by treating the chain of the previous ones as its prior. Experiment results indicate that our method can achieve better translation performance when th"
P13-1079,J97-3002,0,0.0172822,"ence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by sli"
P13-1079,P12-1018,1,0.903351,"(Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008).  More formally, P he, f i; θx , θt are the probability of phrase pairs he, f i, which is parameterized by a phrase pair distribution θt and a symbol distribution θx . θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1"
P13-1079,2007.mtsummit-papers.68,0,0.0386744,"ining data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a m"
P13-1079,P08-1012,0,0.0167267,"er-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronou"
P13-1079,J03-1002,0,0.00573107,"phrase pair would be boosted by the fallback probabilities. Pitman-Yor process is also employed in n-gram language models which are hierarchically represented through the hierarchical Pitman-Yor process with switch priors to integrate different domains in all the levels (Wood and Teh, 2009). Our work incrementally combines the models from different domains by directly employing the hierarchical process through the base measures. 5 Corpus HIT BTEC Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 News News Magazine Magazine Finance 1. GIZA-linear: Phase pairs are extracted in each domain by GIZA++ (Och and Ney, 2003) and the ”grow-diag-final-and” method with a maximum length 7. The phrase tables from various domains are linearly combined by averaging the feature values. 2. Pialign-linear: Similar to GIZA-linear, but we employed the phrasal ITG method described in Section 3 using the pialign toolkit 3 (Neubig et Experiment 1 http://code.google.com/p/plda/ In particular, they come from LDC catalog number: LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47, LDC2006E26, in this order. 3 http://www.phontron.com/pialign/ 2 We evaluate the proposed approach on the Chinese-to-English translation task with three data"
P13-1079,J04-4002,0,0.0224078,"rely on a heuristic phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incremental"
P13-1079,P02-1040,0,0.0878579,"ata consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus. Since pialign cannot handle large data, we did not experiment on"
P13-1083,D09-1037,0,0.0182123,"ls: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1083,P05-1067,0,0.0375111,"syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they ar"
P13-1083,P07-1035,0,0.105138,"Eiichiro Sumita† , Hiroya Takamura‡ , Manabu Okumura‡ † National Institute of Information and Communications Technology {akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp † Precision and Intelligence Laboratory, Tokyo Institute of Technology {takamura, oku}@pi.titech.ac.jp Abstract [Example 1] 1 あなた は インターネット が 利用 でき ない Japanese POS: noun particle This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. [Example 2] particle noun verb auxiliary verb You can not use the Internet . 私 Japane"
P13-1083,D09-1071,0,0.0564114,"Missing"
P13-1083,P06-1121,0,0.0231334,"guage into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceeding"
P13-1083,I11-1136,0,0.0659352,"Missing"
P13-1083,D11-1125,0,0.0332039,"Missing"
P13-1083,W06-3601,0,0.0610141,"Missing"
P13-1083,P10-1145,0,0.0147666,"d method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the othe"
P13-1083,N03-1017,0,0.00724269,"e included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, th"
P13-1083,C12-1120,0,0.0368962,"Missing"
P13-1083,J03-1002,0,0.0064088,"lish sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the fo"
P13-1083,P03-1021,0,0.159428,"Missing"
P13-1083,W04-3250,0,0.192157,"Missing"
P13-1083,P02-1040,0,0.0860707,"Missing"
P13-1083,W02-2016,0,0.0772343,"Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency str"
P13-1083,P05-1034,0,0.0379419,"between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed withou"
P13-1083,D07-1072,0,0.16601,"nd parameters (e.g., ϕ′k and ϕ′′k ) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities πks and observation ′ distributions (ϕsk , ϕks ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st , and we use the pair (st , zt ) as a state representation. β|γ ∼ GEM(γ), πk |α0 , β ∼ DP(α0 , β), ϕk ∼ H, ϕ′k ∼ H ′ , zt′ |zt ∼ Multinomial(πzt ), xt |zt ∼ F (ϕzt ), x′t |zt ∼ F ′ (ϕ′zt"
P13-1083,W11-2119,0,0.0290777,"Missing"
P13-1083,C04-1090,0,0.0333421,"s represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal"
P13-1083,P06-1077,0,0.0845012,"s in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “"
P13-1083,P08-1066,0,0.025975,"S tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the l"
P13-1083,P09-1063,0,0.0382929,"Missing"
P13-1083,N12-1045,0,0.0450049,"Missing"
P13-1083,D08-1022,0,0.0195747,"ations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual"
P13-1083,P11-1125,1,0.791018,"Missing"
P13-1083,N12-1026,1,0.875093,"Missing"
P13-1083,P08-1064,0,0.0187394,"te two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Asso"
P13-1083,P11-1084,0,0.0126965,"joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851, c Sofia, Bulgaria, August 4"
P13-1083,P11-1087,0,\N,Missing
P13-1083,P07-2045,0,\N,Missing
P14-1138,D13-1106,0,0.0157846,"Dependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are su"
P14-1138,P06-1009,0,0.083905,"Missing"
P14-1138,J93-2003,0,0.11607,"nformation and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN a-tamura@ah.jp.nec.com, {taro.watanabe, eiichiro.sumita}@nict.go.jp Abstract This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical"
P14-1138,P11-1042,0,0.0616453,"Rumelhart et al., 1986), which unfolds the network in time (j) and computes gradients over time steps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T ), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as { loss(θ) = max 0, 1 − ∑ + ∑ EΦ [sθ (a|f + , e+ )] (f + ,e+ )∈T } EΦ [sθ (a|f + , e− )] , (11) ("
P14-1138,P08-1112,0,0.0350066,"Missing"
P14-1138,D13-1176,0,0.0436696,"al Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are supervised models. Unfortunately,"
P14-1138,N03-1017,0,0.1479,"Missing"
P14-1138,W04-3250,0,0.0844871,"Missing"
P14-1138,N12-1005,0,0.0140666,"pair (f1J , eI1 ) can be found as a ˆJ1 = argmax p(f1J , aJ1 |eI1 ). aJ 1 (3) For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as 1471 sN N (aJ1 |f1J , eI1 ) = J ∏ ta (aj − aj−1 |c(eaj−1 )) j=1 ·tlex (fj , eaj |c(fj ), c(eaj )), (4) t lex ( fj , ea |f j-1 , eaa -1+1 ) j+1 j Output Layer The computations in the hidden and output layer are as follows2 : j j O× z1 +BO z1 Hidden Layer"
P14-1138,N06-1014,0,0.527699,"the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f . It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads to global optimizat"
P14-1138,C04-1032,0,0.059982,"ch that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f . It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads"
P14-1138,W03-0301,0,0.426192,"Missing"
P14-1138,H05-1011,0,0.0550795,"Missing"
P14-1138,J03-1002,0,0.0908667,"a. All the data in BT EC is word-aligned, and the training data in Hansards is unlabeled data. In F BIS, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (N IST 03 and N IST 04). We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by previously presented model sequence schemes (Och and Ney, 2003): 15 H 5 35 45 , i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (IBM 4). For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer |z1 |to 100, and the window size of contexts to 5. Hence, |z0 |is 300 (30×5×2). Following Yang et al. (2013), the FFNN-based model was trained by the supervised approach described in Section 2.2 (F F N Ns ). For the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer |yj |to 100. Thus, |xj |is 60 ("
P14-1138,P03-1021,0,0.0506179,"Missing"
P14-1138,P02-1040,0,0.0976276,"cause the supervised models are adversely affected by errors in the automatically generated training data. This is especially true when the quality of training data, i.e., the performance of IBM 4, is low. 5.3 Word Alignment Results 5.4 Machine Translation Results Table 2 shows the alignment performance by the F1-measure. Hereafter, M ODEL(R) and M ODEL(I) denote the M ODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ranTable 3 shows the translation performance by the case sensitive BLEU4 metric11 (Papineni et al., 2002). Table 3 presents the average BLEU of three different MERT runs. In N T CIR and F BIS, each alignment model was trained from the ranAlignment IBM 4 F F N Ns (I) RN Ns (I) RN Ns+c (I) RN Nu RN Nu+c F F N Ns (R) RN Ns (R) RN Ns+c (R) Table 2: measure) 7 BT EC 0.4859 0.4770 0.5053+ 0.5174+ 0.5307+ 0.5562+ 0.8224 0.8798+ 0.8921+ Hansards 0.9029 0.9020 0.9068 0.9202+ 0.9037 0.9275+ - Word alignment performance (F1- http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ 8 http://chasen-legacy.sourceforge.jp/ 9 http://nlp.stanford.edu/software/ segmenter.shtml 10 Due to high computational cost, we did not use al"
P14-1138,D13-1140,0,0.0306444,"can be found as a ˆJ1 = argmax p(f1J , aJ1 |eI1 ). aJ 1 (3) For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as 1471 sN N (aJ1 |f1J , eI1 ) = J ∏ ta (aj − aj−1 |c(eaj−1 )) j=1 ·tlex (fj , eaj |c(fj ), c(eaj )), (4) t lex ( fj , ea |f j-1 , eaa -1+1 ) j+1 j Output Layer The computations in the hidden and output layer are as follows2 : j j O× z1 +BO z1 Hidden Layer htanh(H× z0 +BH) z1 = f"
P14-1138,C96-2141,0,0.874258,"nsures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed"
P14-1138,P13-1017,0,0.335221,"ted by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a"
P14-1138,P05-1044,0,0.0543265,"ps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T ), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as { loss(θ) = max 0, 1 − ∑ + ∑ EΦ [sθ (a|f + , e+ )] (f + ,e+ )∈T } EΦ [sθ (a|f + , e− )] , (11) (f + ,e− )∈Ω where Φ is a set of all possible alignments given (f , e), EΦ [sθ ] is the expected value of"
P14-1138,takezawa-etal-2002-toward,1,0.52196,"Missing"
P14-1138,H05-1010,0,0.0207941,"Missing"
P14-1138,P12-1033,0,0.0339331,"Missing"
P14-1138,2007.iwslt-1.1,0,\N,Missing
P14-1138,P07-2045,0,\N,Missing
P14-2026,W13-2239,0,0.0189713,"-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules and Wang et al.’s rules, one can use both dependency and constituency parsers for pre-ordering in Chinese-English PBSMT. This is especially important on the point of the system combination of PBSMT systems, because the diversity of outputs from machine translation systems is important for system combination (Cer et al., 2013). By using both our rules and Wang et al.’s rules, one can obtain diverse machine translation results because the pre-ordering results of these two rule sets are generally different. Another similar work is that of (Xu et al., 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. In contrast, our rule set is for ChineseEnglish PBSMT. That is, the direction of translation is opposite. Because there are a lot of language specific decisions that reflect specific aspects of the source language and the language pair combination, our rule set provi"
P14-2026,C10-1071,0,0.0152835,"procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituen"
P14-2026,W09-2307,0,0.0252693,"parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituent parse tree (i.e. 17). Because dependency parse trees are generally more concise than the constituent ones, they can conduct longdistance reorderings in a finer way. Thus, we attempted to conduct pre-ordering based on dependency parsing. There are two widely-used dependency systems – Stanford typed dependencies and CoNLL typed dependencies. For Chinese, there are 45 types of grammatical relations for Stanford typed dependencies (Chang et al., 2009) and 25 for CoNLL typed dependencies. As we thought that Stanford typed dependencies could describe language phenomena more meticulously owing to more types of grammatical relations, we preferred to use it for searching candidate preordering rules. We designed two types of formats in our dependency-based pre-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules a"
P14-2026,P02-1040,0,0.0985432,",752 547,084 Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the total count of each rule set and the number of sentences they were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency"
P14-2026,P06-1055,0,0.00873215,"were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inferenc"
P14-2026,P12-2003,0,0.0130816,"yed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table"
P14-2026,P05-1066,0,0.155941,"pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of n"
P14-2026,D07-1077,0,0.265653,"approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i"
P14-2026,W12-4207,0,0.229959,"Missing"
P14-2026,I11-1004,0,0.148624,"Missing"
P14-2026,2007.mtsummit-papers.29,0,0.39493,"Missing"
P14-2026,W10-1736,0,0.108857,"Missing"
P14-2026,C04-1073,0,0.257529,"in SMT systems between distant language pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence."
P14-2026,D11-1017,0,0.0265976,"Missing"
P14-2026,N09-1028,0,0.380798,"munications Technology joycetsai99@gmail.com {mutiyama, eiichiro.sumita}@nict.go.jp yjzhang@bjtu.edu.cn Abstract As a kind of constituent structure, HPSG (Pollard and Sag, 1994) parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese (Isozaki et al., 2010; Wu et al., 2011) and Chinese-Japanese (Han et al., 2012). Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007), and EnglishSOV languages (Xu et al., 2009; Katz-Brown et al., 2011). The pre-ordering rules can be made manually (Collins et al., 2005; Wang et al., 2007; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011). The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system. Experiment results showed that our pre-ordering rule set improved the BLEU score on the NIST 2006 evaluation data by 1.61. Moreover, this rule set substantially"
P14-2026,C08-1137,0,0.0190934,"g the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer"
P14-2026,P03-1054,0,0.00382741,"et in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table 1 presents a comparison of the system without pre-ordering, the constituent system using WR07 and two dependency systems employing the converted Berkeley Parser and the Mate Parser, re"
P14-2026,P07-2045,0,0.00682969,"Missing"
P14-2026,W07-0401,0,\N,Missing
P14-2122,J93-2003,0,0.0813025,"h E, formulated as: ′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0"
P14-2122,C10-1092,0,0.0157702,"being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training. We aware that variational bayes (VB) may be used for speeding up the training of DP-based Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine transl"
P14-2122,W08-0336,0,0.0239815,"able to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain. 1 Introduction Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as 1 http://ntcir.nii.ac.jp/PatentMT 752 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–758, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Monolingual and bilingual WS can be formulated as follows, respectively, or PYP-based bilingual UWS. However, VB requires f"
P14-2122,J03-1002,0,0.00915462,"′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0 + (θ + d · nei )G0 ("
P14-2122,D09-1075,0,0.0283316,"r the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it. Thus its complexity is U 2 times the unigram model’s complexity: Omonoling = O(Ni |F|KU 4 ). Type Mono. Mono. Biling. Biling. (17) 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 × 10−6 . For bilingual segmentation,the size of the alignment window is set to 6; the probability λφ of foreign language words being generated by an empty Experiments In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved. 4.1 Experimental Settings 4.1.1 Experimental Corpora Two monolingual corpora and two bilingual corpora are used (Table 2). CHILDES (MacWhinney and Sn"
P14-2122,P03-1021,0,0.0177603,"noling = O(Ni |F|KU 2 ), unigram bigram unigram 4 = O(Ni |F|KU 2 δb ). # Characters 95,809 4,234,824 19,692,605 63,130,757 corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2 . We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources. The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with d"
P14-2122,I05-3017,0,0.194131,"t al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS"
P14-2122,P02-1040,0,0.0934044,"shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is the average number of characters per sentence, and U is the predefined maximum length of words. Fo"
P14-2122,P06-1085,0,0.0420447,") which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy"
P14-2122,P07-2045,0,0.0109645,"or MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is"
P14-2122,P06-1124,0,0.0821838,"Missing"
P14-2122,I05-3027,0,0.0342293,"Missing"
P14-2122,P09-1012,0,0.285732,"d news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilin"
P14-2122,J01-3002,0,0.456396,"Missing"
P14-2122,C08-1128,0,0.047334,"Missing"
P14-2122,I08-1002,0,0.179395,"le to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model"
P14-2122,2008.iwslt-evaluation.1,0,\N,Missing
P15-1113,P05-1022,0,0.426589,"Missing"
P15-1113,A00-2018,0,0.65318,"Missing"
P15-1113,H91-1060,0,0.0470421,"Missing"
P15-1113,D14-1082,0,0.109218,"der large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for transition-based dependency parsing in which limited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the i"
P15-1113,J05-1003,0,0.0182229,"e the best results for each hidden state dimension. Experiments Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 4401151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8 . The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token hunki when looking up their corresponding word representation vector. Similarly, when"
P15-1113,P04-1015,0,0.436603,"nd hidlei, respectively3 . 5 Parameter Estimation  X X Let θ = Hsh , Qsh , · · · ∈ RM be an M dimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j ∗ where the difference of the scores is the largest:   j ∗ j = arg min ρθ (y0 ) − max ρθ (d) . (9) j 10 can be intuitively considered an expected mistake suffered at the maximum violated step j ∗ , which is measured by the Viterbi violation in Equation 9. Note that if we replace EB˜j ∗ [ρθ ] with maxd∈Bj ∗ ρθ (d) in Equation 10, it is exactly the same as the max-violation objective (Huang et al.,"
P15-1113,P97-1003,0,0.561303,"Missing"
P15-1113,J03-4003,0,0.631116,"Missing"
P15-1113,P14-1129,0,0.0927518,"Missing"
P15-1113,P14-1022,0,0.0329376,"that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delaye"
P15-1113,D13-1137,0,0.00902901,"t  ≥ η02 , it is always decayed6 . In our preliminary studies, AdaGrad eventually becomes very conservative to update parameters when training longer iterations. AdaDec fixes the problem by ignoring older histories of sub-gradients in G, which is reflected in the learning rate η. In each update, we employ `1 regularization through FOBOS (Duchi and Singer, 2009) using a hyperparameter λ ≥ 0 to control the fitness in Equation 16 and 17. For testing, we found that taking the average of the pa1 PT rameters over period T +1 t=0 θ t under training iterations T was very effective as demonstrated by Hashimoto et al. (2013). Parameter estimation is performed in parallel by distributing training instances asynchronously Or, setting pθ (d∗ ) = 1 for the Viterbi derivation d∗ = arg maxd∈Bj ∗ ρθ (d) and zero otherwise. 6 Note that AdaGrad is a special case of AdaDec with γ = 1 and  = 0. 1173 5 6 6.1 dev test in each shard and by updating locally copied parameters using the sub-gradients computed from the distributed mini-batches (Dean et al., 2012). The sub-gradients are broadcast asynchronously to other shards to reflect the updates in one shard. Unlike Dean et al. (2012), we do not keep a central storage for mode"
P15-1113,N03-1014,0,0.256072,"ural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for tran"
P15-1113,N07-1051,0,0.152499,"Missing"
P15-1113,P04-1013,0,0.840703,"vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 1990) in backward direction (Henderson, 2004). The stack structure is also modeled similarly to RNNs, and its top item is updated using the previously constructed hidden representations saved in the 1169 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics stack. The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree structure inspired by the recursive neura"
P15-1113,P06-1055,0,0.258141,"Missing"
P15-1113,P82-1020,0,0.859454,"Missing"
P15-1113,N12-1015,0,0.124816,"s for the three actions is 9×m2 +m×m0 +6×m+3 for each non-terminal label X. The scores for a 1172 finish action and an idle action are defined analogous to the unary-X action with special labels for X, hfinishi and hidlei, respectively3 . 5 Parameter Estimation  X X Let θ = Hsh , Qsh , · · · ∈ RM be an M dimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j ∗ where the difference of the scores is the largest:   j ∗ j = arg min ρθ (y0 ) − max ρθ (d) . (9) j 10 can be intuitively considered an expected mistake suffered at the maximum viol"
P15-1113,P03-1054,0,0.0727829,"Missing"
P15-1113,D12-1096,0,0.0716293,"Missing"
P15-1113,D14-1081,0,0.0175063,"., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the"
P15-1113,J93-2004,0,0.0597693,"Missing"
P15-1113,P05-1010,0,0.125817,"Missing"
P15-1113,W05-1513,0,0.196083,"3; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insuffi"
P15-1113,P13-1045,0,0.764865,"mpete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 199"
P15-1113,P14-1138,1,0.767336,"t g t 2 (15) (16) 1 θ t ← arg min kθ − θ t− 1 k22 + λη &gt; t abs(θ). 2 2 θ (17) d∈Bj Then, we define the following hinge-loss function: o Compared with AdaGrad, the squared sum of the n ∗ L(w, y; B, θ) = max 0, 1 − ρθ (y0j ) + EB˜j ∗ [ρθ ] ,sub-gradients decays over time using a constant (10) wherein we consider the subset of sub-derivations ˜j ∗ ⊂ Bj ∗ consisting of those scored higher than B ∗ ρθ (y0j ): n o ˜j ∗ = d ∈ Bj ∗ ρθ (d) &gt; ρθ (y j ∗ ) (11) B 0 exp(ρθ (d)) 0 ˜j ∗ exp(ρθ (d )) d0 ∈B X EB˜j ∗ [ρθ ] = pθ (d)ρθ (d). pθ (d) = P (12) (13) ˜j ∗ d∈B Unlike Huang et al. (2012) and inspired by Tamura et al. (2014), we consider all incorrect sub˜j ∗ through the expected derivations found in B 4 score EB˜j ∗ [ρθ ] . The loss function in Equation 3 Since h1j and qn are constants for the finish and idle acX tions, we enforce Hun = 0 and QX un = 0 for those special actions. 4 We can use all the sub-derivations in Bj ∗ ; however, our ˜j ∗ was better. preliminary studies indicated that the use of B 0 &lt; γ ≤ 1 in Equation 14. The learning rate in Equation 15 is computed element-wise and bounded by a constant  ≥ 0, and if we set  ≥ η02 , it is always decayed6 . In our preliminary studies, AdaGrad eventually be"
P15-1113,N03-1033,0,0.115222,"ments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 4401151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8 . The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token hunki when looking up their corresponding word representation vector. Similarly, when assigning possible POS tags in shift actions, they fell back to their corresponding “word si"
P15-1113,P10-1040,0,0.0741815,"Missing"
P15-1113,P14-1069,0,0.437019,"in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased pa"
P15-1113,W09-3825,0,0.834691,"ategory splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various"
P15-1113,P13-1043,0,0.68089,"manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relation"
P16-1120,N09-1042,0,0.0568986,"Missing"
P16-1120,D15-1131,0,0.0318862,"espond to the English articles used in the experiments in Section 5, from the French Wikipedia database dump (2 June 2015) based on inter-language links. As a result, our English–French corpus comprises 3,159 document pairs. The French articles were preprocessed in the same manner as the English articles: text extraction using the open-source script, segmentation using TreeTagger, removal of function words, and lemmatization. We created a gold-standard translation set for translation extraction experiments using Google Translate service11 in a manner similar to that in Gouws et al. (2015) and Coulmance et al. (2015), translating the French words in our corpus using Google Translate, and then eliminating word pairs that do not appear in the document pairs in our corpus. We used the top 1,000 most frequent French words in the resulting gold-standard set as the evaluation input. Table 6 summarizes ACC1 , ACC10 , and perplexity. It shows that the proposed models are effective also for the English–French Wikipedia corpus. BiSTM and BiSTM+TS outperform BiLDA in terms of perplexity and performance of translation extraction, and BiSTM+TS works well even if the boundaries of segments are unknown. 7 Related Work M"
P16-1120,D12-1049,0,0.0408074,"Missing"
P16-1120,N13-1019,0,0.148457,"ls documents by treating each document as a set of segments, e.g., sections. While previous bilingual topic models, such as bilingual latent Dirichlet allocation (BiLDA) (Mimno et al., 2009; Ni et al., 2009), consider only cross-lingual alignments between entire documents, the proposed model considers cross-lingual alignments between segments in addition to document-level alignments and assigns the same topic distribution to aligned segments. This study also presents a method for simultaneously inferring latent topics and segmentation boundaries, incorporating unsupervised topic segmentation (Du et al., 2013) into BiSTM. Experimental results show that the proposed model significantly outperforms BiLDA in terms of perplexity and demonstrates improved performance in translation pair extraction (up to +0.083 extraction accuracy). 1 㻭㼟㼟㼛㼏㼕㼍㼠㼕㼛㼚㻌㼒㼛㼛㼠㼎㼍㼘㼘 䝃䝑䜹䞊 (football) ƐƐŽĐŝĂƚŝŽŶĨŽŽƚďĂůůŵŽƌĞ ĐŽŵŵŽŶůǇŬŶŽǁŶĂƐĨŽŽƚďĂůů ϭ,ŝƐƚŽƌǇ ŽƌƐŽĐĐ Ğƌ ŝƐĂƐƉŽƌƚƉůĂǇĞĚ ϮƚǇŵŽůŽŐǇĂŶĚŶĂŵĞƐ ďĞƚǁǁŽ ŵƐ ŽĨĞůĞǀĞŶƉůĂǇĞƌƐ ϯ&apos;ĂŵĞƉůĂǇ ǁŝƚŚƐƉ ŚĞƌŝĐĂů ďĂůůƚ ƉůĂǇĞĚďǇϮŶƉůĂǇĞƌƐŝŶ ϰ&gt;ĂǁƐ ŽǀĞƌϭϱϬĐŽƵŶƚƌŝĞƐ ϱ&apos;ŽǀĞƌŶŝŶŐďŽĚŝĞƐ ŵĂŬŝ ŶŐŝƚƚŚĞǁŽƌůĚƐ 䝃䝑䜹䞊䠄ⱥ: soccer䠅䛿䚸⌫ ᙧ䛾䝪䞊 䝹䜢"
P16-1120,P14-1110,0,0.0198039,"2) have been proposed for document-aligned comparable corpora. Fukumasu et al. (2012) applied SwitchLDA (Newman et al., 2006) and Correspondence LDA (Blei and Jordan, 2003), which 1273 11 http://translate.google.com/ were originally intended to work with multimodal data, such as annotated image data, to modeling multilingual text data. They also proposed a symmetric version of Correspondence LDA. Platt et al. (2010) projected monolingual models based on PLSA or Principal Component Analysis into a shared multilingual space with the constraint that document pairs must map to similar locations. Hu et al. (2014) proposed a multilingual tree-based topic model that uses a hierarchical bilingual dictionary in addition to document alignments. Note that these models do not consider segment-level alignments. There are several multilingual topic models tailored for data other than a document-aligned comparable corpus, including bilingual topic models for word alignment and machine translation on parallel sentence pairs (Zhao and Xing, 2006; Zhao and Xing, 2008). Some models have mined multilingual topics from unaligned text data by bridging the gap between different languages using a bilingual dictionary (J"
P16-1120,W13-3523,0,0.2748,"nsion of the topic segmentation, i.e., incorporation of y, for future work. 5 http://alaginrc.nict.go.jp/ WikiCorpus/index_E.html 6 We filtered out the Japanese articles that do not have corresponding English articles. 7 http://dumps.wikimedia.org/enwiki/ 8 https://github.com/attardi/ wikiextractor/ English texts were segmented using MeCab9 and TreeTagger10 (Schmid, 1994), respectively. Then, function words were removed, and the remaining words were lemmatized to reduce data sparsity. For translation extraction experiments, we automatically created a gold-standard translation set according to Liu et al. (2013). We first computed p(we |wf ) and p(wf |we ) by running IBM Model 4 on the original Kyoto Wiki corpus, which is a parallel corpus, using GIZA++ (Och and Ney, 2003), and then extracted word pairs (wˆe , wˆf ) that satisfy both of the following conditions: wˆe = argmaxwe p(we |wf = wˆf ) and wˆf = argmaxwf p(wf |we = wˆe ). Finally, we eliminated word pairs that do not appear in the document pairs in the document-aligned comparable corpus. We used all 7,930 Japanese words in the resulting gold-standard set as the evaluation input. 5.1 Competing Methods We compared the proposed models (BiSTM and"
P16-1120,D09-1092,0,0.0306595,"Missing"
P16-1120,I11-1111,0,0.0133622,"uses a hierarchical bilingual dictionary in addition to document alignments. Note that these models do not consider segment-level alignments. There are several multilingual topic models tailored for data other than a document-aligned comparable corpus, including bilingual topic models for word alignment and machine translation on parallel sentence pairs (Zhao and Xing, 2006; Zhao and Xing, 2008). Some models have mined multilingual topics from unaligned text data by bridging the gap between different languages using a bilingual dictionary (Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Negi, 2011). Boyd-Graber and Blei (2009) used parallel sentences in combination with a bilingual dictionary. However, these models have the drawback that they require a parallel corpus or a bilingual dictionary in advance, which cannot be obtained for some language pairs or domains. In a monolingual setting, some topic models that consider segment-level topics have been proposed. Du et al. (2010) considered a document as a set of segments and generated each per-segment topic distribution from the topic distribution of the related document through a Pitman–Yor process. Others have considered a document as"
P16-1120,J03-1002,0,0.00772549,"ticles that do not have corresponding English articles. 7 http://dumps.wikimedia.org/enwiki/ 8 https://github.com/attardi/ wikiextractor/ English texts were segmented using MeCab9 and TreeTagger10 (Schmid, 1994), respectively. Then, function words were removed, and the remaining words were lemmatized to reduce data sparsity. For translation extraction experiments, we automatically created a gold-standard translation set according to Liu et al. (2013). We first computed p(we |wf ) and p(wf |we ) by running IBM Model 4 on the original Kyoto Wiki corpus, which is a parallel corpus, using GIZA++ (Och and Ney, 2003), and then extracted word pairs (wˆe , wˆf ) that satisfy both of the following conditions: wˆe = argmaxwe p(we |wf = wˆf ) and wˆf = argmaxwf p(wf |we = wˆe ). Finally, we eliminated word pairs that do not appear in the document pairs in the document-aligned comparable corpus. We used all 7,930 Japanese words in the resulting gold-standard set as the evaluation input. 5.1 Competing Methods We compared the proposed models (BiSTM and BiSTM+TS) with a standard bilingual topic model (BiLDA). BiSTM considers each section in Wikipedia articles as a segment. Note that alignments between sections are"
P16-1120,P11-2084,0,0.0520056,"Missing"
P16-1120,P11-1153,0,0.0242332,"a parallel corpus or a bilingual dictionary in advance, which cannot be obtained for some language pairs or domains. In a monolingual setting, some topic models that consider segment-level topics have been proposed. Du et al. (2010) considered a document as a set of segments and generated each per-segment topic distribution from the topic distribution of the related document through a Pitman–Yor process. Others have considered a document as a sequence of segments. Cheng et al. (2009) reflected the underlying sequences of segments’ topics by positing a permutation distribution over a document. Wang et al. (2011) modeled topical sequences in documents with a latent first-order Markov chain, and Du et al. (2012) generated each per-segment topic distribution from the topic distribution of its document and that of its previous segment. Note that none of these models have been extended to a multilingual setting. 8 Conclusions In this paper, we proposed BiSTM, which models a document hierarchically and deals with segmentlevel alignments. BiSTM assigns the same topic distribution to both aligned documents and aligned segments. We also presented an extended model, BiSTM+TS, that infers segmentation boundarie"
P16-1120,P10-1115,0,0.0222041,"ed topic model that uses a hierarchical bilingual dictionary in addition to document alignments. Note that these models do not consider segment-level alignments. There are several multilingual topic models tailored for data other than a document-aligned comparable corpus, including bilingual topic models for word alignment and machine translation on parallel sentence pairs (Zhao and Xing, 2006; Zhao and Xing, 2008). Some models have mined multilingual topics from unaligned text data by bridging the gap between different languages using a bilingual dictionary (Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Negi, 2011). Boyd-Graber and Blei (2009) used parallel sentences in combination with a bilingual dictionary. However, these models have the drawback that they require a parallel corpus or a bilingual dictionary in advance, which cannot be obtained for some language pairs or domains. In a monolingual setting, some topic models that consider segment-level topics have been proposed. Du et al. (2010) considered a document as a set of segments and generated each per-segment topic distribution from the topic distribution of the related document through a Pitman–Yor process. Others have considered"
P16-1120,P06-2124,0,0.0116405,"cted monolingual models based on PLSA or Principal Component Analysis into a shared multilingual space with the constraint that document pairs must map to similar locations. Hu et al. (2014) proposed a multilingual tree-based topic model that uses a hierarchical bilingual dictionary in addition to document alignments. Note that these models do not consider segment-level alignments. There are several multilingual topic models tailored for data other than a document-aligned comparable corpus, including bilingual topic models for word alignment and machine translation on parallel sentence pairs (Zhao and Xing, 2006; Zhao and Xing, 2008). Some models have mined multilingual topics from unaligned text data by bridging the gap between different languages using a bilingual dictionary (Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Negi, 2011). Boyd-Graber and Blei (2009) used parallel sentences in combination with a bilingual dictionary. However, these models have the drawback that they require a parallel corpus or a bilingual dictionary in advance, which cannot be obtained for some language pairs or domains. In a monolingual setting, some topic models that consider segment-level topics have been pro"
P16-1120,D10-1025,0,\N,Missing
P17-2089,D11-1033,0,0.776056,"2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains a"
P17-2089,2014.iwslt-evaluation.1,0,0.0320202,"Missing"
P17-2089,P15-1001,0,0.0256648,"er, we exploit the NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver,"
P17-2089,2016.amta-researchers.8,0,0.537509,"hod can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only foc"
P17-2089,D15-1147,0,0.0659529,"Missing"
P17-2089,P07-2045,0,0.0260817,"Missing"
P17-2089,P13-2119,0,0.356115,"e training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able"
P17-2089,2015.mtsummit-papers.10,0,0.319092,"a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and"
P17-2089,2015.iwslt-evaluation.11,0,0.719606,"Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelr"
P17-2089,P16-2021,0,0.0875788,"bedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017."
P17-2089,D14-1062,0,0.0613509,"Missing"
P17-2089,P10-2041,0,0.40117,"od (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the sam"
P17-2089,P02-1040,0,0.117011,"Missing"
P17-2089,E12-1055,0,0.0294002,"ma,eiichiro.sumita}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained"
P17-2089,P13-1082,0,0.0794448,"ta}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and"
P17-2089,P16-1008,0,0.0219846,"NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30"
P17-2089,C16-1295,1,0.865382,"ms apply a similar sentence representation. In comparison, we adopt a transition layer between the source and target layers and don’t use test data. 2 It is possible to use a sample of the out-of-domain data. In this paper, we use all of them. 561 vector center CFin as d(vf , CFin ) and out-ofdomain vector center CFout as d(vf , CFout ), respectively. We use the difference δ of these two distances to classify each sentence: δf = d(vf , CFin ) − d(vf , CFout ). blog texts. The statistics on data sets were shown in Table 1. These adaptation corpora settings were nearly the same as that used in (Wang et al., 2016). The differences were: (5) By using an English-to-French NMT system NEF , we can obtain a target sentence embedding ve , in-domain target vector center CEin and out-of-domain target vector center CEout . Corresponding distance difference δe is, δe = d(ve , CEin ) − d(ve , CEout ). • For IWSLT, they chose FR-EN translation task, which is popular in PBSMT. We chose EN-FR, which is more popular in NMT; • For NIST, they chose 02-05 as dev set, and we chose 02-04. Because we would report results on two test sets (MT05 and MT06) in comparison with only one (MT06). (6) δf , δe and δf e = δf + δe can"
P17-2089,P14-2122,1,0.851211,", the maximum sequence length were 50, and the beam size for decoding was 10. Default dropout were applied. We used a mini-batch Stochastic Gradient Descent (SGD) algorithm together with ADADELTA optimizer (Zeiler, 2012). Training was conducted on a single Tesla K80 GPU. Each NMT model was trained for 500K batches, taking 7-10 days. For sentence embedding and selection, it only took several hours to process all of sentences in the training data, because decoding was not necessary. • NIST 2006 Chinese (ZH) to English corpus5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014). Chinese-to-English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data set were used as out-ofdomain data. NIST MT 2002-2004 and NIST MT 2005/2006 were used as the development and test data, respectively. We are aware of that there are additional NIST corpora in a similar domain, but because this task was for domain adaptation, we only selected a small subset, which is mainly focused on news and 3 https://wit3.fbk.eu/mt.php?release=2014-01 http://statmt.org/wmt15/translation-task.html 5 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 4 6 https://github.com/lisa-groundhog/ Gro"
P17-2089,D16-1050,0,0.0127489,"urce sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P18-1116,P17-2021,0,0.394426,"forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-ba"
P18-1116,P05-1022,0,0.250252,"Missing"
P18-1116,P17-1177,0,0.21255,"e used beam search, and fixed the beam size to 12. For the case of Forest (SoA), with 1 core of Tesla K80 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed is about 10 sentences per second. 5 https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 7 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06 8 https://github.com/EdinburghNLP/ nematus Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) MT 03 FBIS LDC 27.10 28.21 29.00 29.71 28.34 29.64 28.40 29.60 27.44 29.18 28.61 29.38 28.78 30.65 29.39 30.80 28.06 29.63 29.58 31.07 29.63 31.35 MT 04 FBIS LDC 28.67 30.09 30.24 31.56 30.00 31.25 29.66 31.96 29.73 30.53 30.07 31.58 30.36 32.22 30.25 32.39 29.51 31.41 30.67 32.69 30.31 33.14 MT 05 FBIS LDC 26.57 28.36 28.38 30.33 28.14 29.59 27.74 29.84 27.32 28.80 28.59 30.01 29.31 30.16 29.30 30.61 28.48 29.75 29.26 30.41 29.87 31.23 p value < 0.01 < 0.05 < 0.005 < 0.01 < 0.001 < 0."
P18-1116,N16-1024,0,0.362868,"more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest"
P18-1116,P16-1078,0,0.358869,"f approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on ho"
P18-1116,P17-2012,0,0.0416471,"ked forest with a forest-structured neural network. However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences). In contrast, our proposed method is effective in a largescale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT. Although these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, therefore restricts the scope of application. Other attempts at encoding syntactic trees have also been proposed. Eriguchi et al. (2017) combined the Recurrent Neural Network Grammar (Dyer et al., 2016) with NMT systems, while Li et al. (2017) linearized the constituent tree and encoded it using RNNs. The training of these methods is fast, because of the linear structures of RNNs. However, all these syntax-based NMT systems used only the 1-best parsing tree, making the systems sensitive to parsing errors. Instead of using trees to represent syntactic information, some studies use other data structures to represent the latent syntax of the input sentence. For example, Hashimoto and Tsuruoka (2017) proposed translating using a l"
P18-1116,D17-1012,0,0.034678,"Missing"
P18-1116,P08-1067,0,0.492245,"relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT a"
P18-1116,W04-3250,0,0.0752847,"29.63 31.35 MT 04 FBIS LDC 28.67 30.09 30.24 31.56 30.00 31.25 29.66 31.96 29.73 30.53 30.07 31.58 30.36 32.22 30.25 32.39 29.51 31.41 30.67 32.69 30.31 33.14 MT 05 FBIS LDC 26.57 28.36 28.38 30.33 28.14 29.59 27.74 29.84 27.32 28.80 28.59 30.01 29.31 30.16 29.30 30.61 28.48 29.75 29.26 30.41 29.87 31.23 p value < 0.01 < 0.05 < 0.005 < 0.01 < 0.001 < 0.001 Table 2: English-Chinese experimental results (character-level BLEU). “FS,” “TN,” and “FN” denote forest-based SMT, tree-based NMT, and forest-based NMT systems, respectively. We performed the paired bootstrap resampling significance test (Koehn, 2004) over the NIST MT 03 to 05 corpus, with respect to the s2s baseline, and list the p values in the table. Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) BLEU (test) 34.13 37.52 36.94 36.21 37.10 38.01 38.53 39.42 37.92 41.35 42.17 p value < 0.05 < 0.01 < 0.001 < 0.1 < 0.01 < 0.005 Table 3: English-Japanese experimental results (character-level BLEU). 4.2 Experimental results Table 2 and 3 summarize the experimental results. To avoid the"
P18-1116,P17-1064,0,0.139505,"reebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compare"
P18-1116,J93-2004,0,0.0609437,"endency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993). Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT. It can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation. 2.3 Packed forest The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list S [11] [9] S0,5 NP 5.8665 VP NNP"
P18-1116,D08-1022,0,0.0348446,"within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en1 Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a fores"
P18-1116,P08-1023,0,0.164271,"el can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en1 Zaremoodi and Haffari (2017) have proposed a forestbased NMT me"
P18-1116,P02-1040,0,0.100987,"es in the table. Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) BLEU (test) 34.13 37.52 36.94 36.21 37.10 38.01 38.53 39.42 37.92 41.35 42.17 p value < 0.05 < 0.01 < 0.001 < 0.1 < 0.01 < 0.005 Table 3: English-Japanese experimental results (character-level BLEU). 4.2 Experimental results Table 2 and 3 summarize the experimental results. To avoid the affect of segmentation errors, the performance were evaluated by character-level BLEU (Papineni et al., 2002). We compare our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to-string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree-based models that use the 1-best parsing tree (1-best (No score, SoE, SoA)). For the 1-best models, we preserve the nodes and hyperedges that are used in the 1-best constituent tree in the packed forest, and remove all other nodes and hyperedges, yielding a pruned forest that contains only the 1-best constituent tree. For the “No score” configurations, we force the input score seque"
P18-1116,W06-1608,0,0.0520192,"ization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspi"
P18-1116,E17-3017,0,0.054061,"Missing"
P18-1116,W16-2209,0,0.0382625,"n states of the source-side RNN and target-side RNN, respectively, c is the context vector, and et is the embedding of xt . Bahdanau et al. (2014) introduced an attention mechanism to deal with the issues related to long sequences (Cho et al., 2014). Instead of encoding the source sequence into a fixed vector c, the attention model uses different ci -s when calculating ci = T X αij hj , (6) j=0 exp(a(si−1 , hj )) αij = PT . k=0 exp(a(si−1 , hk )) 2.2 (7) Linear-structured tree-based NMT systems Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated"
P18-1116,P15-1150,0,0.0449153,"Missing"
P18-1116,P17-1065,0,0.0199032,"sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993). Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT. It can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation. 2.3 Packed forest The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list S [11] [9] S0,5 NP 5.8665 VP NNP VBZ John has VP1,"
P18-1116,P17-1139,0,0.026196,"itnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model stru"
P18-2048,2015.iwslt-evaluation.11,0,0.0402112,"r to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,D15-1166,0,0.0664039,"erate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,P02-1040,0,0.100028,"Missing"
P18-2048,K16-1029,0,0.0408088,"Missing"
P18-2048,E17-3017,0,0.0769889,"Missing"
P18-2048,P17-2089,1,0.867888,"Missing"
P18-2048,kocmi-bojar-2017-curriculum,0,0.279027,"n, Kyoto, Japan {wangrui, mutiyama, eiichiro.sumita}@nict.go.jp Abstract in the data or in the model (Wu et al., 2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and"
P18-2048,W04-3250,0,0.397163,"Missing"
P18-2048,D17-1155,1,0.83141,"2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training"
P18-2048,P07-2045,0,0.0140916,"Missing"
P18-2048,I17-2046,0,0.302474,"on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training cost as a dynamic criterion to gradually fine-tune the NMT training. This approach was developed based on the idea that the training cost is a useful measure to determine the translation quality of a sentence. However, some of the sentences that can be potentially improved by training may be deleted using this method. In addition, all of the above works primarily focused on NMT translation performance, instead of training efficiency. In this study, we propose a method of dynamic sentence sampling (DSS) to improve the NMT training efficiency. First, the diff"
P18-2048,W17-4717,0,\N,Missing
P18-2078,W11-3502,0,0.0761458,"Missing"
P18-2078,P00-1031,0,0.124337,"anguage-specific optimization in a future work, via both data- and user-driven studies. The simplification scheme is shown in Fig. 3.1 Generally, the merges are based on the common distribution of consonant phonemes in most natural languages, as well as the etymology of the characters in each abugida. Specifically, three or four Related Work Some optimized keyboard layout have been proposed for specific abugidas (Ouk et al., 2008). Most studies on input methods have focused on Chinese and Japanese characters, where thousands of symbols need to be encoded and recovered. For Chinese characters, Chen and Lee (2000) made an early attempt to apply statistical methods to sentence-level processing, using a hidden Markov model. Others have examined max-entropy models, support vector machines (SVMs), conditional 1 Each script also includes native punctuation marks, digit notes, and standalone vowel characters that are not represented by diacritics. These characters were kept in the experimental texts but not evaluated in the final results, as the usage of these characters is trivial. In addition, white spaces, Latin letters, Arabic digits, and non-native punctuation marks were normalized into placeholders in"
P19-1119,D16-1250,0,0.0277561,"ods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findi"
P19-1119,P17-1042,0,0.0444313,"The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been applied to UNMT (Artetxe et al., 2018c; Lample et al., 2018a). These rely solely on monolingual corpora in each language via UBWE initialization, denoising auto-encoder, and back-translation. A shared encode"
P19-1119,P18-1073,0,0.117216,"BWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement regularization and UBWE adversarial training, to maintain the quality of UBWE during NMT"
P19-1119,D18-1399,0,0.0466684,"Missing"
P19-1119,P19-1019,0,0.0982651,"Missing"
P19-1119,J82-2005,0,0.756838,"Missing"
P19-1119,Q17-1010,0,0.039396,"UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language independently with fastText3 (Bojanowski et al., 2017) (default settings). The word embeddings were normalized by length and mean centered before bilingual projection. We then used VecMap4 (Artetxe et al., 2018a) (default settings) to project two monolingual word embeddings into one space. To evaluate the quality of UBWE, we selected the accuracy of word translation using the top-1 predicted candidate in the MUSE test set as the criterion. 5.3 UNMT Settings In the training process for UNMT, we used the transformer-based UNMT toolkit5 and the settings of Lample et al. (2018b). That is, we used four 3 https://github.com/facebookresearch/ fastText 4"
P19-1119,C16-1171,1,0.803699,"orm conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approache"
P19-1119,D17-1304,1,0.782183,"Missing"
P19-1119,C18-1271,0,0.0472765,"seline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 20"
P19-1119,E14-1049,0,0.0395704,"erformance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng S"
P19-1119,P18-1192,0,0.062598,"adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been appl"
P19-1119,N16-1162,0,0.097674,"Missing"
P19-1119,N15-1028,0,0.0983804,"Missing"
P19-1119,W18-6419,1,0.821417,"ur method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods can mitigat"
P19-1119,P16-1009,0,0.124354,"et al., 2016). The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, is optimized by minimizing the Lauto = EX∼φL1 [−logPL1 →L1 (X|C(X)] + EY ∼φL2 [−logPL2 →L2 (Y |C(Y )], (2) where C(X) and C(Y ) are noisy versions of sentences X and Y , PL1 →L1 (PL2 →L2 ) denotes the reconstruction probability in the language L1 (L2 ). 2.3 Back-translation The denoising auto-encoder acts as a language model that has been trained in one language and does not consider the final goal of translating between two languages. Therefore, backtranslation (Sennrich et al., 2016) was adapted to train translation systems in a true translation setting based on monolingual corpora. Formally, given the sentences X and Y , the sentences YP (X) and XP (Y ) would be produced by the model at the previous iteration. The pseudo-parallel sentence pair (YP (X), X) and (XP (Y ), Y ) would be obtained to train the new translation model. Finally, the back-translation process is optimized by minimizing the following objective function: Lbt = EX∼φL1 [−logPL2 →L1 (X|YP (X)] + EY ∼φL2 [−logPL1 →L2 (Y |XP (Y )], (3) where PL1 →L2 (PL2 →L1 ) denotes the translation probability across two"
P19-1119,P18-1072,0,0.043009,"Missing"
P19-1119,P17-1179,0,0.327433,"NMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement reg"
P19-1119,P16-1131,0,0.0253662,"eed of the model. Baseline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods ("
P19-1119,C18-1269,0,0.0524389,"Missing"
P19-1119,D14-1023,1,0.81634,"s initialization process for UBWE in our method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs sh"
P19-1119,N15-1104,0,0.0375279,"gnificantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internshi"
P19-1119,P18-1005,0,0.321075,"racy Base-Fr enc-En dec AR-Fr-En AR-Fr enc-En dec AT-Fr-En UBWEAT-Fr accuracy enc-En dec AR-Fr-En BLEU 20 40 60 80 100 120 140 Epoch Base-Ja-En UBWE accuracy UBWE accuracy Base-Ja enc-En dec AR-Ja-En AR-Ja enc-En dec Base-Fr-En BLEU AT-Ja-En UBWE AT-Ja accuracy enc-En dec AT-Fr-En BLEU AR-Ja-En BLEU (a) Fr-En Base-Ja-En BLEU AT-Ja-En BLEU (b) Ja-En Figure 4: The trends of UBWE quality and BLEU score for baseline (Base), UBWE agreement regularization (AR), and UBWE adversarial training (AT) during UNMT training on the Fr-En and Ja-En dataset Methods Artetxe et al. (2018c) Lample et al. (2018a) Yang et al. (2018) Lample et al. (2018b) UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language indepen"
P19-1174,J93-2003,0,0.124809,"lel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transfor"
P19-1174,P17-1177,0,0.0856638,"utively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural netwo"
P19-1174,D17-1304,1,0.836837,"Missing"
P19-1174,P05-1066,0,0.763417,"are sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address t"
P19-1174,P13-2071,0,0.149567,"Missing"
P19-1174,C14-1041,0,0.0608164,"Missing"
P19-1174,P11-1105,0,0.0991813,"Missing"
P19-1174,P16-1078,0,0.0203508,"g RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the p"
P19-1174,2010.amta-papers.22,0,0.0465578,"illman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reorde"
P19-1174,N13-1003,0,0.0494421,"Missing"
P19-1174,P13-1032,0,0.0308283,"Missing"
P19-1174,P05-1033,0,0.216411,"The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional p"
P19-1174,D08-1089,0,0.301049,"into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-toGerman, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system. 1 Introduction The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words"
P19-1174,P17-1012,0,0.0293336,"ration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. T"
P19-1174,N10-1129,0,0.03344,"Missing"
P19-1174,D15-1165,0,0.0408643,"Missing"
P19-1174,P18-1192,0,0.0268229,"reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward"
P19-1174,P17-4012,0,0.10494,"Missing"
P19-1174,N03-1017,0,0.379592,"ce pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ord"
P19-1174,P17-1064,0,0.0207119,"roving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the positional embeddi"
P19-1174,D13-1054,0,0.0422787,"Missing"
P19-1174,C14-1179,0,0.0358165,"Missing"
P19-1174,C18-1271,0,0.0358463,"echanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT"
P19-1174,P18-3004,0,0.15555,"Missing"
P19-1174,W18-6419,1,0.792532,"anslation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. Thus, the reordering problem in NMT has not been studied extensively, especially in Transformer. In this paper, we propose a reordering mechanism for the Transformer translation system. We dynamically penalize the given positional embedding of a word depending on its contextual information, thus generating a reordering embedding for each word. The reorderi"
P19-1174,D16-1096,0,0.0297414,"Missing"
P19-1174,P06-1090,0,0.185574,"ation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly"
P19-1174,W18-6319,0,0.0149356,"ng was set to 0.1, and the attention dropout and residual dropout were p = 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warmup steps of 8,000. For evaluation, we validated the model with an interval of 1,000 batches on the dev set. Following the training of 200,000 batches, the model with the highest BLEU score of the dev set was selected to evaluate on the test sets. During the decoding, the beam size was set to four. All models were trained and evaluated on a single P100 GPU. SacreBELU (Post, 2018) was used as the evaluation metric of EN-DE, and the multi-bleu.perl1 was used the evaluation metric of ZH-EN and JA-EN tasks. The signtest (Collins et al., 2005) was as statistical significance test. We re-implemented all methods (“this work” in the tables) on the OpenNMT toolkit (Klein et al., 1 https://github.com/mosessmt/mosesdecoder/tree/RELEASE-4.0/scripts/generic/multibleu.perl 2017). 6.4 Main Results To validate the effectiveness of our methods, the proposed models were first evaluated on the WMT14 EN-DE translation task as in the original Transformer translation system (Vaswani et al."
P19-1174,P16-1162,0,0.0678125,"ative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair encoding algorithm (Sennrich et al., 2016) was adopted and the size of the vocabulary was set to 32,000. The number of dimensions of all input and output 1791 System Wu et al. (2016) Gehring et al. (2017b) Vaswani et al. (2017) Vaswani et al. (2017) this work Architecture newstest2014 Existing NMT systems GNMT 26.3 CONVS2S 26.36 Transformer (base) 27.3 Transformer (big) 28.4 Our NMT systems Transformer (base) 27.24 +Additional PEs 27.10 +Relative PEs 27.63 +Encoder REs 28.03++ +Decoder REs 27.61+ +Both REs 28.22++ Transformer (big) 28.34 +Both REs 29.11++ #Speed1 #Speed2 #Params N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 65.0M 213.0M 991"
P19-1174,N18-2074,0,0.0292439,"T07, LDC2004T08, and LDC2005T06. The MT06 and the MT02/MT03/MT04/MT05/MT08 datasets were used as the dev set and test set, respectively. 3) For the JA-EN translation task, the training dataset consisted of two million bilingual sentence pairs from the ASPEC corpus (Nakazawa et al., 2016). The dev set consisted of 1,790 sentence pairs and the test set of 1,812 sentence pairs. 6.2 Baseline Systems These baseline systems included: Transformer: a vanilla Transformer with absolute positional embedding (Vaswani et al., 2017), for example Transformer (base) and Transformer (big) models. Relative PE (Shaw et al., 2018): incorporates relative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair en"
P19-1174,N04-4026,0,0.113282,"n reordering-aware sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al.,"
P19-1174,P16-1008,0,0.0276056,"networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sources"
P19-1174,P17-2089,1,0.579914,"Missing"
P19-1174,D17-1155,1,0.862726,"f reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpfu"
P19-1174,P18-2048,1,0.899817,"Missing"
P19-1174,P96-1021,0,0.289443,"eordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) orig"
P19-1174,J97-3002,0,0.637414,"Missing"
P19-1174,P06-1066,0,0.0442704,"model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attent"
P19-1174,W06-3108,0,0.0419669,"nslation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reorde"
P19-1174,P17-1140,0,0.233489,"ding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly enc"
P19-1174,D18-1511,1,0.794361,". (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving tra"
P19-1174,P16-1131,0,0.0292113,"posed a reordering mechanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Sc"
P19-1174,D18-1036,0,0.0254378,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1174,L18-1143,0,0.0642441,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1296,D17-1304,1,0.807793,"Missing"
P19-1296,I17-1002,1,0.860783,"Missing"
P19-1296,P18-1192,0,0.0281199,"mize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when co"
P19-1296,P18-1164,0,0.250005,"conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when conducting this work. Based on this hypothesis, Kuang et al. (2018) proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation. Tu et al. (2017) incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation. However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships (Zhang and Zhao, 2019). Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT,"
P19-1296,C18-1271,0,0.0150768,"ose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming"
P19-1296,D15-1166,0,0.122472,"Missing"
P19-1296,P02-1040,0,0.103188,"Missing"
P19-1296,W18-6319,0,0.0337367,"Missing"
P19-1296,W16-0533,0,0.0307462,"Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and"
P19-1296,W16-2323,0,0.0605544,"Missing"
P19-1296,P16-1162,0,0.158734,"Missing"
P19-1296,P16-1131,0,0.017386,"this paper, we propose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target w"
P19-1296,P16-1008,0,0.0212731,"nction in between: FFN(x) = max(0, xW1 + b1 )W2 + b2 , (2) where W1 and W2 are both linear transformation networks, b1 and b2 are both bias. We define Henc as the sentence representation of X via the self-attention layers in encoder, and Hdec as the sentence representation of words Y via embedding layers in decoder. The parameters of Transformer are trained to minimize the following objective function on a set of training examples {(X n , Y n )}N n=1 : Lmle = − 3 N Iy 1 XX n logP (yin |y<i , Henc , Hdec ). N n=1 i=1 (3) Agreement on Source and Target Sentence Some studies (Luong et al., 2015; Tu et al., 2016; Chen et al., 2017a,b; Kuang et al., 2018) showed that improving word alignment is beneficial to machine translation. Their idea is based on word-level agreement and make the embeddings of source words and corresponding target words similar. In this paper, we investigate the sentence-level relationship between the source and target sentences. We propose a sentence-level agreement method which can make the sentencelevel semantics of the source and target closer. The entire architecture of the proposed method is illustrated in Figure 1. 3.1 Sentence-Level Agreement First, we need to get the sen"
P19-1296,P17-2089,1,0.919928,"Missing"
P19-1296,C18-1269,0,0.0400623,"Missing"
P19-1296,D17-1155,1,0.822564,"ntion is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and the sentences in the 3076 Proceedings of the 57th Annual Meeting of the Association for Computational L"
P91-1024,C88-2091,0,0.0275049,"Missing"
P91-1024,C86-1023,0,0.0162921,"example (2), we finally get &quot;The chief is sharp&quot;. Although it is possible to obtain the same result by a word selection rule using fme-tuned semantic restriction, note that translation here is obtained by retrieving similar examples to the input. When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT. (Ca) Translation rule formation is difficult. (Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &quot;- de&quot;, &quot;~ hi&quot;,...) • subordinate conjunction (&quot;- ba -&quot;, &quot;~ nagara -&quot;, &quot;~ tara -&quot;,...,&quot;- baai ~&quot;,...) • noun phrases of the form &apos;~1 no N2&quot; • sentences of the form &quot;N~ wa N2 da&quot; • sentences lacking the main verb (eg. sentences of the form &quot;~ o-negaishimasu&quot;) • fragmental expressions Chai&quot;, &quot;sou-desu&quot;, &quot;wakarimashita&quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending C-tainodesuga&quot;, &quot;~seteitadakimasu&quot;, ...) (Furuse et al. 1990"
P91-1024,1990.tc-1.1,0,0.0281192,"a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &quot;- de&quot;, &quot;~ hi&quot;,...) • subordinate conjunction (&quot;- ba -&quot;, &quot;~ nagara -&quot;, &quot;~ tara -&quot;,...,&quot;- baai ~&quot;,...) • noun phrases of the form &apos;~1 no N2&quot; • sentences of the form &quot;N~ wa N2 da&quot; • sentences lacking the main verb (eg. sentences of the form &quot;~ o-negaishimasu&quot;) • fragmental expressions Chai&quot;, &quot;sou-desu&quot;, &quot;wakarimashita&quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending C-tainodesuga&quot;, &quot;~seteitadakimasu&quot;, ...) (Furuse et al. 1990) • simple sentences (Sato and Nagao 1989) • Example Database (data for &quot;kireru&apos;[cut / be sharp]) (1) houchou wa k l r s r u -&gt; The kitchen knife c u t s . (2) kanojo w a k i r e r u -&gt; She Is s h a r p . • Input kachouwa k l r e r u o&gt;? • Retrieval of similar examples (Syntax) (Semantics) Input = (1), (2) kachou/== houehou kachou ,= kanojo (Total) Input ==(2) • OUt0Ut -&gt; The chief Is ~ h a r D, Figure I Mimicking Similar Examples This paper discusses a detailed experiment for &quot;N~ no N2&quot;"
P91-1024,C90-3044,0,0.394324,"Missing"
P91-1024,P87-1018,0,0.0143101,"Missing"
P91-1024,1988.tmi-1.13,1,0.825003,"Missing"
P91-1024,J90-2002,0,0.12195,"tion performance improved by attaching EBMT components. This is in the line with the proposal in Nagao (1984). Subsequently, we proposed a practical method of integration in 3 BROAD APPLICABILITY AND INTEGRATION 3.1 BROAD APPLICABILITY EBMT is applicable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others 186 expressions). EBMT has the advantage that it can directly return a translation by adapting examples without reasoning through a long chain of rules. previous papers (Sumita et al. 1990a, b). 4 EBMT FOR &quot;N x no Nz&quot; 4.1 T H E P R O B L E M &quot;N~ no N2&quot; is a common Japanese noun phrase form. &quot;no&quot; in the &quot;Nt no Nz&quot; is a Japanese adnominal particle. There are other variants, including &quot;deno&quot;, &quot;karano&quot;, &quot;madeno&quot; and so on. Roughly speaking, Japanese noun phrases of the form &quot;N~ no N2&quot; correspond to English noun phrases of the form &quot;N2 of N:&quot; as shown in the examples at the top of Figure 2. Japanese English youka n o gogo kaigi no mokuteki the afternoon o f the 8th the object o f the conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 I M"
P98-1107,W97-0404,0,0.0740875,"Missing"
P98-2233,P95-1017,0,0.0745593,"Missing"
P98-2233,C96-2137,0,0.0886788,"on of ellipses is performed by another module, such as a parser. We only considered ellipses that are commonly and dearly identified. 2 When t o R e s o l v e E l l i p s i s in M T ? As described above, our major application for ellipsis resolution is in machine translation. In an MT process, there can be several approaches about the timing of ellipsis resolution: when analyzing the source language, when generating the target language, or at the same time as translating process. Among these candidates, most of the previous works with Japanese chose the source-language approach. For instance, Nakaiwa and Shirai (1996) attempted to resolve Japanese ellipsis in the source language analysis of J-to-E MT, despite utilizing targetdependent resolution candidates. We originally thought that ellipsis resolution in the MT was a generation problem, namely a target-driven problem which utilizes some help, if necessary, of source-language information. This is because the problem is outputdependent and it relies on demands from a target language. In the J-to-Korean or J-toChinese MT, all or most of the ellipses that must be resolved in J-to-E are not necessary to resolve. However, we adopted source-language policy in t"
P98-2233,C94-2116,0,0.0297388,"psis is anaphoric; in case we need to refer back to the antecedent in the dialogue. In this paper we are not concerned with resolving the antecedent that such ellipses refer to, because it is necessary to have another module to deal with the context for resolving such endophoric ellipses, and the main target of this paper is the exophoric ellipses. 3.2 Since a huge text corpus has become widely available, the machine-learning approach has been utilized for some problems in natural language processing. The most popular touchstone in this field is the verbal case frame or the translation rules (Tanaka, 1994). Machine-learning algorithm has also been attempted to solve some Meaning first person, singular first person, plural second person, singular second person, plural person(s) ~n general anaphoric Learning Method We used the C~.5 algorithm by Quinlan (1993), which is a well-known automatic classifier that produces a binary decision tree. Although it may be necessary to prune decision trees, no pruning is performed throughout this experiment, since we want to concentrate the discussion on the feasibility of machine learning. As shown in the experiment by Aone and BenTable 2: Number of training a"
P98-2233,J97-1001,0,\N,Missing
shimohata-etal-2004-building,W02-1611,1,\N,Missing
shimohata-etal-2004-building,W01-1401,1,\N,Missing
shimohata-etal-2004-building,P03-1057,1,\N,Missing
shimohata-sumita-2002-automatic,P97-1004,0,\N,Missing
shimohata-sumita-2002-automatic,W01-1401,1,\N,Missing
shimohata-sumita-2002-automatic,P01-1008,0,\N,Missing
shimohata-sumita-2002-automatic,P98-1116,0,\N,Missing
shimohata-sumita-2002-automatic,C98-1112,0,\N,Missing
shimohata-sumita-2002-automatic,P98-2127,0,\N,Missing
shimohata-sumita-2002-automatic,C98-2122,0,\N,Missing
takezawa-etal-2002-toward,W01-1401,1,\N,Missing
takezawa-etal-2002-toward,P01-1068,1,\N,Missing
takezawa-etal-2002-toward,W00-0900,0,\N,Missing
W01-1401,1999.mtsummit-1.75,0,0.0179482,"ns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 2.1 The proposed method Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignment and translation; and (3) thes"
W01-1401,J90-2002,0,0.0863253,"viable for machine translation. The background is as follows: l Demands have been increasing for machine translation systems to handle a wider range of languages and domains. l MT requires bulk knowledge consisting of rules and dictionaries. Building knowledge consumes considerable time and money. Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3)"
W01-1401,C00-1019,0,0.187154,"orpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 2.1 The proposed method Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignment and translation; and (3) thesauri of both l"
W01-1401,1999.mtsummit-1.37,0,0.146863,"T): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 2.1 The proposed method Configuration As shown in Figure 1, our resources are (1) a bilingual corpus, in which sentences are aligned beforehand; (2) a bilingual dictionary, which is used for word alignme"
W01-1401,P91-1024,1,0.897875,"lingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 2.1 The proposed method Configuration As shown in Figure 1, our resourc"
W01-1401,1999.mtsummit-1.34,1,0.33956,"Missing"
W01-1401,C90-3044,0,0.204091,"money. Bilingual/multilingual translations have become widely available. There are two approaches in corpus-based translation: 1. Statistical Machine Translation (SMT): SMT learns models for translation from corpora and dictionaries and searches for the best translation according to the models in run-time (Brown et al., 1990; Knight, 1997; Ney et al., 2000). 2. Example-Based Machine Translation (EBMT): EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and adjusts the examples to obtain the translation (Nagao, 1981; Sadler 1989; Sato and Nagao, 1990; Sumita and Iida, 1991; Kitano, 1993; Furuse et al., 1994; Watanabe and Maruyama, 1994; Cranias et al., 1994; Jones, 1996; Veale and Way, 1997; Carl, 1999, Andriamanankasina et al., 1999; Brown, 2000). Input sentence (1) SentenceAligned Bilingual Corpus (2) Bilingual Dictionary Retrieval + Adjustment (3) Thesauri Target sentence Figure 1 Configuration This paper pursues EBMT and proposes a new approach by using the distance between word sequences. The following sections show the algorithm, experimental results, and implications and prospects. 2 2.1 The proposed method Configuration As shown i"
W01-1401,1993.tmi-1.5,0,0.0126559,"EBMT studies (Sato and Nagao, 1990; Sato, 1991; Furuse et al., 1994; Sadler, 1989) assume the existence of a bank of aligned bilingual trees or a set of translation patterns. However, building such knowledge is done by humans and is very expensive. Methods for automating knowledge building are still being developed. In contrast, our proposal does not rely on such a high-level analysis of the corpus and requires only word-level knowledge, i.e., morphological tags and dictionaries. Dynamic programming Dynamic programming has been used within the EBMT paradigm (1) for technical term translation (Sato, 1993), and (2) for translation support (Cranias et al., 1994). Sato translates technical terms, which are usually compound nouns, while we translate sentences. He uses a corpus in which translation units of a pair of technical terms are aligned, while we do not require the alignment of translation units. He defines the matching score and we define the distance between word sequences, which are different. However, both are computed by a standard dynamic programming technique. Based on surface structures and content words, Cranias defined a similarity score between texts and introduced the idea of cl"
W01-1401,J96-4008,0,\N,Missing
W01-1401,C94-1014,0,\N,Missing
W01-1615,W99-0207,1,\N,Missing
W01-1615,P00-1053,0,\N,Missing
W01-1615,P95-1017,0,\N,Missing
W02-0701,2001.mtsummit-papers.3,1,0.818553,"al approaches to automatic evaluation include methods (Su, 1992; Yasuda et al., 2001) that automatically assign one of several ranks to MT output according to a single edit distance between an MT output and a correct translation example. To improve performance, we proposed an automatic ranking method that, by using multiple edit distances, encodes machine-translated sentences with a rank assigned by humans into multi-dimensional vectors from which a classifier of ranks is learned in the form of a decision tree. The proposed method assigns a rank to MT output through the learned decision tree (Akiba et al., 2001). Experimental results show that the proposed method is more accurate than the single-edit-distance-based ranking methods in both closed and open tests. The proposed method has the potential to accurately estimate the quality of outputs of machine translation systems. 5.2 Multiple-engine Machine Translation System Every researcher has his own way of acquiring translation knowledge by generalizing translation instances in a corpus. Our approach is no exception to this rule. Our MTs are based on different paradigms, different development styles, and different development periods. This results in"
W02-0701,P00-1056,0,0.040406,"Missing"
W02-0701,shimohata-sumita-2002-automatic,1,0.887673,"Missing"
W02-0701,yasuda-etal-2002-automatic,0,\N,Missing
W02-0701,2002.tmi-papers.9,0,\N,Missing
W02-0701,C02-1076,1,\N,Missing
W02-0701,2002.tmi-papers.20,1,\N,Missing
W02-0701,W01-1401,1,\N,Missing
W02-0701,J90-2002,0,\N,Missing
W02-0701,C92-2067,0,\N,Missing
W02-0701,P01-1067,0,\N,Missing
W02-0701,1999.mtsummit-1.34,1,\N,Missing
W02-0701,takezawa-etal-2002-toward,1,\N,Missing
W02-0701,watanabe-etal-2002-statistical,1,\N,Missing
W02-0701,2001.mtsummit-papers.67,0,\N,Missing
W02-1611,C00-1019,0,0.0246807,"apanese to English (J to E). English SE clusters were used for E to J, and Japanese SE clusters were used for J to E. 4.3 Expansion of Coverage 4.4 Quality of Translations The e ect of coverage expansion is divided into two types of input sentences: those acceptable only to with&quot; EBMT and those acceptable to both w/o&quot; and with&quot; EBMT. The former is evaluated by the expansion of acceptable input sentences. The latter is evaluated by the expansion of retrieved translations. Expansion of retrieved translations is useful since many EBMT systems (Sumita, 2001) (Veale and Way, 1997) (Carl, 1999) (Brown, 2000) utilize plural translations from similar sentences to acquire output translation. The results of the two types are shown in table 4. Exp.&quot; denotes the expansion ratio of with to w/o. The results show an obvious e ect on the expansion of the coverage. Interestingly, the expansion e ect is similar in English and Japanese. The quality of translation was evaluated by native speakers of the target languages. They evaluated translations as correct (Cor.) or not. Cor.&quot; means that the translation is basically appropriate for the translation of input sentence. Small di erences, such as degree of pol"
W02-1611,1999.mtsummit-1.37,0,0.0301679,"J) and from Japanese to English (J to E). English SE clusters were used for E to J, and Japanese SE clusters were used for J to E. 4.3 Expansion of Coverage 4.4 Quality of Translations The e ect of coverage expansion is divided into two types of input sentences: those acceptable only to with&quot; EBMT and those acceptable to both w/o&quot; and with&quot; EBMT. The former is evaluated by the expansion of acceptable input sentences. The latter is evaluated by the expansion of retrieved translations. Expansion of retrieved translations is useful since many EBMT systems (Sumita, 2001) (Veale and Way, 1997) (Carl, 1999) (Brown, 2000) utilize plural translations from similar sentences to acquire output translation. The results of the two types are shown in table 4. Exp.&quot; denotes the expansion ratio of with to w/o. The results show an obvious e ect on the expansion of the coverage. Interestingly, the expansion e ect is similar in English and Japanese. The quality of translation was evaluated by native speakers of the target languages. They evaluated translations as correct (Cor.) or not. Cor.&quot; means that the translation is basically appropriate for the translation of input sentence. Small di erences, such as"
W02-1611,W01-1401,1,0.906482,"ions: from English to Japanese (E to J) and from Japanese to English (J to E). English SE clusters were used for E to J, and Japanese SE clusters were used for J to E. 4.3 Expansion of Coverage 4.4 Quality of Translations The e ect of coverage expansion is divided into two types of input sentences: those acceptable only to with&quot; EBMT and those acceptable to both w/o&quot; and with&quot; EBMT. The former is evaluated by the expansion of acceptable input sentences. The latter is evaluated by the expansion of retrieved translations. Expansion of retrieved translations is useful since many EBMT systems (Sumita, 2001) (Veale and Way, 1997) (Carl, 1999) (Brown, 2000) utilize plural translations from similar sentences to acquire output translation. The results of the two types are shown in table 4. Exp.&quot; denotes the expansion ratio of with to w/o. The results show an obvious e ect on the expansion of the coverage. Interestingly, the expansion e ect is similar in English and Japanese. The quality of translation was evaluated by native speakers of the target languages. They evaluated translations as correct (Cor.) or not. Cor.&quot; means that the translation is basically appropriate for the translation of input"
W02-1611,takezawa-etal-2002-toward,1,0.834603,"ry verbs would&quot; and could&quot; are interchangeable if they are used in euphemistic request sentences like (could j would) you pass me the salt?&quot; but are not synonymous in other sentences. The common words surrounding di erent words are used as a contextual condition. They have the advantage that they are e ective enough as a contextual condition and are easy to acquire. For example, the expressions 	ake pictures&quot; and 	ake photos&quot; are synonymous in most cases. The same applies to the expressions #1 Would you&quot; and # Could you.&quot; An example of SE extracted from a corpus of travel conversation (Takezawa et al., 2002) (detail is described in 4.1) are shown in gure 2. The clusters tagged with E* represent English SE clusters based on Japanese translation, and those tagged with J* represent Japanese SE clusters based on English. The surrounding words of E1 properly work as a contextual condition. Unfortunately, many SE have unnecessary conditions or need other contextual conditions. 2.2.2 In uence on Target Language 2.2.3 Unrestricted to Types Extracted SE have in uence on the target language, since the synonymy of SE depends on the equivalence of translations in the target language. It is important that the"
W03-0311,P01-1004,0,0.013974,"ty and tense. Consequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant"
W03-0311,C00-1019,0,0.0760963,"Missing"
W03-0311,1999.mtsummit-1.37,0,0.0756867,"Missing"
W03-0311,W02-0718,0,0.0275687,"lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text . Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungramYuji Matsumoto Nara Institute of Science and Technology matsu@is.aist-nara.ac.jp matical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. On"
W03-0311,C92-4203,0,0.00964272,"onsequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information."
W03-0311,1988.tmi-1.13,1,0.610238,"pus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corresponds to rough translations, which aim not at exact translation with narrow"
W03-0311,W01-1401,1,0.874799,"ted Translated Concise Conversational 100 80 Table 1: Number of Words by Sentences 60 40 20 0 Test 2-5 6-10 11-15 16- Sentence Length (Words) Figure 1: Distribution of Untranslated Inputs by Length 2 Difficulty in Example-based S2ST 2.1 Translation Degradation by Input Length A major problem with machine translation, regardless of the translation method, is that performance drops rapidly as input sentences become longer. For EBMT, the longer input sentences become, the fewer similar example sentences exist in the example corpus. Figure 1 shows translation difficulty in long sentences in EBMT (Sumita, 2001). The EBMT system is given 591 test sentences and returns translation result as translated/untranslated. Untranslated means that there exists no similar example sentences for the input. Although the EBMT is equipped with a large example corpus (about 170K sentences), it often failed to translate long inputs. 2.2 Language English Japanese 5.4 6.2 7.9 8.9 Style Differences between Concise and Conversational The performance of example-based S2ST greatly depends on the example corpus. It is advantageous for an example corpus to have a large volume and the same style as the input sentences. A corpu"
W03-0311,takezawa-etal-2002-toward,1,0.921832,"texts dictated from conversational speech is favorable for S2ST. Unfortunately, it is very difficult to prepare such an example corpus since this task requires laborious work such as speech recording and speech transcription. Therefore, we cannot avoid using a written-style corpus, such as phrasebooks, to prepare a sufficiently large volume of examples. Contained texts are almost grammatical and rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both Concise Conversational Language Model Concise Conversational 16.4 58.3 72.3 16.3 Table 2: Cross Perplexity English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 200"
W03-0311,P99-1049,0,0.0174701,"a (my baggage was stolen.) question past negation Meaning-equivalent Sentence baggu wo nusuma re ta (My bag was stolen). Sentence3 hoteru wo yoyaku shi mashi ta ka? (Did you reserve this hotel?) hoteru wo yoyaku shi tei masen (I do not reserve this hotel.) Modality & Tense4 C1 C2 C3 C4 C5 C6 Figure 3: Sentences and their Modality and Tense sample sentences and their modality and tense. Clues are underlined. A speech act is a concept similar to modality in which speakers’ intentions are represented. The two studies introduced information of the speech act in their S2ST systems (Wahlster, 2000; Tanaka and Yokoo, 1999). The two studies and our method differ in the effect of speech act information. Their effect of speech act information is so small that it is limited to generating the translation text. Translation texts are refined by selecting proper expressions according to the detected speakers’ intention. 3.3 Retrieved sentences are ranked by the conditions described below. Conditions are described in order of priority. If there is more than one sentence having the highest score under these conditions, the most similar sentence is selected randomly. C1: C2: C3: C4: C5: C6: Sentences that satisfy the cond"
W03-0311,J00-4006,0,\N,Missing
W03-0318,takezawa-etal-2002-toward,1,0.936714,"are a) D3 does not assume syntactic parsing and bilingual tree-banks; b) D3 generates translation patterns on the fly according to the input and the retrieved translation examples as needed; c) D3 uses examples sentence-by-sentence and does not combine examples. Because of c), D3's result is pretty good when a similar example is retrieved, but very bad otherwise. Therefore, we usually decide a threshold. If there is no example whose dist is within the given threshold, we must give up performing translation. In an experiment using Basic Travel Expression Corpus (BTEC, described as BE-corpus in Takezawa, 2002), D3’s translation quality is very high. The experiment also shows a clear correlation between dist and the quality of translation. In other words, the accuracy decreases as the dist increases. In particular, the longer input sentences are, the more difficult for D3 to find examples with a small dist. 4.2 Applying Method-T to D3 As there is a correlation between dist and the translation quality, we can make use of dist as a confidence factor. To make the combined-reliability, each partial translation is weighted with its source word's number. That is, for each partial translation, its dist is"
W03-0318,W01-1401,1,0.936516,"n experiment with an EBMT system, in which prior methods cannot work or work badly, the proposed split-and-translate method achieves much better results in translation quality. 1 Introduction To achieve translation technology that is adequate for speech translation, the possibilities of several corpusbased approaches are being investigated. Among these methods, DP-match Driven transDucer (D3) has been proposed as an Example-Based Machine Translation (EBMT). When D3 is adapted to Japanese-to-English translation in a travel conversation domain, the method can achieve a high translation quality (Sumita, 2001 and 2002). On the other hand, the translation method is sensitive to the long sentence problem, where longer input sentences make it more difficult for a machine translation (MT) system to perform good translation. To overcome this problem, the technique of splitting an input sentence 1 and translating the split sentences appears promising. The methods of previous studies related to this approach can be roughly classified into two types: one splits sentences before translation and the other splits them in the parsing phase of translation. We’ll call the former pre-process-splitting, the latte"
W03-0318,J96-1002,0,\N,Missing
W03-0318,W02-0701,1,\N,Missing
W03-0318,1999.mtsummit-1.34,1,\N,Missing
W03-0318,P98-1070,0,\N,Missing
W03-0318,C98-1067,0,\N,Missing
W04-1708,2002.tmi-tutorials.2,0,0.0357521,"Missing"
W04-1708,P02-1040,0,0.102728,"t examination designers to make the questions and the alternative “detractor” answers. In this paper, we propose a method for the automatic measurement of English language proficiency by applying automatic evaluation techniques. The proposed method selects adequate test sentences from an existing corpus. Then, it automatically evaluates the translations of test sentences done by users. The core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of Machine Translation (MT) technology (Su et al., 1992; Papineni et al., 2002; NIST, 2002). In the proposed method, we apply these MT evaluation technologies to the measurement of human English language proficiency. The proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking. It does not measure elementary capabilities including vocabulary or grammar. This method also proposes a test sentence selection scheme to enable efficient testing. Section 2 describes several automatic evaluation methods applied to the proposed method. Section 3 introduces the proposed evaluation scheme. Section 4 show"
W04-1708,shimohata-sumita-2002-automatic,1,0.827135,"Missing"
W04-1708,C92-2067,0,0.27597,"r costs for expert examination designers to make the questions and the alternative “detractor” answers. In this paper, we propose a method for the automatic measurement of English language proficiency by applying automatic evaluation techniques. The proposed method selects adequate test sentences from an existing corpus. Then, it automatically evaluates the translations of test sentences done by users. The core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of Machine Translation (MT) technology (Su et al., 1992; Papineni et al., 2002; NIST, 2002). In the proposed method, we apply these MT evaluation technologies to the measurement of human English language proficiency. The proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking. It does not measure elementary capabilities including vocabulary or grammar. This method also proposes a test sentence selection scheme to enable efficient testing. Section 2 describes several automatic evaluation methods applied to the proposed method. Section 3 introduces the proposed evaluation"
W04-1708,1999.mtsummit-1.44,1,0.732327,"automatic evaluation methods of translation. These methods were proposed to evaluate MT output, but they are applicable to translation by humans. All of these methods are based on the same idea, that is, to compare the target translation for evaluation with high-quality reference translations that are usually done by skilled translators. Therefore, these methods require a corpus of high-quality human reference translations. We call these translations as “references”. 2.1 DP-based Method The DP score between a translation output and references can be calculated by DP matching (Su et al., 1992; Takezawa et al., 1999). First, we define the DP score between sentence (i.e., word array) Wa and sentence Wb by the following formula. T −S−I −D (1) T where T is the total number of words in Wa , S is the number of substitution words for comparing Wa to Wb , I is the number of inserted words for comparing Wa to Wb , and D is the number of deleted words for comparing Wa to Wb . Using Equation 1, (Si (j)), that is, the test sentence unit DP-score of the translation of test sentence j done by subject i, can be calculated by the following formula. SDP (Wa , Wb ) = SDPi (j) = max n k=1 to Nref o SDP (Wref (k) (j), Wsub("
W04-1708,takezawa-etal-2002-toward,1,0.786432,"2 shows the flow of measuring English proficiency. In the parameter-estimation phase, for each subject, we first calculate the test set unit automatic score by using Equation 3, 6 or 7. Next, we apply regression analysis using the automatic scores and subjects’ TOEIC scores. In the testing phase, we calculate a user’s TOEIC score using the automatic score of the user and the regression line calculated in the parameter-estimation phase. 4 Experiments 4.1 Experimental Conditions 4.1.1 Test sets For the experiments, we employ two different test sets. One is BTEC (Basic Travel Expression Corpus) (Takezawa et al., 2002) and the other is SLTA1 (Takezawa, 1999). Both BTEC and SLTA1 are parts of bilingual corpora that have been collected for research on speech translation systems. However, they have different features. A detailed analysis of these corpora was done by Kikui et al. (2003). Here, we briefly explain these test sets. In this study, we use the Japanese side as a test set and the English side as a reference for automatic evaluation. BTEC BTEC was designed to cover expressions for every potential subject in travel conversation. This test set was collected by investigating “phrasebooks” that contain Jap"
W05-0210,W03-0203,0,0.0807562,"Missing"
W05-0210,W04-1708,1,\N,Missing
W05-0210,J03-3001,0,\N,Missing
W05-0210,W04-2012,0,\N,Missing
W08-0334,W05-0909,0,0.0298061,"riments. Tuning the interpolation weights The interpolation weights were tuned by maximizing the BLEU score on the development set over a set of weights ranging from 0 to 1 in increments of 0.1. Figure 1 shows the behavior of two of our models with respect to their weight parameter. Evaluation schemes To obtain a balanced view of the merits of our proposed approach, in our experiments we used 6 evaluation techniques to evaluate our systems. These were: BLEU (Papineni, 2001), NIST (Doddington, 2002), WER (Word Error Rate), PER (Position-independent WER), GTM (General Text Matcher), and METEOR (Banerjee and Lavie, 2005). 4.2 Classification Accuracy The performance of the classifier (from 10-fold cross-validation on the training set) is shown in Table 2. We give classification accuracy figures for predicting both source (same language) and target (English) punctuation. Unsurprisingly, all systems were better at predicting their own punctuation. The poorer scores in the table might reflect linguistic characteristics (perhaps questions in the source language are often expressed as statements in the target), or characteristics of the corpus itself. For all languages the accuracy of the classifier seemed satisfac"
W08-0334,A94-1010,0,0.0397707,"hat fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class. The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. 1 Introduction Topic-dependent modeling has proven to be an effective way to improve quality the quality of models in speech recognition (Iyer and Osendorf, 1994; Carter, 1994). Recently, experiments in the field of machine translation (Hasan and Ney, 2005; Yamamoto and Sumita, 2007; Finch et al. 2007, Foster and Kuhn, 2007) have shown that classspecific models are also useful for translation. † National Institute for Science and Technology ‡ Advanced Telecommunications Research Laboratories In the method proposed by Yamamoto and Sumita (2007), topic dependency was implemented by partitioning the data into sets before the decoding process commenced, and subsequently decoding these sets independently using different models that were specific to the class predicted fo"
W08-0334,W07-0722,0,0.175874,"Missing"
W08-0334,W07-0717,0,0.570953,". The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. 1 Introduction Topic-dependent modeling has proven to be an effective way to improve quality the quality of models in speech recognition (Iyer and Osendorf, 1994; Carter, 1994). Recently, experiments in the field of machine translation (Hasan and Ney, 2005; Yamamoto and Sumita, 2007; Finch et al. 2007, Foster and Kuhn, 2007) have shown that classspecific models are also useful for translation. † National Institute for Science and Technology ‡ Advanced Telecommunications Research Laboratories In the method proposed by Yamamoto and Sumita (2007), topic dependency was implemented by partitioning the data into sets before the decoding process commenced, and subsequently decoding these sets independently using different models that were specific to the class predicted for the source sentence by a classifier that was run over the source sentences in a pre-processing pass. Our approach is in many ways a generalization o"
W08-0334,N03-1017,0,0.0292428,"Missing"
W08-0334,koen-2004-pharaoh,0,0.038143,"Before adopting the mixture-based approach set out in this paper, we first pursued an obvious and intuitively appealing way of using this classifier. We applied it as a filter to the output of the decoder, to force source sentences that the classifier predicts should generate questions in the target to actually generate questions in the target. This approach was unsuccessful due to a number of issues. 211 4.1 Experimental Conditions Decoder The decoder used to in the experiments, CleopATRa is an in-house phrase-based statistical decoder that can operate on the same principles as the PHARAOH (Koehn, 2004) and MOSES (Koehn et Source BLEU NIST WER PER GTM METEOR ar 0.4457 (0.00) 8.9386 (0.00) 0.4458 (0.00) 0.3742 (0.00) 0.7469 (0.00) 0.6766 (0.00) da 0.6640 (0.64) 11.4500 (1.64) 0.2560 (0.08) 0.2174 (2.42) 0.8338 (0.68) 0.8154 (1.23) de 0.6642 (0.79) 11.4107 (0.44) 0.2606 (2.18) 0.2105 (0.14) 0.8348 (-0.13) 0.8132 (-0.07) es 0.7345 (0.00) 12.1384 (0.00) 0.2117 (0.00) 0.1668 (0.00) 0.8519 (0.00) 0.8541 (0.00) fr 0.6666 (0.95) 11.7443 (0.63) 0.2548 (4.82) 0.2172 (6.50) 0.8408 (0.48) 0.8293 (1.29) id 0.5295 (9.56) 10.3459 (4.11) 0.3899 (21.17) 0.3239 (4.65) 0.7960 (1.35) 0.7521 (2.35) it 0.6702 (1."
W08-0334,J03-1002,0,0.00321587,"Missing"
W08-0334,P03-1021,0,0.00883029,"improvement of the proposed method’s score relative to the alternative method. Cells with bold borders indicate those conditions where performance was degraded. sary to decode the sentence in hand. This reduced the memory overhead considerably when loading multiple models, without noticeably affecting decoding time. Moreover, it is also possible to precompute the interpolated probabilities for most of the models for each sentence before the search commences, reducing both search memory and processing time. 213 Decoding Conditions For tuning of the decoder&apos;s parameters, minimum error training (Och 2003) with respect to the BLEU score using was conducted using the respective development corpus. A 5-gram language model, built using the SRI language modeling toolkit (Stolcke, 1999) with Witten-Bell smoothing was used. The model included a length model, and also the simple distance-based distortion model used by the PHARAOH decoder (Koehn, 2004). 4.3 zh id BLEU score 0.46 0.44 0.42 0.4 0.38 0 0.2 0.4 0.6 0.8 Model interpolation weight 1 Figure 3. Graph showing the BLEU score on the developmment set plotted against the general model’s interpolation weight (a weight of 0 meaning no contribution fr"
W08-0334,2001.mtsummit-papers.68,0,0.0414493,"Missing"
W08-0334,2006.amta-papers.25,0,0.0360089,"Missing"
W08-0334,P02-1040,0,\N,Missing
W08-0334,2005.eamt-1.17,0,\N,Missing
W08-0334,2007.iwslt-1.15,1,\N,Missing
W08-0335,P03-1021,0,0.00865291,"NIST MT05 evaluation which is defined in the LDC corpus as LDC2006E38. We used the corpus, LDC2006E43, as the development data for loglinear model optimization. We used a phrase-based SMT system that is based on a log-linear model incorporating multiple features. The training and decoding system of our SMT used the publicly available Pharaoh (Koehn et al., 2003)2 . GIZA++ was used for word alignment. The Pharaoh decoder was used exclusively in all the experiments. No additional features but the defaults defined by Pharaoh were used. The feature weights were optimized against the BLEU scores (Och, 2003). We chose automatic metrics to evaluate CWS and SMT. We used the F-score for CWS and BLEU for SMT. The BLEU is BLEU4, computed using the NIST-provided “mt-eval” script. 2 http://www.iccs.informatics.ed.ac.uk/˜pkoehn 218 3.2 Implementation of CWS schemes To determine the effect of CWS on SMT, we created 14 CWS schemes which are shown in Table 3. Schemes 1 to 12 were implemented using the in-house tool, Achilles, and schemes 13 and 14 using off-the-shelf tools. The CWS schemes are named according to the specifications (AS, CITYU, MSR, PKU), implementing methods (CRF-based or dictionary-based),"
W08-0335,I05-3017,0,0.243971,"wo methods of combining advantages of different specifications: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving quality of translations. 1 Introduction Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT). The research on CWS independently from SMT has been conducted for decades. As an evidence, the CWS evaluation campaign, the Sighan Bakeoff (Emerson, 2005),1 , has been held four times since 2004. However, works on relations between CWS and SMT are scarce. Generally, two factors need to be considered in constructing a CWS system. The first one is the specifications for CWS, i.e., the rules or guidelines for word segmentation, and the second one is the CWS methods. There are many CWS specifications used by different organizations. Unfortunately, these organizations do not seem to have any intention of reaching a unified specification. More than five or six specifications have been used in the four Sighan Bakeoffs. There is also significant disagr"
W08-0335,C04-1081,0,0.0079789,"likelihood of development data as has been reported previously (Foster and Kuhn, 2007). However, optimization is computationally expensive and the effect was not satisfactory. Therefore, we decided not optimizing the αs in this work. 5 Related work and Discussions CWS has been the subject of intensive research in recent years, as is evident from the last four international evaluations, the Sighan Bakeoffs, and many approaches have been proposed over the past decade. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to CRF (Peng and McCallum, 2004) approach. We used dictionary-based and CRF-based CWS approaches to demonstrate the effect of CWS on SMT, both without and with OOV recognition. SMT is a very complicated system to study. Its response to CWS schemes is intractable and it is very hard to use one or two measures to describe 222 the relationship between CWS and SMT, in a similar way to describing the relationship between the alignment error rate (AER) and SMT (Fraser and Marcu, 2007). The CWS and SMT are related by a series of factors such as the specifications, OOVs, lexicons, and F-scores. None of these factors can be directly"
W08-0335,W07-0717,0,0.0423297,"ed multiple translation models by using different word segmenters. Each translation model corresponded to a word segmenter. The same type of features as in the log-linear model were added linearly. For example, the phrase translation model p(e |f ) can be linP early interpolated as, p(e |f ) = Si=1 αi pi (e |f ) where pi (e |f ) is the phrase translation model corresponding to the i-th CWSs. αi is the weight, and S is the P total number of models. Si=1 αi = 1. αs can be obtained by maximizing the likelihood or BLEU scores of the development data. Optimizing the α has been described elsewhere (Foster and Kuhn, 2007). p(e |f ) is the phrase translation model generated. In addition to the phrase translation model, we used the same approach to integrate three other features: phrase inverse probability p( f |e), lexical probability lex(e |f, a), and lexical inverse probability lex( f |e, a). We integrated the CWS schemes ranked in the top five in Table 4: ICTCLAS, dict-hybrid, dictLDC-PKU, dict-CITYU, and CRF-AS. We labeled the five schemes A, B, C, D, and E, respectively, as shown in Table 6. The first line of Table 6 represents the test data segmented by the five CWS schemes. “tst-A” means the test data wa"
W08-0335,W03-1730,0,0.0132228,"were trained using a specific Sighan corpus. For example, schemes 1 to 3 were trained using the AS corpus, schemes 4 to 6 using the CITYU corpus, and so on. The meaning of the name of the CWS scheme can be derived from the table – the name is defined by specifications, methods and lexicon sources. For example, the CRF-AS scheme performs CRF-based segmentation; and its lexicon is from the AS corpus provided by the Sighan. The CRF-AS segmenter can be easily trained, as described by Achilles. The second group contains two schemes 13 and 14. The ICTCLAS is a HHMM-based hierarchical HMM segmenter (Zhang et al., 2003) that uses the specifications of PKU. This segmenter incorporates parts-of-speech information in the probability models and generates multiple HMM models for solving segmentation ambiguities. The MSRSEG was developed by Gao et al. (Gao et al., 2004). This segmenter is based on the MSR specifications. It uses a log-linear model that integrates multiple features. The segmenters of the first group, dict-AS and dict-LDC-AS, are two dictionary-based CWS schemes. They differ in lexicon size and lexicon extracting source. The former used a lexicon extracted directly from the Sighan AS training data w"
W08-0335,J07-3002,0,0.00784033,"er the past decade. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to CRF (Peng and McCallum, 2004) approach. We used dictionary-based and CRF-based CWS approaches to demonstrate the effect of CWS on SMT, both without and with OOV recognition. SMT is a very complicated system to study. Its response to CWS schemes is intractable and it is very hard to use one or two measures to describe 222 the relationship between CWS and SMT, in a similar way to describing the relationship between the alignment error rate (AER) and SMT (Fraser and Marcu, 2007). The CWS and SMT are related by a series of factors such as the specifications, OOVs, lexicons, and F-scores. None of these factors can be directly related to the SMT. While we have completed many experiments, based on changing the CWS specifications and methods used, to determine the relationship between CWS and SMT, we have not established any overwhelming rules. However, we believe the following guidelines are appropriate in considering a CWS system for SMT. Firstly, the F-score is not a reliable guide to SMT quality. A very high F-score may produce the lowest quality translations, as was"
W08-0335,zhang-etal-2004-interpreting,0,0.0757069,"Missing"
W08-0335,P04-1059,0,0.0387099,"Missing"
W08-0335,N06-2049,1,0.93356,"ter sequences into a word. Using this specification could generate tens of thousands of new words, which can cause data sparseness for SMT. In addition to using the four specifications, we also downloaded the training and test corpora of the second Sighan Bakeoff. We used each of the training corpora provided to create a CWS scheme and evaluated the performance of the schemes on our test data. This enabled us to examine the effect of CWS specifications on SMT. We used a Chinese word segmentation tool, Achilles, to implement word segmentation. Part of the work using this tool was described by (Zhang et al., 2006). The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff. Moreover, this tool meets our need to test the effect of the two kinds of CWS approaches for SMT. We can easily train a dictionary-based and a CRF-based CWS by using this tool. By turning the program’s option for the CRF model on and off, we can use the Achilles as a dictionary-based approach and as a CRF-based CWS. In fact, the dictionary-based approach is the default approach for Achilles. 3 Experiments 3.1 SMT resources We followed the instructions for the 2005 NIST M"
W08-0335,N03-1017,0,0.00334294,"Missing"
W08-0401,P05-1067,0,0.0318129,"Missing"
W08-0401,P06-1121,0,0.0302732,"Missing"
W08-0401,2006.amta-papers.8,0,0.261705,"Missing"
W08-0401,P06-1077,0,0.281609,"Missing"
W08-0401,W02-1018,0,0.0342724,"Missing"
W08-0401,W06-1606,0,0.151335,"Missing"
W08-0401,P04-1083,0,0.0726394,"Missing"
W08-0401,P03-1021,0,0.0203996,"t corpus E/J Train E/J Dev E/J Eval 5.2.1 # of sent. 1.8M 916 899 Total words 60M/64M 30K/32K 29K/32K # of entries 188K/118K 4,072/3,646 3,967/3,682 English-to-Japanese Translation The translation direction of the first experiment was English-to-Japanese (E-J). For phrase-based translation model training, we used the GIZA++ toolkit (Och et al., 2003). For language model training, the SRI language model tool kit (Stolcke 2002) was used. The language model type was word 5gram smoothed by Kneser-Ney discounting (Kneser 1995). For tuning of decoder parameters, we conducted minimum error training (Och 2003) with respect to the BLEU score using 916 development sentence pairs. For extraction of source sentence tree structure, we used the Charniak parser (Charniak 2000). We used Chasen for segmentation of the Japanese. The numbers of entries in the language models were 0.1 M, 2.1 M, 4.3 M, 6.2 M, and 6.9 M for 1, 2, 3, 4, and 5grams respectively. The number of entries in the phrase-table was 76 M. For decoding, we used an in-house decoder that is a close relative to the Moses decoder. The performance of this decoder was configured to be the same as Moses. Another conditions are the same as the defa"
W08-0401,J04-4002,0,0.193218,"Missing"
W08-0401,P02-1040,0,0.0760954,"are included in u. Phrases [eb ec ed ] or [eb ec ed ee ] are also accepted since these include u. • If a sub-tree consists of only TRANSLATED sub-trees, this sub-tree is also TRANSLATED. 3. If a new candidate includes NG sub-trees, this candidate is rejected. • If a sub-tree consists of only leaf word nodes, and all leaf words are not yet translated, this sub-tree is defined as UNTRANSLATED. • If a sub-tree consists of only UNTRANSLATED sub-trees, this sub-tree is also UNTRANSLATED. 4 5 Experiments 5.1 Evaluation Measures We evaluated the proposed method using four evaluation measures, BLEU (Papineni et al., 2002), NIST (Doddington 2002), WER(word error rate), and PER(position independent word error rate). Before discussing the evaluation, the characteristics of each one are analyzed. • BLEU: This evaluation measure takes into account middle range word order, but does not take into account global word order. When the translation result is [w1 , w2 , ..., wj−1 , X, wj+1 , ..., wn ] for reference translation [w1 , w2 , ..., wn ], both WER and BLEU scores will be high. For a translation result [wj+1 , ..., wn , X, w1 , w2 , ..., wj−1 ], the BLEU score will be the same as the previous result since BLEU onl"
W08-0401,P05-1034,0,0.243982,"Missing"
W08-0401,P03-1010,0,0.0349749,"Missing"
W08-0401,J97-3002,0,0.068148,"Missing"
W08-0401,C04-1030,1,0.939418,"Missing"
W08-0401,A00-2018,0,\N,Missing
W08-0401,N04-4026,0,\N,Missing
W08-0401,P01-1067,0,\N,Missing
W08-0401,J03-1002,0,\N,Missing
W08-0401,P03-1019,0,\N,Missing
W09-0418,2007.mtsummit-papers.48,1,0.737342,"ocabulary (OOV) words, i.e., source language words that do not occur in the training corpus. For unknown words, no translation entry is available in the statistical translation model (phrase-table). As a result, these OOV words cannot be translated. Dealing with languages with a rich morphology like Spanish and having a limited amount of bilingual resources make this problem even more severe. There have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from"
W09-0418,W05-0909,0,0.0471768,"Missing"
W09-0418,P02-1040,0,0.0750922,"Missing"
W09-0418,W08-0309,0,0.0308403,"n quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the new"
W09-0418,W08-0334,1,0.83212,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,I08-8003,1,0.924723,", we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1 words with phonetically"
W09-0418,W07-0717,0,0.0215096,"ning of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-specific statistical models. As the official WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and disProceedings of the Fourth Workshop on Statistical Machine Translation , pages 105–109, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 105 katakana1"
W09-0418,P97-1017,0,0.0702819,"posed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese Abstract This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation. We participated in the Spanish-English translation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machi"
W09-0418,D07-1091,0,0.0188625,"have been several efforts in dealing with OOV words to improve translation quality. In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007). However, these approaches depend on the coverage of the utilized external dictionaries. Data sparseness problems due to inflectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006). A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. However, these approaches still suffer from the data sparseness problem, since lemmata and inflectional forms never seen in the training corpus cannot be translated. In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introdu"
W09-0418,2005.mtsummit-papers.11,0,0.0369022,"ation task. The focus of this year’s participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation. We participated in the SpanishEnglish translation task under the Constrained Condition. For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde. In order to adapt SMT systems to a specific domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a). Statistical models can be trained on in-domain and out-of-domain data sets and combined"
W09-0418,niessen-etal-2000-evaluation,0,0.0181976,"8.36 4.3 Effects of Transliteration In order to investigate the effects of transliteration, we trained three different transliteration using the phrase-table of the baseline systems trained on (a) only the NC corpus, (b) only the EP corpus, and (c) on the merged corpus (NC+EP). The performance of these phrase-based transliteration models is evaluated for 2000 randomly selected transliteration examples. Table 5 summarizes the haracter-based automatic evaluation scores for the word error rate (WER) metrics, i.e., the edit distance between the system output and the closest reference translation (Niessen et al., 2000), as well as the BLEU and METEOR metrics. The best performance is achieved when training examples from both domains are exploit to transliterate unknown Spanish words into English. Therefore, the NC+EP transliteration model was applied to the translation outputs of all mixture models described in Section 4.2. The effects of the transliteration post-process are summarized in Table 6. Transliteration consisTable 7: Testset 2009 Performance weight NC Eval optimization BLEU tm 21.07 tm+lm 20.95 tm+dm 21.45 tm+lm+dm 21.67∗ EP Eval BLEU 20.81 20.59 21.32 21.27 5 Conclusion The work for this year’s s"
W09-0418,J03-1002,0,0.0112243,"Missing"
W09-0418,P03-1021,0,0.0130472,"Phrase penalty Language model probability Lexical reordering probability Simple distance-based distortion model Word penalty For the training of the statistical models, standard word alignment (GIZA++ (Och and Ney, 2003)) and language modeling (SRILM (Stolcke, 2002)) tools were used. We used 5-gram language models trained with modified Knesser-Ney smoothing. The language models were trained on the target side of the provided training corpora. Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoder’s parameters, and performed using the technique proposed in (Och, 2003). For the translation, the inhouse multi-stack phrase-based decoder CleopATRa was used. 4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system (Finch and Sumita, 2008a) built within the framework of a feature-based exponential model containing the following features: The automatic evaluation scores of the baseline systems trained on (a) only the NC corpus and (b) only on the EP corpus are summarized in Table 3. 107 Table 3: Baseline Performance baseline NC Eval BLEU METEOR 17.56 40.52 Table 5: Transliteration Performance EP Eval BLEU METEOR 33.00 56.50 Trai"
W09-0418,N03-1017,0,\N,Missing
W09-2309,A00-2018,0,0.0186181,"language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was conﬁgured to be the same as Moses. Other conditions were the same as the default conditions of the Moses decoder. In this experiment, the following three methods were compared. • Baseline : The IBM constraints and the lexical reordering model were used for target word BLEU Baseline 27.87 IST-ITG 29.31 Proposed 29.80 English Train Table 3: BLEU score results for E-J translation. (1reference) Dev Test reordering."
W09-2309,W02-1039,0,0.0265043,"hreshold and unseen subtrees are clustered to deal with the data sparseness problem for robust model estimations. After creating word alignments of a training parallel corpus, there are target word orders which are not derived from rotating nodes of source-side parsetrees. Figure 3 shows a sample which is not derived from rotating nodes. Some are due to linguistic reasons, structual differences such as negation (French “ne...pas” and English “not”), adverb, modal and so on. Others are due to non-linguistic reasons, errors of automatic word alignments, syntactic analysis, or human translation (Fox, 2002). The proposed method discards such problematic cases. In Figure 3, the subtree s1 is then removed from training samples, and the subtrees s2 and s3 are used as training samples. 3.3 Decoding Using the Proposed Reordering Model In this section, we describe a one-pass phrase-based decoding algorithm that uses the proposed reordering model in the decoder. The translation target sentence is sequentially generated from left (sentence 73 Figure 4: Example of a target candidate including a phrase. head) to right (sentence tail), and all reordering is conducted on the source side. To introduce the pr"
W09-2309,N04-1035,0,0.066933,"Missing"
W09-2309,2006.amta-papers.8,0,0.0258706,"ranslation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constrai"
W09-2309,N03-1017,0,0.0187068,"e, the proposed method conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model. 1 Introduction Statistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permu"
W09-2309,W06-1606,0,0.0205975,"he-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG con"
W09-2309,P04-1083,0,0.0464455,"popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of"
W09-2309,J03-1002,0,0.0175091,"el is as follows. 1. The training process begins with a wordaligned corpus. We obtained the word alignments using Koehn et al.’s method (2003), 1 Subtree type S+PP+,+NP+VP+. PP+IN+NP NP+DT+NN+NN VP+AUX+VP VP+VBN+PP NP+NP+PP NP+DT+JJ+NN NP+DT+JJ+VBP+NN NP+DT+NN+CC+VB 2,3,4 4 2,3 2 3 Monotone probability 0.764 0.816 0.664 0.864 0.837 0.805 0.653 0.412 0.357 Table 1: Example of proposed reordering models. Figure 2: Example of a source-side parse-tree with word alignments using the training algorithm of the proposed model. which is based on Och and Ney’s work (2004). This involves running GIZA++ (Och and Ney, 2003) on the corpus in both directions, and applying reﬁnement rules (the variant they designate is “ﬁnal-and”) to obtain a single many-tomany word alignment for each sentence. 2. Source-side parse-trees are created using a source language phrase structure parser, which annotates each node with a syntactic label. A source-side parse-tree consists of several subtrees with syntactic labels. For example, the parse-tree “(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))” is obtained from the source sentence “This is a pen” which consists of four words. 3. Word alignments and source-side parse-"
W09-2309,P03-1021,0,0.0101573,"model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was conﬁgured to be the same as Moses. Other conditions were the same as the defa"
W09-2309,J04-4002,0,0.0562306,"od conducts a probabilistic evaluation of target word reorderings. In Englishto-Japanese and English-to-Chinese translation experiments, the proposed method resulted in a 0.49-point improvement (29.31 to 29.80) and a 0.33-point improvement (18.60 to 18.93) in word BLEU-4 compared with IST-ITG constraints, respectively. This indicates the validity of the proposed reordering model. 1 Introduction Statistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constra"
W09-2309,P05-1034,0,0.0309353,"tatistical machine translation has been wiedely applied in many state-of-the-art translation systems. A popular statistical machine translation paradigms is the phrase-based model (Koehn et al., 2003; Och and Ney, 2004). In phrase-based statistical machine translation, errors in word reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on I"
W09-2309,P02-1040,0,0.0771608,"2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the evaluation and development sets, a single reference was used. For the creation of English sentence parse trees and segmentation of the English, we used the Charniak parser (Charniak, 2000). We used Chasen for segmentation of the Japanese sentences. For decoding, we used an in-house decoder that is a close relative of the Moses decoder. The performance of this decoder was conﬁgured to be the same as Moses. Other conditions were the same as the default conditions of the Moses decoder. In this experiment, the"
W09-2309,N04-4026,0,0.028741,"rd reordering, especially global reordering, are one of the most serious problems. To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the ﬁrst and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reorderi"
W09-2309,2007.mtsummit-papers.63,0,0.0311279,"lso to be assigned a higher probability. 4 Experiments To evaluate the proposed model, we conducted two experiments: English-to-Japanese and English-toChinese translation. 74 4.1 English-to-Japanese Paper Abstract Translation Experiments The ﬁrst experiment was the English-to-Japanese (E-J) translation. Table 2 shows the training, development and test corpus statistics. JST JapaneseEnglish paper abstract corpus consists of 1.0M parallel sentences were used for model training. This corpus was constructed from 2.0M JapaneseEnglish paper abstract corpus belongs to JST by NICT using the method of Uchiyama and Isahara (2007). For phrase-based translation model training, we used the GIZA++ toolkit (Och and Ney, 2003), and 1.0M bilingual sentences. For language model training, we used the SRI language model toolkit (Stolcke, 2002), and 1.0M sentences for the translation model training. The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser and Ney, 1995). To tune the decoder parameters, we conducted minimum error rate training (Och, 2003) with respect to the word BLEU score (Papineni et al., 2002) using 2.0K development sentence pairs. The test set with 2.0K sentences is used. In the eva"
W09-2309,J97-3002,0,0.0319424,". To resolve this problem, many 69 word-reordering constraint techniques have been proposed. These techniques are categorized into two types. The ﬁrst type is linguistically syntaxbased. In this approach, tree structures for the source (Quirk et al., 2005; Huang et al., 2006), target (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the ﬁrst and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints. For example, ISTITG constraints allow"
W09-2309,W08-0401,1,0.564512,"get (Yamada and Knight, 2000; Marcu et al., 2006), or both (Melamed, 2004) are used for model training. The second type is formal constraints on word permutations. IBM constraints (Berger et al., 1996), the lexical word reordering model (Tillmann, 2004), and inversion transduction grammar (ITG) constraints (Wu, 1995; Wu, 1997) belong to this type of approach. For ITG constraints, the target-side word order is obtained by rotating nodes of the sourceside binary tree. In these node rotations, the source binary tree instance is not considered. Imposing a source tree on ITG (IST-ITG) constraints (Yamamoto et al., 2008) is an extension of ITG constraints and a hybrid of the ﬁrst and second type of approach. IST-ITG constraints directly introduce a source sentence tree structure. Therefore, IST-ITG can obtain stronger constraints for word reordering than the original ITG constraints. For example, ISTITG constraints allows only eight word orderings for a four-word sentence, even though twenty-two word orderings are possible with respect to the original ITG constraints. Although IST-ITG constraints efﬁciently suppress erroneous target word orderings, the method cannot assign the probability to the target word o"
W09-2309,P01-1067,0,\N,Missing
W09-2309,D08-1076,0,\N,Missing
W09-3510,N03-1017,0,0.0157969,"erated in the target • The number of types (the vocabulary size) in both source and target languages is considerably less for the transliteration task We take a statistical machine translation paradigm (Brown at al., 1991) as the basis for our systems. The work in this paper is related to the work of (Finch and Sumita, 2008) who also use SMT directly to transliterate. 3 Experimental Conditions In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles as the publicly available MOSES decoder (Koehn et al., 2003). Like MOSES we utilize a future cost in our calculations. Our decoder was modified to be able to run two instances of the decoder at the same 52 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52–56, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Word 1 Word 2 Word m Segment into individual words and decode each word independently Decode Decode Decode n-best n-best n-best hypothesis 1 hypothesis 2 hypothesis 1 hypothesis 1 Search for the best path hypothesis 2 hypothesis 2 ... ... ... hypothesis n hypothesis n hypothesis n Figure 1: The decoding process for mul"
W09-3510,P03-1021,0,0.00484161,"hase (where output translations are compared against a set of references) we restored the case for Russian by simply capitalizing the first character of each word. We chose not to perform any tokenization for any of the language pairs in the shared task. We chose this approach for several reasons: • It allowed us to have a single unified approach for all language pairs • It was in the spirit of the evaluation, as it did not require specialist knowledge outside of the supplied corpora 3.3 Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is 53 En-Ch En-Ja En-Ko En-Ru Jn-Jk After tuning 0.908 0.772 0.622 0.914 0.769 Before tuning 0.871 0.635 0.543 0.832 0.737 Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score were able to perform a full search. For the rare long word sequences in the data, a beam search strategy was adopted. commonly used to evaluate the performance of machine translation systems and is a function of the geometric mean of n-gram precision. Table 1 shows the effect of tun"
W09-3510,J93-2003,0,\N,Missing
W09-3510,P04-1021,0,\N,Missing
W09-3510,P97-1017,0,\N,Missing
W09-3510,D08-1076,0,\N,Missing
W10-1760,W07-0718,0,0.0433782,"Missing"
W10-1760,P02-1040,0,0.0790848,"th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus"
W10-1760,W08-0336,0,0.0752938,"stent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are small enough to be translatable, but large enough to be meaningful in the context of the given input sentence, achieving a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suita"
W10-1760,W96-0213,0,0.138718,"e shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: Iteration 1 segmented SRC Iteration 2 segmented SRC SMT1 … (7) extract (4) annotate (5) resegment ME1 classifier (9) resegment decode evalchr decode eval1 decode evalJ-1 better (6) train SRCtoken TRGword alignment (8) annotate better (J-1) train SMTJ-1 … extract (J) train SMTJ worse evalJ SRCtoken TRGword alignment ME2 classifier … MEJ-1 classifier Iteration J-1 segmented SRC Selected Word Segmenter … Figure 2: Iterative Bootstrap Method The feature set is given in Table 1. The lexical context features consist of target words annotated with a tag t. w0 denotes the wor"
W10-1760,D09-1075,0,0.0563363,"erive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a gi"
W10-1760,N09-2019,0,0.0478899,"Missing"
W10-1760,J01-3002,0,0.0418668,"els trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools. 1 (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information p"
W10-1760,P08-1115,0,0.067533,"learned using a parallel corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn the source language word In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence 401 dicator where only two tags are used, i.e, “E” (end-of-word character tag) and “I” (in-word character tag). The annotations are derived from the SMT training corpus as described in Figure 1. segmentation that is consistent with the translation model of an SMT system trained on the resegmented bitext. The process is iterated until no further improvement in translation quality is achieved. In order to integrate multiple word segmentation into a single SMT system, the statistical translation models t"
W10-1760,N09-1046,0,0.0169293,"ons like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. The problems of word segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dict"
W10-1760,zhang-etal-2004-interpreting,0,0.0314116,"Missing"
W10-1760,P06-1085,0,0.207335,"rds such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information provided by these tokens might not be enough to obtain a good translation. For example, a Japanese-English SMT engine might translate the two successive characters “ ” (“white”) and “ ” (“bird”) as “white bird”, while a"
W10-1760,W08-0335,1,0.835024,"of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using segmentation information implicit in the phrase-table to generate the target and thus avoids issues of consistency between phrasetable and input representation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. • it uses segmentations at all iterative leve"
W10-1760,W02-0704,0,0.0334855,"of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. • it works for any language pair where the source language is unsegmented and the target language segmentation is known. • it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available. • it applies machine learning techniques to identify segmentation schemes that improve translation quality for a given language pair. • it decodes directly from unsegmented text using"
W10-1760,W07-0734,0,0.0113547,"No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step iteraExperiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sent"
W10-1760,E09-1063,0,0.162335,"segmentation are:   400 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: “consistent with SMT models” is one that identifies translation units that are"
W10-1760,J03-1002,0,0.0117672,"s of the iterative bootstrap method (dev), and the evaluation of translation quality (test). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence. The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with repl"
W10-1760,C08-1128,0,\N,Missing
W10-1760,J96-1002,0,\N,Missing
W10-2406,W09-3528,0,0.110337,"ize - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model SMT Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles"
W10-2406,W09-0438,0,0.022089,"each other well. We now briefly describe the component systems. Joint Multigram Model 2.1 Introduction The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extension of the use of variable-length n-grams (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model"
W10-2406,I08-8003,1,0.873708,"ect to joint multigram size - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model SMT Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exact"
W10-2406,W09-3510,1,0.607773,"hai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official evaluation on the test data on all performance metrics. machine translation systems and is a function of the geometric mean of n-gram precision. The use of BLEU score as a proxy has been shown to be a reasonable strategy for the metrics used in these experiments (Finch and Sumita, 2009). Nonetheless, it is reasonable to assume that one would be able to improve the performance in a particular evaluation metric by doing minimum error rate training specifically for that metric. The interpolation weight was tuned by a grid search to find the value that gave the maximal fscore (according to the official f-score evaluation metric for the shared task) on the development data, the process for English-Japanese is shown in Figure 3. 4 existed between our system and the top-ranked system, there is much room for improvement. One of the strengths in terms of the utility of our approach i"
W10-2406,N03-1017,0,0.0612569,"English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with • The symbols in the source and target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics F-Score 0.9 3 0.7 3.1 0.4 0.3 2 3 4 5 6 7 8 9 10 Joint Multigram Size Figure 1: The effect on F-score by tuning with resp"
W10-2406,P03-1021,0,0.00726501,"g Multi-Word Sequences 3.4 The data for some languages contained some multi-word sequences. To handle these we had to consider the following strategies: • • Building the Models Introduce a <space&gt; token into the sequence, and treat it as one long character sequence to transliterate; or 3.6 Segment the word sequences into individual words and transliterate these independently, combining the n-best hypothesis lists for all the individual words in the sequence into a single output sequence. Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is commonly used to evaluate the performance of 50 Language Pair Accuracy in top-1 Mean F-score MRR MAPref English ➝ Thai 0.412 0.883 0.550 0.412 Thai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official"
W10-2406,P04-1021,0,0.321009,"target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics F-Score 0.9 3 0.7 3.1 0.4 0.3 2 3 4 5 6 7 8 9 10 Joint Multigram Size Figure 1: The effect on F-score by tuning with respect to joint multigram size - • Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. Phrase-based SMT 2.2 It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed"
W10-2406,W10-2402,0,\N,Missing
W10-2406,J93-2003,0,\N,Missing
W10-2406,P97-1017,0,\N,Missing
W10-2406,D08-1076,0,\N,Missing
W10-3508,2009.mtsummit-posters.10,1,0.748465,"o QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accep"
W10-3508,P09-4005,0,0.0320061,"ble translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools installed in MNH. 3.1 QRedit QRedit is a translation"
W10-3508,P09-4008,0,0.0291935,"ictionary that was made from the English Wikipedia. This enable translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools i"
W10-3508,W06-1703,0,0.0239343,"www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accept these term candidates as the translations of those Japanese term candidates. 4 Fostering language resources Being a “one stop” translation aid tool for online translators, MNH incorporates mechanisms which enable users to naturally foster import"
W10-3508,P03-1010,1,0.7968,"em which is designed for volunteer translators working mainly online (Abekawa and Kageura, 2007). When a URL of a source language (SL) text is given to QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term ex"
W10-3804,P06-2014,0,0.0240457,"word alignment model to find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine trans"
W10-3804,P08-1009,0,0.0407929,"Missing"
W10-3804,P05-1033,0,0.165473,"Missing"
W10-3804,D07-1079,0,0.0344199,"Missing"
W10-3804,P07-1003,0,0.0198412,"o find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine translation system, which rel"
W10-3804,W08-0334,1,0.852581,"e state-of-the-art alignment tool such as GIZA++ 2 can not always find alignments for every word in the sentence pair. The possible reasons could be: its frequency is too low, noisy data, auxiliary words or function words which have no obvious correspondence in the opposite language. In the automatically aligned parallel corpus, unaligned words are frequent enough to be noticeable (see section 4.1 in this paper). How to decide the translation of unaligned word is left to the phrase extraction algorithm. An unaligned 2 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexicalized reordering model was built by using the “msdbidirectional-fe” configuration in our experiments. The translation model was created from the FBIS parallel corpus. We used a 5-gram language model trained with modified Kneser-Ney smoothing. The lang"
W10-3804,W08-0306,0,0.0146045,"cision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links. The similarity is that both Ma et al. (2008) and this work utilize structure information to find appropriate translations for words which are difficult to align. The difference is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage. There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006; DeNero and Klein, 2007; Fossum et al., 2008; Hermjakob, 2009). Johnson et al., (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher’s exact test. They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold. Yang and Zheng (2008) extended the work in Johnson et al., (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). Tomeh et al., (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical"
W10-3804,W04-3243,0,0.0702217,"Missing"
W10-3804,2002.tmi-papers.9,0,0.0838836,"Missing"
W10-3804,W99-0604,0,0.0822253,"is pair’s contribution to the translation model. In typical PBMT systems such as MOSES (Koehn, 2007), phrase pairs are extracted from wordaligned parallel corpora. Figure 1 shows the form of training example. f1 | e1 f2 | e2 f3 f4 | e3 Figure 1: An example parallel sentence pair and word alignment Since there is no phrase segmentation information in the word-aligned sentence pair, in practice all pairs of “source word sequence target word sequence” that are consistent with word alignments are collected. The words in a legal phrase pair are only aligned to each other, and not to words outside (Och et al., 1999). For example, given a sentence pair and its word alignments shown in Figure1, the following nine phrase pairs will be extracted: Table 1: Phrase pairs extracted from the example in Figure 1 Note that neither the source phrase nor the not a legal phrase pair. Phrase pairs are extracted over the entire training corpus. Given all the collected phrase pairs, we can estimate the phrase translation probability distribution by relative frequency. The collected phrase pairs will also be used to 29 build the lexicalized reordering model. For more details of the lexicalized reordering model, please ref"
W10-3804,P03-1021,0,0.0181132,"requent enough to be noticeable (see section 4.1 in this paper). How to decide the translation of unaligned word is left to the phrase extraction algorithm. An unaligned 2 Experiments Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008). For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder. Our decoder can operate on the same principles as the MOSES decoder. Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder’s parameters, and it is performed using the standard technique of Och (2003). A lexicalized reordering model was built by using the “msdbidirectional-fe” configuration in our experiments. The translation model was created from the FBIS parallel corpus. We used a 5-gram language model trained with modified Kneser-Ney smoothing. The language model was trained on the target side of the FBIS corpus and the Xinhua news in the GIGAWORD corpus. The development and test sets are from the NIST MT08 evaluation campaign. Table 3 shows the statistics of the corpora used in our experiments. Table 2: Phrase pairs extracted from the example in Figure 2 f1 source word should be trans"
W10-3804,N03-1017,0,0.0379239,"Missing"
W10-3804,P06-1077,0,0.0605332,"Missing"
W10-3804,P07-1089,0,0.0319879,"Missing"
W10-3804,W08-0409,0,0.039275,"Missing"
W10-3804,W06-1606,0,0.0522068,"Missing"
W10-3804,P08-1114,0,0.0319459,"Missing"
W10-3804,P02-1040,0,0.0855271,"to automatically align them to the target words. Unaligned word 的 , 在 一 中 个 是 上 了 不 Frequency 77776 29051 9414 8768 8543 7471 7365 6155 5945 5450 Syntactic Constraints Number of distinct phrase pairs BLEU None 14,195,686 17.26 Full constraint 4,855,108 16.51 Selectively constraint 10,733,731 17.78 Table 5: Comparison of different constraints on phrase pair extraction by translation quality Table 4: Frequently unaligned words from the training corpus 4.2 settings were the identical in the three experiments. Table 5 summarizes the SMT performance. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002) which estimates the accuracy of translation output with respect to a set of reference translations. Experiments on Chinese-English SMT In order to confirm that it is advantageous to apply appropriate syntactic constraints on phrase extraction, we performed three translation experiments by using different ways of phrase extraction. In the first experiment, we used the method introduced in Section 2 to extract all possible phrase translation pairs without using any constraints arising from knowledge of syntax. The second experiment used source language syntactic constraints to filter out all no"
W10-3804,P05-1034,0,0.067201,"Missing"
W10-3804,P05-1069,0,0.0198178,"example, given a sentence pair and its word alignments shown in Figure1, the following nine phrase pairs will be extracted: Table 1: Phrase pairs extracted from the example in Figure 1 Note that neither the source phrase nor the not a legal phrase pair. Phrase pairs are extracted over the entire training corpus. Given all the collected phrase pairs, we can estimate the phrase translation probability distribution by relative frequency. The collected phrase pairs will also be used to 29 build the lexicalized reordering model. For more details of the lexicalized reordering model, please refer to Tillmann and Zhang (2005) and section 2.7.2 of the MOSES’s manual 1 . The main problem of such a phrase pair extraction procedure is the resulting phrase translation table is very large, especially when a large quantity of parallel data is available. This is not desirable in real application where speed and memory consumption are often critical concerns. In addition, some phrase translation pairs are generated from training data errors and word alignment noise. Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al., 2007; Yang and Zheng, 2009). 3"
W10-3804,2009.mtsummit-papers.17,0,0.0779538,"Missing"
W10-3804,P09-1036,0,0.0378316,"Missing"
W10-3804,P09-2060,0,0.0164599,"illmann and Zhang (2005) and section 2.7.2 of the MOSES’s manual 1 . The main problem of such a phrase pair extraction procedure is the resulting phrase translation table is very large, especially when a large quantity of parallel data is available. This is not desirable in real application where speed and memory consumption are often critical concerns. In addition, some phrase translation pairs are generated from training data errors and word alignment noise. Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al., 2007; Yang and Zheng, 2009). 3 Syntactic Constraints on Phrase Pair Extraction We can divide all the possible phrases into two types: syntactic phrases and non-syntactic phrases. A “syntactic phrase” is defined as a word sequence that is covered by a single subtree in a syntactic parse tree (Imamura, 2002). Intuitively, we would think syntactic phrases are much more reliable while the non-syntactic phrases are useless. However, (Koehn et al., 2003) showed that restricting phrasal translation to only syntactic phrases yields poor translation performance – the ability to translate nonsyntactic phrases (such as “there are”"
W10-3804,P08-1064,0,0.0285611,"Missing"
W10-3804,W06-3119,0,0.063016,"Missing"
W10-3804,D07-1103,0,\N,Missing
W10-3804,P09-1058,0,\N,Missing
W10-3804,P01-1067,0,\N,Missing
W10-3804,P07-2045,0,\N,Missing
W10-3804,D09-1024,0,\N,Missing
W10-3804,P06-1121,0,\N,Missing
W11-2601,2008.iwslt-papers.1,0,0.33914,"language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality o"
W11-2601,J93-2003,0,0.0213426,"rce channel model, also called the n-gram transliteration model, is a joint probability model that captures information on how the source and target sentences can be generated simultaneously using transliteration pairs, i.e., the most likely sequence of source characters and target words according to a joint language model built from the co-segmentation from the Bayesian model. 4 (5) where p(src|trg) is called a translation model (T M ) and represents the generation probability from trg into src, and p(trg) is called a language model (LM ) and represents the likelihood of the target language (Brown et al., 1993). During the translation process (decoding), a score based on the statistical model probabilities is assigned to each translation hypothesis and the one that gives the highest probability is selected as the best translation. The translation quality of SMT approaches heavily depends on the amount and coverage of the bilingual language resources available to train the statistical models. In the context of dialect translation, where only few bilingual language resources (if any at all) are available for the dialect and the foreign language, only a relatively low translation quality can be obtaine"
W11-2601,P08-2006,0,0.0306173,"and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentat"
W11-2601,D08-1033,0,0.0535578,"Missing"
W11-2601,2010.iwslt-papers.7,1,0.645429,"rsion of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the joint-source channel model approach as a weighted finite state transducer (FST) using the OpenFst toolkit (Allauzen et al., 2007). The FST takes the sequence of dialect characters as its input and outputs the co-segmented bilingual segments from which the standard language segments are extracted. 2.3 Pivot-based SMT Recent research on speech translation focuses on corpus-based approaches, and in particular on statistical machine translation (SMT), which is a machine translation paradigm where translations are generated on the bas"
W11-2601,W05-0703,0,0.031257,"lects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmen"
W11-2601,W06-1108,0,0.0145447,"guage) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect reso"
W11-2601,W10-2405,0,0.0150812,"ained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for"
W11-2601,W03-0317,0,0.0363333,"a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuiti"
W11-2601,P04-1021,0,0.270354,"tween character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric with respect to source and target language. Intuitively, the model has two basic components: a model for generating an outcome that has already been generated at least once before, and a second model that assigns a probability to an outcome that has not yet been produced. Ideally, to encourage the re-use of model parameters, the probability of generating a novel bilingual sequence pair should be considerably lower then the probability of generating a previously observed sequence pair. The probability distribution over these bilingual sequence pairs (including a"
W11-2601,P09-1012,0,0.0156796,"teration units. Then, an n-gram transliteration model is defined as the transliteration probability of a transliteration pair < l, s >k depending on its immediate n preceding transliteration pairs: P (σ, ω, γ) = K Y P (< l, s >k |< l, s >k−1 k−n+1 ) (4) k=1 In this equation, N is the total number of bilingual sequence pairs generated so far and N ((sk , tk )) is the number of times the sequence pair (sk , tk ) has occurred in the history. G0 and α are the base measure and concentration parameter as before. We used a blocked version of a Gibbs sampler for training, which is similar to that of (Mochihashi et al., 2009). We extended their forward filtering / backward sampling (FFBS) dynamic programing algorithm in order to deal with bilingual segmentations (see Algorithm 1). We found our sampler converged rapidly without annealing. The number of iterations was set by hand after observing the convergence behavior of the algorithm in pilot experiments. We used a value of 75 iterations through the corpus in all experiments reported in this paper. For more details on the Bayesian co-segmentation process, please refer to (Finch and Sumita, 2010). For the experiments reported in this paper, we implemented the join"
W11-2601,W97-1102,0,0.0712968,"to a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguis"
W11-2601,W10-2408,0,0.0192295,"input and the translation models. In the second step, a state-of-the-art phrase-based SMT system trained on a large amount of bilingual data is applied to obtain high-quality foreign language translations as described in Section 2.3. 2.1 Bayesian Co-segmentation The method for mapping the dialect sentences into the standard language word segments is a direct character-to-character mapping between the languages. This process is known as transliteration. Many transliteration methods have previously been proposed, including methods based on stringsimilarity measures between character sequences (Noeman and Madkour, 2010) or generation-based models (Lee and Chang, 2003; Tsuji and Kageura, 2006; Jiampojamarn et al., 2010). In this paper, we use a generative Bayesian model similar to the one from (DeNero et al., 2008) which offers several benefits over standard transliteration techniques: (1) the technique has the ability to train models whilst avoiding over-fitting the data, (2) compact models that have only a small number of well-chosen parameters are constructed, (3) the underlying generative transliteration model is based on the joint source-channel model (Li et al., 2004), and (4) the model is symmetric wit"
W11-2601,J03-1002,0,0.00720165,"Missing"
W11-2601,P02-1040,0,0.0804258,"Missing"
W11-2601,N09-2056,1,0.856333,"nly a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improve translation quality of certain language pairs, especially when translating from or into Asian languages (Paul et al., 2009). This paper focuses on the translation of dialects, i.e., a variety of a language that is characteristic of a particular group of the language’s speakers, into a foreign language. A standard dialect (or standard language) is a dialect that is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–"
W11-2601,2010.amta-papers.5,0,0.0321811,"distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often insufficient. For example, the task of word segmentation, i.e., the identification of word boundaries in continuous text, is one of the fundamental preprocessing steps of MT applications. In contrast to Indo-European languages like English, many Asian languages like Japanese do not use a whitespace character to separate meaningful word units. However, the application of a linguistically motivated standard language word segmentation tool to a dialect corpus results in a poor segmentation quality"
W11-2601,P07-3010,0,0.0202508,"t is recognized as the ”correct” spoken and written form of the language. Dialects typically differ in terms of morphology, vocabulary and pronunciation. Various 1 LDC: http://www.ldc.upenn.edu, ELRA: http://www.elra.info 1 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 1–9, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics methods have been proposed to measure relatedness between dialects using phonetic distance measures (Nerbonne and Heeringa, 1997), string distance algorithms (Heeringa et al., 2006; Scherrer, 2007), or statistical models (Chitturi and Hansen, 2008). Concerning data-driven natural language processing (NLP) applications like machine translation (MT), however, linguistic resources and tools usually are available for the standard language, but not for dialects. In order to create dialect language resources, previous research utilized explicit knowledge about the relation between the standard language and the dialect using rule-based and statistical models (Habash et al., 2005; Sawaf, 2010). In addition, applying the linguistic tools for the standard language to dialect resources is often in"
W11-2601,N07-1061,0,0.180595,"ds on the amount and coverage of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot l"
W11-2601,P07-1108,0,0.202943,"ge of the bilingual language resources available to train the statistical models. There are several data collection initiatives1 amassing and distributing large amounts of textual data. For frequently used language pairs like French-English, large-sized text data sets are readily available. However, for less frequently used language pairs, only a limited amount of bilingual resources are available, if any at all. In order to overcome language resource limitations, recent research on multilingual SMT focuses on the use of pivot languages (de Gispert and Marino, 2006; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Koehn et al., 2009). Instead of a direct translation between two languages where only a limited amount of bilingual resources is available, the pivot translation approach makes use of a third language that is more appropriate due to the availability of more bilingual corpora and/or its relatedness to the source/target language. In most of the previous research, English has been the pivot language of choice due to the richness of available language resources. However, recent research on pivot translation has shown that the usage of non-English pivot languages can improv"
W11-2601,C08-1128,0,0.022935,"hen the sequence itself is generated given the length. Note that this model is able to assign a probability to arbitrary bilingual sequence pairs of any length in the source and target sequence, but favors shorter sequences in both. The generative model is given in Equation 3. The equation assigns a probability to the k th bilingual sequence pair (sk , tk ) in a derivation of the corpus, given all of the other sequence pairs in the history so far (s−k , t−k ). Here −k is read as: “up to but not including k”. p((sk , tk ))|(s−k , t−k )) N ((sk , tk )) + αG0 ((sk , tk )) = N +α (3) 3 Following (Xu et al., 2008), we assign the parameters λs , λt and α, the values 2, 2 and 0.3 respectively. Input: Random initial corpus segmentation Output: Unsupervised co-segmentation of the corpus according to the model foreach iter=1 to NumIterations do foreach bilingual word-pair w ∈ randperm(W) do foreach co-segmentation γi of w do Compute probability p(γi |h) where h is the set of data (excluding w) and its hidden co-segmentation end Sample a co-segmentation γi from the distribution p(γi |h) Update counts end end Algorithm 1: Blocked Gibbs Sampling Suppose that we have a dialect sentence σ = l1 l2 . . . lL and a"
W11-2601,2009.mtsummit-papers.7,0,\N,Missing
W11-3203,P04-1021,0,0.233247,"ase-based translation model lacks contextual information, and in the experiments of (Finch and Sumita, 2010a), the model gained this contextual information implicitly by the use of agglomerated phrases. In other words, Introduction In the NEWS2010 workshop, (Finch and Sumita, 2010b) reported that the performance of a phrasebased statistical machine transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 23–27, Chiang Mai, Thailand, November 12, 2011. the target language model to generate derivations that are too short. the longer phrases carried with them their own built-in c"
W11-3203,2005.mtsummit-papers.36,0,0.0302575,"ne transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 23–27, Chiang Mai, Thailand, November 12, 2011. the target language model to generate derivations that are too short. the longer phrases carried with them their own built-in context. In our model these contextual dependencies are made explicit and modeled directly by the joint source-channel model. The termination condition for our Bayesian cosegmentation algorithm was set based on pilot experiments that showed very little gain in system performance after iteration 10, and no loss in performance by continuing the training"
W11-3203,J03-1002,0,0.00277059,"xt of graphemes and grapheme sequences in both source and target languages. The segmentation for our approach was performed using a non-parametric Bayesian co-segmentation model, and in this paper we present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters"
W11-3203,P03-1021,0,0.00383897,"ource-channel model was trained from the Viterbi co-segmentation arising from the final iteration of the Bayesian segmentation process on the training data (for model used in parameter tuning), and the training data added to the development data (for the model used to decode the test data). We used the MIT language modeling toolkit (Bo-june et al., 2008) with modified Knesser-Ney smoothing to build this model. In all experiments we used a language model of order 5. The exponential log-linear model weights of our system are set by tuning the system on development data using the MERT procedure (Och, 2003) by means of the publicly available ZMERT toolkit 1 (Zaidan, 2009). The systems reported in this paper used a metric based on the word-level F-score, an official evaluation metric for the shared tasks, which measures the relationship of the longest common subsequence of the transliteration pair to the lengths of both source and target sequences. 2.2.2 2.4 Official Results Target Language model The target model was trained from target side of the training data (for model used in parameter tuning), and the training data added to the development data (for the model used to decode the test data)."
W11-3203,I08-8003,1,0.657881,"ramework as (Finch and Sumita, 2010a), and replace the agglomeration heuristics by incorporating a joint source-channel model directly into the decoder as an additional feature. Our motivation for this was simply that the phrase-based translation model lacks contextual information, and in the experiments of (Finch and Sumita, 2010a), the model gained this contextual information implicitly by the use of agglomerated phrases. In other words, Introduction In the NEWS2010 workshop, (Finch and Sumita, 2010b) reported that the performance of a phrasebased statistical machine transliteration system (Finch and Sumita, 2008; Rama and Gali, 2009) could be improved significantly by combining it with a model based on the n-gram context of source-target grapheme sequence pairs: a joint source-channel model similar to that of (Li et al., 2004). Their system integrated the two approaches by using a re-scoring step at the end of the decoding process. Our system goes one step further and integrates a joint source-channel model directly into the SMT decoder to allow the probabilities from it to be taken into account within a single search process in the similar manner to (Banchs et al., 2005). 23 Proceedings of the 2011"
W11-3203,W09-3528,0,0.151423,"aring the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian approach offered higher"
W11-3203,2010.iwslt-papers.7,1,0.667333,"present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian a"
W11-3203,W10-2406,1,0.87351,"present experiments comparing the effectiveness of this segmentation relative to the publicly available state-of-the-art m2m alignment tool. In all our experiments we have taken a strictly language independent approach. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian a"
W11-3203,P11-2094,0,0.0579273,"h. Each of the language pairs were processed automatically with no special treatment. 1 The typical method of deriving a translation-model for a machine translation is to use GIZA++ (Och and Ney, 2003) to perform word alignment and a set of heuristics for phrase-pair extraction. A commonly used set of heuristics is known as growdiag-final-and. This type of approach was taken by (Finch and Sumita, 2010b; Rama and Gali, 2009) to train their models. An alternative approach is to use a nonparametric Bayesian technique to co-segment both source and target in a single step (Finch and Sumita, 2010a; Huang et al., 2011). This approach has the advantage of being symmetric with respect to source and target languages, and furthermore Bayesian techniques tend to give rise to models with few parameters that do not overfit the data in the same way as traditional maximum likelihood training. In experiments on an EnglishJapanese transliteration task, (Finch and Sumita, 2010a) showed that that a Bayesian approach offered higher performance than using GIZA++ together with heuristic phrase-pair extraction. Their approach unfortunately required a simple set of agglomeration heuristics in order get good performance from"
W11-3203,N07-1047,0,0.0537841,"rding to the correct linguistic readings of the kanji. We investigate this further in the next section. shown (Finch and Sumita, 2010a) that in transliteration, this Bayesian approach can give rise to a smaller and more useful phrase-table than that derived by using GIZA++ for alignment and the grow-diag-final-and heuristics which have been shown to be effective for transliteration (Rama and Gali, 2009). In these experiments we compare the Bayesian segmenter to a similar state-of-the-art segmentation tool that is capable of many-to-many alignments: the publicly available m2m alignment tool 2 (Jiampojamarn et al., 2007) that is trained using the EM algorithm and is based on the principles set out in (Ristad and Yianilos, 1998). We used a similar system to that in the shared task, but without the maximum entropy model. The experiments were run in the same way using the same script, the only difference being the choice of aligner used. We used data from the 2009 NEWS workshop for our experiments, and evaluated using the F-score metric used for the shared task evaluation. The aligners were run with their default settings, and with the same limits for source and target segment size. It may have been possible to"
W11-3203,P07-2045,0,0.00954349,"es are made explicit and modeled directly by the joint source-channel model. The termination condition for our Bayesian cosegmentation algorithm was set based on pilot experiments that showed very little gain in system performance after iteration 10, and no loss in performance by continuing the training. We arbitrarily chose iteration 30 in all our experiments as the final iteration. The decoding was performed using a specially modified version of the CLEOPATRA decoder (Finch et al., 2007), an in-house multi-stack phrase-based decoder that operates on the same principles as the MOSES decoder (Koehn et al., 2007). The system we used in this shared task is a log-linear combination of 5 different models, the following sections describe each of these models in detail. Due to the small size of many of the data sets in the shared tasks, we used all of the data to build models for the final systems. 2.2.4 Maximum-entropy model In a typical phrase-based SMT system, the translation model contains a context-independent probability of the target grapheme sequence (phrase) given the source. Our system replaces this with a more sophisticated maximum entropy model that takes the local context of source and target"
W11-3203,D08-1076,0,\N,Missing
W11-3203,2007.iwslt-1.15,1,\N,Missing
W12-4406,P11-2094,0,\N,Missing
W12-4406,P07-1081,0,\N,Missing
W12-4406,2010.iwslt-papers.7,1,\N,Missing
W12-4406,P03-1021,0,\N,Missing
W12-4406,W11-3203,1,\N,Missing
W14-5503,P06-1085,0,0.0349653,"odel The generative model is given in Equation 3 below. The equation assignes a probability to the k th segment sk in a derivation of the corpus, given all of the other segments in the history so far s−k . Here −k is read as: “up to but not including k”. p(sk |s−k ) = N (sk ) + αG0 (sk ) N +α (3) In this equation, N is the total number of segments generated so far, N (sk ) is the number of times the segment sk has occurred in the history. G0 and α are the base measure and concentration parameter as before. 3.1.3 Bayesian Inference We used a blocked version of a Gibbs sampler for training. In (Goldwater et al., 2006) they report issues with mixing in the sampler that were overcome using annealing. In (Mochihashi et al., 2009) this issue was overcome by using a blocked sampler together with a dynamic programming approach. Our algorithm is an extension of application the forward filtering backward sampling (FFBS) algorithm (Scott, 2002) to the problem of word segmentation presented in (Mochihashi et al., 2009). We extend their approach to handle the joint segmentation and alignment of character sequences. We refer the reader to (Mochihashi et al., 2009) for a complete description of the FFBS process. In ess"
W14-5503,I08-7006,0,0.116272,"mar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used (based on bigram information), whereas (Htay and Murthy, 2008) utilize a word list extracted from a monolingual Myanmar corpus. In a related study (Thu et al., 2013a), various Myanmar word segmentation approaches including character segmentation, syllable segmentation, human lexical/phrasal segmentation, unsupervised and semi-supervised word segmentation, were investigated. They reported that the highest quality machine translation was attained either without word segmentation using simply sequences of syllables, or by a process of Maximum Matching with a monolingual dictionary. In this study the effectiveness of approaches unsupervised word segmentation"
W14-5503,O91-1004,0,0.433216,"Missing"
W14-5503,P98-2206,0,0.104297,"hop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they requir"
W14-5503,P09-1012,0,0.122441,"ational Conference on Computational Linguistics, pages 20–27, Dublin, Ireland, August 23-29 2014. 2 Related Work In this section, we will briefly introduce some proposed word segmentation methods with an emphasis on the schemes that have been applied to Myanmar. Many word segmentation methods have been proposed especially for the Thai, Khmer, Lao, Chinese and Japanese languages. These methods can be roughly classified into dictionary-based (Sornlertlamvanich, 1993; Srithirath and Seresangtakul, 2013) and statistical methods (Wu and Tseng, 1993; Maosong et al., 1998; Papageorgiou and P., 1994; Mochihashi et al., 2009; Jyun-Shen et al., 1991). In dictionary-based methods, only words that are stored in the dictionary can be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time require"
W14-5503,H94-1054,0,0.265939,"Missing"
W14-5503,J00-3004,0,0.023792,"n be identified and the performance depends to a large degree upon the coverage of the dictionary. New words appear constantly and thus, increasing size of the dictionary is a not a solution to the out of vocabulary word (OOV) problem. On the other hand, although statistical approaches can identify unknown words by utilizing probabilistic or cost-based scoring mechanisms, they also suffer from some drawbacks. The main issues are: they require large amounts of data; the processing time required; and the difficulty in incorporating linguistic knowledge effectively into the segmentation process (Teahan et al., 2000). For low-resource languages such as Myanmar, there is no freely available corpus and dictionary based or rule based methods are being used as a temporary solution. If we only focus on Myanmar language word segmentation, as far as the authors are aware there have been only two published methodologies, and one study. Both of the proposed methodologies operate according using a process of syllable breaking followed by Maximum Matching; the differences in the approaches come from the manner in which the segmentation boundary decision is made. In (Thet et al., 2008) statistical information is used"
W14-5503,H01-1057,0,0.0397598,"d from the data, as to whether the segment would be generated from the unsupervised sub-model or the dictionary sub-model. In this model, the decision to generate from the dictionary model is refined into a number of decisions to generate from a number of subsets of the dictionary, each with its own probability. These probabilities were re-estimated from the sampled segmentation of the corpus at the end of each iteration of the training (in a similar manner to the dictionary augmented model). A diagram showing the generative process is shown in Figure 1. 3.4 Language Model Augmented Model In (Theeramunkong and Usanavasin, 2001) dictionary approaches were deliberately avoided in order to address issues with unknown words. Instead a decision tree model for segmentation was proposed. Our approach although different in character (since a generative model is used), shares the insight that knowledge of how words are constructed is key to segmentation when dictionary information is absent. In this model we used the dictionary resource, but in a more indirect manner. We use a language model to capture the notion of exactly what constitutes a segment. To do this words in the dictionary were first segmented into syllables. Th"
W14-5503,1985.tmi-1.4,0,0.361798,"Missing"
W14-5503,C98-2201,0,\N,Missing
W14-7001,W14-7005,0,0.0335781,"Missing"
W14-7001,W14-7011,1,0.826327,"Missing"
W14-7001,P11-2093,0,0.053248,"h language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tree syntax-based SMT system. • max-chart-span = 1000 • Phrase score option: GoodTuring 5 6 http://nlp.stanford.edu/software/segmenter.shtml 3 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html 4 Figure 1: The submission web page for participants For Japanese segmentation we use three different tools, which are Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 7 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0 8 . For Chinese segmentation we use two different tools, which are KyTea 0.4.6 with Full SVM Model in MSR model and Stanford Word Segmenter version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model 9 (Tseng, 2005). For English segmentation we use tokenizer.perl 10 in the Moses toolkit. The detailed procedures for the automatic evaluation are shown at WAT2014 evaluation web page 11 . after WAT2014. Everybody can use the system by registering on the registration web page 12 . 5 Human Evaluat"
W14-7001,W14-7007,0,0.033389,"Missing"
W14-7001,W14-7002,0,0.0428991,"ieved better quality WEBLIO-EJ1 1 in the automatic evaluation, howthan RBMT system. ever it is much worse in the human evaluation. According to the descriptions of the two submissions, • The translation quality of the widely used the difference of the two is whether it uses the forsystems was Phrase-based SMT < Hierarchiest input or not. It is natural that using the forcal PBSMT < Syntax-based SMT (S2T and est input improves the translation quality, thus we T2S). conducted the human evaluation of WEBLIO-EJ1 • Forest-to-String Syntax-based SMT system 2 compared to WEBLIO-EJ1 1, which means we (Neubig, 2014) achieved the best quality for used WEBLIO-EJ1 1 as the baseline for the huall the translation directions. man evaluation. The HUMAN score was 2.50 ± 4.17 which Statistical Significance Testing between means there is no significant difference between Submissions the two, and this result is far from the results of Tables 5, 6, 7 and 8 show the results of statistical the official results. Actually, taking the confidence significance testings of JE, EJ, JC and CJ transintervals into consideration, this conclusion can be lations respectively where all the pairs of submisderived under some probabil"
W14-7001,P13-2121,0,0.018775,"Missing"
W14-7001,W14-7006,0,0.0412465,"Missing"
W14-7001,2009.iwslt-papers.4,0,0.110246,"ine system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the d"
W14-7001,P02-1040,0,0.11454,"Words. • distortion-limit = 20 • msd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ c"
W14-7001,P06-1055,0,0.0857644,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this paper. We describe the detail of the baseline SMT systems. 2.2 ASPEC-JC ASPEC-JC is a parallel corpus consisting of J"
W14-7001,W14-7008,0,0.0763585,"Missing"
W14-7001,D10-1092,0,0.117188,"sd-bidirectional-fe lexicalized reordering • Phrase score option: GoodTuring The default values were used for the other system parameters. The default values were used for the other system parameters. 3.4 Hierarchical Phrase-based SMT 4 Automatic Evaluation We used the following Moses’ configuration for the hierarchical phrase-based SMT system. 4.1 Procedure of Calculating Automatic Evaluation Score • max-chart-span = 1000 • Phrase score option: GoodTuring We calculated automatic evaluation scores of the translation results applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated with multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated with RIBES.py version 1.02.4 6 . All scores of each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results have been tokenized with word segmentation tools on each language. The default values were used for the other system parameters. 3.5 String-to-Tree Syntax-based SMT We used Berkeley parser to obtain target language syntax. We used the following Moses’ configuration for the string-to-tr"
W14-7001,W14-7012,1,0.854354,"Missing"
W14-7001,P07-2045,0,0.0113934,"ilarity score. baseline system at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, five commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and translating using the systems were published on the WAT 2014 web page3 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 2. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Since our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system description of these systems are anonymized in this pap"
W14-7001,W14-7003,0,0.0378915,"Missing"
W14-7001,W04-3250,0,0.570886,"Missing"
W14-7001,2007.mtsummit-papers.63,0,0.0465476,"submit translation results at any time. 2.1 ASPEC-JE The training data of ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the paper abstracts are kind • Domain and language pairs WAT is the world’s first workshop that uses 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 1 Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 1‒19, Tokyo, Japan, 4th October 2014. 2014 Copyright is held by the author(s). of comparable corpora, the sentence correspondences are automatically found using the method of (Utiyama and Isahara, 2007). Each sentence pair is accompanied with the similarity score and the field symbol. The similarity scores are calculated by the method of (Utiyama and Isahara, 2007). The field symbols are single letters AZ and show the scientific field of each document2 . The correspondance between the symbols and field names, along with the frequency and occurance ratios for the training data, are given in the README of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from Japanese-English paper abstracts owned by JST that are not contained in the training data"
W14-7001,W14-7010,0,0.0441404,"Missing"
W14-7001,W14-7004,0,0.0675856,"Missing"
W14-7001,W15-5003,0,\N,Missing
W14-7001,W15-5009,0,\N,Missing
W14-7001,W15-5007,0,\N,Missing
W14-7001,W14-7009,0,\N,Missing
W14-7001,W15-5008,0,\N,Missing
W14-7001,W15-5012,0,\N,Missing
W14-7001,W15-5010,0,\N,Missing
W14-7001,W15-5013,0,\N,Missing
W14-7001,W15-5005,0,\N,Missing
W14-7011,I13-1147,0,0.0240338,"Missing"
W14-7011,N03-1017,0,0.0207031,"ntences into Chinese-like word order, before a baseline phrase-based (PB) SMT system applied. Experimental results on the ASPEC-JC data show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the"
W14-7011,P07-2045,0,0.0576512,"-like word order, before a baseline phrase-based (PB) SMT system applied. Experimental results on the ASPEC-JC data show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the target-side word ord"
W14-7011,2005.mtsummit-papers.11,0,0.111268,"a show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the target-side word order, before a standard SMT system is applied, on both training and decoding phases. 77 Proceedings of the 1s"
W14-7011,W12-4207,0,0.0141584,"5-8573, Japan • {tei@mibel., myama@}cs.tsukuba.ac.jp {mutiyama, eiichiro.sumita}@nict.go.jp Abstract An effective rule-based approach, head finalization has been proposed for English-to-Japanese translation (Isozaki et al., 2012). The approach takes advantage of the head final property of Japanese on the target-side. It designs a head finalization rule to move the head word based on the parsing result by a head-driven phrase structure grammar (HPSG) parser. Generally, the idea can be applied to other SVO-to-Japanese translation tasks, such as its application in Chinese-toJapanese translation (Dan et al., 2012). However, the head finalization cannot be applied on the reverse translation task, i.e. Japaneseto-SVO translation, which becomes a more difficult task. Specifically, Japanese-to-English translation has been studied and several rule-based prereordering approaches have been proposed, taking advantage of the characters of Japanese and English (Komachi et al., 2006; Katz-Brown and Collins, 2008; Sudoh et al., 2011; Hoshino et al., 2013; Ding et al., 2014). A comparison of these approaches is reported in Ding et al. (2014). Because both Chinese and English are SVO languages, to transfer approache"
W14-7011,2006.iwslt-evaluation.11,0,0.0180473,"the head word based on the parsing result by a head-driven phrase structure grammar (HPSG) parser. Generally, the idea can be applied to other SVO-to-Japanese translation tasks, such as its application in Chinese-toJapanese translation (Dan et al., 2012). However, the head finalization cannot be applied on the reverse translation task, i.e. Japaneseto-SVO translation, which becomes a more difficult task. Specifically, Japanese-to-English translation has been studied and several rule-based prereordering approaches have been proposed, taking advantage of the characters of Japanese and English (Komachi et al., 2006; Katz-Brown and Collins, 2008; Sudoh et al., 2011; Hoshino et al., 2013; Ding et al., 2014). A comparison of these approaches is reported in Ding et al. (2014). Because both Chinese and English are SVO languages, to transfer approaches of Japanese-toEnglish to Japanese-to-Chinese translation is a natural idea. Based on the framework of Ding et al. (2014), we propose dependency-based prereordering rules for Japanese-to-Chinese translation in this paper. Contrary to our expectations, from the experimental results on ASPEC-JC data, we discover that the rule-based pre-reordering cannot improve th"
W14-7011,W02-2016,0,0.0690907,"rts are corresponding translation. The right example shows an extra-chunk move. A Japanese post-positioned case-marker is arranged to the left-most position of the range it governs. This move makes the Japanese postposition phrase have an identical order to the Chinese preposition phrase. 4 We further delete several Japanese functional morphemes which do not have exact Chinese translations. They are as follows. Experiment We tested our approach on the ASPEC-JC data (Nakazawa et al., 2014). For the source side Japanese sentences, we used MeCab (IPA dictionary)2 for morpheme analysis, CaboCha3 (Kudo and Matsumoto, 2002) for chunking and dependency parsing. We used the Stanford Chinese Word Segmenter4 (Tseng et al., 2005) with the Chinese Penn Treebank standard (CTB) to seg• topic marker wa • nominative case-marker ga • accusative case-marker wo • conjunctive particle te 2 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 3 https://code.google.com/p/cabocha/ 4 http://www-nlp.stanford.edu/software/ segmenter.shtml We illustrate examples of our pre-reordering approach in Figs. 3 and 4. 79 ment each Chinese sentence. We used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a ba"
W14-7011,J03-1002,0,0.0069743,"gmenter4 (Tseng et al., 2005) with the Chinese Penn Treebank standard (CTB) to seg• topic marker wa • nominative case-marker ga • accusative case-marker wo • conjunctive particle te 2 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 3 https://code.google.com/p/cabocha/ 4 http://www-nlp.stanford.edu/software/ segmenter.shtml We illustrate examples of our pre-reordering approach in Figs. 3 and 4. 79 ment each Chinese sentence. We used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a baseline SMT system. Word alignment was automatically generated by GIZA++6 (Och and Ney, 2003) with the default setting of Moses, and symmetrized by the grow-diagfinal-and heuristics (Koehn et al., 2003). In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring. The language model used in decoding is an interpolated modified Kneser-Ney discounted 5-gram model, trained on the English side of the training corpus by SRILM7 (Stolcke, 2002). In decoding, the distortion-limit was 9. The MERT (Och, 2003) was used to tune the feature weights on the development set and the translation performance was evaluated on the test set with the tuned weights. We used identical"
W14-7011,P03-1021,0,0.0267577,"used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a baseline SMT system. Word alignment was automatically generated by GIZA++6 (Och and Ney, 2003) with the default setting of Moses, and symmetrized by the grow-diagfinal-and heuristics (Koehn et al., 2003). In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring. The language model used in decoding is an interpolated modified Kneser-Ney discounted 5-gram model, trained on the English side of the training corpus by SRILM7 (Stolcke, 2002). In decoding, the distortion-limit was 9. The MERT (Och, 2003) was used to tune the feature weights on the development set and the translation performance was evaluated on the test set with the tuned weights. We used identical decoding settings on development and test sets. Our approach reached a test set BLEU of 28.18 with CTB segmentation in the final evaluation, which had a slight improvement compared with the 28.01 of the organizer’s PB SMT baseline. As to the reordering measure RIBES, our approach reached a score of 0.8087, which had a mediocre improvement compared with the organizer’s 0.7926. In the human evaluation, our approach also had an improv"
W15-3909,P04-1021,0,\N,Missing
W15-3909,P02-1040,0,\N,Missing
W15-3909,P07-1081,0,\N,Missing
W15-3909,W14-4012,0,\N,Missing
W15-3909,W12-4401,0,\N,Missing
W15-3909,2010.iwslt-papers.7,1,\N,Missing
W15-3909,2007.iwslt-1.15,1,\N,Missing
W15-3909,D08-1076,0,\N,Missing
W15-3909,W11-3203,1,\N,Missing
W15-5001,W15-5008,0,0.0448879,"Missing"
W15-5001,W15-5013,0,0.0370801,"Missing"
W15-5001,W15-5002,0,0.0289439,"Missing"
W15-5001,W14-7001,1,0.205867,"ve been submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Evaluation method Evaluation is done both automatically and manually. For human evaluation, WAT uses crowdsourcing, which is low cost and allows multiple evaluations, as the first-stage evaluation. Also, JPO adequacy evaluation is conducted for the selected submissions according to the crowdsourcing evaluation results. Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshop WAT2014(Nakazawa et al., 2014), WAT2015 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We are working toward the practical use of machine translation among all Asian countries. For the 2nd WAT, we adopt new translation subtasks “Chinese-to-Japanese and Koreanto-Japanese patent translation” in addition to the subtasks that were conducted in WAT2014. WAT is unique for the following reasons: 2 Dataset WAT uses the Asian Scientific Paper Excerpt Corpus (ASPEC)1 and JPO Patent Corpus (JPC) 2 as the dataset. 2.1 ASPEC ASPEC is constructed by t"
W15-5001,P11-2093,1,0.853461,"on results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific papers subtask (J ↔ E, J ↔ C); – Patents subtask (C → J, K → J); • Method (SMT, RBMT, SMT and RBMT, EBMT, Other); 10 http://code.google.com/p/mecab/downloads/detail? name=mecab-ipadic-2.7.0-20070801.tar.gz 11 http://nlp.stanford.edu/software/segmenter.shtml 12 https://bitbucket.org/eunjeon/mecab-ko/ 13 https://github.com/moses-smt/mosesdecoder/tree/ RELEASE-2.1.1/scripts/tokenizer/tokenizer.perl 14 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html http://w"
W15-5001,P13-2121,0,0.0884737,"Missing"
W15-5001,2009.iwslt-papers.4,0,0.108842,"em as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignment"
W15-5001,W15-5003,1,0.888058,"Missing"
W15-5001,D10-1092,0,0.23015,"2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005) 8 9 • Subtask: – Scientific paper"
W15-5001,P02-1040,0,0.109103,"the other system parameters. 4 4.2 Automatic Evaluation System The participants submit translation results via an automatic evaluation system deployed on the WAT2015 web page, which automatically gives evaluation scores for the uploaded results. Figure 1 shows the submission interface for participants. The system requires participants to provide the following information when they upload translation results: Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We calculated automatic evaluation scores for the translation results by applying two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). BLEU scores were calculated using multi-bleu.perl distributed with the Moses toolkit (Koehn et al., 2007); RIBES scores were calculated using RIBES.py version 1.02.4 8 . All scores for each task were calculated using one reference. Before the calculation of the automatic evaluation scores, the translation results were tokenized with word segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with Full SVM model 9 and MeCab 0.996 (Kudo, 2005)"
W15-5001,P06-1055,0,0.112959,"we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To obtain word alignments, GIZA++ and growdiag-final-and heuristics were used. We used 5gram language models with mod"
W15-5001,P07-2045,0,0.0208005,"ich is the same system as that at WAT 2014. In addition to the results for the baseline phrasebased SMT system, we produced results for the baseline systems that consisted of a hierarchical phrase-based SMT system, a string-to-tree syntaxbased SMT system, a tree-to-string syntax-based SMT system, seven commercial rule-based machine translation (RBMT) systems, and two online translation systems. The SMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page4 . We used Moses (Koehn et al., 2007; Hoang et al., 2009) as the implementation of the baseline SMT systems. The Berkeley parser (Petrov et al., 2006) was used to obtain syntactic annotations. The baseline systems are shown in Table 3. The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. To"
W15-5001,W15-5006,1,0.742862,"Missing"
W15-5001,W04-3250,0,0.568022,"Missing"
W15-5001,W15-5010,0,0.0468531,"Missing"
W15-5001,W15-5005,0,0.0362327,"Missing"
W15-5001,W15-5012,0,0.0480478,"Missing"
W15-5001,W15-5009,0,0.0622163,"Missing"
W15-5001,2007.mtsummit-papers.63,0,0.08182,"2015. 2015 Copyright is held by the author(s). LangPair ASPEC-JE ASPEC-JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Test 1,812 2,107 LangPair JPC-CJ JPC-KJ Table 1: Statistics for ASPEC. Train 1,000,000 1,000,000 Dev 2,000 2,000 DevTest 2,000 2,000 Test 2,000 2,000 Table 2: Statistics for JPC. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by the NICT from approximately 2 million Japanese-English scientific paper abstracts owned by the JST. Because the abstracts are comparable corpora, the sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score and the field symbol. The similarity scores are calculated by the method from (Utiyama and Isahara, 2007). The field symbols are single letters A-Z and show the scientific field for each document3 . The correspondence between the symbols and field names, along with the frequency and occurrence ratios for the training data, are given in the README file from ASPECJE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts owned by JST that are not contained in the tr"
W15-5001,W15-5011,0,0.0304094,"Missing"
W15-5001,W15-5007,0,0.153198,"Missing"
W15-5004,N03-1017,0,0.0147896,"tactic analysis at all, while the DEP-REO makes a good use of the dependency structure of Japanese sentences. As both approaches have been described in detail in their original papers, We do not give repeated descriptions but just state several details in experiments. For DEP-REO, the processes were completely identical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-ba"
W15-5004,P07-2045,0,0.00623357,"16.08 17.58 18.02 17.78 17.54 RIBES .6356 .6512 .6520 .6545 .6488 .6497 .6499 .6556 .6586 .6707 .6751 .6733 .6691 Table 1: Devtest set BLEU score and RIBES on JE translation. +Lex.-Reo. −Lex.-Reo. + Bracket Balanc. DL 0 3 6 0 0 BLEU 66.79 66.64 66.80 66.74 66.98 RIBES .9222 .9221 .9228 .9221 .9224 Table 2: Devtest set BLEU score and RIBES on KJ translation (morpheme level, by MeCab). guage model training (interpolated modified Kneser-Ney discounting; 5-gram on English for JE translation and 9-gram on Japanese for KJ translation). Experiment and Evaluation We used the PB SMT system in Moses12 (Koehn et al., 2007) for JE and KJ translation tasks. Basically, we used identical settings as the organizer used in the baseline. However, there were several differences as follows. • We used MeCab (IPA) and CaboCha to process Japanese sentences in JE translation. • We used no tools for Korean and Japanese morphological analysis in KJ translation, instead, the max-phrase-length were set to 9 in translation model training. • We used SRILM13 (Stolcke, 2002) for lan4 otherwise the reordering will become excessive. i.e., the ordinary comma. 6 “fullwidth comma”, the Chinese comma. 7 “ideographic comma”, the Japanese"
W15-5004,W02-2016,0,0.0984305,"fficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io/mecab/ 3 http://taku910.github.io/cabocha/ 2 42 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 42‒47, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). (Kudo and Matsumoto, 2002) based on IPA system for Japanese morphemes was used. For REVREO, an important point is to avoid the reordering across punctuations4 . In the experiments, we used four marks to compose the punctuation set: U+002C5 , U+FF0C6 , U+30017 , and U+30028 . For the Japanese topic marker wa, which plays the key role of the approach, we did not judge it only by the surface form, but also referred to the specific tag joshi, kakarijoshi9 . 3 BASELINE +DEP-REO +REV-REO Character-based KJ Translation As Korean and Japanese share so many similar features, we tried a purely character-based approach in WAT 201"
W15-5004,P13-4016,0,0.0155952,"but just state several details in experiments. For DEP-REO, the processes were completely identical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io"
W15-5004,W14-7002,0,0.0118816,"dentical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io/mecab/ 3 http://taku910.github.io/cabocha/ 2 42 Proceedings of the 2nd Workshop on Asian Tr"
W16-2711,I08-8003,1,0.781894,"Agtarbidir. 1 • A target-bidirectional agreement model was employed. • Ensembles of neural networks were used rather than just a single network. • The ensembles were selected from different training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower th"
W16-2711,W12-4406,1,0.904887,"Missing"
W16-2711,W15-3909,1,0.851947,"eneration is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically from the character sequence representation supplied for the shared tasks, with no language specific treatment for any of the language pairs. Furthermore no preprocessing was performed on any of"
W16-2711,P07-1081,0,0.524071,"Missing"
W16-2711,P04-1021,0,0.182904,"training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically"
W16-4606,N12-1047,0,0.0247922,"in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer gramma"
W16-4606,P05-1033,0,0.657321,"bles high quality images to be formed.]] Figure 1: Example of sublanguage-specific bilingual sentences requiring global reordering. A, B, C are the sentence segments constituting global sentence structures. was combined with syntactic pre-ordering. A statistically significant improvement was observed against the syntactic pre-ordering alone, and a substantial gain of more than 25 points in RIBES score against the baseline was observed for both Japanese-to-English and English-to-Japanese translations, and the BLEU scores remained comparable. 2 Related Work The hierarchical phrase-based method (Chiang, 2005) is one of the early attempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation"
W16-4606,N15-1105,0,0.0599862,"Missing"
W16-4606,2015.mtsummit-papers.1,1,0.799688,"method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. This paper describes a novel global reordering method for capturing sublanguage-specific global sentence structure to supplement the performance of conventional syntactic reordering. The method learns a global pre-ordering model from non-annotated corpora without using syntactic parsing and uses this model to perform global pre-ordering on newly"
W16-4606,D08-1089,0,0.0313098,"hen reorder these detected segments globally. In step (ii), we experiment with a detection method based on heuristics, as well as a method based on machine learning. Steps (i) and (ii) are described in the following subsections. 3.1 Extraction of Sentence Pairs Containing Global Reordering We extract sentences containing global reordering from the training corpus and store them in the global reordering corpus; they can subsequently be used for training and prediction. We consider that a sentence pair contains global reordering if the segments in the target sentence appear in swap orientation (Galley and Manning, 2008) to the source segments, when the sentences are divided into two or three segments each. Figure 2 shows an example of a sentence pair involving global reordering with the sentence divided into three segments. We take the following steps: 1. We divide each source and target sentence into two or three segments. The candidate segments start at all possible word positions in the sentence. Here, a sentence pair consisting of K segments is represented as (ϕ1 , ϕ2 · · · ϕK ), where ϕk consists of the k th phrase of the source sentence and αk th phrase of the target sentence. These segments meet the s"
W16-4606,P13-2121,0,0.0130852,"sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the b"
W16-4606,P15-2023,0,0.0136127,"tactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of the finding that sublanguage-specific global structures can be detecte"
W16-4606,W15-3058,0,0.0128803,"g global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respectively, listing the RIBES and BLEU scores computed for each of the four reordering configurations. The numbers in the brackets refer to the improvement over the baseline phrase-based SMT with a distortion limit of six. A substantial gain of more than 25 points in the RIBES scores compared to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89"
W16-4606,D10-1092,0,0.0355675,"Missing"
W16-4606,W10-1736,0,0.120586,"tructure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of th"
W16-4606,P07-2045,0,0.0042033,"ponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berke"
W16-4606,W04-3250,0,0.0354322,"ed to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89 Table 1: Evaluation of Japanese-to-English translation where glob-pre denotes global pre-ordering and pre denotes conventional syntactic pre-ordering, dl denotes distortion limit, HPB denotes hierarchical phrase-based SMT and TDBTG denotes reordering based on top-down bracketing transduction grammar. The bold numbers indicate a statistically insignificant difference from the best system performance according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 44.9 17.9 T1 PB dl=20 53.7 (+8.8) 21.3 (+3.4) HPB 54.9 (+10.0) 23.1 (+5.2) √ PB dl=6 heuristic 61.7 (+16.8) 19.6 (+1.7) T2 PB dl=6 SVM 61.0 (+16.1) 19.3 (+1.4) √ PB dl=6 TDBTG 64.6 (+19.7) 22.3 (+4.4) T3 PB dl=6 syntactic 64.9 (+20.0) 25.5 (+7.6) √ √ PB dl=6 heuristic syntactic 71.3 (+26.4) 25.3 (+7.4) T4 PB dl=6 SVM syntactic 72.1 (+27.2) 25.6 (+7.7) T1 T2 T3 T4 Table 2: Evaluation of English-to-Japanese translation Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 43.2 27.9"
W16-4606,2006.iwslt-evaluation.11,1,0.718282,"ge to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our p"
W16-4606,E91-1054,0,0.534414,"and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence st"
W16-4606,A00-2002,0,0.0838799,"unction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skele"
W16-4606,2006.eamt-1.24,0,0.0750942,"Missing"
W16-4606,P15-1021,0,0.0168744,"ts of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer grammar (TDBTG) (Nakagawa, 2015). We use the output of mkcls and SyMGIZA++ obtained during the preparation of the baseline SMT for training TDBTG-based reordering. Global pre-ordering Global pre-ordering consists of the detection of segment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sen"
W16-4606,P02-1040,0,0.101581,"egment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sentence pairs containing global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respective"
W16-4606,2007.mtsummit-papers.63,1,0.694872,"am for Japanese input. Figure 5 shows the same for English input. The accuracy is the average accuracy of a ten-fold cross-validation for the global reordering corpus. From the calibration shown in the tables, we select the settings producing the highest prediction accuracy, namely, a value of f ive for the n of n-grams and a size of 100k for the global reordering corpus, for both Japanese and English inputs. 4.3 Translation Experiment Setup Data As our experimental data, we use the Patent Abstracts of Japan (PAJ), the English translations of Japanese patent abstracts. We automatically align (Utiyama and Isahara, 2007) PAJ with the corresponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Mose"
W16-4606,P14-2092,0,0.076127,"In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. Thi"
W16-4606,P01-1067,0,0.249784,"tempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extra"
W16-4611,N12-1047,0,0.0149631,"ulti-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of the bilingual corpora using KenLM (Heafield et al., 2013). In addition, we included the Google n-gram language models for Japanese and English as the external knowledge. These are back-off models estimated using maximum likelihood. The Japanese model was constructed from Web Japanese N-gram Version 1,3 and the English model was constructed from Web 1T 5-gram Version 1 (LDC2006T13). For optimization, we used k-best batch MIRA (Cherry and Foster, 2012). 3.3 Translation The decoder used here is a clone of the Moses PBSMT decoder. It accepts feature augmentation, i.e., it can use multiple submodels and set an empty value. 2 This preorderer modifies word order based on parse trees output by the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). 3 http://www.gsk.or.jp/catalog/gsk2007-c/ 129 Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 34.58 35.64 35.68 JPC En-Ja Ja-Zh 38.06 33.35 38.61 34.27 39.03 34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model"
W16-4611,P11-2031,0,0.0123879,"34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 35.12(-) 36.22 36.29 JPC En-Ja Ja-Zh 37.40(-) 31.96(-) 38.03(-) 32.92(-) 38.48 33.36 Zh-Ja 38.15(-) 39.68(-) 39.85 Table 4: BLEU Scores on JPO Corpus (MultEval scores) 4 Experimental Results For evaluation, we used two toolkits based on BLEU (Papineni et al., 2002). One is the official BLEU scores provided by the WAT2016 committee. Because the official tool cannot measure a significance level of two systems, we also used the MultEval tool (Clark et al., 2011), which can measure significance levels based on bootstrap resampling. Since we represent the mean scores of three optimizations, the MultEval scores differ from the official scores. 4.1 JPO Corpus (without External Knowledge) For JPO corpus experiments, we did not use external knowledge and compared translations of the singledomain model, corpus concatenation, and domain adaptation. The JPO corpus was divided into four domains (chemistry, electricity, machine, and physics). Tables 3 and 4 show the results evaluated by the official scorer and MultEval tools, respectively. The symbol (-) indica"
W16-4611,P07-1033,0,0.175276,"Missing"
W16-4611,P13-2121,0,0.0452534,"n Grammar (TDBTG) trained by the JPO corpus as the preorderer without external knowledge. For the preorderer with external knowledge, we used the one developed in-house (Chapter 4.5 of Goto et al. (2015)),2 which was tuned for patent translation. 3.2 Training and Optimization We used the Moses toolkit (Koehn et al., 2007) to train the phrase tables and lexicalized reordering models. We used multi-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of the bilingual corpora using KenLM (Heafield et al., 2013). In addition, we included the Google n-gram language models for Japanese and English as the external knowledge. These are back-off models estimated using maximum likelihood. The Japanese model was constructed from Web Japanese N-gram Version 1,3 and the English model was constructed from Web 1T 5-gram Version 1 (LDC2006T13). For optimization, we used k-best batch MIRA (Cherry and Foster, 2012). 3.3 Translation The decoder used here is a clone of the Moses PBSMT decoder. It accepts feature augmentation, i.e., it can use multiple submodels and set an empty value. 2 This preorderer modifies word"
W16-4611,2016.amta-researchers.7,1,0.899683,"tem employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality. 1 Introduction In this paper, we describe the NICT-2 translation system for the 3rd Workshop on Asian Translation (WAT2016) (Nakazawa et al., 2016a). The proposed system employs Imamura and Sumita (2016)’s domain adaptation technique, which improves translation quality using other domain data when the target domain data is insufficient. The method employed in this paper assumes multiple domains and improves the quality inside the domains (cf., Section 2). For WAT2016, the Japan Patent Office (JPO) Corpus can be regarded as multi-domain data because it includes chemistry, electricity, machine, and physics patents with their domain ID, and thus it is suitable for observing the effects of domain adaptation. WAT2016 provides the JPO corpora in Japanese and English (Ja-En), Japanese and Chinese (J"
W16-4611,N03-1017,0,0.0372434,"wc and wi denote subvectors of the weight vector w. Φc (e, f ) and Φi (e, f ) are feature functions that return feature subvectors (cf., Section 2.2). 2 Domain Adaptation We used the domain adaptation method proposed by Imamura and Sumita (2016). This method adapts a weight vector by feature augmentation (Daum´e, 2007) and a feature vector using a corpus-concatenated model. Since this method only operates in feature space, it can be applied to various translation strategies, such as tree-to-tree translation. In this study, we applied it to phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003; Koehn et al., 2007). 2.1 Adaptation of Weight Vector by Feature Augmentation Most statistical machine translation employs log-linear models that interpolate feature function values obtained from various submodels, such as phrase tables and language models (LMs). The likelihood of a translation is computed as follows: log P (e|f ) ∝ w · h(e, f ), (1) where h(e, f ) denotes a feature vector and w denotes its weight vector. Figure 1 shows a feature space structure of feature augmentation. When we translate texts of D domains, the feature space is segmented into D + 1 subspaces: common, domain 1"
W16-4611,P07-2045,0,0.0275413,"vectors of the weight vector w. Φc (e, f ) and Φi (e, f ) are feature functions that return feature subvectors (cf., Section 2.2). 2 Domain Adaptation We used the domain adaptation method proposed by Imamura and Sumita (2016). This method adapts a weight vector by feature augmentation (Daum´e, 2007) and a feature vector using a corpus-concatenated model. Since this method only operates in feature space, it can be applied to various translation strategies, such as tree-to-tree translation. In this study, we applied it to phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003; Koehn et al., 2007). 2.1 Adaptation of Weight Vector by Feature Augmentation Most statistical machine translation employs log-linear models that interpolate feature function values obtained from various submodels, such as phrase tables and language models (LMs). The likelihood of a translation is computed as follows: log P (e|f ) ∝ w · h(e, f ), (1) where h(e, f ) denotes a feature vector and w denotes its weight vector. Figure 1 shows a feature space structure of feature augmentation. When we translate texts of D domains, the feature space is segmented into D + 1 subspaces: common, domain 1, · · · domain D. A f"
W16-4611,P15-1021,0,0.0169397,"Moses Toolkit Moses Toolkit - Table 2: Summary of Preprocessing, Training, and Translation Optimization Imamura and Sumita (2016) proposed joint optimization and independent optimization. We employ independent optimization, which can use existing optimizers. 3 System Description In this section, we describe the preprocessing, training, and translation components of the proposed system (Table 2). 3.1 Preprocessing Preprocessing is nearly the same as the baseline system provided by the WAT2016 committee. However, preorderers are added because our system is phrase-based with preordering. We used Nakagawa (2015)’s Top-Down Bracketing Transduction Grammar (TDBTG) trained by the JPO corpus as the preorderer without external knowledge. For the preorderer with external knowledge, we used the one developed in-house (Chapter 4.5 of Goto et al. (2015)),2 which was tuned for patent translation. 3.2 Training and Optimization We used the Moses toolkit (Koehn et al., 2007) to train the phrase tables and lexicalized reordering models. We used multi-threaded GIZA++ for word alignment. For the language models of the corpus-concatenated and single-domain models, we constructed 5gram models from the target side of t"
W16-4611,W16-4601,0,0.0611743,"Missing"
W16-4611,L16-1350,1,0.872835,"Missing"
W16-4611,P02-1040,0,0.0945046,"v and Klein, 2007). 3 http://www.gsk.or.jp/catalog/gsk2007-c/ 129 Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 34.58 35.64 35.68 JPC En-Ja Ja-Zh 38.06 33.35 38.61 34.27 39.03 34.64 Zh-Ja 39.54 40.96 41.09 Table 3: BLEU Scores on JPO Corpus (official scores) Method Single-Domain Model Corpus Concatenation Domain Adaptation Ja-En 35.12(-) 36.22 36.29 JPC En-Ja Ja-Zh 37.40(-) 31.96(-) 38.03(-) 32.92(-) 38.48 33.36 Zh-Ja 38.15(-) 39.68(-) 39.85 Table 4: BLEU Scores on JPO Corpus (MultEval scores) 4 Experimental Results For evaluation, we used two toolkits based on BLEU (Papineni et al., 2002). One is the official BLEU scores provided by the WAT2016 committee. Because the official tool cannot measure a significance level of two systems, we also used the MultEval tool (Clark et al., 2011), which can measure significance levels based on bootstrap resampling. Since we represent the mean scores of three optimizations, the MultEval scores differ from the official scores. 4.1 JPO Corpus (without External Knowledge) For JPO corpus experiments, we did not use external knowledge and compared translations of the singledomain model, corpus concatenation, and domain adaptation. The JPO corpus"
W16-4611,P06-1055,0,0.0716582,"Missing"
W16-4611,N07-1051,0,\N,Missing
W16-4613,N12-1048,0,0.152918,"Missing"
W16-4613,2014.iwslt-papers.8,1,0.818639,"ines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Sto"
W16-4613,2015.iwslt-evaluation.9,0,0.106224,"Missing"
W16-4613,2005.mtsummit-papers.11,0,0.0404732,"imental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior"
W16-4613,D10-1018,0,0.31642,"Missing"
W16-4613,2005.iwslt-1.19,0,0.666287,"m model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Stolcke and others, 2002) tools. The machine translation system was an in-house phrasebased system that pre-ordered the input. 4.2 Experimental Results The performance of the interpretation systems using different sentence segmenters is presented in Table 2. The following observations can"
W16-4613,2006.iwslt-papers.1,0,0.207774,"roparl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in t"
W16-4613,P14-2090,0,0.0468592,"k based on confidence scores, denoted as bi . The final output is a segmented sentence, e.g. w0 , · · · , wi . The proposed segmenters work in an online manner as follows: words are input one by one. The sequence of input words and the derived confidence scores are maintained as states. Once an word is input, its confidence score is calculated and added into the sequence (which is labeled as a in Figure 2). Then a segmentation strategy is applied to the sequence (labeled as (b) in Figure 2). In case that the 1 Fujita et al. (2013)’s method may work on word streams without sentence boundaries; Oda et al. (2014)s’ segmentation model uses linear SVMs and local features extracted from just three word lookahead, so it might be adapted. 141 Algorithm 1 Online Sentence Segmenter Require: w0 , w1 , w2 , . . ., 1: W ← []; S ← [] 2: for wk in stream of words do 3: W ← W + [wk ] 4: sk−1 ← confidence of segmenting before wk 5: S ← S + [sk−1 ] 6: B ← apply segmentation strategy to S 7: if bi = 1 (0 ≤ i ≤ k − 1) then 8: output [w0 , w1 , . . . , wi ] as a segment 9: remove first i elements from W and S 10: end if 11: end for ⊲ assume W = [w0 , w1 , . . . , wk−1 , wk ] ⊲ assume S = [s0 , s1 , . . . , sk−1 ] ⊲ ass"
W16-4613,P02-1040,0,0.101875,"and MT, yet most of them require a long context of future words that follow sentence boundaries. In addition, they are often computationally expensive. These shortages make them unattractive for use in simultaneous interpretation. To the best of our knowledge, there are no published ready-to-use online sentence segmenters, and this motivated this paper. The proposed method is crafted in a way that requires little computation and minimum future words in order to achieve efficiency. Also the proposed method is directly optimized against the widely used measurement of translation quality – BLEU (Papineni et al., 2002) – in order to achieve effectiveness. We believe that this work can directly contribute to the development of real-world simultaneous interpretation systems. The main contributions of this paper are, • proposing a segment boundary confidence score; 139 Proceedings of the 3rd Workshop on Asian Translation, pages 139–148, Osaka, Japan, December 11-17 2016. Figure 1: Illustration of Online Sentence Segmenter in Simultaneous Interpretation System • proposing a hybrid online sentence segmenter; • an empirical study and analysis of the proposed method on two translation tasks. The rest of this paper"
W16-4613,2011.iwslt-papers.7,0,0.0928464,"tegy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation. 1 Introduction Simultaneous interpretation performs spoken language translation in a online manner. A spoken language translation system automatically translates text from an automatic speech recognition (ASR) system into another language. Spoken language translation itself is an important application of machine translation (MT) because it takes one of the most natural forms of human communication – speech – as input (Peitz et al., 2011). Simultaneous interpretation is even more demanding than spoken language translation because the processing must occur online. Simultaneous interpretation can bridge the language gap in people’s daily lives transparently because of its ability to respond immediately to users’ speech input. Simultaneous interpretation systems recognize and translate speech at the same time the speakers are speaking, thus the audience can hear the translation and catch the meaning without delay. Potential applications of simultaneous interpretation include interpreting speeches and supporting cross-lingual conv"
W16-4613,takezawa-etal-2002-toward,1,0.387634,"95,054 103,638 95,176 Table 1: Experimental Corpora.† Including punctuations. 4 Experiments 4.1 Experimental Settings Experiments were performed on translation between Japanese and English in both directions. The word orders of these two languages are very different, thus long-distance reordering is often obligatory during translation. This makes simultaneous interpretation a very challenging task, and therefore we choose this language pair for experiments. The experimental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines"
W16-4613,I13-1141,0,0.0397647,"Missing"
W16-4614,W07-0705,0,0.0139778,"e observation, we think the Thai-Laotian and Malay-Indonesian pairs can be handled simultaneously and harmoniously in further research, including corpus annotation, technique development, and NLP applications. The remaining of the paper is arranged as following. In section 2, we introduce the background of the languages discussed in this paper. In section 3, we describe the experiment settings used and discuss the numerical results obtained. Section 4 concludes the paper and provides our future work. 2 Background Specific approaches to process similar languages is an interesting topic in NLP (Vilar et al., 2007; Ding et al., 2015). In this research direction, a priori knowledge of the languages is required and specific approaches can thus be designed by taking advantage of the similarities to outperform a general approach. Here we provide outlines of linguistic features of the four languages mentioned in this paper. 1 http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/index.html 149 Proceedings of the 3rd Workshop on Asian Translation, pages 149–156, Osaka, Japan, December 11-17 2016. Thai Malay คุ ณ จะ ทำ อ ยำ ง ไร ? apa yang akan anda lakukan ? ທ າ ນ ຈະ ເຮັ ດ ແນ ວ ໃດ ? apa yang akan anda laku"
W16-4614,J93-2003,0,\N,Missing
W17-5705,J82-2005,0,0.74764,"Missing"
W17-5705,P11-1022,0,0.0408254,"Missing"
W17-5705,W16-2378,0,0.0116472,"0.584 0.480 Table 12: Results for the APE task. Method Raw MT output (a) APE w/ gold data only (b) (a) + bitext back-off (c) (b) + pseudo training data Ja→En 43.74 43.38 44.00 43.90 BLEU (↑) Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly"
W17-5705,2011.mtsummit-papers.35,0,0.112321,"Missing"
W17-5705,N16-1059,0,0.079743,"ting HTER score, irrespective of the evaluation metrics: Pearson’s correlation coefficient r, mean average error (MAE), and root mean squared error (RMSE). Given a pair of source text (src) and MT output (hyp), the task of sentence-level QE is to predict how good the entire hyp is, with respect to src. We conducted experiments on both of the HTER prediction and binary classification tasks. 4.3.1 Prediction of HTER In WMT, this task is to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of src and hyp. First, word embeddings with 300 dimensions were learned from each part of the DLC corpus u"
W17-5705,P09-5002,0,0.0164932,"lation (ref ) 4. Manual grading of MT output (grade) 5. Manual post-editing of MT output (pe) For the latter three tasks (detailed in Sections 2.3, 2.4 and 2.5, respectively), we allocated adult native speakers of the target language who also understand Japanese. 2.1 Collecting Japanese utterances (src) First, we collected the following two sets of utterances in Japanese that have been used with our speech translation service. 2.2 Generation of MT outputs (hyp) The collected Japanese segments (src) were then translated by our in-house MT systems, which implement a phrase-based statistical MT (Koehn, 2009). The Ja→En translations were obtained in 2013, with the system trained on 736k sentence pairs. The Ja→Zh and Ja→Ko translations were generated later in 2016, with the systems trained on 1.44M and 1.40M sentence pairs, respectively. Table 1 summarizes the statistics of src and hyp. These segments are relatively shorter than sentences in written texts, such as news articles and patent documents. Travel-related utterances (travel): From the log data that our speech translation service accumulates, we randomly sampled 20,000 identical transcribed segments5 that were identified as Japanese by its"
W17-5705,W15-3001,0,0.0220631,"orpus using Moses. 4.2 Word-level QE (WQE) Given a pair of source text (src) and MT output (hyp), the task of word-level QE is to predict a sequence of tags with the same length as hyp, where each tag indicates how good the corresponding word in hyp is. While some previous studies, such as Bach et al. (2011), addressed to gauge the quality of each word with a real-valued score, WMT adopted a coarse-grained binary tag, i.e., {OK, BAD}, presumably because this form of tags can be automatically generated as the byproduct of computing HTER score by comparing hyp with its post-edited version (pe) (Bojar et al., 2015). Following the recent convention in WMT, we automatically generated a sequence of binary tags for each pair of src and hyp using TERCOM. As the evaluation metrics, we used F1 score of detecting “OK” tags (F1 -OK), that for “BAD” tags (F1 -BAD), and their product (F1 -mult) as in Bojar et al. (2016). As a system for WQE, we adopted an implementation15 based on a feed-forward neural net15 Ja 274,746 14,388 Step 2. Japanese sentences in the remaining half of the DLC corpus were decoded by the MT systems. Step 3. Tag sequences for the MT outputs were given in the same manner as the manually creat"
W17-5705,P17-2062,1,0.874248,"Missing"
W17-5705,P02-1040,0,0.0983965,"Zh Ja→Ko 26.18 38.85 69.44 81.98 39.73 49.11 30.38 51.01 86.45 93.52 34.29 54.16 • If the grade is either “S” or “A” but pe is not identical to the given hyp, both grading and post-editing are performed again. TER (↓) Ja→Zh 50.81 19.20 38.79 48.54 8.63 43.78 Ja→Ko 43.43 12.25 34.75 32.44 4.12 30.00 First, the results of manual grading are summarized in Table 3. While MT outputs for the travel domain were much better than the hospital domain in the Ja→En task, the segments in the hospital domain were better translated by the Ja→Zh and Ja→Ko MT systems. Table 4 shows proximity in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), between translations obtained through different ways. (a) “hyp against ref ” presents what is measured in standard evaluation of MT outputs. The scores in these rows reflect the distribution of MT outputs shown in Table 3. On the other hand, (b) “hyp against pe” gauges the amount of post-edits. As we asked to perform only necessary edits to assure at least grade “A,” the scores in these rows should be good in general. Only the exception is the hospital domain in the Ja→En task. As most of the MT outputs were of low quality, the workers tended to abandon them rat"
W17-5705,potet-etal-2012-collection,0,0.0493193,"Missing"
W17-5705,P16-1009,0,0.134765,"Missing"
W17-5705,W16-2392,0,0.107576,"to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of src and hyp. First, word embeddings with 300 dimensions were learned from each part of the DLC corpus using word2vec17 with its default parameters. Then, the embedding for a given segment is computed by averaging the embeddings of its constituent words, assuming the additive compositionality (Mikolov et al., 2013). During the computation, unknown words were mapped to a zero vector. Finally, values for each of 300 dimensions were regarded as additional features. 4.3.2 Binary Classification We assume that users of speech translation servi"
W17-5705,N07-1064,0,0.0397932,") Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly improved the naive baseline system (a), but the translation quality was not consistently better depending on the language pair. Finally, we introduced in the third syst"
W17-5705,W07-0728,0,0.0298995,") Ja→Zh Ja→Ko 73.14 85.52 72.28 84.87 73.01 85.53 73.15 85.57 Ja→En 42.21 42.33 41.87 41.95 Ja→En 0.517 0.438 0.563 TER (↓) Ja→Zh 16.98 17.53 17.05 16.97 F1 -OK (↑) Ja→Zh Ja→Ko 0.711 0.817 0.667 0.770 0.702 0.825 Ja→Ko 9.87 10.31 9.87 9.82 4.4 APE 5 Conclusion The task of APE is to automatically post-edit MT outputs (hyp). Although there are a number of methods that also refer to src (B´echara et al., 2011; Junczys-Dowmunt and Grundkiewicz, 2016), we have so far examined only classic baseline methods based on phrase-based statistical MT. The first system (a) was trained only on the gold data (Simard et al., 2007a) using Moses. However, this system tended to deteriorate the translation quality in terms of BLUE and TER, presumably due to the scarcity of training data. Then, our second model (b) introduced identical pairs of sentences in the target side of our DLC corpus in order to conservatively retain grammatical fragment within hyp. By (re-)decoding the hyp using the multiple decoding path ability of Moses,20 this model significantly improved the naive baseline system (a), but the translation quality was not consistently better depending on the language pair. Finally, we introduced in the third syst"
W17-5705,2006.amta-papers.25,0,0.299193,"oviders, such as doctors and nurses, and patients, containing 2,225 identical segments of utterances. They were surely spoken language, although they were manually written and more formal than those in the travel domain. Procedure of Corpus Construction We have created our QE/APE datasets, regarding Japanese as the source language. We have so far regarded English, Chinese, and Korean as the target languages, considering that the speakers of these languages hold the largest proportion of visitors to Japan (Japan National Tourism Organization, 2017). Following the procedure in previous studies (Snover et al., 2006; Potet et al., 2012) and practices in WMT (Bojar et al., 2014, 2015, 2016), we determined the following five-step process. We have been examining the installation of our speech translation service into several practical situations where such system helps cross-lingual communication between humans. For this purpose, we have manually created role-play dialogs between Japanese and non-Japanese speakers. The hospital data is one of them. The extracted segments, especially those in the travel domain, include ungrammatical ones, nonunderstandable ones, and those containing inappropriate expressions"
W17-5705,P15-4020,0,0.0854108,"justifies that sentence embeddings obtained by such a naive way19 can improve the performance of predicting HTER score, irrespective of the evaluation metrics: Pearson’s correlation coefficient r, mean average error (MAE), and root mean squared error (RMSE). Given a pair of source text (src) and MT output (hyp), the task of sentence-level QE is to predict how good the entire hyp is, with respect to src. We conducted experiments on both of the HTER prediction and binary classification tasks. 4.3.1 Prediction of HTER In WMT, this task is to predict the HTER score, directly from (src, hyp) pair (Specia et al., 2015), or indirectly through predicting the necessary edits in a similar manner to WQE (Kim and Lee, 2016). We implemented a tool to extract a set of 17 features16 of QuEst++ (Specia et al., 2015), which is regarded as the baseline of this task. To compute the features based on language models, we used the corresponding part of the DLC corpus. To estimate the translation-related features, such as the number of translations per word in src, we trained a phrase-table on the DLC corpus using Moses. Following the findings in Shah et al. (2016), we also incorporated the distributed representations of sr"
W17-5705,W14-3302,0,\N,Missing
W17-5705,W16-2301,0,\N,Missing
W17-5705,W17-4717,0,\N,Missing
W17-5711,W08-0336,0,0.0134164,"enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the highest when the n-best size was 5, and slowly decreased when the size increased above 5. 3.2 Optimal Size of N-best List To output n-best translations using the beam search, beam width is better to set equal or more than n. In our experiments, we set the beam width"
W17-5711,P07-2045,0,0.0174661,"6.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the highest when the n-best size was 5, and slowly decreased when the size increased above 5. 3.2 Optimal Size of N-best List To output n-best translations using the beam search,"
W17-5711,P17-4012,0,0.017969,"Japanese and then translated into English. The byte-pair encoding (Sennrich et al., 2016) rules were acquired from a training set of each corpus, and they were applied to the training, development, and test sets. The number of sub-word types is 34–35k in the JIJI Corpus and 20–21k in the MED Corpus. We used sentences with 80 or fewer sub-words for training. Preprocessing, Postprocessing: Table 3 shows a summary of our system. As shown in the table, we used the same preprocessing and postprocessing steps as the WAT baseline systems (Nakazawa et al., 2016a). Translation System: We used OpenNMT (Klein et al., 2017)1 as the base translation system. The encoder comprises a two-layer bi-directional LSTM (long short-term memory), in which the number of units is 500 each. The decoder comprises a two-layer LSTM (1000 Experiments Using Small Data Sets We perform Japanese-English translation experiments using small data (with approximately 200k sentences) to clarify characteristics of the ensemble and the bi-directional reranking approaches. 1 129 http://opennmt.net/ Preprocessing Training and Translation Postprocessing Character Normalization Tokenizer TrueCaser Byte Pair Encoding System Encoder Decoder Attent"
W17-5711,W04-3230,0,0.251116,"Missing"
W17-5711,N16-1046,1,0.898576,"Missing"
W17-5711,D15-1166,0,0.0281317,"Tokenizer Japanese English Chinese NFKC Normalization of Unicode MeCab (Kudo Moses Toolkit Stanford Segmenter (CTB) et al., 2004) – Moses Toolkit – In-house Encoder OpenNMT (modified for right-to-left decoding and the ensemble method) Word embedding: 500 units, two-layer Bi-LSTM (500 + 500 units) Word embedding: 500 units, two-layer LSTM (1,000 units) Global Attention Mini Batch Size:64, SGD Optimization (10+6 epochs), Dropout:0.3 Beam Width:5 (c.f., Sec. 3.2) – Moses Toolkit – WAT Official’s Moses Toolkit WAT Official’s Table 3: Summary of the NICT-2 NMT System 18.0 units). Global Attention (Luong et al., 2015) was utilized. We used the stochastic gradient descent (SGD) method for the optimization. The learning rate was 1.0 for the first ten epochs, and then annealing was performed for six epochs while decreasing the learning rate by half. To implement the methods described in Section 2.3, we modified OpenNMT as follows. 17.5 BLEU Score 17.0 16.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-b"
W17-5711,W16-4601,0,0.0305515,"Missing"
W17-5711,L16-1350,1,0.890132,"Missing"
W17-5711,N04-1021,0,0.141948,"Missing"
W17-5711,P02-1040,0,0.0980273,"was performed for six epochs while decreasing the learning rate by half. To implement the methods described in Section 2.3, we modified OpenNMT as follows. 17.5 BLEU Score 17.0 16.5 16.0 left-to-right right-to-left 15.5 • We enabled the ensemble in the translator. • We enabled right-to-left decoding in the trainer and translator. bi-directional reranking 15.0 0 5 10 15 20 N-best Size (=Beam Width) Figure 2: BLEU Scores According to N-best Size The n-best size for the reranking was determined by the experiment in Section 3.2. Evaluation: Of the WAT official evaluation metrics, we employ BLEU (Papineni et al., 2002) for the evaluation. WAT official scores are changed by word segmenters. In this paper, we use JUMAN (Kurohashi et al., 1994) for Japanese, Moses tokenizer (Koehn et al., 2007) for English, and Stanford Word Segmenter (Chinese Penn Treebank Model) (Chang et al., 2008) for Chinese evaluation. In all methods, the BLEU scores changed according to the size of the n-best list. For left-toright and right-to-left decoding, the BLEU scores were highest when the n-best size was 4, and the scores decreased when the n-best size increased above 4. After the bi-directional reranking, the BLEU score was the"
W17-5711,P16-1162,0,0.0416405,"as small data sets. The first is the JIJI Corpus, which consists of newswires. Japanese and English articles were automatically aligned sentence by sentence. Note that the translations are sometimes not literal because the original articles were not translated sentence by sentence. The second is the corpus of pseudo-dialogues at hospitals (MED Corpus). This corpus is a collection of conversations between patients and hospital staffs, which were created by writers (developed in-house). The pseudo-dialogues were first written in Japanese and then translated into English. The byte-pair encoding (Sennrich et al., 2016) rules were acquired from a training set of each corpus, and they were applied to the training, development, and test sets. The number of sub-word types is 34–35k in the JIJI Corpus and 20–21k in the MED Corpus. We used sentences with 80 or fewer sub-words for training. Preprocessing, Postprocessing: Table 3 shows a summary of our system. As shown in the table, we used the same preprocessing and postprocessing steps as the WAT baseline systems (Nakazawa et al., 2016a). Translation System: We used OpenNMT (Klein et al., 2017)1 as the base translation system. The encoder comprises a two-layer bi"
W17-5712,P11-2027,0,0.0225845,"https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and ensemble models, we also calc"
W17-5712,D10-1092,1,0.7921,"W P = 0.75. 2 Model Ensembling https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and"
W17-5712,W16-4601,0,0.192201,"The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture. 1 Introduction Pr(e|f ) = 2.1 Pr(et |e<t , f ), (1) t=1 Neural machine translation (NMT) methods became one of the main-stream techniques in current machine translation studies. Previous WAT campaign showed that NMT methods can achieve higher translation accuracy in spite of simple model configurations (Nakazawa et al., 2016a). In this year, we chose the NMT architecture as our translation systems submitted for WAT2017 English-Japanese Scientific Paper Translation Task (Nakazawa et al., 2017). The main translation model is constructed by an encoder-decoder model (Sutskever et al., 2014) enforced by an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). This paper describes the details of our system, including whole model architecture, training criteria, decoding strategy, and data preparation. Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT"
W17-5712,L16-1350,1,0.888778,"Missing"
W17-5712,P02-1040,0,0.104299,"ystem penalizes shorter sentences, and tends to generate longer sentences. Note that if the beam width is 1, there is no effect from word penalty, because the translation system can generate only 1-best results. Results We trained all translation systems varied by model/training/tokenization hyper-parameters described in previous sections, and performed a grid search to find an optimal set of hyper-parameters for this task. For the training data, we used top 2M sentences in ASPEC corpus (Nakazawa et al., 2016b) provided by the organizer. We chose the optimal model that achieves the best BLEU (Papineni et al., 2002) score over the dev corpus. For the optimal model, we also performed a grid search about decoding-time hyper-parameters. All the optimal hyper-parameters described in previous sections are found as the results of these searches. We submitted two results generated from the same optimal model: one-best results, i.e., the results with fixing BW = 1, and adjusted results, i.e., the results with optimal BW and W P deHyper-parameters In our decoding strategy, We have 2 hyper-parameters: beam width BW and word penalty factor W P . We varied BW from 1 to 128, and W P from 0 to 1.5, and finally chose B"
W18-2707,P16-1185,0,0.0450977,"17) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LSTM Figure 2: Decoding Process of Back-Translator by the back-translator to"
W18-2707,P11-2031,0,0.181239,"Missing"
W18-2707,P16-1009,0,0.612859,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,W17-4715,0,0.0716399,"lator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models using synthetic parallel sentences. The method of Sennrich et al. (2016a) generates synthetic parallel corpora through back-translation and learns models from such corpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Che"
W18-2707,P16-1162,0,0.855078,"ro Sumita National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {kenji.imamura,atsushi.fujita,eiichiro.sumita}@nict.go.jp Abstract coder is accurately trained because the target side of the synthetic parallel texts consists of manually created (correct) sentences. Consequently, this method provides steady improvements. However, this approach may not contribute to the improvement of the encoder because the source side of the synthetic parallel texts are automatically generated. In this paper, we extend the method proposed by Sennrich et al. (2016a) to enhance the encoder and attention using target monolingual corpora. Our proposed method generates multiple source sentences by sampling when each target sentence is translated back. By using multiple source sentences, we aim to achieve the following. A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a me"
W18-2707,D17-1158,0,0.0425771,"nd, monolingual corpora are readily available in large quantities. Sennrich et al. (2016a) proposed a method using synthetic parallel texts, in which target monolingual corpora are translated back into the source language (Figure 1). The advantage of this method is that the de2 Related Work One approach of using target monolingual corpora is to construct a recurrent neural network language model and combine the model with the decoder (G¨ulc¸ehere et al., 2015; Sriram et al., 2017). Similarly, there is a method of training language models, jointly with the translator, using multitask learning (Domhan and Hieber, 2017). These 55 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 55–63 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics Target Monolingual Corpus Back-Translator Target → Source Synthetic Source Sentences Base Parallel Corpus Training (Forward) Translator Source → Target Training Filter Test Sentence Synthetic Parallel Corpus Translation Figure 1: Flow of Our Approach methods only enhance the decoder and require a modification of the NMT. Another approach of using monolingual corpora of the target language is to learn models us"
W18-2707,W16-2392,0,0.0290266,"r not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learn"
W18-2707,W17-5705,1,0.838981,"nces, each of which contains less than 1024 characters. We assume practical situations in which the domains of parallel and monolingual corpora are not identical. All sentences were segmented into words using an in-house word segmenter. The words were further segmented into 16K sub-words based on the byte-pair encoding rules (Sennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and ta"
W18-2707,P16-1159,0,0.0282914,"t changed. It must be noted that if the domains of the base parallel and the target monolingual corpora are different, it is better to perform “further training” using the base parallel corpus for domain adaptation (Freitag and Al-Onaizan, 2016; Servan et al., 2016). 3 (2) yt where samplingy (P ) denotes the sampling operation of y based on the probability distribution P . The decoding continues until the end-of-sentence symbol is generated.2 We repeat the above process to generate multiple synthetic sentences. Note that this generation method is the same as that of the minimum risk training (Shen et al., 2016). In NMT, even if a low-probability word is selected by the sampling, the subsequent word would become fluent because it is conditioned by the history. Table 1 presents examples of the synthetic source sentences produced by the back-translator. Most of the synthetic source sentences are identical, or close to, the manual backtranslation (i.e., the reference translation). On the other hand, the last example is quite different from the perspective of word order because the clauses are inverted. Such a synthetic sentence is usually not produced by the n-best translation because of the low likelih"
W18-2707,P15-4020,0,0.0128306,"ennrich et al., 2016b) acquired from the base parallel corpus for each language independently. 3.3.2 Confidence Filtering The second method involves filtering with the confidence of translation used in the translation quality estimation task. We use the data provided by Fujita and Sumita (2017), which is a collection of manual labels indicating whether the translation is acceptable or not. We train the support vector machines (SVMs) on the sentence-level data and regard the classifier’s score as the confidence score. The features of the SVM classifier include the 17 basic features of QuEst++ (Specia et al., 2015).4 They are roughly categorized into the following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation"
W18-2707,L18-1545,1,0.800428,"h an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora The corpus sizes used here are shown in Table 2. We used the global communication plan corpus (the GCP corpus, (Imamura and Sumita, 2018)), which is an in-house parallel corpus of daily life conversations and consists of Japanese (Ja), English (En), and Chinese (Zh). The experiments were performed on Englishto-Japanese and Chinese-to-Japanese translation tasks. We randomly selected 400K sentences for the base parallel corpus, and the remaining (1.55M sentences) were used as the Japanese monolingual corpus. The reason for dividing the parallel corpus into two corpora is to measure the upper-bound of quality improvement by using existing parallel texts on the same domain as the manual backtranslation. We also used the Balanced Co"
W18-2707,P17-4012,0,0.0440349,"e following two types. • Language model features of each of the source and target sentences. • Features based on the parallel sentences such as the average number of translation hypotheses per word. In addition, we add the source and target word embeddings. The sentence features are computed by averaging all word embeddings (Shah et al., 2016). The hyperparameters for the training are set using the grid search on the development set. In the expriments of Section 4, features are extracted from the base parallel corpus. 4 Translation System The translation system used in this study was OpenNMT (Klein et al., 2017). We modified it to accept Sections 3.1 and 3.2. 5 http://www.quest.dcs.shef.ac.uk/ 58 http://pj.ninjal.ac.jp/corpus center/bccwj/en/ The encoder was comprised of a two-layer BiLSTM (500 + 500 units), the decoder included a two-layer LSTM (1,000 units), and the stochastic gradient descent was used for optimization. The learning rate for the base parallel corpus was 1.0 for the first 14 epochs, followed by the annealing of 6 epochs while decreasing the learning rate by half. The mini-batch size was 64. At the translation stage, we generated 10-best translations and selected the best among them"
W18-2707,W17-5706,0,0.157005,"Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same length as the manual back-translation. We set the word penalty such that the lengths of the translation and reference translation on the development set are approximately equal, using line search. 3.3.3 Random Filtering The third method is random filtering. This is identical to the reduction of the number of synthetic source sentences to be generated. 4 Experiments 4.1 Experimental Settings Corpora Th"
W18-2707,D16-1160,0,0.0383788,"rpora. Our proposed method is an extension of this method. Currey et al. (2017) generated synthetic parallel sentences by copying target sentences to the source. This method utilizes a feature in which some words, such as named entities, are often identical across the source and target languages and do not require translation. However, this method provides no benefits to language pairs having different character sets, such as English and Japanese. On the other hand, the basis of source monolingual corpora, a pre-training method based on an autoencoder has been proposed to enhance the encoder (Zhang and Zong, 2016). However, the decoder is not enhanced by this method. Cheng et al. (2016) trained two autoencoders using source and target monolingual corpora, while translation models are trained using a parallel corpus. This method enhances both the encoder and decoder, but it requires two monolingual corpora, respectively. Our proposed method enhances not only the decoder but also the encoder and attention using target monolingual corpora. 3 Output Word Output Word Sampling Sampling Word Distribution Generator & Attention Mech. Word Distribution Generator & Attention Mech. Contexts Contexts LSTM States LS"
W18-2707,W17-5712,1,0.825262,"hood Filtering The first method is filtering by the likelihood output from the back-translator. We consider the likelihood as an indicator of translation quality, and low-likelihood synthetic sentences are filtered 2 The back-translator does not use the beam search because the sampling is independently performed for each word. 3 57 We did not perform “further training” in this paper. Type Base Development Test Monolingual GCP Corpus (Japanese) BCCWJ out. Note that the likelihood is corrected with the length of the synthetic source sentence. We call this the length biased log-likelihood lllen (Oda et al., 2017). lllen (y|x) = ∑ Parallel log Pr(yt |x, y&lt;t )+W P ·T, (3) # Sentences 400,000 2,000 2,000 1,552,475 4,791,336 Table 2: Corpus Statistics t where the first term on the right-hand side is the log-likelihood, W P denotes the word penalty (W P ≥ 0), and T denotes the number of words in the synthetic source sentence. NMTs tend to generate shorter translations than the expectation (Morishita et al., 2017). The word penalty works to increase the likelihood of long hypotheses when it is set to a positive value. With an appropriate value, we can obtain synthetic sentences that are almost of the same l"
W18-2707,D16-1163,0,0.0229488,"ic source sentences and the BLEU score on the GCP corpus of En-Ja and Zh-Ja translation tasks, respectively. The graphs and tables in the figures present the same data for overviews and for analyzing the data in detail. Note that the method of Sennrich et al. (2016a) corresponds to the case of one synthetic source 6 4.3 Results with BCCWJ Table 3 shows the results using BCCWJ as a monolingual corpus (the results of the GCP cor7 Unfortunately, it is unknown in this experiment whether the encoder or attention were enhanced. We plan to investigate which module is enhanced by freezing parameters (Zoph et al., 2016) of the encoder and attention through the training. https://github.com/jhclark/multeval 59 32.0 Manual Back-Translation 31.0 BLEU 30.0 29.0 28.0 Sennrich et al. (2016) Likelihood Filtering Confidence Filtering Random Filtering N-best Generation 27.0 26.0 Base Corpus Only 25.0 0 2 4 6 8 10 Number of Synthetic Source Sentences # of Synthetic Sentences Base Corpus Only Sennrich et al. (2016a) 1 2 4 6 8 10 Manual Back-Translation En-Ja Confidence Random Filtering Filtering 26.19 28.61 (+2.42) 28.49 (+2.30) 28.85 (+2.66) 29.26 (+3.07) 30.30 (+4.11) 30.26 (+4.07) 30.08 (+3.89) 30.59 (+4.40) 30.27 (+"
W18-2707,P02-1040,0,0.100513,"Missing"
W18-2713,1983.tc-1.13,0,0.75701,"Missing"
W18-2713,W17-5706,0,0.0169257,"eal that the length ratio should be constant for fair comparison when we compare different systems because they generate translations of different lengths. Therefore, we compare different models and settings by tuning the word penalty to maintain the stable length ratio on the development set (newstest2013). In this experiment, we show results of the two length ratios based on the “original parallel corpus only” of the transformer model. Note that the submitted system employs the first setting. 4.1 Word Penalty / Length Ratio The BLEU score significantly changes due to the translation length (Morishita et al., 2017). For instance, Figure 2 shows BLEU scores of our submitted system (a) when the word penalty was changed from 0.0 to 2.0 and (b) on various length ratios (LRs), which indicate the ratios of the number of words of the system outputs to the reference translations (sys/ref). As shown in Figure 2 (a), the BLEU scores change over 0.5 when we change the word penalty. The penalties of the peaks are different among the development/test sets. The BLEU score peaks were at W P = 1.2, 0.2, and 0.5 in the newstest2013, newstest2014, and newstest2015 sets, respectively. Therefore, the BLEU scores significan"
W18-2713,P11-2031,0,0.115317,"Missing"
W18-2713,W17-5712,1,0.804157,"ynthetic parallel set, and train the model using the mixture of the synthetic and original parallel sets. 3 3.3 Forward Translator 2: Seq-to-Seq Model Applied Systems The other forward translator used herein is OpenNMT based on the seq-to-seq model. The settings were almost the same as the back-translator. SGD was used for the optimization, but the learning rate was set to 0.5 because all target sentences appear twice in an epoch. At the translation, we translated the source sentence into 10-best, and the best hypothesis was selected using the length reranking based on the following equation (Oda et al., 2017). In this paper, we apply the proposed self-training approach to two translator types; the seq-to-seq model (Sutskever et al., 2014; Bahdanau et al., 2014) implemented by OpenNMT (LUA version) (Klein et al., 2017) and the transformer model (Vaswani et al., 2017) implemented by Marian NMT (Junczys-Dowmunt et al., 2018). Table 1 summerizes the system description. 3.1 Back-Translator The back-translator used herein is OpenNMT, which employs an RNN-based seq-to-seq model. The training corpus for the back-translation is preprocessed using the byte-pair encoding (BPE) (Sennrich et al., 2016b). For e"
W18-2713,W18-2707,1,0.924678,"d approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies the effect of our selftraining"
W18-2713,P16-1009,0,0.232298,"tion 2 describes the proposed approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies th"
W18-2713,P18-4020,0,0.0347549,"Missing"
W18-2713,P16-1162,0,0.251699,"tion 2 describes the proposed approach. Section 3 describes the details of our system. Section 4 explains the results of experiments, and Section 5 concludes the paper. Introduction In this study, we introduce the NICT neural translation system at the Second Workshop on Neural Machine Translation and Generation (NMT-2018) (Birch et al., 2018). A characteristic of the system is that translation qualities are improved by introducing self-training, using open-source neural translation systems and defined training data. The self-training method discussed herein is based on the methods proposed by Sennrich et al. (2016a) and Imamura et al. (2018), and they are applied to a self training strategy. It extends only the source side of the training data to increase variety. The merit of the proposed self-training strategy is that it does not influence the efficiency of the translation, such as the translation speed, because it does not change the model structure. (However, the training time increases due to an increase in the training data size.) The proposed approach can be applied to any translation method. However, we want to confirm on which model our approach is practically effective. This paper verifies th"
W18-2713,P17-4012,0,0.0300075,"OpenNMT based on the seq-to-seq model. The settings were almost the same as the back-translator. SGD was used for the optimization, but the learning rate was set to 0.5 because all target sentences appear twice in an epoch. At the translation, we translated the source sentence into 10-best, and the best hypothesis was selected using the length reranking based on the following equation (Oda et al., 2017). In this paper, we apply the proposed self-training approach to two translator types; the seq-to-seq model (Sutskever et al., 2014; Bahdanau et al., 2014) implemented by OpenNMT (LUA version) (Klein et al., 2017) and the transformer model (Vaswani et al., 2017) implemented by Marian NMT (Junczys-Dowmunt et al., 2018). Table 1 summerizes the system description. 3.1 Back-Translator The back-translator used herein is OpenNMT, which employs an RNN-based seq-to-seq model. The training corpus for the back-translation is preprocessed using the byte-pair encoding (BPE) (Sennrich et al., 2016b). For each language, 16K subword types were independently computed. The model was optimized using the stochastic gradient descent (SGD) whose learning rate was 1.0. For the back-translation, we modified OpenNMT to genera"
W18-2713,P18-1007,0,0.0240338,"Self-training they proposed such self-training strategy and confirmed the effect on their own corpus. Figure 1 shows the flow of self-training. The procedure is summarized as follows. 2.3 Dynamic Generation 1. First, train the back-translator that translates the target language into the source using original parallel corpus. A problem in the research proposed by Imamura et al. (2018) is that the training time increases (N + 1)-times with an increase in the training data size. To alleviate this problem, we introduce dynamic generation that uses different synthetic parallel sets for each epoch (Kudo, 2018). Specifically, a synthetic parallel sentence set, which contains one synthetic source sentence per target sentence, is used for an epoch of the training. By changing the synthetic parallel sentence set for each epoch, we expect a similar effect to using multiple source sentences in the training. For implementation, we do not embed the dynamic generation in the training program but perform it offline. Multiple synthetic source sentences were generated in advance, whose number N is 20 this time, and N synthetic parallel sets are constructed. During training, a synthetic set is selected for each"
W18-6419,N12-1047,0,0.0542077,"r training data. Consequently, for Tr-En and Zh-En we simply trained regular phrase-based models using MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models and used the default distortion limit of 6. We trained two 4-gram language models: one on the entire monolingual data concatenated to the target side of the parallel data, and another one on the in-domain “News Crawl” corpora only, using LMPLZ (Heafield et al., 2013). For English, all singletons were pruned due to the large size of the monolingual data. To tune the SMT model weights, we used KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 decoding runs. 4 target monolingual data i=i+1 source to target NMT target to source NMT synthetic parallel data synthetic parallel data ki=rki-1 sentences ki=rki-1 sentences ki back-translated sentences ki back-translated sentences Figure 1: Our incremental training framework. systems are trained, from scratch, on their respective new training data comprising the mixture of the original parallel data and the synthetic parallel data whose source side is back-translated from the target side. At this stage, we"
W18-6419,P13-2121,0,0.05331,"Missing"
W18-6419,W18-2703,0,0.069259,"the mixture of the synthetic and original parallel data, we back-translate a larger number of monolingual sentences, including the same sentences backtranslated at the first iteration. Since we have better NMT systems than those at the first iteration, we can expect the back-translation to be of a better quality. We mix this new synthetic parallel data to the original one and train again from scratch a source-to-target and a target-to-source NMT systems to obtain further improved translation models. Note that this procedure is partially similar to the work proposed by Zhang et al. (2018) and Hoang et al. (2018), but differs in the sense that we increase incrementally our back-translated data. Given the number of sentences used in the first iteration, k1 , and an expansion factor, r, we determine ki , the number of monolingual sentences back-translated at iteration i, as follows: Back-translation of Monolingual Data 4.1 bilingual parallel data Incremental Back-Translation with Et-En, Fi-En, and Tr-En We introduced an incremental training framework for NMT aiming to iteratively increase the quality and quantity of the synthetic parallel data used for training. In this framework, we first simultaneousl"
W18-6419,P18-4020,0,0.0575771,"Missing"
W18-6419,I17-1016,1,0.84595,"the scores given by right-to-left NMT models that we trained for each translation direction with the same parameters as left-to-right NMT models. The two right-to-left NMT models, each achieving the best BLEU and the best perplexity scores on the development data, were selected, giving us two other features for each translation direction. Since the Tr-En training parallel data are much smaller, we were able to perform one more right-to-left train15 In practice, adding one more right-to-left model for reranking did not significantly improve the BLEU score on the development data. 453 7 score (Zhang et al., 2017) thanks to the small size of the phrase table learned for this language pair. Also only for this language pair, we computed the scores for each hypothesis given by the so-called minimum Bayes risk (MBR) decoding for n-best list using two metrics: sBLEU and chrF++ (Popovi´c, 2017). The reranking framework was trained on n-best lists produced by the decoding of the same development data that we used to validate NMT system’s training and to tune SMT’s model weights. 6 Conclusion We participated in eight translation directions and for all of them we did experiments to compare SMT and NMT performan"
W18-6419,P07-2045,0,0.0179829,"9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for Zh-En, and Newsdev2018 for Et-En. 2.2 Tokenization, Truecasing and Cleaning We used Moses tokenizer (Koehn et al., 2007) and truecaser for English, Estonian, Finnish, and Turkish. The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For Chinese, we used Jieba3 for tokenization but did not perform truecasing. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Tables 1 and 2 present the statistics of the parallel and monoli"
W18-6419,W17-3204,0,0.0263216,"rie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017).12 Consequently, we also produced with Marian the 10-best lists, for Zh-En, and 12-best lists for the other language pairs, and merged them with Marian’s 100-best lists to obtain lists containing up to 110 or 112 hypotheses.13 In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size.14 Then, we merged the lists produced by Marian and Moses. We rescored all the hypotheses in the resulting lists with a reranking framework using features to better model the fluency and the adequacy of each hySetting for Zh-En For the Zh-En language pai"
W18-6419,W18-1811,1,0.930099,"1M 1M 200k 2 2 2 2 2 4 #lines back-translated 10M 20M 40M Table 3: Parameters used for our incremental training. For each language pair, the same parameters were used for both translation directions. In our preliminary experiments, we found that setting r = 2 and k1 very close to, or smaller than, the size of the original parallel data consistently gives good results across language pairs. Fine-tuning r and k1 would result in a better translation quality but at a greater cost. our primary submissions for WMT18 are the results of a simple combination of NMT and SMT. Indeed, as demonstrated by Marie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 20"
W18-6419,P02-1040,0,0.102926,"ords --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 6 --dec-depth 6 --transformer-dropout 0.1 --learn-rate 0.0003 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 --dim-vocabs 50000 50000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing. For ZhEn, we did not use --dropout-src 0.1 --dropout-trg 0.1 since the training data is much larger. We performed NMT decoding with an ensemble of a total of six models according to the best BLEU (Papineni et al., 2002) and the best perplexity scores,7 produced by three independent training runs. #tokens 29.4M (Et) 52.9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for"
W18-6419,W17-4770,0,0.0236645,"Missing"
W18-6419,P16-1009,0,0.397624,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6419,P16-1162,0,0.730129,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6489,P18-4020,0,0.08313,"Missing"
W18-6489,W18-2709,0,0.0269454,"00 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description W"
W18-6489,W18-6453,0,0.0251529,"etwork structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data include newstest2018, iwslt2017, Acquis, EMEA, Global Voices, and KDE.2 The statistics of the noisy data to filter are"
W18-6489,P12-3005,0,0.0214345,"Tesla P100 GPUs. Our settings were the same for all of the NMT systems. For each method, we use their score to select the top 100M and 10M sentences to train the corresponding NMT systems. In Table 4, “Original” means the original corpus without any filtering. “Aggressive Filtering” is the method which we introduced in Section 3.1. “Hunalign” indicates the baseline corpus filtering method (Varga et al., 2007)8 given by the organizers. “Classifier” indicates the classifier that we proposed in Section 3.3. “Classifier + LangID” indicates that we also use a language identification tool, LangID (Lui and Baldwin, 2012)9 , to filter the sentence pairs containing sentences that are not German or English. The results were evaluated on the development data newstest2016. Classifier We chose a logistic regression classifier to compute a score for each sentence pair using the features presented in Section 3.2. We trained our classifier on Newstest2014, that we used as positive examples of good sentence pairs, and created the same number of negative examples using the following procedure. We created three-type of negative examples, each of which contains one third of the sentence number of Newstest2014: • Misaligne"
W18-6489,P17-2062,1,0.829142,"3.2 We scored each of the remaining sentence pairs with four NMT transformer models, trained with Marian (Junczys-Dowmunt et al., 2018)4 , on all the parallel data provided for the shared news translation task (excluding the “paracrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of th"
W18-6489,P18-1073,0,0.0238372,"robability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take into account the sentence length: the number of tokens in the source and target sentences, and the difference, and its absolute value, between them. We summarize the features that we used in Table 2. Sentence Pairs Scoring The task"
W18-6489,D17-1155,1,0.758833,"sy data. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov"
W18-6489,D17-1319,0,0.0246581,"to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data inclu"
W18-6489,Q17-1010,0,0.0149258,"acrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take"
W19-5313,P18-4020,0,0.0305142,"Missing"
W19-5313,P07-2045,0,0.0106799,"Missing"
W19-5313,W18-6419,1,0.860496,"algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a equal contribution 168 Proceedings of the Fourth Conference on Machine Translation (WMT), Volu"
W19-5313,P02-1040,0,0.103864,"was set to 2048. The number of attention heads in each encoder and decoder layer was set to eight. During training, the value of label smoothing was set to 0.1, and the attention dropout and residual dropout were set to 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warm-up steps of 16,000. All NMT models for ZH↔EN tasks were consistently trained on four P100 GPUs. We validated the model with an interval of 5,000 batches on the development set and selected the best model according to BLEU (Papineni et al., 2002) score on the newsdev2018 data set. We performed the following training run independently for five times to obtain the models for ensembling. First, an initial model was trained on the provided parallel data and used to generate pseudo-parallel data through back-translation. A new model was then trained from scratch on the mixture of the original parallel data and the pseudo-parallel data. The new model was further 12 13 Results Table 2 shows the results of ZH↔EN tasks. It is obvious that the back-translation, fine-tuning, and ensemble methods are greatly effective for the ZH↔EN tasks. In part"
W19-5313,Y17-1038,1,0.807473,"ansfer Learning In addition to the approaches in Section 3.1, we also use fine-tuning for transfer learning. Zoph et al. (2016) proposed to train a robust L3→L1 parent model using a large L3–L1 parallel corpus and then fine-tune it on a small L2–L1 corpus to obtain a robust L2→L1 child model. The underlying assumption is that the pre-trained L3→L1 model contains prior probabilities for translation into L1. The prior information is divided into two parts: language modeling information (strong prior) and cross-lingual information (weak or strong depending on the relationship between L3 and L2). Dabre et al. (2017) have shown that linguistically similar L3 and L2 allow for better transfer learning. As such, we transliterate L3 to L2 before pre-training a parent model. This could help in faster convergence, ensure cognate overlap, and potentially lead to a better translation quality. In this participation, we used Hindi as the helping language, L3. Results Refer to rows 1 and 2 of Table 1 for the various automatic evaluation scores. For Kazakh→English our submitted system achieved a cased BLEU score of 26.2 placing our system at 3rd rank out of 9 primary systems. On the other hand, our English→Kazakh per"
W19-5313,P16-1009,0,0.456958,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,N16-1101,0,0.279052,", 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transfo"
W19-5313,P16-1162,0,0.806805,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,D16-1163,0,0.143248,"English corpus. Chinese↔English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which l"
W19-5330,P18-1073,0,0.354971,"sesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings (BWE), and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. In total, the induced phrase table contains 90M phrase pairs. BWE of 512 dimensions were obtained using word embeddings trained with fastText9 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018b)10 for this induction. In total four scores, to be used as features in the phrase table, for each of these phrase pairs were computed to mimic phrasebased SMT: forward and backward phrase and lexical translation probabilities. Then, the phrase table was plugged into a Moses system that was tuned on the development data using KB-MIRA. We performed four refinement steps to improve the system using at each step 3M synthetic parallel sentences generated by the forward and backward translation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetx"
W19-5330,D18-1399,0,0.473732,"Missing"
W19-5330,P07-2045,0,0.00885169,"Missing"
W19-5330,W17-3204,0,0.021935,"work. To account for hypotheses length, we added the difference, and its absolute value, between the number of tokens in the translation hypothesis and the source sentence. The reranking framework was trained on n-best lists generated by decoding the first 3k sentence pairs of the development data that we also used to validate the training of UNMT and PNMT systems and to tune the weights of USMT models. Table 3: Parameters for training Marian. 4 Generation of n-best Lists 12 We generated n-best with different beam size for decoding since translation quality can decrease with larger beam size (Koehn and Knowles, 2017). https://marian-nmt.github.io/ 297 # Methods de-cs 1 2 Single UNMT system Single USMT system 15.5 11.1 3 4 5 6 7 Single NMT system pseudo-supervised by UNMT Single NMT system pseudo-supervised by USMT Single Pseudo-supervised MT system Ensemble Pseudo-supervised MT system Re-ranking Pseudo-supervised MT system 15.9 15.3 16.2 16.5 17.0 8 9 10 Fine-tuning Pseudo-supervised MT system Fine-tuning Pseudo-supervised MT system + fixed quotes Fine-tuning + re-ranking Pseudo-supervised MT system + fixed quotes 18.7 19.6 20.1 Table 4: BLEU scores of UMT. #10 is our primary system submitted to the organ"
W19-5330,N12-1047,0,0.0485966,"on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation probabilities using the lexical translation probabilities learned by mgiza during the training of our USMT system. We also used two 4-gram language models to compute two features for each hypothesis. One is the same language model used by our USMT system while the other is a small model trained on all the"
W19-5330,P17-2061,0,0.0185166,"pus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT ("
W19-5330,2015.iwslt-evaluation.11,0,0.0294481,"ge in-domain corpora. For Gujarati and Kazakh, we used Common Crawl and News Crawl corpora, in addition to the provided News Commentary corpus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system a"
W19-5330,C18-1111,1,0.806407,"atistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), me"
W19-5330,W18-1811,1,0.86005,"result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (“constraint”), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and EnglishKazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions. reranking using different informative features as proposed by Marie and Fujita (2018a). This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (Bojar et al., 2019). In addition to the official track, we also present the unsupervised systems for English-Gujariti and English-Kazakh for contrastive experiments with much more distant language pairs. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our UNMT, USMT, and pseudosupervised MT systems. Then, the combination of pseudo-supervised NMT and USMT is described in Secti"
W19-5330,N16-1162,0,0.0471887,"The other parameters for training the language model were set as listed in Table 1. Then we trained a Transformer-based UNMT model with the pre-trained cross-lingual language model using XLM toolkit. The auto-encoder of UNMT architecture cannot learn useful knowledge without some constraints; it would merely become a copying task that learns to copy the input words one by one (Lample et al., 2018). To alleviate this issue, we utilized a denoising auto-encoder (Vincent et al., 2010), and added noise in the form of random token swapping in input sentences to improve the model learning ability (Hill et al., 2016; He et al., 2016). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair e"
W19-5330,W18-6419,1,0.803583,"pairs generated by UNMT. To train this pseudo-supervised NMT (PNMT) system, we chose Marian (Junczys-Dowmunt et al., 2018)11 since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. Specifically, the pseudo-supervised NMT system for de-cs was trained on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation pr"
W19-5330,P18-4020,0,0.0370204,"Missing"
W19-5330,P16-1009,0,0.474495,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P16-1162,0,0.857696,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P18-1072,0,0.0953411,"Missing"
W19-5330,P17-2089,1,0.888859,"Missing"
W19-5330,D17-1155,1,0.833261,"preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), merging pseudo-parall"
W19-5362,D14-1179,0,0.0698778,"Missing"
W19-5362,P17-2061,1,0.912568,"English text that be leveraged for this purpose. In this paper, we describe the systems for Japanese↔English translation, that we developed and submitted for WMT 2019 under the team name “NICT”. In particular our observations can be summarized as follows: In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest. 1 Japanese↔English translation dramatically fails given the limited amount of noisy training data. Fine-Tuning is simple but has over-fitting risks. Mixed-Fine-Tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry"
W19-5362,D16-1163,0,0.164342,"but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details a"
W19-5362,Q17-1024,0,0.078108,"Missing"
W19-5362,P07-2045,0,0.00751066,"isy training data. Fine-Tuning is simple but has over-fitting risks. Mixed-Fine-Tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models"
W19-5362,W19-5303,0,0.0403835,"in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper. 2 Approaches We used domain adaptation approaches on top of the transformer model. 2.1 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components: the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors. The decoder, on the other hand, produces the target word sequence by predicti"
W19-5362,D18-1050,0,0.423737,"atistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper. 2 Approaches We used domain adaptation approaches on top of the transformer model. 2.1 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components: the encoder and the decoder. The encoder converts the input word sequenc"
W19-5362,P11-2093,0,0.0312858,"Computational Linguistics input sequence representations. The reader is encouraged to read the original paper (Vaswani et al., 2017) for a deeper understanding. 2.2 avoid out-of-memory errors. We tried a naive paragraph splitting method where we split a paragraphs into sentences and keep the splits if there are an equal number of sentences. Upon manual investigation we found out that this splitting leads to correct splits most of the times. As a result, the number of training sentences for English→Japanese translation increases to 10,060 pairs. We pre-processed the Japanese text using KyTea (Neubig et al., 2011). Other than this, we do not perform any pre-processing. Mixed Fine Tuning for Domain Adaptation The fastest way to adapt an out-of-domain model to an in-domain task is to first train a L1→L2 model on the large out-of-domain data and then fine tune it on the small in-domain data. However, given that NMT models overfit quickly on small data (Zoph et al., 2016), it is important to consider learning rate modification, regularization and sophisticated training schedules. All this can be avoided by performing Mixed-Fine-Tuning (MFT) (Chu et al., 2017) where the out-of-domain model is fine-tuned on"
W19-5362,W18-6319,0,0.0242931,"io thereby ensuring that the model sees an equal number of training examples from both domains. 2 https://github.com/tensorflow/ tensor2tensor 534 Task BLEU BLEU cased IGNORE BLEU (11b) English→Japanese Japanese→English 11.1 8.1 11.1 7.4 11.1 8.1 IGNORE BLEU-cased (11b) 11.1 7.4 IGNORE BLEU-cased-norm BEER 2.0 11.1 7.8 0.354 0.352 Table 1: Results for Japanese↔English translation for the robustness task. Approach Bidirectional FT Bidirectional MFT Ja→En 9.6 9.2 En→Ja 10.5 13.4 the training data. We did not use this test set for training or tuning. The BLEU scores are obtained using SacreBLEU (Post, 2018). We can see that while the performance of Japanese to English slightly degrades (not statistically significant), English to Japanese translation improves by approximately 2 BLEU points. As such MFT is either comparable to or significantly better than regular fine-tuning and was the reason why we chose it for the final submission. Table 2: BLEU scores on the non-blind test set for Japanese–English translation. We show that MFT is either comparable to or significantly better than regular fine-tuning. because the model had converged sufficiently by 150,000 iterations. We then used this model to"
W19-6601,N12-1048,0,0.042591,"Missing"
W19-6601,P07-2045,0,0.00614914,"Missing"
W19-6601,2015.iwslt-papers.8,0,0.628516,"Missing"
W19-6601,2005.mtsummit-papers.11,0,0.0833525,"decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016.php 6 ftp://jaguar.ncsl.nist.gov/mt/resources/ 5 Proceedings of MT Summit XVII, volume 1 Two operations are applied in order to simulate the transcripts generated by ASR following the setting in (Wang et al., 2016) and (Cho et al., 2017). First, because ASR engines normally do not produce punctuation, punctuation is re"
W19-6601,2015.iwslt-evaluation.9,0,0.0925523,"Missing"
W19-6601,L16-1147,0,0.0201567,"d vectors 5: for θ in the beginning of Θ do 6: remove θ from Θ 7: if θ′ not in dict then 8: dict ← dict ∪ {θ} 9: s ← decode D using θ and evaluate 10: if s &gt; s∗ − ν then 11: if s &gt; s∗ then 12: s∗ ← s 13: θ∗ ← θ 14: end if 15: for k in 1 to m do 16: θ′ ← increase/decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016.php 6 ftp://jaguar.ncsl.nist.gov/mt/resources/ 5 Proceedings of MT Summ"
W19-6601,2015.iwslt-evaluation.11,0,0.0631315,"Missing"
W19-6601,D15-1166,0,0.0445983,"Missing"
W19-6601,2005.iwslt-1.19,0,0.863852,"Missing"
W19-6601,tiedemann-2012-parallel,0,0.0211808,"eshold vector 4: dict ← {} ⊲ a dictionary of visited threshold vectors 5: for θ in the beginning of Θ do 6: remove θ from Θ 7: if θ′ not in dict then 8: dict ← dict ∪ {θ} 9: s ← decode D using θ and evaluate 10: if s &gt; s∗ − ν then 11: if s &gt; s∗ then 12: s∗ ← s 13: θ∗ ← θ 14: end if 15: for k in 1 to m do 16: θ′ ← increase/decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016."
W19-6601,I13-1141,0,0.0451441,"Missing"
W19-7201,D17-1151,0,0.0296082,"Missing"
W19-7201,P16-1160,0,0.0311178,"Missing"
W19-7201,P15-1001,0,0.115725,"Missing"
W19-7201,P17-4012,0,0.406549,"its high accuracy. A downside of NMT is it requires a long training time. For instance, training a Seq2Seq RNN machine translation (MT) with attention (Luong et al., 2015) could take over 10 days using 10 million sentence pairs. A natural solution to this is to use multiple GPUs. There are currently two common approaches for reducing the training time of NMT models. One approach is by using data parallel approach, while the other approach is through the use of the model parallel approach. 1 The data parallel approach is common in many neural network (NN) frameworks. For instance, OpenNMT-lua (Klein et al., 2017) 1 , an NMT toolkit, uses multiple GPUs in training NN models using the data parallel approach. In this approach, the same model is distributed to different GPUs as replicas, and each replica is updated using different data. Afterward, the gradients obtained from each replica are accumulated, and parameters are updated. The model parallel approach has been used for training a Seq2Seq RNN MT with attention (Wu et al., 2016). In this approach, the model is distributed across multiple GPUs, that is, each GPU has only a part of the model. Subsequently, the same data are processed by all GPUs so th"
W19-7201,D15-1166,0,0.141649,"Missing"
W19-7201,W18-6301,0,0.119803,"fficult and can worsen accuracy of the tasks (Krizhevsky, 2014; Keskar et al., 2017). Another important factor to be considered is the ratio of processing time needed for synchronization and forward-backward process on each GPU. If synchronization takes much longer than the forward-backward process, the advantage of using multiple GPUs diminishes. In summary, depending on models, data parallelism may not work effectively. In such a case, there are methods that can be used to achieve synchronization after several mini-batches or to overlap backward and synchronization process at the same time (Ott et al., 2018). However, these advanced synchronization methods are out of the scope of this study. Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 2.2 Model parallelism In this approach, each GPU has different parameters (and computation) of different parts of a model. Most of the communication occurs when passing intermediate results between GPUs. In other words, multiple GPUs do not need to synchronize the values of the parameters. In contrast to data parallelism, most DNN frameworks do not implement automatic model parallelism. Programmers have to implement it depending o"
W19-7201,P16-1009,0,0.0253533,", the conditional probabilities of the target sentence words can be computed as (5) Softmax ,⋯, ,⋯, | ,⋯, , 4.1 We used datasets of WMT14 (Bojar et al., 2014)2 and WMT17 (Bojar et al., 2017)3 English-German shared news translation tasks in the experiments. Both datasets were pre-processed using the scripts of the Marian toolkit (Junczys-Dowmunt et al., 2018)4. Table 1 shows the number of sentences in these datasets. For the WMT17 dataset, first, we duplicated the provided parallel corpus, and then we augmented the parallel corpus with the pseudo-parallel corpus obtained using backtranslation (Sennrich et al., 2016a) of the provided German monolingual data of 10 million (M) sentences. Overall, we used 19 M sentence pairs in the training. We also used the word vocabulary of 32 thousand (K) types from joint source and target byte pair encoding (BPE; Sennrich et al., 2016b). 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt17/translation-task.html 4 https://github.com/marian-nmt/marian3 Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 4492K ― 4492K 3000 3003 4561K 10000K 19122K 2999 3004 Parameter Experiments Data statistics WMT17 Table 1. Datasets"
W19-7201,Q16-1027,0,0.0498595,"Missing"
W19-7201,P16-1162,0,0.0272778,", the conditional probabilities of the target sentence words can be computed as (5) Softmax ,⋯, ,⋯, | ,⋯, , 4.1 We used datasets of WMT14 (Bojar et al., 2014)2 and WMT17 (Bojar et al., 2017)3 English-German shared news translation tasks in the experiments. Both datasets were pre-processed using the scripts of the Marian toolkit (Junczys-Dowmunt et al., 2018)4. Table 1 shows the number of sentences in these datasets. For the WMT17 dataset, first, we duplicated the provided parallel corpus, and then we augmented the parallel corpus with the pseudo-parallel corpus obtained using backtranslation (Sennrich et al., 2016a) of the provided German monolingual data of 10 million (M) sentences. Overall, we used 19 M sentence pairs in the training. We also used the word vocabulary of 32 thousand (K) types from joint source and target byte pair encoding (BPE; Sennrich et al., 2016b). 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt17/translation-task.html 4 https://github.com/marian-nmt/marian3 Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 4492K ― 4492K 3000 3003 4561K 10000K 19122K 2999 3004 Parameter Experiments Data statistics WMT17 Table 1. Datasets"
W19-7201,W17-4717,0,\N,Missing
W19-7201,P18-4020,0,\N,Missing
W19-7201,W17-4739,0,\N,Missing
W99-0207,P98-1011,0,0.106169,"Missing"
W99-0207,C88-1021,0,0.529577,"Missing"
W99-0207,J95-2003,0,0.352919,"Missing"
W99-0207,C96-1021,0,0.0509278,"Missing"
W99-0207,P98-2143,0,0.108075,"Missing"
W99-0207,C96-2137,0,0.155725,"Missing"
W99-0207,A88-1003,0,0.032175,"Missing"
W99-0207,P98-2204,0,0.0979801,"Missing"
W99-0207,P95-1017,0,\N,Missing
W99-0207,C98-1011,0,\N,Missing
W99-0207,C98-2138,0,\N,Missing
W99-0207,C98-2199,0,\N,Missing
watanabe-etal-2002-statistical,N01-1009,0,\N,Missing
watanabe-etal-2002-statistical,J93-2003,0,\N,Missing
watanabe-etal-2002-statistical,C00-2123,0,\N,Missing
watanabe-etal-2002-statistical,P01-1008,0,\N,Missing
watanabe-etal-2002-statistical,1999.mtsummit-1.34,1,\N,Missing
watanabe-etal-2002-statistical,shimohata-sumita-2002-automatic,1,\N,Missing
Y15-1030,2012.eamt-1.42,0,0.201113,"SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For PACLIC 29 Source-Target Syllable Word PBSMT HPBSMT OSM PBSMT HPBSMT OSM km-ar 29.87 23.33 30.08 42.74 42.46 42.60 km-da 41.53 23.68 40.88 52.22 52.05 52.66 km-de 35.03 19.44 35.03 48.79 47.58 48.99 km-en 49.07 36.79 49.20 59.51 57.83 60.02 km-es 42.17 30.82 41.14 52.97 52.45 53.53 km-fr 40.85 34.00 40.96 50.79 49.76 51.63 km-hi 26.30 8.82 26.22 40.53 42.05 40.87 km-id 43.26 32.18 43.78 53.26 52.14 53.65 km-it 37.60 29.15 37.03 47.27 46.87 47.79 km-ja 23.46 16.06 23.43 34.27 36.42 33.78 km-ko 21.37 22.57 21.53 32.21 33.61 32.13 km-ms 42.90 33.55 43.03 53.85 52.52 53.56 km-my 27.43 24.40 2"
Y15-1030,P96-1041,0,0.398816,"ed SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is r"
Y15-1030,J07-2003,0,0.12215,"gnment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For P"
Y15-1030,J15-2001,0,0.188134,".95 km-vi 45.67 27.20 46.91 53.39 52.57 53.86 km-zh 23.72 8.14 23.87 32.09 32.99 32.22 Table 2: BLEU scores for translating from Khmer. the experiments in this paper we used the implementation of hierarchical model provided by the Moses machine translation toolkit (both the hierarchical decoder and training procedure provided by the experiment management system), using the default settings. 3.4 Operation Sequence Model (OSM) The operation sequence model is a model for statistical MT that combines the benefits of two state-of-the-art SMT frameworks, namely ngram-based SMT and phrase-based SMT (Durrani et al., 2015). It is a generative model that performs the translation process as a linear sequence of operations that jointly generate the source and target sentences. The operation 263 types are (i) generation of a sequence of source and/or target words (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations which perform the actual reordering. The probability of a sequence of operations is given by an n-gram model. The OSM integrates translation and reordering into a single model which provides a natural reordering mechanism that is ab"
Y15-1030,D10-1092,0,0.0465491,"5 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for distant language pairs such as Khmer and English, Khmer and Korean, Khmer and Myanmar (Isozaki et al., 2010"
Y15-1030,W09-0429,0,0.131977,"others domains. The CRF segmenter achieved 99.15 Precision, 95.72 Recall and 97.31 F-Score. This CRF word segmenter was used to segment the Khmer BTEC data for the experiments in the next section. 3 Experimental Methodology 3.1 Corpus Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 20"
Y15-1030,N03-1017,0,0.0900442,"Missing"
Y15-1030,W04-3250,0,0.114954,"is the target language. It is clear from the results in the experiments, that syllable segmentation is a far worse segmentation strategy for SMT than word segmentation. This is not always the case, and for 265 languages such as Myanmar it has been shown (Thu et al., 2013) that syllable segmentation can give rise to machine translation scores that are competitive with other approaches. However, for Khmer the proposed word segmentation strategy gave rise to considerable gains in performance and is therefore to be preferred in all cases. Statistical significance tests using bootstrap resampling (Koehn, 2004) were run for all experiments involving the two segmentation schemes. For all experiments the differences were significant (p &lt; 0.01). For most languages combinations the OSM approach gave the highest scores. It is not surprising that is was able to exceed the performance of the phrase-based approach which it extends. However, in all-but-one of the evaluations involving Japanese and Korean the HPBSMT approach gave rise to the highest scores. Looking at the Kendall’s tau distances in Figure 2 it can be seen that Japanese and Korean are the two most distant languages from Khmer in terms of this"
Y15-1030,P00-1056,0,0.278169,"us Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT a"
Y15-1030,P03-1021,0,0.0861983,", 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a"
Y15-1030,2001.mtsummit-papers.68,0,0.0744377,"-km 33.82 25.84 33.94 38.25 31.83 38.15 nl-km 44.85 43.05 45.22 53.51 53.98 53.96 pt-km 44.89 44.13 45.55 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for dist"
Y15-1030,N04-4026,0,\N,Missing
Y15-1030,P02-1040,0,\N,Missing
Y15-1030,D08-1076,0,\N,Missing
Y18-3003,D14-1179,0,0.016896,"Missing"
Y18-3003,P17-2061,1,0.937207,"ish-Japanese translation, UCSY MyanmarEnglish translation and Indic multilingual translation directions. The techniques we focused on for each translation task can be summarized as below: • For the ASPEC translation tasks, we mostly relied on multilingual Transformer (Vaswani et al., 2017) models and experimented with Recurrently Stacked NMT (RS-NMT) (Dabre and Fujita, 2018) models in order to determine the trade-off between compactness of models and the loss in their performance. • For the UCSY Myanmar-English translation task, we tried domain adaptation techniques such as Mixed Fine Tuning (Chu et al., 2017) since the the final objective was to achieve high quality translation for a low-resource domain (ALT). • For the Indic multilingual task, we explored the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on"
Y18-3003,P07-2045,0,0.00542266,"perience a large loss in translation quality despite having significantly fewer parameters compared to the vanilla NMT models. An interesting observation is that systems with the best BLEU might not be the best in terms of human evaluation. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et Anoop Kunchukuttan Microsoft AI and Research, India ankunchu@microsoft.com Eiichiro Sumita NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan eiichiro.sumita@nict.go.jp al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2018, we participated as team “NICT5” and worked on ASPEC Chinese-Japanese and English-Japanese translation, UCSY MyanmarEnglish translation an"
Y18-3003,N15-3017,1,0.843788,"ingle model to translate from English to all the Indic languages. This is essentially the reverse of the XX-En model. We also trained this model for 500k iterations. gle model to translate from all the Indic languages to English and vice versa. Unlike the previous multilingual models, we trained this model only for 180k iterations due to lack of time. • Multilingual Shared Indic Script XX-En model: This model is similar to the XX-En model except that the scripts for all the Indic languages are mapped to a common script. We used Devanagari as the common script, and used the Indic NLP Library5 (Kunchukuttan et al., 2015) for script conversion. As such, this increases the chance of vocabulary sharing. Because the training corpus diversity is significantly reduced we trained this model for 100k iterations because it is technically equivalent 5 • Multilingual XX-YY model: We trained a sinhttps://github.com/anoopkunchukuttan/ indic_nlp_library/ 957 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 Task Bengali-English Bengali-English Bengali-English Bengali-English Hindi-Englis"
Y18-3003,Y18-3001,1,0.838941,"d the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 participants, kindly refer to the overview paper (Nakazawa et al., 2018). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-tosequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensionality. The decoder, on the other hand, produces the target word sequence by predicting the words using a combination of the previously predicted word and relevant p"
Y18-3003,D16-1163,0,0.0754938,"Missing"
Y18-3006,P03-1021,0,0.0360105,"neni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are ev"
Y18-3006,P02-1040,0,0.109271,"ni-batch 16 --valid-freq 5000 --learn-rate 0.0003 --lr-decay-inv-sqrt 16000 --lr-warmup 16000 --lr-report --sync-sgd --devices 0 1 2 3 --dim-vocabs 50000 50000 --exponential-smoothing 2 https://marian-nmt.github.io/, version 1.4.0 It is fully implemented in pure C++ and supports multiGPU training. 4 NVIDIA® Tesla® P100 16Gb. 3 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings --mini-batch-fit --early-stopping 5 --label-smoothing 0.1 --valid-metrics ce-mean-words perplexity translation We performed NMT decoding with an ensemble of a total of 4 models according to the best BLEU (Papineni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and sel"
Y18-3006,2014.iwslt-papers.5,1,0.84444,"-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are evaluated on the ALT test set and the results5 are shown in Table 2. Our observations from are as follows: 1) Our NMT (Marian) system performed much better than SMT (Moses) system in My-to-En. That is, nearly 7 BLEU scores. However, there is no significant"
Y18-3006,W18-6419,1,0.806536,"slation task (Nakazawa et al., 2018), specifically Myanmar (My) - English (En) for both translation directions. All of our systems are constrained, i.e., we used only the parallel adata provided by the organizers to train and tune our systems. The remainder of this paper is organized as follows. In Section 2, we present the data preprocessing. In Section 3, we introduce the details 1 of our NMT and SMT systems with pre-ordering technology. Empirical results obtained with our systems are analyzed in Section 4 and we conclude this paper in Section 5. This system is based on our WMT-2018 system (Marie et al., 2018). Corpus train(ALT) train(UCSY) dev(ALT) test(ALT) #lines #tokens (My/En) 17.9K 208.6K 0.9K 1.0K 1.0M / 410.2K 5.8M / 2.6M 57.4K / 22.1K 58.3K / 22.7K We used Moses tokenizer and truecaser for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. 972 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translatio"
Y18-3007,N12-1047,0,0.0634823,"and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We trained phrase-based SMT models and MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data only, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. 8k and 32k. 7 R Tesla R P100 16Gb. NVIDIA Back-translation # backtr. my→en en→my None (baseline) 0 300k 1M 300k 1M 19.0 20.1 23.3 23.2 25.1∗ 27.6 29.0∗ 27.9 22.5 17.6 Wikipedia CommonCrawl Table 3: BLEU scores for our NMT systems on the official test set of the tasks. The “Corpus” column denotes the origin of the back-translated data and the “#backtr.” column denotes the number of back-translated sentences mixed with the bilingual data for training. “∗” indicates the best configuration for each"
Y18-3007,D18-1045,0,0.0257642,"Missing"
Y18-3007,P13-2121,0,0.0392325,"were trained on the tokenized parallel data using mgiza. Source-totarget and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We trained phrase-based SMT models and MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data only, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. 8k and 32k. 7 R Tesla R P100 16Gb. NVIDIA Back-translation # backtr. my→en en→my None (baseline) 0 300k 1M 300k 1M 19.0 20.1 23.3 23.2 25.1∗ 27.6 29.0∗ 27.9 22.5 17.6 Wikipedia CommonCrawl Table 3: BLEU scores for our NMT systems on the official test set of the tasks. The “Corpus” column denotes the origin of the back-translated data and the “#backtr.” column denotes the number of back-translated sentences mixed with the b"
Y18-3007,P18-4020,0,0.0379332,"Missing"
Y18-3007,P07-2045,0,0.0108559,"we decided to remove lines in both corpora that fulfill at least one of the following conditions: Data set Train Development Test • contains more than 80 tokens For contrastive experiments, we also prepared CommonCrawl and Wikipedia corpora for English and cleaned them in the same manner. For the CommonCrawl corpus, we sampled 2M lines from the entire CommonCrawl corpus provided by WMT18, while for the Wikipedia corpus we sampled 1M lines from the entire dump of the English Wikipedia of 2017/06/01. We tokenized and truecased English data respectively with the tokenizer and truecaser of Moses (Koehn et al., 2007). The truecaser was trained on the English side of the parallel data. Truecasing was performed on all the tokenized data. For Myanmar, the provided bilingual data were already tokenized into writing units and Romanized.4 However, we were not able to take advantage of this preprocessing and chose to reverse it and tokenize the bilingual and monolingual data by ourselves with an in-house tokenizer. We did not apply truecasing on the Myanmar data. Note that for the en→my task, the outputs generated in the Myanmar language must be processed as done by the organizers before submission. For cleaning"
Y18-3007,W18-1811,1,0.796036,"aper describes neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) to the WAT2018 Myanmar–English translation task (Nakazawa et al., 2018).1 We present systems built using only the parallel data provided by the organizers. For contrastive experiments, we also present systems that use monolingual data not provided by the organizers. For both translation directions, we trained NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018). This simple combination method achieved the best results among the submitted MT systems for this task according to BLEU (Papineni et al., 2002). We also 1 The team ID of our participation is “NICT-4”. show that the use of monolingual data can dramatically improve translation quality. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our NMT and SMT systems. The back-translation of monolingual data used by some of our systems is described in Section 4. Then, the combination of NMT and SMT is describ"
Y18-3007,P02-1040,0,0.101122,"and Communications Technology (NICT) to the WAT2018 Myanmar–English translation task (Nakazawa et al., 2018).1 We present systems built using only the parallel data provided by the organizers. For contrastive experiments, we also present systems that use monolingual data not provided by the organizers. For both translation directions, we trained NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018). This simple combination method achieved the best results among the submitted MT systems for this task according to BLEU (Papineni et al., 2002). We also 1 The team ID of our participation is “NICT-4”. show that the use of monolingual data can dramatically improve translation quality. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our NMT and SMT systems. The back-translation of monolingual data used by some of our systems is described in Section 4. Then, the combination of NMT and SMT is described in Section 5. Empirical results achieved by our systems are showed and analyzed in Section 6, and Section 7 concludes this paper. 2 Data prepr"
Y18-3007,P16-1009,0,0.107692,"nsformer architecture (Vaswani et al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep RNN) and convolutional neural network (CNN). We chose Marian5 (JunczysDowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available. In order to limit the size of the vocabulary of the NMT models, we further segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 8k operations for both languages.6 All 5 https://marian-nmt.github.io/, version 1.6 The number of operations was chosen among 8k,16k,32k according to the best BLEU score obtained on the development set. We observed around 2 BLEU points of difference between 6 976 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 our NMT systems were consistently trained on 4 GPUs,7 with the following parameters for Marian: --type transformer --max-length 80 --mini-b"
Y18-3007,P16-1162,0,0.356602,"nsformer architecture (Vaswani et al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep RNN) and convolutional neural network (CNN). We chose Marian5 (JunczysDowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available. In order to limit the size of the vocabulary of the NMT models, we further segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 8k operations for both languages.6 All 5 https://marian-nmt.github.io/, version 1.6 The number of operations was chosen among 8k,16k,32k according to the best BLEU score obtained on the development set. We observed around 2 BLEU points of difference between 6 976 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 our NMT systems were consistently trained on 4 GPUs,7 with the following parameters for Marian: --type transformer --max-length 80 --mini-b"
