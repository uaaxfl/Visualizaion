2021.findings-acl.170,An {E}valuation of {D}isentangled {R}epresentation {L}earning for {T}exts,2021,-1,-1,2,1,7909,krishnapriya vishnubhotla,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.starsem-1.8,Knowledge Graphs meet Moral Values,2020,-1,-1,4,0,14529,ioana hulputextcommabelows,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Operationalizing morality is crucial for understanding multiple aspects of society that have moral values at their core {--} such as riots, mobilizing movements, public debates, etc. Moral Foundations Theory (MFT) has become one of the most adopted theories of morality partly due to its accompanying lexicon, the Moral Foundation Dictionary (MFD), which offers a base for computationally dealing with morality. In this work, we exploit the MFD in a novel direction by investigating how well moral values are captured by KGs. We explore three widely used KGs, and provide concept-level analogues for the MFD. Furthermore, we propose several Personalized PageRank variations in order to score all the concepts and entities in the KGs with respect to their relevance to the different moral values. Our promising results help to progress the operationalization of morality in both NLP and KG communities."
W19-5025,Can Character Embeddings Improve Cause-of-Death Classification for Verbal Autopsy Narratives?,2019,0,0,3,0,23960,zhaodong yan,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"We present two models for combining word and character embeddings for cause-of-death classification of verbal autopsy reports using the text of the narratives. We find that for smaller datasets (500 to 1000 records), adding character information to the model improves classification, making character-based CNNs a promising method for automated verbal autopsy coding."
W19-2504,Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama,2019,0,0,3,1,7909,krishnapriya vishnubhotla,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"According to the literary theory of Mikhail Bakhtin, a dialogic novel is one in which characters speak in their own distinct voices, rather than serving as mouthpieces for their authors. We use text classification to determine which authors best achieve dialogism, looking at a corpus of plays from the late nineteenth and early twentieth centuries. We find that the SAGE model of text generation, which highlights deviations from a background lexical distribution, is an effective method of weighting the words of characters{'} utterances. Our results show that it is indeed possible to distinguish characters by their speech in the plays of canonical writers such as George Bernard Shaw, whereas characters are clustered more closely in the works of lesser-known playwrights."
P19-1166,Understanding Undesirable Word Embedding Associations,2019,0,8,3,0,8878,kawin ethayarajh,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus."
P19-1315,Towards Understanding Linear Word Analogies,2019,0,11,3,0,8878,kawin ethayarajh,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity."
D19-1472,Text-based inference of moral sentiment change,2019,0,0,3,0,14908,jing xie,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We present a text-based framework for investigating moral sentiment change of the public via longitudinal corpora. Our framework is based on the premise that language use can inform people{'}s moral perception toward right or wrong, and we build our methodology by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as slavery and democracy over centuries at three incremental levels: moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying natural language processing toward characterizing moral sentiment change in society."
W18-5620,Listwise temporal ordering of events in clinical notes,2018,0,2,2,1,23961,serena jeblee,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"We present metrics for listwise temporal ordering of events in clinical notes, as well as a baseline listwise temporal ranking model that generates a timeline of events that can be used in downstream medical natural language processing tasks."
W18-5509,Automated Fact-Checking of Claims in Argumentative Parliamentary Debates,2018,-1,-1,2,1,13712,nona naderi,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),0,"We present an automated approach to distinguish true, false, stretch, and dodge statements in questions and answers in the Canadian Parliament. We leverage the truthfulness annotations of a U.S. fact-checking corpus by training a neural net model and incorporating the prediction probabilities into our models. We find that in concert with other linguistic features, these probabilities can improve the multi-class classification results. We further show that dodge statements can be detected with an F1 measure as high as 82.57{\%} in binary classification settings."
W18-5214,Using context to identify the language of face-saving,2018,0,3,2,1,13712,nona naderi,Proceedings of the 5th Workshop on Argument Mining,0,We created a corpus of utterances that attempt to save face from parliamentary debates and use it to automatically analyze the language of reputation defence. Our proposed model that incorporates information regarding threats to reputation can predict reputation defence language with high confidence. Further experiments and evaluations on different datasets show that the model is able to generalize to new utterances and can predict the language of reputation defence in a new dataset.
W18-2302,Multi-task learning for interpretable cause of death classification using key phrase prediction,2018,0,0,3,1,23961,serena jeblee,Proceedings of the {B}io{NLP} 2018 workshop,0,We introduce a multi-task learning model for cause-of-death classification of verbal autopsy narratives that jointly learns to output interpretable key phrases. Adding these key phrases outperforms the baseline model and topic modeling features.
naderi-hirst-2017-recognizing,Recognizing Reputation Defence Strategies in Critical Political Exchanges,2017,17,0,2,1,13712,nona naderi,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"We propose a new task of automatically detecting reputation defence strategies in the field of computational argumentation. We cast the problem as relation classification, where given a pair of reputation threat and reputation defence, we determine the reputation defence strategy. We annotate a dataset of parliamentary questions and answers with reputation defence strategies. We then propose a model based on supervised learning to address the detection of these strategies, and report promising experimental results."
naderi-hirst-2017-classifying,Classifying Frames at the Sentence Level in News Articles,2017,0,5,2,1,13712,nona naderi,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Previous approaches to generic frame classification analyze frames at the document level. Here, we propose a supervised based approach based on deep neural networks and distributional representations for classifying frames at the sentence level in news articles. We conduct our experiments on the publicly available Media Frames Corpus compiled from the U.S. Newspapers. Using (B)LSTMs and GRU networks to represent the meaning of frames, we demonstrate that our approach yields at least 14-point improvement over several baseline methods."
P17-2039,Argumentation Quality Assessment: Theory vs. Practice,2017,4,7,5,0,6983,henning wachsmuth,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Argumentation quality is viewed differently in argumentation theory and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by theory. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other."
I17-1051,Cross-Lingual Sentiment Analysis Without (Good) Translation,2017,25,0,2,0,22203,mohamed abdalla,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Current approaches to cross-lingual sentiment analysis try to leverage the wealth of labeled English data using bilingual lexicons, bilingual vector space embeddings, or machine translation systems. Here we show that it is possible to use a single linear transformation, with as few as 2000 word pairs, to capture fine-grained sentiment relationships between words in a cross-lingual setting. We apply these cross-lingual sentiment models to a diverse set of tasks to demonstrate their functionality in a non-English context. By effectively leveraging English sentiment knowledge without the need for accurate translation, we can analyze and extract features from other languages with scarce data at a very low cost, thus making sentiment and related analyses for many languages inexpensive."
E17-1017,Computational Argumentation Quality Assessment in Natural Language,2017,49,12,7,0,6983,henning wachsmuth,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment."
W16-0417,Semi-supervised and unsupervised categorization of posts in Web discussion forums using part-of-speech information and minimal features,2016,18,3,2,0,34103,krish perumal,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Web discussion forums typically contain posts that fall into different categories such as question, solution, feedback, spam, etc. Automatic identification of these categories can aid information retrieval that is tailored for specific user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying only one or two categories, or do not discuss category-specific performance. In contrast, this work proposes methods for identifying multiple categories, and also analyzes the category-specific performance. These methods are based on sequence models (specifically, hidden Markov Models) that can model language for each category using both probabilistic word and part-of-speech information, and minimal manually specified features. The unsupervised version initializes the models using clustering, whereas the semi-supervised version uses few manually labeled forum posts. Empirical evaluations demonstrate that these methods are more accurate than previous ones."
W16-0301,Detecting late-life depression in {A}lzheimer{'}s disease through analysis of speech and language,2016,54,9,3,1,12761,kathleen fraser,Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology,0,"Alzheimerxe2x80x99s disease (AD) and depression share a number of symptoms, and commonly occur together. Being able to differentiate between these two conditions is critical, as depression is generally treatable. We use linguistic analysis and machine learning to determine whether automated screening algorithms for AD are affected by depression, and to detect when individuals diagnosed with AD are also showing signs of depression. In the first case, we find that our automated AD screening procedure does not show false positives for individuals who have depression but are otherwise healthy. In the second case, we have moderate success in detecting signs of depression in AD (accuracy = 0.658), but we are not able to draw a strong conclusion about the features that are most informative to the classification."
W15-0915,Building a Lexicon of Formulaic Language for Language Learners,2015,21,2,5,1,28552,julian brooke,Proceedings of the 11th Workshop on Multiword Expressions,0,"Though the multiword lexicon has long been of interest in computational linguistics, most relevant work is targeted at only a small portion of it. Our work is motivated by the needs of learners for more comprehensive resources reflecting formulaic language that goes beyond what is likely to be codified in a dictionary. Working from an initial sequential segmentation approach, we present two enhancements: the use of a new measure to promote the identification of lexicalized sequences, and an expansion to include sequences with gaps. We evaluate using a novel method that allows us to calculate an estimate of recall without a reference lexicon, showing that good performance in the second enhancement depends crucially on the first, and that our lexicon conforms much more with human judgment of formulaic language than alternatives."
W15-0705,{G}uten{T}ag: an {NLP}-driven Tool for Digital Humanities Research in the {P}roject {G}utenberg Corpus,2015,16,12,3,1,28552,julian brooke,Proceedings of the Fourth Workshop on Computational Linguistics for Literature,0,"This paper introduces a software tool, GutenTag, which is aimed at giving literary researchers direct access to NLP techniques for the analysis of texts in the Project Gutenberg corpus. We discuss several facets of the tool, including the handling of formatting and structure, the use and expansion of metadata which is used to identify relevant subcorpora of interest, and a general tagging framework which is intended to cover a wide variety of future NLP modules. Our hope that the shared ground created by this tool will help create new kinds of interaction between the computational linguistics and digital humanities communities, to the benefit of both."
R15-1057,Semi-Supervised Never-Ending Learning in Rhetorical Relation Identification,2015,22,5,2,0,24628,erick maziero,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Some languages do not have enough la- beled data to obtain good discourse pars- ing, specially in the relation identification step, and the additional use of unlabeled data is a plausible solution. A workflow is presented that uses a semi-supervised learning approach. Instead of only a pre- defined additional set of unlabeled data, texts obtained from the web are continu- ously added. This obtains near human per- fomance (0.79) in intra sentential rhetori- cal relation identification. An experiment for English also shows improvement using a similar workflow."
P15-2075,Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge,2015,23,11,3,1,4412,tong wang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries. Both proposals provide low-entropy lexical cooccurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks. 1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words. As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Although supported by the distributional hypothesis (Harris, 1954), this definition suffers from two major limitations. Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies. Secondly, a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words. The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsuming false-positive (n1)-grams. Several existing studies have addressed these limitations of window-based contexts. Nonetheless, we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks."
P15-1051,Encoding Distributional Semantics into Triple-Based Knowledge Ranking for Document Enrichment,2015,28,3,4,0,37495,muyu zhang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Document enrichment focuses on retrieving relevant knowledge from external resources, which is essential because text is generally replete with gaps. Since conventional work primarily relies on special resources, we instead use triples of Subject, Predicate, Object as knowledge and incorporate distributional semantics to rank them. Our model first extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirichlet Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a ranking problem, our model significantly outperforms multiple strong baselines. Moreover, we conduct a task-based evaluation by incorporating these triples as additional features into document classification and enhances the performance by 3.02%."
N15-1087,Sentence segmentation of aphasic speech,2015,25,7,3,1,12761,kathleen fraser,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Automatic analysis of impaired speech for screening or diagnosis is a growing research field; however there are still many barriers to a fully automated approach. When automatic speech recognition is used to obtain the speech transcripts, sentence boundaries must be inserted before most measures of syntactic complexity can be computed. In this paper, we consider how language impairments can affect segmentation methods, and compare the results of computing syntactic complexity metrics on automatically and manually segmented transcripts. We find that the important boundary indicators and the resulting segmentation accuracy can vary depending on the type of impairment observed, but that results on patient data are generally similar to control data. We also find that a number of syntactic complexity metrics are robust to the types of segmentation errors that are typically made."
N15-1115,Encoding World Knowledge in the Evaluation of Local Coherence,2015,16,7,4,0,37495,muyu zhang,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Previous work on text coherence was primarily based on matching multiple mentions of the same entity in di erent parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents."
2015.lilt-12.2,Distinguishing Voices in The Waste Land using Computational Stylistics,2015,0,0,3,1,28552,julian brooke,"Linguistic Issues in Language Technology, Volume 12, 2015 - Literature Lifts up Computational Linguistics",0,"T. S. Eliot{'}s poem The Waste Land is a notoriously challenging example of modernist poetry, mixing the independent viewpoints of over ten distinct characters without any clear demarcation of which voice is speaking when. In this work, we apply unsupervised techniques in computational stylistics to distinguish the particular styles of these voices, offering a computer{'}s perspective on longstanding debates in literary analysis. Our work includes a model for stylistic segmentation that looks for points of maximum stylistic variation, a k-means clustering model for detecting non-contiguous speech from the same voice, and a stylistic profiling approach which makes use of lexical resources built from a much larger collection of literary texts. Evaluating using an expert interpretation, we show clear progress in distinguishing the voices of The Waste Land as compared to appropriate baselines, and we also offer quantitative evidence both for and against that particular interpretation."
W14-3420,Using statistical parsing to detect agrammatic aphasia,2014,23,4,2,1,12761,kathleen fraser,Proceedings of {B}io{NLP} 2014,0,"Agrammatic aphasia is a serious language impairment which can occur after a stroke or traumatic brain injury. We present an automatic method for analyzing aphasic speech using surface level parse features and context-free grammar production rules. Examining these features individually, we show that we can uncover many of the same characteristics of agrammatic language that have been reported in studies using manual analysis. When taken together, these parse features can be used to train a classifier to accurately predict whether or not an individual has aphasia. Furthermore, we find that the parse features can lead to higher classification accuracies than traditional measures of syntactic complexity. Finally, we find that a minimal amount of pre-processing can lead to better results than using either the raw data or highly processed data."
W14-3203,Comparison of different feature sets for identification of variants in progressive aphasia,2014,32,8,2,1,12761,kathleen fraser,Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,0,"We use computational techniques to extract a large number of different features from the narrative speech of individuals with primary progressive aphasia (PPA). We examine several different types of features, including part-of-speech, complexity, context-free grammar, fluency, psycholinguistic, vocabulary richness, and acoustic, and discuss the circumstances under which they can be extracted. We consider the task of training a machine learning classifier to determine whether a participant is a control, or has the fluent or nonfluent variant of PPA. We first evaluate the individual feature sets on their classification accuracy, then perform an ablation study to determine the optimal combination of feature sets. Finally, we rank the features in four practical scenarios: given audio data only, given unsegmented transcripts only, given segmented transcripts only, and given both audio and segmented transcripts. We find that psycholinguistic features are highly discriminative in most cases, and that acoustic, context-free grammar, and part-of-speech features can also be important in some circumstances."
P14-2087,Applying a Naive {B}ayes Similarity Measure to Word Sense Disambiguation,2014,30,4,2,1,4412,tong wang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We replace the overlap mechanism of the Lesk algorithm with a simple, generalpurpose Naive Bayes model that measures many-to-many association between two sets of random variables. Even with simple probability estimates such as maximum likelihood, the model gains significant improvement over the Lesk algorithm on word sense disambiguation tasks. With additional lexical knowledge from WordNet, performance is further improved to surpass the state-of-the-art results."
P14-1048,A Linear-Time Bottom-Up Discourse Parser with Constraints and Post-Editing,2014,21,54,2,1,37678,vanessa feng,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be applied in practice. In this work, we develop a much faster model whose time complexity is linear in the number of sentences. Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers. To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF. In addition to efficiency, our parser also significantly outperforms the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy."
D14-1056,Resolving Shell Nouns,2014,32,7,2,1,28596,varada kolhatkar,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Shell nouns, such as fact and problem, occur frequently in all kinds of texts. These nouns themselves are unspecific, and can only be interpreted together with the shell content. We propose a general approach to automatically identify shell content of shell nouns. Our approach exploits lexicosyntactic knowledge derived from the linguistics literature. We evaluate the approach on a variety of shell nouns with a variety of syntactic expectations, achieving accuracies in the range of 62% (baseline = 33%) to 83% (baseline = 74%) on crowd-annotated data."
C14-1071,Unsupervised Multiword Segmentation of Large Corpora using Prediction-Driven Decomposition of n-grams,2014,20,7,3,1,28552,julian brooke,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We present a new, efficient unsupervised approach to the segmentation of corpora into multiword units. Our method involves initial decomposition of common n-grams into segments which maximize within-segment predictability of words, and then further refinement of these segments into a multiword lexicon. Evaluating in four large, distinct corpora, we show that this method creates segments which correspond well to known multiword expressions; our model is particularly strong with regards to longer (3 word) multiword units, which are often ignored or minimized in relevant work."
C14-1089,The Impact of Deep Hierarchical Discourse Structures in the Evaluation of Text Coherence,2014,16,29,3,1,37678,vanessa feng,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Previous work by Lin et al. (2011) demonstrated the effectiveness of using discourse relations for evaluating text coherence. However, their work was based on discourse relations annotated in accordance with the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which encodes only very shallow discourse structures; therefore, they cannot capture long-distance discourse dependencies. In this paper, we study the impact of deep discourse structures for the task of coherence evaluation, using two approaches: (1) We compare a model with features derived from discourse relations in the style of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), which annotate the full hierarchical discourse structure, against our re-implementation of Lin et al.xe2x80x99s model; (2) We compare a model encoded using only shallow RST-style discourse relations, against the one encoded using the complete set of RST-style discourse relations. With an evaluation on two tasks, we show that deep discourse structures are truly useful for better differentiation of text coherence, and in general, RST-style encoding is more powerful than PDTBstyle encoding in these settings."
C14-1205,Supervised Ranking of Co-occurrence Profiles for Acquisition of Continuous Lexical Attributes,2014,45,10,2,1,28552,julian brooke,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Certain common lexical attributes such as polarity and formality are continuous, creating challenges for accurate lexicon creation. Here we present a general method for automatically placing words on these spectra, using co-occurrence profiles, counts of co-occurring words within a large corpus, as a feature vector to a supervised ranking algorithm. With regards to both polarity and formality, we show this method consistently outperforms commonly-used alternatives, both with respect to the intrinsic quality of the lexicon and also when these newly-built lexicons are used in downstream tasks."
W13-2314,Annotating Anaphoric Shell Nouns with their Antecedents,2013,23,9,3,1,28596,varada kolhatkar,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information (Schmid, 2000). We examine the feasibility of annotating such anaphoric nouns using crowdsourcing. In particular, we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so. We also evaluated the quality of crowd annotation using experts. The results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns."
W13-1725,Using Other Learner Corpora in the 2013 {NLI} Shared Task,2013,20,2,2,1,28552,julian brooke,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Our efforts in the 2013 NLI shared task focused on the potential benefits of external corpora. We show that including training data from multiple corpora is highly effective at robust, cross-corpus NLI (i.e. open-training task 1), particularly when some form of domain adaptation is also applied. This method can also be used to boost performance even when training data from the same corpus is available (i.e. open-training task 2). However, in the closed-training task, despite testing a number of new features, we did not see much improvement on a simple model based on earlier work."
W13-1401,A Tale of Two Cultures: Bringing Literary Analysis and Computational Linguistics Together,2013,26,13,3,1,24665,adam hammond,Proceedings of the Workshop on Computational Linguistics for Literature,0,"There are cultural barriers to collaborative effort between literary scholars and computational linguists. In this work, we discuss some of these problems in the context of our ongoing research project, an exploration of free indirect discourse in Virginia Woolfxe2x80x99s To The Lighthouse, ultimately arguing that the advantages of taking each field out of its xe2x80x9ccomfort zonexe2x80x9d justifies the inherent difficulties."
W13-1406,Clustering Voices in The Waste Land,2013,19,5,2,1,28552,julian brooke,Proceedings of the Workshop on Computational Linguistics for Literature,0,"T.S. Eliotxe2x80x99s modernist poem The Waste Land is often interpreted as collection of voices which appear multiple times throughout the text. Here, we investigate whether we can automatically cluster existing segmentations of the text into coherent, expert-identified characters. We show that clustering The Waste Land is a fairly difficult task, though we can do much better than random baselines, particularly if we begin with a good initial segmentation."
W13-1008,"Automatically Assessing Whether a Text Is Cliched, with Applications to Literary Analysis",2013,7,3,2,0.279025,1013,paul cook,Proceedings of the 9th Workshop on Multiword Expressions,0,"Cliches, as trite expressions, are predominantly multiword expressions, but not all MWEs are cliches. We conduct a preliminary examination of the problem of determining how cliched a text is, taken as a whole, by comparing it to a reference text with respect to the proportion of more-frequent n-grams, as measured in an external corpus. We find that more-frequent n-grams are over-represented in cliched text. We apply this finding to the xe2x80x9cEumaeusxe2x80x9d episode of James Joycexe2x80x99s novel Ulysses, which literary scholars believe to be written in a deliberately cliched style."
N13-1078,A Multi-Dimensional {B}ayesian Approach to Lexical Style,2013,37,9,2,1,28552,julian brooke,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions."
J13-3004,Computing Lexical Contrast,2013,41,59,3,0.522537,13005,saif mohammad,Computational Linguistics,0,"Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not classify opposites into the different kinds, however. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and freezing. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly related. (For example, there exists the pair of opposites hot and cold such that tropical is related to hot, and freezing is related to cold.) We will call this the contrast hypo..."
I13-1010,Hybrid Models for Lexical Acquisition of Correlated Styles,2013,27,10,2,1,28552,julian brooke,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Automated lexicon acquisition from corpora represents one way that large datasets can be leveraged to provide resources for a variety of NLP tasks. Our work applies techniques popularized in sentiment lexicon acquisition and topic modeling to the broader task of creating a stylistic lexicon. A novel aspect of our approach is a focus on multiple related styles, first extracting initial independent estimates of style based on co-occurrence with seeds in a large corpus, and then refining those estimates based on the relationship between styles. We compare various promising implementation options, including vector space, Bayesian, and graph-based representations, and conclude that a hybrid approach is indeed warranted."
I13-1039,Detecting Deceptive Opinions with Profile Compatibility,2013,20,44,2,1,37678,vanessa feng,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We propose using profile compatibility to differentiate genuine and fake product reviews. For each product, a collective profile is derived from a separate collection of reviews. Such a profile contains a number of aspects of the product, together with their descriptions. For a given unseen review about the same product, we build a test profile using the same approach. We then perform a bidirectional alignment between the test and the collective profile, to compute a list of aspect-wise compatible features. We adopt Ott et al. (2011)xe2x80x99s op spam v1.3 dataset for identifying truthful vs. deceptive reviews. We extend the recently proposed N-GRAMSYN model of Feng et al. (2012a) by incorporating profile compatibility features, showing such an addition significantly improves upon their state-ofart classification performance."
D13-1030,Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data,2013,22,12,3,1,28596,varada kolhatkar,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Interpreting anaphoric shell nouns (ASNs) such as this issue and this fact is essential to understanding virtually any substantial natural language text. One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data. We tackle this challenge by exploiting cataphoric shell nouns (CSNs) whose construction makes them particularly easy to interpret (e.g., the fact that X). We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs. We achieve precisions in the range of 0.35 (baseline = 0.21) to 0.72 (baseline = 0.44), depending upon the shell noun."
W12-2504,Unsupervised Stylistic Segmentation of Poetry with Change Curves and Extrinsic Features,2012,34,11,3,1,28552,julian brooke,Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature,0,"The identification of stylistic inconsistency is a challenging task relevant to a number of genres, including literature. In this work, we carry out stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot, which is traditionally analyzed in terms of numerous voices which appear throughout the text. Our method, adapted from work in topic segmentation and plagiarism detection, predicts breaks based on a curve of stylistic change which combines information from a diverse set of features, most notably co-occurrence in larger corpora via reduced-dimensionality vectors. We show that this extrinsic information is more useful than (within-text) distributional features. We achieve well above baseline performance on both artificial mixed-style texts and The Waste Land itself."
W12-2205,Building Readability Lexicons with Unannotated Corpora,2012,32,4,5,1,28552,julian brooke,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"Lexicons of word difficulty are useful for various educational applications, including readability classification and text simplification. In this work, we explore automatic creation of these lexicons using methods which go beyond simple term frequency, but without relying on age-graded texts. In particular, we derive information for each word type from the readability of the web documents they appear in and the words they co-occur with, linearly combining these various features. We show the efficacy of this approach by comparing our lexicon with an existing coarse-grained, low-coverage resource and a new crowdsourced annotation."
P12-1007,Text-level Discourse Parsing with Rich Linguistic Features,2012,22,107,2,1,37678,vanessa feng,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we develop an RST-style text-level discourse parser, based on the HILDA discourse parser (Hernault et al., 2010b). We significantly improve its tree-building step by incorporating our own rich linguistic features. We also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourse-parsing performance under different discourse conditions."
brooke-hirst-2012-measuring,Measuring Interlanguage: Native Language Identification with {L}1-influence Metrics,2012,19,21,2,1,28552,julian brooke,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The task of native language (L1) identification suffers from a relative paucity of useful training corpora, and standard within-corpus evaluation is often problematic due to topic bias. In this paper, we introduce a method for L1 identification in second language (L2) texts that relies only on much more plentiful L1 data, rather than the L2 texts that are traditionally used for training. In particular, we do word-by-word translation of large L1 blog corpora to create a mapping to L2 forms that are a possible result of language transfer, and then use that information for unsupervised classification. We show this method is effective in several different learner corpora, with bigram features being particularly useful."
E12-1032,Extending the Entity-based Coherence Model with Multiple Ranks,2012,17,13,2,1,37678,vanessa feng,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. We associate multiple ranks with the set of permutations originating from the same source document, as opposed to the original pairwise rankings. We also study the effect of the permutations used in training, and the effect of the coreference component used in entity extraction. With no additional manual annotations required, our extended model is able to outperform the original model on two tasks: sentence ordering and summary coherence rating."
D12-1115,Resolving {``}This-issue{''} Anaphora,2012,26,4,2,1,28596,varada kolhatkar,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We annotate and resolve a particular case of abstract anaphora, namely, this-issue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issue-specific and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance."
C12-1025,"Robust, Lexicalized Native Language Identification",2012,33,26,2,1,28552,julian brooke,Proceedings of {COLING} 2012,0,"Previous approaches to the task of native language identification (Koppel et al., 2005) have been limited to small, within-corpus evaluations. Because these are restrictive and unreliable, we apply cross-corpus evaluation to the task. We demonstrate the efficacy of lexical features, which had previously been avoided due to the within-corpus topic confounds, and provide a detailed evaluation of various options, including a simple bias adaptation technique and a number of classifier algorithms. Using a new web corpus as a training set, we reach high classification accuracy for a 7-language task, performance which is robust across two independent test sets. Although we show that even higher accuracy is possible using crossvalidation, we present strong evidence calling into question the validity of cross-validation evaluation using the standard dataset."
Y11-1028,Automatic identification of words with novel but infrequent senses,2011,16,8,2,0.279025,1013,paul cook,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"We propose a statistical method for identifying words that have a novel sense in one corpus compared to another based on differences in their lexico-syntactic contexts in those corpora. In contrast to previous work on identifying semantic change, we focus specifically on infrequent word senses. Given the challenges of evaluation for this task, we further propose a novel evaluation method based on synthetic examples of semantic change that allows us to simulate differing degrees of sense change. Our proposed method is able to identify rather subtle simulated sense changes, and outperforms both a random baseline and a previously-proposed approach."
P11-1099,Classifying arguments by scheme,2011,27,105,2,1,37678,vanessa feng,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accuracies of 63--91% in one-against-others classification and 80--94% in pairwise classification (baseline = 50% in both cases)."
I11-1160,Predicting Word Clipping with Latent Semantic Analysis,2011,13,0,3,1,28552,julian brooke,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we compare a resourcedriven approach with a task-specific classification model for a new near-synonym word choice sub-task, predicting whether a full or a clipped form of a word will be used (e.g. doctor or doc) in a given context. Our results indicate that the resourcedriven approach, the use of a formality lexicon, can provide competitive performance, with the parameters of the taskspecific model mirroring the parameters under which the lexicon was built."
D11-1093,Refining the Notions of Depth and Density in {W}ord{N}et-based Semantic Similarity Measures,2011,18,17,2,1,4412,tong wang,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in WordNet-based semantic similarity measures. We show that the intuition for including these notions in WordNet-based similarity measures does not always stand up to empirical examination. In particular, the traditional definitions of depth and density as ordinal integer values in the hierarchical structure of WordNet does not always correlate with human judgment of lexical semantic similarity, which imposes strong limitations on their contribution to an accurate similarity measure. We thus propose several novel definitions of depth and density, which yield significant improvement in degree of correlation with similarity. When used in WordNet-based semantic similarity measures, the new definitions consistently improve performance on a task of correlating with human judgment."
C10-2011,Automatic Acquisition of Lexical Formality,2010,25,33,3,1,28552,julian brooke,Coling 2010: Posters,0,"There has been relatively little work focused on determining the formality level of individual lexical items. This study applies information from large mixed-genre corpora, demonstrating that significant improvement is possible over simple word-length metrics, particularly when multiple sources of information, i.e. word length, word counts, and word association, are integrated. Our best hybrid system reaches 86% accuracy on an English near-synonym formality identification task, and near perfect accuracy when comparing words with extreme formality differences. We also test our word association method in Chinese, a language where word length is not an appropriate metric for formality."
C10-1133,Near-synonym Lexical Choice in Latent Semantic Space,2010,12,21,2,1,4412,tong wang,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We explore the near-synonym lexical choice problem using a novel representation of near-synonyms and their contexts in the latent semantic space. In contrast to traditional latent semantic analysis (LSA), our model is built on the lexical level of co-occurrence, which has been empirically proven to be effective in providing higher dimensional information on the subtle differences among near-synonyms. By employing supervised learning on the latent features, our system achieves an accuracy of 74.5% in a fill-in-the-blank task. The improvement over the current state-of-the-art is statistically significant.n n We also formalize the notion of subtlety through its relation to semantic space dimensionality. Using this formalization and our learning models, several of our intuitions about subtlety, dimensionality, and context are quantified and empirically tested."
R09-1084,Extracting Synonyms from Dictionary Definitions,2009,34,8,2,1,4412,tong wang,Proceedings of the International Conference {RANLP}-2009,0,"Automatic extraction of synonyms and/or semantically related words has various applications in Natural Language Processing (NLP). There are currently two mainstream extraction paradigms, namely, lexicon-based and distributional approaches. The former usually suffers from low coverage, while the latter is only able to capture general relatedness rather than strict synonymy. In this paper, two rule-based extraction methods are applied to definitions from a machine-readable dictionary. Extracted synonyms are evaluated in two experiments by solving TOEFL synonym questions and being compared against existing thesauri. The proposed approaches have achieved satisfactory results in both evaluations, comparable to published studies or even the state of the art."
D08-1103,Computing Word-Pair Antonymy,2008,23,77,3,1,13005,saif mohammad,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances."
S07-1071,"Tor, {T}or{M}d: Distributional Profiles of Concepts for Unsupervised Word Sense Disambiguation",2007,12,6,2,1,13005,saif mohammad,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words---the distributional profile of a concept (DPC)---without the use of manually annotated data. We implemented an unsupervised naive Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese-English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its performance was poor."
D07-1060,Cross-Lingual Distributional Profiles of Concepts for Measuring Semantic Distance,2007,23,32,3,1,13005,saif mohammad,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present the idea of estimating semantic distance in one, possibly resource-poor, language using a knowledge source in another, possibly resource-rich, language. We do so by creating cross-lingual distributional profiles of concepts, using a bilingual lexicon and a bootstrapping algorithm, but without the use of any sense-annotated data or word-aligned corpora. The cross-lingual measures of semantic distance are evaluated on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Readerxe2x80x99s Digest xe2x80x98Word Powerxe2x80x99 problems. In task (1), cross-lingual measures are superior to conventional monolingual measures based on a wordnet. In task (2), cross-lingual measures are able to solve more problems correctly, and despite scores being affected by many tied answers, their overall performance is again better than the best monolingual measures."
W06-1605,Distributional measures of concept-distance: A task-oriented evaluation,2006,20,71,2,1,13005,saif mohammad,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences. We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept--concept matrix roughly .01% the size of that created by existing measures. We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors. In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures."
J06-2003,Building and Using a Lexical Knowledge Base of Near-Synonym Differences,2006,62,60,2,0.666667,8015,diana inkpen,Computational Linguistics,0,"The initial knowledge base is later enriched with information from other machine-readable dictionaries. Information about the collocational behavior of the near-synonyms is acquired from free text. The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in specific situations."
J06-1003,Evaluating {W}ord{N}et-based Measures of Lexical Semantic Relatedness,2006,55,1214,2,0,50596,alexander budanitsky,Computational Linguistics,0,"The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness."
E06-1016,Determining Word Sense Dominance Using a Thesaurus,2006,9,44,2,1,13005,saif mohammad,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The degree of dominance of a sense of a word is the proportion of occurrences of that sense in text. We propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus. Unlike the McCarthy et al. (2004) system, these methods can be used on relatively small target texts, without the need for a similarly-sensedistributed auxiliary text. We perform an extensive evaluation using artificially generated thesaurus-sense-tagged data. In the process, we create a wordxe2x80x93category cooccurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well."
W04-2607,Non-Classical Lexical Semantic Relations,2004,25,96,2,0,51394,jane morris,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,"NLP methods and applications need to take account not only of classical lexical relations, as found in WordNet, but the less-structural, more context-dependent non-classical relations that readers intuit in text. In a reader-based study of lexical relations in text, most were found to be of the latter type. The relationships themselves are analyzed, and consequences for NLP are discussed."
W04-0509,Analysis of Semantic Classes in Medical Text for Question Answering,2004,-1,-1,2,1,51688,yun niu,Proceedings of the Conference on Question Answering in Restricted Domains,0,None
W03-2502,Testing the Efficacy of Part-of-Speech Information in Word Completion,2003,8,39,2,0,10736,afsaneh fazly,Proceedings of the 2003 {EACL} Workshop on Language Modeling for Text Entry Methods,0,"We investigate the effect of incorporating syntactic information into a word-completion algorithm. We introduce two new algorithms that combine part-of-speech tag trigrams with word bigrams, and evaluate them with a test-bench constructed for the purpose. The results show a small but statistically significant improvement in keystroke savings for one of our algorithms over baselines that use only word n-grams."
W03-1310,Answering Clinical Questions with Role Identification,2003,18,41,2,1,51688,yun niu,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"We describe our work in progress on natural language analysis in medical question-answering in the context of a broader medical text-retrieval project. We analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems, and describe an alternative approach whose organizing principle is the identification of semantic roles in both question and answer texts that correspond to the fields of PICO format."
N03-5001,Introduction to Non-Statistical Natural Language Processing,2003,0,0,1,1,7910,graeme hirst,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Tutorial Abstracts,0,"Many problems in natural language processing are best understood and approached by symbolic rather than statistical methods, especially problems in using the structure and meaning of sentences, paragraphs, texts, and dialogues. This tutorial will introduce symbolic methods in syntactic analysis, semantic structures and the representation of meaning, discourse and dialogue structure, and the pragmatics of language in use."
W02-0909,Acquiring Collocations for Lexical Choice between Near-Synonyms,2002,13,43,2,0.666667,8015,diana inkpen,Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition,0,We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between near-synonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry.
J02-2001,Near-Synonymy and Lexical Choice,2002,78,155,2,0,53578,philip edmonds,Computational Linguistics,0,"We develop a new computational model for representing the fine-grained meanings of near-synonyms and the differences between them. We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation. This research has direct applications in machine translation and text generation.We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy. We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms. That is, near-synonyms cluster together.We then develop a clustered model of lexical knowledge, derived from the conventional ontological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning. The model groups near-synonyms into subconceptual clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style. The model is general enough to account for other types of variation, for instance, in collocational behavior.An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles. The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters. We describe our prototype implementation of the system, called I-Saurus."
J01-1006,Book Reviews: Longman Grammar of Spoken and Written {E}nglish,2001,4,0,1,1,7910,graeme hirst,Computational Linguistics,0,None
J99-4008,Book Reviews: {E}uro{W}ord{N}et: A Multilingual Database with Lexical Semantic Networks,1999,-1,-1,1,1,7910,graeme hirst,Computational Linguistics,0,None
W98-1413,Generating Warning Instructions by Planning Accidents and Injuries,1998,11,8,2,0,55109,daniel ansari,Natural Language Generation,0,"We present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances; that is able to automatically generate safety warnings tO the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of t h e device, and these 'injury sub-plans, are then used to instruct the user to avoM these situations. 1 I n t r o d u c t i o n We xe2x80xa2present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances xe2x80xa2 , that is able to automatically generate safety Warnings to the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of the device, and these injury sub-plans are then used to instruct the user to avoid these situations. Thus, unlike other instruction generation systems, our xe2x80xa2system tells the user what not to do as well as what to do. We will show how knowledge about a device that is assumed to already exist as part of the engineering effort, together with adequate, domain-independent knowledge about .the environment, can be used for this. We also put forth the notion that actions are performed on the materials that thedevice operates upon, that the states of these materials may change as a result of these actions, and that the goal of the system should be defined in xe2x80xa2 terms of the final states of the materials. W e take the stand that a complete natural language instruction generation system for a device should have, at the top level, knowledge of the device (as suggested by Delin et al. (1993)). This is one facet of instruction generation that many NLG systems have largely ignored by instead incorporating the knowledge xe2x80xa2of the task at their top level, i.e., the basic content of thexe2x80xa2 instructions is assumed to already exist and does not need to be planned for. In our approach, all the knowledge necessary for the planning stage of a system i s contained (possibly in a more abstract form) in the knowledge of the artifact together with the world knowledge. The kinds of knowledge that Should .be sufficient for this planning are device knowledge xe2x80xa2(topological, kinematic, electrical, thermodynamic, and electronic) and world knowledge. The IDAS project of Reiter et al. (1992; 1995) served as a key motivation for this work. One of the primary goals of the IDAS project was to automatically generate technical documentation I Address correspondence to the second author. E-maili gh @cs.toront0.edu"
P95-1020,A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances,1995,20,0,2,0,31937,daniel marcu,33rd Annual Meeting of the Association for Computational Linguistics,1,"Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics. This paper provides a theoretical framework, called stratified logic, that can accommodate defeasible pragmatic inferences. The framework yields an algorithm that computes the conversational, conventional, scalar, clausal, and normal state implicatures; and the presuppositions that are associated with utterances. The algorithm applies equally to simple and complex utterances and sequences of utterances."
J95-4001,The Repair of Speech Act Misunderstandings by Abductive Inference,1995,60,103,2,1,47796,susan mcroy,Computational Linguistics,0,"During a conversation, agents can easily come to have different beliefs about the meaning or discourse role of some utterance. Participants normally rely on their expectations to determine whether the conversation is proceeding smoothly: if nothing unusual is detected, then understanding is presumed to occur. Conversely, when an agent says something that is inconsistent with another's expectations, then the other agent may change her interpretation of an earlier turn and direct her response to the reinterpretation, accomplishing what is known as a fourth-turn repair.Here we describe an abductive account of the interpretation of speech acts and the repair of speech act misunderstandings. Our discussion considers the kinds of information that participants use to interpret an utterance, even if it is inconsistent with their beliefs. It also considers the information used to design repairs. We describe a mapping between the utterance-level forms (semantics) and discourse-level acts (pragmatics), and a relation between the discourse acts and the beliefs and intentions that they express. We specify for each discourse act, the acts that might be expected, if the hearer has understood the speaker correctly. We also describe our account of belief and intention, distinguishing the beliefs agents actually have from the ones they act as if they have when they perform a discourse act. To support repair, we model how misunderstandings can lead to unexpected actions and utterances and describe the processes of interpretation and repair. To illustrate the approach, we show how it accounts for an example repair."
J95-3003,Collaborating on Referring Expressions,1995,33,0,2,0,40772,peter heeman,Computational Linguistics,0,"This paper presents a computational model of how conversational participants collaborate in order to make a referring action successful. The model is based on the view of language as goal-directed behavior. We propose that the content of a referring expression can be accounted for by the planning paradigm. Not only does this approach allow the processes of building referring expressions and identifying their referents to be captured by plan construction and plan inference, it also allows us to account for how participants clarify a referring expression by using meta-actions that reason about and manipulate the plan derivation that corresponds to the referring expression. To account for how clarification goals arise and how inferred clarification plans affect the agent, we propose that the agents are in a certain state of mind, and that this state includes an intention to achieve the goal of referring and a plan that the agents are currently considering. It is this mental state that sanctions the adoption of goals and the acceptance of inferred plans, and so acts as a link between understanding and generation."
W93-0205,A Goal-Based Grammar of Rhetoric,1993,-1,-1,2,1,49352,chrysanne dimarco,Intentionality and Structure in Discourse Relations,0,None
J93-3002,A Computational Theory of Goal-Directed Style in Syntax,1993,31,32,2,1,49352,chrysanne dimarco,Computational Linguistics,0,"The problem of style is highly relevant to computational linguistics, but current systems deal only superficially, if at all, with subtle but significant nuances of language. Expressive effects, together with their associated meaning, contained in the style of a text are lost to analysis and absent from generation.We have developed an approach to the computational treatment of style that is intended to eventually incorporate three selected components---lexical, syntactic, and semantic. In this paper, we concentrate on certain aspects of syntactic style. We have designed and implemented a computational theory of goal-directed stylistics that can be used in various applications, including machine translation, second-language instruction, and natural language generation.We have constructed a vocabulary of style that contains both primitive and abstract elements of style. The primitive elements describe the stylistic effects of individual sentence components. These elements are combined into patterns that are described by a stylistic meta-language, the abstract elements, that define the concordant and discordant stylistic effects common to a group of sentences. Higher-level patterns are built from the abstract elements and associated with specific stylistic goals, such as clarity or concreteness. Thus, we have defined rules for a syntactic stylistic grammar at three interrelated levels of description: primitive elements, abstract elements, and stylistic goals. Grammars for both English and French have been constructed, using the same vocabulary and the same development methodology. Parsers that implement these grammars have also been built.The stylistic grammars codify aspects of language that were previously defined only descriptively. The theory is being applied to various problems in which the form of an utterance conveys an essential part of meaning and so must be precisely represented and understood."
E93-1033,Abductive Explanation of Dialogue Misunderstandings,1993,29,29,2,1,47796,susan mcroy,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"To respond to an utterance, a listener must interpret what others have said and why they have said it. Misunderstandings occur when agents differ in their beliefs about what has been said or why. Our work combines intentional and social accounts of discourse, unifying theories of speech act production, interpretation, and the repair of misunderstandings. A unified theory has been developed by characterizing the generation of utterances as default reasoning and using abduction to characterize interpretation and repair."
J91-2004,Does Conversation Analysis Have a Role in Computational Linguistics?,1991,33,21,1,1,7910,graeme hirst,Computational Linguistics,0,"Computational linguists are a vicious warrior tribe. Unconstrained by traditional dis- ciplinary boundaries, they invade and plunder neighboring disciplines for attractive theories to incorporate into their own. The latest victim of these attacks is sociology-- in particular, a branch of ethnomethodological sociolinguistics called Conversation Analysis (hereafter,"
J91-1002,Lexical Cohesion Computed by Thesaural relations as an indicator of the structure of text,1991,13,794,2,0,51394,jane morris,Computational Linguistics,0,"In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being about the same thing, and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences."
J90-4005,Book Reviews: A Connectionist Approach to Word Sense Disambiguation,1990,3,0,1,1,7910,graeme hirst,Computational Linguistics,0,None
J90-2007,"Book Reviews: Computational Linguistics: An International Handbook on Computer Oriented Language Research and Applications / COMPUTERLINGUISTIK: EIN INTERNATIONALES {H}ANDBUCH ZUR COMPUTERGEST{\\\U}TZTEN {S}PRACHFORSCHUNG UND IHRER {A}NWENDUNGEN""",1990,-1,-1,1,1,7910,graeme hirst,Computational Linguistics,0,None
C88-1031,Stylistic Grammars in Language Translation,1988,4,6,2,1,49352,chrysanne dimarco,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We are developing stylistic grammars to provide the basis for a French and English stylistic parser. Our stylistic grammar is a branching stratificational model, built upon a foundation dealing with lexical, syntactic, and semantic stylistic realizations. Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals, such as clarity and concreteness, with patterns of these elements.Overall, we are implementing a computational schema of stylistics in French-to-English translation. We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output."
C88-1052,Presuppositions as Beliefs,1988,11,11,2,0,57955,diane horton,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Most theories of presupposition implicitly assume that presuppositions are facts, and that all agents involved in a discourse share belief in the presuppositions that it generates. These unrealistic assumptions can be eliminated if each presupposition is treated as the belief of an agent. However, it is not enough to consider only the beliefs of the speaker; we show that the beliefs of other agents are often involved. We describe a new model, including an improved definition of presupposition, that treats presuppositions as beliefs and considers the beliefs of all agents involved in the discourse. We show that treating presuppositions as beliefs makes it possible to explain phenomena that cannot be explained otherwise."
P86-1030,The detection and representation of ambiguities of intension and description,1986,9,4,2,0,58147,brenda fawcett,24th Annual Meeting of the Association for Computational Linguistics,1,"Ambiguities related to intension and their consequent inference failures are a diverse group, both syntactically and semantically. One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed. The different readings lead to different inferences in a system modeling the beliefs of external agents.We propose that a unified approach to the representation of the alternative readings of intension-related ambiguitoes can be based on the notion of a descriptor that is evaluated with repsoect to intensionality, the beliefs of agents, and a time of application. We describe such a representation, built on a standard modal logic, and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts."
J85-2009,Automatic Semantic Interpretation: A Computer Model of Understanding Natural Language,1985,0,4,1,1,7910,graeme hirst,Computational Linguistics,0,None
P83-1010,A Foundation for Semantic Interpretation,1983,30,13,1,1,7910,graeme hirst,21st Annual Meeting of the Association for Computational Linguistics,1,"Traditionally, translation from the parse tree representing a sentence to a semantic representation (such as frames or procedural semantics) has always been the most ad hoc part of natural language understanding (NLU) systems. However, recent advances in linguistics, most notably the system of formal semantics known as Montague semantics, suggest ways of putting NLU semantics onto a cleaner and firmer foundation. We are using a Montague-inspired approach to semantics in an integrated NLU and problem-solving system that we are building. Like Montague's, our semantics are compositional by design and strongly typed, with semantic rules in one-to-one correspondence with the meaning-affecting rules of a Marcus-style parser. We have replaced Montague's semantic objects, functors and truth conditions, with the elements of the frame language Frail, and added a word sense and case slot disambiguation system. The result is a foundation for semantic interpretation that we believe to be superior to previous approaches."
