2021.naacl-main.203,{UDALM}: Unsupervised Domain Adaptation through Language Modeling,2021,-1,-1,3,0,3882,constantinos karouzos,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74{\%} accuracy, which is an 1.11{\%} absolute improvement over the state-of-the-art."
P19-1385,Attention-based Conditioning Methods for External Knowledge Integration,2019,0,1,3,0,8739,katerina margatina,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture."
N19-1071,{SEQ}{\\^{}}3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression,2019,0,12,4,1,8137,christos baziotis,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ{\^{}}3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets."
N19-1110,Cross-Topic Distributional Semantic Representations Via Unsupervised Mappings,2019,33,0,3,0,4011,eleftheria briakou,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"In traditional Distributional Semantic Models (DSMs) the multiple senses of a polysemous word are conflated into a single vector space representation. In this work, we propose a DSM that learns multiple distributional representations of a word based on different topics. First, a separate DSM is trained for each topic and then each of the topic-based DSMs is aligned to a common vector space. Our unsupervised mapping approach is motivated by the hypothesis that words preserving their relative distances in different topic semantic sub-spaces constitute robust semantic anchors that define the mappings between them. Aligned cross-topic representations achieve state-of-the-art results for the task of contextual word similarity. Furthermore, evaluation on NLP downstream tasks shows that multiple topic-based embeddings outperform single-prototype models."
N19-1213,An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models,2019,0,10,3,1,3263,alexandra chronopoulou,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity."
W18-6209,{NTUA}-{SLP} at {IEST} 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification,2018,22,0,4,1,3263,alexandra chronopoulou,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703."
S18-1037,{NTUA}-{SLP} at {S}em{E}val-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive {RNN}s and Transfer Learning,2018,40,8,8,1,8137,christos baziotis,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: {``}Affect in Tweets{''}. We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E {``}Multi-Label Emotion Classification{''}, 2nd in Subtask A {``}Emotion Intensity Regression{''} and achieved competitive results in other subtasks."
S18-1069,{NTUA}-{SLP} at {S}em{E}val-2018 Task 2: Predicting Emojis using {RNN}s with Context-aware Attention,2018,19,0,6,1,8137,christos baziotis,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 {``}Multilingual Emoji Prediction{''}. We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a {``}context vector{''} which is taken as the aggregation of a tweet{'}s meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2nd out of 48 teams."
S18-1100,{NTUA}-{SLP} at {S}em{E}val-2018 Task 3: Tracking Ironic Tweets using Ensembles of Word and Character Level Attentive {RNN}s,2018,24,3,7,1,8137,christos baziotis,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present two deep-learning systems that competed at SemEval-2018 Task 3 {``}Irony detection in English tweets{''}. We design and ensemble two independent models, based on recurrent neural networks (Bi-LSTM), which operate at the word and character level, in order to capture both the semantic and syntactic information in tweets. Our models are augmented with a self-attention mechanism, in order to identify the most informative words. The embedding layer of our word-level model is initialized with word2vec word embeddings, pretrained on a collection of 550 million English tweets. We did not utilize any handcrafted features, lexicons or external datasets as prior information and our models are trained end-to-end using back propagation on constrained data. Furthermore, we provide visualizations of tweets with annotations for the salient tokens of the attention layer that can help to interpret the inner workings of the proposed models. We ranked 2nd out of 42 teams in Subtask A and 2nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-of-the-art results ranking 1st for both subtasks."
C18-1243,Neural Activation Semantic Models: Computational lexical semantic models of localized neural activations,2018,0,1,3,0,26123,nikos athanasiou,Proceedings of the 27th International Conference on Computational Linguistics,0,"Neural activation models have been proposed in the literature that use a set of example words for which fMRI measurements are available in order to find a mapping between word semantics and localized neural activations. Successful mappings let us expand to the full lexicon of concrete nouns using the assumption that similarity of meaning implies similar neural activation patterns. In this paper, we propose a computational model that estimates semantic similarity in the neural activation space and investigates the relative performance of this model for various natural language processing tasks. Despite the simplicity of the proposed model and the very small number of example words used to bootstrap it, the neural activation semantic model performs surprisingly well compared to state-of-the-art word embeddings. Specifically, the neural activation semantic model performs better than the state-of-the-art for the task of semantic similarity estimation between very similar or very dissimilar words, while performing well on other tasks such as entailment and word categorization. These are strong indications that neural activation semantic models can not only shed some light into human cognition but also contribute to computation models for certain tasks."
S17-2112,Tweester at {S}em{E}val-2017 Task 4: Fusion of Semantic-Affective and pairwise classification models for sentiment analysis in {T}witter,2017,0,2,10,0,11089,athanasia kolovou,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper, we describe our submission to SemEval2017 Task 4: Sentiment Analysis in Twitter. Specifically the proposed system participated both to tweet polarity classification (two-, three- and five class) and tweet quantification (two and five-class) tasks."
E17-2093,Structural Attention Neural Networks for improved sentiment analysis,2017,17,23,2,0,32333,filippos kokkinos,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree."
W16-0424,A semantic-affective compositional approach for the affective labelling of adjective-noun and noun-noun pairs,2016,27,1,4,0,32334,elisavet palogiannidi,"Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Motivated by recent advances in the area of Compo-sitional Distributional Semantic Models (CDSMs), we propose a compositional approach for estimating continuous affective ratings for adjective-noun (AN) and noun-noun (NN) pairs. The ratings are computed for the three basic dimensions of continuous affective spaces, namely, valence, arousal and dominance. We propose that similarly to the semantic modification that underlies CDSMs, affective modification may occur within the framework of affec-tive spaces, especially when the constituent words of the linguistic structures under investigation form modifier-head pairs (e.g., AN and NN). The affective content of the entire structure is determined from the interaction between the respective constituents, i.e., the affect conveyed by the head is altered by the modifier. In addition, we investigate the fusion of the proposed model with the semantic-affective model proposed in (Malandrakis et al., 2013) applied both at word-and phrase-level. The automatically computed affective ratings were evaluated against human ratings in terms of correlation. The most accurate estimates are achieved via fusion and absolute performance improvement up to 5% and 4% is reported for NN and AN, respectively."
S16-1023,Tweester at {S}em{E}val-2016 Task 4: Sentiment Analysis in {T}witter Using Semantic-Affective Model Adaptation,2016,42,11,9,0,32334,elisavet palogiannidi,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We describe our submission to SemEval2016 Task 4: Sentiment Analysis in Twitter. The proposed system ranked first for the subtask B. Our system comprises of multiple independent models such as neural networks, semantic-affective models and topic modeling that are combined in a probabilistic way. The novelty of the system is the employment of a topic modeling approach in order to adapt the semantic-affective space for each tweet. In addition, significant enhancements were made in the main system dealing with the data preprocessing and feature extraction including the employment of word embeddings. Each model is used to predict a tweetxe2x80x99s sentiment (positive, negative or neutral) and a late fusion scheme is adopted for the final decision."
L16-1016,The {S}pe{D}ial datasets: datasets for Spoken Dialogue Systems analytics,2016,24,4,8,0,16658,jose lopes,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The SpeDial consortium is sharing two datasets that were used during the SpeDial project. By sharing them with the community we are providing a resource to reduce the duration of cycle of development of new Spoken Dialogue Systems (SDSs). The datasets include audios and several manual annotations, i.e., miscommunication, anger, satisfaction, repetition, gender and task success. The datasets were created with data from real users and cover two different languages: English and Greek. Detectors for miscommunication, anger and gender were trained for both systems. The detectors were particularly accurate in tasks where humans have high annotator agreement such as miscommunication and gender. As expected due to the subjectivity of the task, the anger detector had a less satisfactory performance. Nevertheless, we proved that the automatic detection of situations that can lead to problems in SDSs is possible and can be a promising direction to reduce the duration of SDS{'}s development cycle."
L16-1195,Cognitively Motivated Distributional Representations of Meaning,2016,0,3,3,0.363636,30893,elias iosif,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Although meaning is at the core of human cognition, state-of-the-art distributional semantic models (DSMs) are often agnostic to the findings in the area of semantic cognition. In this work, we present a novel type of DSMs motivated by the dual-processing cognitive perspective that is triggered by lexico-semantic activations in the short-term human memory. The proposed model is shown to perform better than state-of-the-art models for computing semantic similarity between words. The fusion of different types of DSMs is also investigated achieving results that are comparable or better than the state-of-the-art. The used corpora along with a set of tools, as well as large repositories of vectorial word representations are made publicly available for four languages (English, German, Italian, and Greek)."
L16-1458,Affective Lexicon Creation for the {G}reek Language,2016,23,10,4,0,32334,elisavet palogiannidi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Starting from the English affective lexicon ANEW (Bradley and Lang, 1999a) we have created the first Greek affective lexicon. It contains human ratings for the three continuous affective dimensions of valence, arousal and dominance for 1034 words. The Greek affective lexicon is compared with affective lexica in English, Spanish and Portuguese. The lexicon is automatically expanded by selecting a small number of manually annotated words to bootstrap the process of estimating affective ratings of unknown words. We experimented with the parameters of the semantic-affective model in order to investigate their impact to its performance, which reaches 85{\%} binary classification accuracy (positive vs. negative ratings). We share the Greek affective lexicon that consists of 1034 words and the automatically expanded Greek affective lexicon that contains 407K words."
L16-1627,Crossmodal Network-Based Distributional Semantic Models,2016,35,1,2,0.363636,30893,elias iosif,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Despite the recent success of distributional semantic models (DSMs) in various semantic tasks they remain disconnected with real-world perceptual cues since they typically rely on linguistic features. Text data constitute the dominant source of features for the majority of such models, although there is evidence from cognitive science that cues from other modalities contribute to the acquisition and representation of semantic knowledge. In this work, we propose the crossmodal extension of a two-tier text-based model, where semantic representations are encoded in the first layer, while the second layer is used for computing similarity between words. We exploit text- and image-derived features for performing computations at each layer, as well as various approaches for their crossmodal fusion. It is shown that the crossmodal model performs better (from 0.68 to 0.71 correlation coefficient) than the unimodal one for the task of similarity computation between words."
W15-1105,Fusion of Compositional Network-based and Lexical Function Distributional Semantic Models,2015,30,2,3,0,34916,spiros georgiladakis,Proceedings of the 6th Workshop on Cognitive Modeling and Computational Linguistics,0,"Distributional Semantic Models (DSMs) have been successful at modeling the meaning of individual words, with interest recently shift- ing to compositional structures, i.e., phrases and sentences. Network-based DSMs repre- sent and handle semantics via operators ap- plied on word neighborhoods, i.e., seman- tic graphs containing a target's most similar words. We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of vari- able size activation. The proposed method is evaluated against and combined with the lexi- cal function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved."
W15-0121,Feeling is Understanding: From Affective to Semantic Spaces,2015,27,0,2,0.444444,30893,elias iosif,Proceedings of the 11th International Conference on Computational Semantics,0,"Motivated by theories of language development we investigate the contribution of affect to lexical semantics in the context of distributional semantic models (DSMs). The relationship between semantic and affective spaces is computationally modeled for the task of semantic similarity computation between words. It is shown that affective spaces contain salient information for lexical semantic tasks. We further investigate specific semantic relationships whe re affective information plays a prominent role. The relations between semantic similarity and opposition are studied in the framework of a binary classification problem applied for the discrimination of synonyms and antonyms. For the case of antonyms, the use of affective features results in 33% relat ive improvement in classification accuracy compared to the use of semantic features."
S14-2002,{S}em{E}val-2014 Task 2: Grammar Induction for Spoken Dialogue Systems,2014,21,0,4,0,38939,ioannis klasinas,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper we present the SemEval2014 Task 2 on spoken dialogue grammar induction. The task is to classify a lexical fragment to the appropriate semantic category (grammar rule) in order to construct a grammar for spoken dialogue systems. We describe four subtasks covering two languages, English and Greek, and three speech application domains, travel reservation, tourism and finance. The classification results are compared against the groundtruth. Weighted and unweighted precision, recall and fmeasure are reported. Three sites participated in the task with five systems, employing a variety of features and in some cases using external resources for training. The submissions manage to significantly beat the baseline, achieving a f-measure of 0.69 in comparison to 0.56 for the baseline, averaged across all subtasks."
S14-2089,{SAIL}: Sentiment Analysis using Semantic Similarity and Contrast Features,2014,22,9,5,1,26543,nikolaos malandrakis,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper describes our submission to SemEval2014 Task 9: Sentiment Analysis in Twitter. Our model is primarily a lexicon based one, augmented by some preprocessing, including detection of MultiWord Expressions, negation propagation and hashtag expansion and by the use of pairwise semantic similarity at the tweet level. Feature extraction is repeated for sub-strings and contrasting sub-string features are used to better capture complex phenomena like sarcasm. The resulting supervised system, using a Naive Bayes model, achieved high performance in classifying entire tweets, ranking 7th on the main set and 2nd when applied to sarcastic tweets."
S14-2119,tuc{S}age: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection,2014,23,0,5,0,34749,arodami chorianopoulou,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEvalxe2x80x9914: Task 2. A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments. Posterior probabilities produced by the fragment selection model are fused with estimates of phraselevel similarity based on lexical and contextual information. Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages.
zervanou-etal-2014-word,Word Semantic Similarity for Morphologically Rich Languages,2014,15,6,3,0,25346,kalliopi zervanou,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this work, we investigate the role of morphology on the performance of semantic similarity for morphologically rich languages, such as German and Greek. The challenge in processing languages with richer morphology than English, lies in reducing estimation error while addressing the semantic distortion introduced by a stemmer or a lemmatiser. For this purpose, we propose a methodology for selective stemming, based on a semantic distortion metric. The proposed algorithm is tested on the task of similarity estimation between words using two types of corpus-based similarity metrics: co-occurrence-based and context-based. The performance on morphologically rich languages is boosted by stemming with the context-based metric, unlike English, where the best results are obtained by the co-occurrence-based metric. A key finding is that the estimation error reduction is different when a word is used as a feature, rather than when it is used as a target word."
C14-1069,Low-Dimensional Manifold Distributional Semantic Models,2014,38,2,3,0,39040,georgia athanasopoulou,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Motivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributed semantic model (DSM) that consists of low-dimensional manifolds built on semantic neighborhoods. Each semantic neighborhood is sparsely encoded and mapped into a low-dimensional space. Global operations are decomposed into local operations in multiple sub-spaces; results from these local operations are fused to come up with semantic relatedness estimates. Manifold DSM are constructed starting from a pairwise word-level semantic similarity matrix. The proposed model is evaluated on semantic similarity estimation task significantly improving on the state-of-the-art."
W13-0205,Semantic Similarity Computation for Abstract and Concrete Nouns Using Network-based Distributional Semantic Models,2013,21,3,2,0.666667,30893,elias iosif,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"Motivated by cognitive lexical models, network-based distributional semantic models (DSMs) were proposed in [Iosif and Potamianos (2013)] and were shown to achieve state-of-the-art performance on semantic similarity tasks. Based on evidence for cognitive organization of concepts based on degree of concreteness, we investigate the performance and organization of network DSMs for abstract vs. concrete nouns. Results show a xe2x80x9cconcreteness effectxe2x80x9d for semantic similarity estimation. Network DSMs that implement the maximum sense similarity assumption perform best for concrete nouns, while attributional network DSMs perform best for abstract nouns. The performance of metrics is evaluated against human similarity ratings on an English and a Greek corpus."
S13-2072,{SAIL}: A hybrid approach to sentiment analysis,2013,17,8,3,1,26543,nikolaos malandrakis,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes our submission for SemEval2013 Task 2: Sentiment Analysis in Twitter. For the limited data condition we use a lexicon-based model. The model uses an affective lexicon automaticallygeneratedfrom a very large corpus of raw web data. Statistics are calculated over the word and bigram affective ratings and used as features of a Naive Bayes tree model. For the unconstrained data scenario we combine the lexicon-based model with a classifier built on maximum entropy language models and trained on a large external dataset. The two models are fused at the posterior level to produce a final output. The approach proved successful, reaching rankings of 9th and 4th in the twitter sentiment analysis constrained and unconstrained scenario respectively, despite using only lexical features."
S13-1014,"{D}eep{P}urple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation",2013,34,3,4,1,26543,nikolaos malandrakis,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"This paper describes our submission for the *SEM shared task of Semantic Textual Similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47."
W12-1801,Up from Limited Dialog Systems!,2012,3,1,3,0,2754,giuseppe riccardi,{NAACL}-{HLT} Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data ({SDCTD} 2012),0,"In the last two decades, information-seeking spoken dialog systems (SDS) have moved from research prototypes to real-life commercial applications. Still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problem-solving machines or mobile personal assistants. Future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms (from structured to unstructured). The Web research community have striven to build large scale and open multilingual resources (e.g. Wikipedia) and knowledge bases (e.g. Yago). We argue that a) it is crucial to leverage this massive amount of Web lightly structured knowledge and b) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community."
S12-1082,{D}eep{P}urple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets,2012,31,8,3,0,42613,nikos malandrakis,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length. Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric. State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval'12. Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set."
iosif-potamianos-2012-semsim,{S}em{S}im: Resources for Normalized Semantic Similarity Computation Using Lexical Networks,2012,14,8,2,1,30893,elias iosif,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We investigate the creation of corpora from web-harvested data following a scalable approach that has linear query complexity. Individual web queries are posed for a lexicon that includes thousands of nouns and the retrieved data are aggregated. A lexical network is constructed, in which the lexicon nouns are linked according to their context-based similarity. We introduce the notion of semantic neighborhoods, which are exploited for the computation of semantic similarity. Two types of normalization are proposed and evaluated on the semantic tasks of: (i) similarity judgement, and (ii) noun categorization and taxonomy creation. The created corpus along with a set of tools and noun similarities are made publicly available."
iosif-etal-2012-associative,Associative and Semantic Features Extracted From Web-Harvested Corpora,2012,28,2,4,1,30893,elias iosif,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We address the problem of automatic classification of associative and semantic relations between words, and particularly those that hold between nouns. Lexical relations such as synonymy, hypernymy/hyponymy, constitute the fundamental types of semantic relations. Associative relations are harder to define, since they include a long list of diverse relations, e.g., ''''''``Cause-Effect'''''''', ''''''``Instrument-Agency''''''''. Motivated by findings from the literature of psycholinguistics and corpus linguistics, we propose features that take advantage of general linguistic properties. For evaluation we merged three datasets assembled and validated by cognitive scientists. A proposed priming coefficient that measures the degree of asymmetry in the order of appearance of the words in text achieves the best classification results, followed by context-based similarity metrics. The web-based features achieve classification accuracy that exceeds 85{\%}."
poesio-etal-2010-babyexp,{B}aby{E}xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do,2010,15,1,5,0,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions."
