2020.acl-main.462,P17-1008,0,0.0245193,"and their associated operations (§3 and §4) offer principled constraints that render the search for coherence more tractable. How, then, should we proceed? Our goal is for construal dimensions such as those highlighted in §3 to be incorporated into any research program aspiring to human-level linguistic behavior. Below, we describe several concrete recommendations for how to do this. More meaningful metrics. Taking construal seriously means rethinking how NLP tasks are designed and evaluated. Construal dimensions can provide a rubric for assessing tasks, datasets, and meaning representations (Abend and Rappoport, 2017) for which meaningful distinctions they make or require. (E.g.: Does it capture the level of RESO LUTION at which entities and events are described? Does it represent METAPHOR? Is it sensitive to the PROMINENCE of different event participants?) Such questions might also help guard against unintended biases like those recently found in NLP evaluations and systems (e.g., Caliskan et al., 2017; Gururangan et al., 2018). Popular NLU benchmarks (like SuperGLUE; Wang et al., 2019a) should be critically examined for potential construal biases, and contrasts should be introduced deliberately to probe"
2020.acl-main.462,W13-2322,1,0.65774,"al language. Recipes, for example, are notoriously telegraphic, with rampant omissions of information that a human cook could easily infer in context (Ruppenhofer and Michaelis, 2010; Malmaud et al., 2014). Consider (5): (5) In a medium bowl, cream together the sugar and butter. Beat in the eggs, one at a time, then stir in the vanilla. The italicized words provide crucial constraints that would help a cook (human or robot) track the evolving spatial relations. The first in establishes 7 Indeed, the needs of human-robot interaction have motivated extensions to Abstract Meaning Representation (Banarescu et al., 2013) beyond predicate-argument structure and entities to capture tense and aspect, spatial information, and speech acts (Bonial et al., 2019). the bowl as the reference point for the creaming action, whose result—the mixture of sugar and butter together—becomes the implicit landmark for the subsequent beating in of eggs and vanilla. Systems following instructions also require a means of segmenting continuous sensorimotor data and linking it to discrete linguistic categories (Regneri et al., 2013; Yagcioglu et al., 2018) (cf. the symbol grounding problem (Harnad, 1990)). This mapping may depend on"
2020.acl-main.462,2020.acl-main.463,0,0.0334875,"ngagements in the study of construal. The psycholinguistics literature is a particularly rich source of construal-related data and human language benchmarks. Psycholinguistic data could also be used to probe neural language models (Futrell et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018; Ettinger, 2020). How well do such models capture the phenomena reviewed in §3, and where do they fall short? A fuller account of the constellation of factors involved in construal should also take seriously the grounded, situated nature of language use (Harnad, 1990; Kiros et al., 2018; Bender and Koller, 2020; Bisk et al., 2020). Frameworks motivated by the linguistic insights mentioned in §2 (such as the work on computational construction grammar referenced in §4) and by growing evidence of embodied simulations as the basis for meaning (Narayanan, 1999; Bergen and Chang, 2005; Feldman, 2006; Bergen, 2012; Tamari et al., 2020) are especially relevant lines of inquiry. Much work remains to flesh out the construal dimensions, operations and phenomena preliminarily identified in §3 and §4, especially in connecting to typological, sociolinguistic, developmental, and neural constraints on conceptualiza"
2020.acl-main.462,W15-1405,0,0.0206162,"effects of metaphor on event conceptualization have been found across other domains, such as cancer (Hauser and Schwarz, 2015; Hendricks et al., 2018) and climate change (Flusberg et al., 2017) (see Thibodeau et al. (2017) for a thorough review). Relevant NLP research. Considerable NLP work has addressed the challenge of metaphor detection and understanding (Narayanan, 1999; Shutova et al., 2010, 2013; Shutova, 2015). This work has made use of both statistical, bottom-up approaches to language modeling (Gutiérrez et al., 2016; Shutova et al., 2013), as well as knowledge bases such as MetaNet (Dodge et al., 2015; Stickles et al., 2014; David and Dancygier, 2017). 3.6 Summary The selective review of construal dimensions presented here is intended to be illustrative, not exhaustive or definitive. Returning to the visual anal5174 ogy, we can see these dimensions as primarily concerned with how (and what part of) a conceptual “scene” is perceived (PERSPECTIVE, PROMI NENCE ); the choice or categorization of which schematic structures are present (CONFIGURATION and METAPHOR); or both (RESOLUTION). We have omitted another high-level categorization dimension, SCHEMATIZATION, which includes concepts related t"
2020.acl-main.462,W14-2408,0,0.0274663,"ading. In fact, construal depends on the interaction of many factors, including degree of conventionality (where push and blow are prototypical caused motion verbs), embodied and world knowledge (the relative forces of sneeze and sleep to napkin weight), and context.5 There is extensive psycholinguistic evidence of constructional coercion and the many factors influencing ease of construal (see Goldberg (2003, 2019) for reviews). Some of these phenomena have been analyzed within computational implementations of construction grammar (Bergen and Chang, 2005; Bryant, 2008; Bergen and Chang, 2013; Dodge and Petruck, 2014; Steels, 2017; Steels and Feldman, 2017; Matos et al., 2017), and have also been incorporated in corpus annotation schemes (Bonial et al., 2011; Hwang et al., 2014; Lyngfelt et al., 2018). Metonymy and metaphor. Metonymy and metaphor are associated with semantic mismatches 5 A related theory is Dowty’s (1991) semantic proto-roles account, which links the grammatical subject/object asymmetry to two clusters of semantic features that are more agent-like (e.g., animacy) or patient-like (e.g., affectedness), respectively; associations between these proto-roles and grammatical subjects and objects"
2020.acl-main.462,P17-1025,0,0.0200473,"resolve conflicts between lexical and grammatical meaning (Goldberg, 1995, 2019): (4) a. She sneezed the napkin off the table. b. She {pushed / blew / sneezed / ?slept} the napkin off the table. Here, the verb sneeze, though not typically transitive or causal, appears in a Caused Motion argument structure construction, which pairs obliquetransitive syntax with a caused motion scene. The resulting conflict between its conventional meaning and its putative causal role is resolvable, however, by a commonsense inference that sneezing expels air, which can plausibly cause the napkin’s motion (cf. Forbes and Choi, 2017). This coercion, also described as role fusion, differs from the previous examples in manipulating the PROMINENCE of a latent component of meaning. Coercion doesn’t always succeed, however: presumably sneezing could only move a boulder with contextual support, and sleeping has a less plausibly forceful reading. In fact, construal depends on the interaction of many factors, including degree of conventionality (where push and blow are prototypical caused motion verbs), embodied and world knowledge (the relative forces of sneeze and sleep to napkin weight), and context.5 There is extensive psycho"
2020.acl-main.462,P14-2085,0,0.0184038,"cluding differences in word recognition time (Gillon et al., 1999) (see Fieder et al. (2014) for a review). Relevant NLP research. Configurational properties are closely linked to well-studied challenges at the syntax-semantic interface, in particular nominal and aspectual coercion effects (§4). Several approaches explicitly model coercion operations based on event structure representations (Moens and Steedman, 1988; Passonneau, 1988; Pulman, 1997; Chang et al., 1998), while others explore statistical learning of aspectual classes and features (Siegel and McKeown, 2000; Mathew and Katz, 2009; Friedrich and Palmer, 2014). Lexical resources have also been developed for aspectual annotation (Donatelli et al., 2018) and the count/ mass distinction (Schiehlen and Spranger, 2006; Kiss et al., 2017). 3.5 Metaphor The dimension of METAPHOR is broadly concerned with cross-domain comparison, in which speakers “conceptualize two distinct structures in relation to one another” (Langacker, 1993, p. 450). Metaphors have been analyzed as structured mappings that allow a target domain to be conceptualized in terms of a source domain (Lakoff and Johnson, 1980). Metaphors pervade language use, and exhibit highly systematic, e"
2020.acl-main.462,N09-1057,0,0.0536895,"ve constructions (“The napkin had ignited”). In language production, there are a number of factors influencing which construction a speaker chooses (e.g., current items in discourse focus (Bresnan et al., 2007), lexical and syntactic priming (Pickering and Ferreira, 2008)). 3.3 Relevant NLP research. Recovering implicit information is widely studied in NLP, and deciding which information to express is key to NLG and summarization. We mention three examples exploring how choices of form lend prominence to certain facets of meaning in ways that strongly resonate with our claims about construal. Greene and Resnik (2009) show that syntactic framing—e.g. active (Prisoner murders guard) vs. passive (Guard is murdered)—is relevant to detecting speaker sentiment about violent events. Hwang et al. (2017) present an annotation scheme for capturing adpositional meaning construal (as in (2b)). Rather than disambiguate the adposition with a single label, they separately annotate an adposition’s role with respect to a scene (e.g. employment) and the aspect of meaning brought into prominence by the adposition itself (e.g., benefactive for vs. locative at). This more flexibly accounts for meaning extensions and resolves"
2020.acl-main.462,N18-2017,0,0.024828,"Missing"
2020.acl-main.462,P16-1018,0,0.0243028,"g., whether participants tended to endorse enforcement- or reform-based solutions. Similar effects of metaphor on event conceptualization have been found across other domains, such as cancer (Hauser and Schwarz, 2015; Hendricks et al., 2018) and climate change (Flusberg et al., 2017) (see Thibodeau et al. (2017) for a thorough review). Relevant NLP research. Considerable NLP work has addressed the challenge of metaphor detection and understanding (Narayanan, 1999; Shutova et al., 2010, 2013; Shutova, 2015). This work has made use of both statistical, bottom-up approaches to language modeling (Gutiérrez et al., 2016; Shutova et al., 2013), as well as knowledge bases such as MetaNet (Dodge et al., 2015; Stickles et al., 2014; David and Dancygier, 2017). 3.6 Summary The selective review of construal dimensions presented here is intended to be illustrative, not exhaustive or definitive. Returning to the visual anal5174 ogy, we can see these dimensions as primarily concerned with how (and what part of) a conceptual “scene” is perceived (PERSPECTIVE, PROMI NENCE ); the choice or categorization of which schematic structures are present (CONFIGURATION and METAPHOR); or both (RESOLUTION). We have omitted another"
2020.acl-main.462,S17-1022,1,0.879754,"Missing"
2020.acl-main.462,hwang-etal-2014-criteria,0,0.0249492,"odied and world knowledge (the relative forces of sneeze and sleep to napkin weight), and context.5 There is extensive psycholinguistic evidence of constructional coercion and the many factors influencing ease of construal (see Goldberg (2003, 2019) for reviews). Some of these phenomena have been analyzed within computational implementations of construction grammar (Bergen and Chang, 2005; Bryant, 2008; Bergen and Chang, 2013; Dodge and Petruck, 2014; Steels, 2017; Steels and Feldman, 2017; Matos et al., 2017), and have also been incorporated in corpus annotation schemes (Bonial et al., 2011; Hwang et al., 2014; Lyngfelt et al., 2018). Metonymy and metaphor. Metonymy and metaphor are associated with semantic mismatches 5 A related theory is Dowty’s (1991) semantic proto-roles account, which links the grammatical subject/object asymmetry to two clusters of semantic features that are more agent-like (e.g., animacy) or patient-like (e.g., affectedness), respectively; associations between these proto-roles and grammatical subjects and objects are attested in comprehension (Kako, 2006; Pyykkönen et al., 2010) and have been investigated computationally (Reisinger et al., 2015; Rudinger et al., 2018). that"
2020.acl-main.462,P18-1085,0,0.0190413,"interdisciplinary engagements in the study of construal. The psycholinguistics literature is a particularly rich source of construal-related data and human language benchmarks. Psycholinguistic data could also be used to probe neural language models (Futrell et al., 2018; Linzen and Leonard, 2018; van Schijndel and Linzen, 2018; Ettinger, 2020). How well do such models capture the phenomena reviewed in §3, and where do they fall short? A fuller account of the constellation of factors involved in construal should also take seriously the grounded, situated nature of language use (Harnad, 1990; Kiros et al., 2018; Bender and Koller, 2020; Bisk et al., 2020). Frameworks motivated by the linguistic insights mentioned in §2 (such as the work on computational construction grammar referenced in §4) and by growing evidence of embodied simulations as the basis for meaning (Narayanan, 1999; Bergen and Chang, 2005; Feldman, 2006; Bergen, 2012; Tamari et al., 2020) are especially relevant lines of inquiry. Much work remains to flesh out the construal dimensions, operations and phenomena preliminarily identified in §3 and §4, especially in connecting to typological, sociolinguistic, developmental, and neural con"
2020.acl-main.462,S17-1023,0,0.0632477,"Missing"
2020.acl-main.462,N19-1349,0,0.0142781,"s as basic-level depends on the speaker’s domain expertise (Tanaka and Taylor, 1991). Speakers may deviate from basic-level terms under certain circumstances, e.g., when a more specific term is needed for disambiguation (Graf et al., 2016). Conceptualization is thus a flexible process that varies across both individual cognizers (e.g., as a function of their world knowledge) and specific communicative contexts. Relevant NLP research. Resolution is already recognized as important for applications such as text summarization and dialogue generation (Louis and Nenkova, 2012; Li and Nenkova, 2015; Ko et al., 2019a; Li et al., 2016; Ko et al., 2019b), e.g., in improving human judgments of informativity and relevance (Ko et al., 2019b). Also relevant is work on knowledge representation in the form of inheritance-based ontologies and lexica (e.g., FrameNet (Fillmore and Baker, 2009), ConceptNet (Liu and Singh, 2004)). 3.4 Configuration CONFIGURATION refers to internal-structural properties of entities, groups of entities, and events, 5173 indicating their schematic “shape” and “texture”: multiplicity (or plexity), homogeneity, boundedness, part-whole relations, etc. (Langacker, 1993; Talmy, 2000). To bor"
2020.acl-main.462,L16-1620,0,0.0274471,"Missing"
2020.acl-main.462,louis-nenkova-2012-corpus,0,0.0158459,"et al., 1976). Importantly, however, what counts as basic-level depends on the speaker’s domain expertise (Tanaka and Taylor, 1991). Speakers may deviate from basic-level terms under certain circumstances, e.g., when a more specific term is needed for disambiguation (Graf et al., 2016). Conceptualization is thus a flexible process that varies across both individual cognizers (e.g., as a function of their world knowledge) and specific communicative contexts. Relevant NLP research. Resolution is already recognized as important for applications such as text summarization and dialogue generation (Louis and Nenkova, 2012; Li and Nenkova, 2015; Ko et al., 2019a; Li et al., 2016; Ko et al., 2019b), e.g., in improving human judgments of informativity and relevance (Ko et al., 2019b). Also relevant is work on knowledge representation in the form of inheritance-based ontologies and lexica (e.g., FrameNet (Fillmore and Baker, 2009), ConceptNet (Liu and Singh, 2004)). 3.4 Configuration CONFIGURATION refers to internal-structural properties of entities, groups of entities, and events, 5173 indicating their schematic “shape” and “texture”: multiplicity (or plexity), homogeneity, boundedness, part-whole relations, etc."
2020.acl-main.462,W14-2407,1,0.791862,"se study 2: Human-robot interaction Situated interactions between humans and robots require the integration of language with other modalities (e.g., visual or haptic).7 Clearly, any spatially grounded referring expressions must be tailored to the interlocutors’ PERSPECTIVE (whether shared or not) (Kunze et al., 2017). Focus of attention (PROMINENCE) is especially important for systems that must interpret procedural language. Recipes, for example, are notoriously telegraphic, with rampant omissions of information that a human cook could easily infer in context (Ruppenhofer and Michaelis, 2010; Malmaud et al., 2014). Consider (5): (5) In a medium bowl, cream together the sugar and butter. Beat in the eggs, one at a time, then stir in the vanilla. The italicized words provide crucial constraints that would help a cook (human or robot) track the evolving spatial relations. The first in establishes 7 Indeed, the needs of human-robot interaction have motivated extensions to Abstract Meaning Representation (Banarescu et al., 2013) beyond predicate-argument structure and entities to capture tense and aspect, spatial information, and speech acts (Bonial et al., 2019). the bowl as the reference point for the cre"
2020.acl-main.462,J88-2005,0,0.355608,"ictions about the consequences of a political candidate’s behavior on reelection (Fausey and Matlock, 2011). The mass/count distinction has attested psychological implications, including differences in word recognition time (Gillon et al., 1999) (see Fieder et al. (2014) for a review). Relevant NLP research. Configurational properties are closely linked to well-studied challenges at the syntax-semantic interface, in particular nominal and aspectual coercion effects (§4). Several approaches explicitly model coercion operations based on event structure representations (Moens and Steedman, 1988; Passonneau, 1988; Pulman, 1997; Chang et al., 1998), while others explore statistical learning of aspectual classes and features (Siegel and McKeown, 2000; Mathew and Katz, 2009; Friedrich and Palmer, 2014). Lexical resources have also been developed for aspectual annotation (Donatelli et al., 2018) and the count/ mass distinction (Schiehlen and Spranger, 2006; Kiss et al., 2017). 3.5 Metaphor The dimension of METAPHOR is broadly concerned with cross-domain comparison, in which speakers “conceptualize two distinct structures in relation to one another” (Langacker, 1993, p. 450). Metaphors have been analyzed a"
2020.acl-main.462,Q19-1043,0,0.0179181,"king it to discrete linguistic categories (Regneri et al., 2013; Yagcioglu et al., 2018) (cf. the symbol grounding problem (Harnad, 1990)). This mapping may depend on flexibly adjusting RESO LUTION and CONFIGURATION based on linguistic cues (e.g., cut/dice/slice/sliver the apple). 5.3 Case study 3: Paraphrase generation Despite many advances, paraphrase generation systems remain far from human performance. One vexing issue is the lack of evaluation metrics that correlate with human judgments for tasks like paraphrase, image captioning, and textual entailment (see, e.g., Bhagat and Hovy, 2013; Pavlick and Kwiatkowski, 2019; Wang et al., 2019b). In particular, it is unclear how closely a good paraphrase should hew to all aspects of the source sentence. For example, should active/passive descriptions of the same scene, or the sets of sentences in (2), be considered meaning-equivalent? Or take the putative paraphrase below: (6) a. The teacher sat on the student’s left. b. Next to the children was a mammal. These could plausibly describe the same scene; should their differences across multiple dimensions (PERSPECTIVE, PROMINENCE, RESOLUTION) be rewarded or penalized for this diversity? A first step out of this quan"
2020.acl-main.462,P19-1334,0,0.0145256,"scribed? Does it represent METAPHOR? Is it sensitive to the PROMINENCE of different event participants?) Such questions might also help guard against unintended biases like those recently found in NLP evaluations and systems (e.g., Caliskan et al., 2017; Gururangan et al., 2018). Popular NLU benchmarks (like SuperGLUE; Wang et al., 2019a) should be critically examined for potential construal biases, and contrasts should be introduced deliberately to probe whether systems are modeling lexical choices, grammatical choices, and meaning in the desired way (Naik et al., 2018; Kaushik et al., 2020; McCoy et al., 2019; Gardner et al., 2020). As a broader suggestion, datasets should move away from a one-size-fits-all attitude based on gold annotations. Ideally, evaluation metrics should take into account not only partial structure matches, but also similarity to alternate construals. Cognitive connections. The many connections between construal and the rest of cognition highlight the need for further interdisciplinary engagements in the study of construal. The psycholinguistics literature is a particularly rich source of construal-related data and human language benchmarks. Psycholinguistic data could also"
2020.acl-main.462,J88-2003,0,0.888102,"Wittenberg, 2019) to predictions about the consequences of a political candidate’s behavior on reelection (Fausey and Matlock, 2011). The mass/count distinction has attested psychological implications, including differences in word recognition time (Gillon et al., 1999) (see Fieder et al. (2014) for a review). Relevant NLP research. Configurational properties are closely linked to well-studied challenges at the syntax-semantic interface, in particular nominal and aspectual coercion effects (§4). Several approaches explicitly model coercion operations based on event structure representations (Moens and Steedman, 1988; Passonneau, 1988; Pulman, 1997; Chang et al., 1998), while others explore statistical learning of aspectual classes and features (Siegel and McKeown, 2000; Mathew and Katz, 2009; Friedrich and Palmer, 2014). Lexical resources have also been developed for aspectual annotation (Donatelli et al., 2018) and the count/ mass distinction (Schiehlen and Spranger, 2006; Kiss et al., 2017). 3.5 Metaphor The dimension of METAPHOR is broadly concerned with cross-domain comparison, in which speakers “conceptualize two distinct structures in relation to one another” (Langacker, 1993, p. 450). Metaphors ha"
2020.acl-main.462,C18-1198,0,0.022729,"UTION at which entities and events are described? Does it represent METAPHOR? Is it sensitive to the PROMINENCE of different event participants?) Such questions might also help guard against unintended biases like those recently found in NLP evaluations and systems (e.g., Caliskan et al., 2017; Gururangan et al., 2018). Popular NLU benchmarks (like SuperGLUE; Wang et al., 2019a) should be critically examined for potential construal biases, and contrasts should be introduced deliberately to probe whether systems are modeling lexical choices, grammatical choices, and meaning in the desired way (Naik et al., 2018; Kaushik et al., 2020; McCoy et al., 2019; Gardner et al., 2020). As a broader suggestion, datasets should move away from a one-size-fits-all attitude based on gold annotations. Ideally, evaluation metrics should take into account not only partial structure matches, but also similarity to alternate construals. Cognitive connections. The many connections between construal and the rest of cognition highlight the need for further interdisciplinary engagements in the study of construal. The psycholinguistics literature is a particularly rich source of construal-related data and human language ben"
2020.acl-main.462,Q13-1003,0,0.0109971,", the needs of human-robot interaction have motivated extensions to Abstract Meaning Representation (Banarescu et al., 2013) beyond predicate-argument structure and entities to capture tense and aspect, spatial information, and speech acts (Bonial et al., 2019). the bowl as the reference point for the creaming action, whose result—the mixture of sugar and butter together—becomes the implicit landmark for the subsequent beating in of eggs and vanilla. Systems following instructions also require a means of segmenting continuous sensorimotor data and linking it to discrete linguistic categories (Regneri et al., 2013; Yagcioglu et al., 2018) (cf. the symbol grounding problem (Harnad, 1990)). This mapping may depend on flexibly adjusting RESO LUTION and CONFIGURATION based on linguistic cues (e.g., cut/dice/slice/sliver the apple). 5.3 Case study 3: Paraphrase generation Despite many advances, paraphrase generation systems remain far from human performance. One vexing issue is the lack of evaluation metrics that correlate with human judgments for tasks like paraphrase, image captioning, and textual entailment (see, e.g., Bhagat and Hovy, 2013; Pavlick and Kwiatkowski, 2019; Wang et al., 2019b). In particul"
2020.acl-main.462,Q15-1034,0,0.0487666,"Missing"
2020.acl-main.462,P18-1210,1,0.811072,"e.g. active (Prisoner murders guard) vs. passive (Guard is murdered)—is relevant to detecting speaker sentiment about violent events. Hwang et al. (2017) present an annotation scheme for capturing adpositional meaning construal (as in (2b)). Rather than disambiguate the adposition with a single label, they separately annotate an adposition’s role with respect to a scene (e.g. employment) and the aspect of meaning brought into prominence by the adposition itself (e.g., benefactive for vs. locative at). This more flexibly accounts for meaning extensions and resolves some annotator difficulties. Rohde et al. (2018) studied the construction of discourse coherence by asking participants to insert a conjunction (and, or, but, so, because, before) where none was originally present, before an Resolution Concepts can be described at many levels of RESO LUTION —from highly detailed to more schematic. We include here both specificity (e.g., pug < dog < animal < being) and granularity (e.g., viewing a forest at the level of individual leaves vs. branches vs. trees). Lexical items and larger expressions can evoke and combine concepts at varying levels of detail (“The gymnast triumphantly landed upright” vs. “A pe"
2020.acl-main.462,D18-1114,0,0.0379811,"Missing"
2020.acl-main.462,schiehlen-spranger-2006-mass,0,0.0364649,"es are closely linked to well-studied challenges at the syntax-semantic interface, in particular nominal and aspectual coercion effects (§4). Several approaches explicitly model coercion operations based on event structure representations (Moens and Steedman, 1988; Passonneau, 1988; Pulman, 1997; Chang et al., 1998), while others explore statistical learning of aspectual classes and features (Siegel and McKeown, 2000; Mathew and Katz, 2009; Friedrich and Palmer, 2014). Lexical resources have also been developed for aspectual annotation (Donatelli et al., 2018) and the count/ mass distinction (Schiehlen and Spranger, 2006; Kiss et al., 2017). 3.5 Metaphor The dimension of METAPHOR is broadly concerned with cross-domain comparison, in which speakers “conceptualize two distinct structures in relation to one another” (Langacker, 1993, p. 450). Metaphors have been analyzed as structured mappings that allow a target domain to be conceptualized in terms of a source domain (Lakoff and Johnson, 1980). Metaphors pervade language use, and exhibit highly systematic, extensible structure. For example, in English, events are often construed either as locations in space or as objects moving through space. Our experience of"
2020.acl-main.462,J15-4002,0,0.0175261,"a beast or as a virus elicited markedly different suggestions about how best to address the problem, e.g., whether participants tended to endorse enforcement- or reform-based solutions. Similar effects of metaphor on event conceptualization have been found across other domains, such as cancer (Hauser and Schwarz, 2015; Hendricks et al., 2018) and climate change (Flusberg et al., 2017) (see Thibodeau et al. (2017) for a thorough review). Relevant NLP research. Considerable NLP work has addressed the challenge of metaphor detection and understanding (Narayanan, 1999; Shutova et al., 2010, 2013; Shutova, 2015). This work has made use of both statistical, bottom-up approaches to language modeling (Gutiérrez et al., 2016; Shutova et al., 2013), as well as knowledge bases such as MetaNet (Dodge et al., 2015; Stickles et al., 2014; David and Dancygier, 2017). 3.6 Summary The selective review of construal dimensions presented here is intended to be illustrative, not exhaustive or definitive. Returning to the visual anal5174 ogy, we can see these dimensions as primarily concerned with how (and what part of) a conceptual “scene” is perceived (PERSPECTIVE, PROMI NENCE ); the choice or categorization of whi"
2020.acl-main.462,C10-1113,0,0.0857546,"Missing"
2020.acl-main.462,J13-2003,0,0.0255574,"tended to endorse enforcement- or reform-based solutions. Similar effects of metaphor on event conceptualization have been found across other domains, such as cancer (Hauser and Schwarz, 2015; Hendricks et al., 2018) and climate change (Flusberg et al., 2017) (see Thibodeau et al. (2017) for a thorough review). Relevant NLP research. Considerable NLP work has addressed the challenge of metaphor detection and understanding (Narayanan, 1999; Shutova et al., 2010, 2013; Shutova, 2015). This work has made use of both statistical, bottom-up approaches to language modeling (Gutiérrez et al., 2016; Shutova et al., 2013), as well as knowledge bases such as MetaNet (Dodge et al., 2015; Stickles et al., 2014; David and Dancygier, 2017). 3.6 Summary The selective review of construal dimensions presented here is intended to be illustrative, not exhaustive or definitive. Returning to the visual anal5174 ogy, we can see these dimensions as primarily concerned with how (and what part of) a conceptual “scene” is perceived (PERSPECTIVE, PROMI NENCE ); the choice or categorization of which schematic structures are present (CONFIGURATION and METAPHOR); or both (RESOLUTION). We have omitted another high-level categorizat"
2020.acl-main.462,J00-4004,0,0.137367,"ction has attested psychological implications, including differences in word recognition time (Gillon et al., 1999) (see Fieder et al. (2014) for a review). Relevant NLP research. Configurational properties are closely linked to well-studied challenges at the syntax-semantic interface, in particular nominal and aspectual coercion effects (§4). Several approaches explicitly model coercion operations based on event structure representations (Moens and Steedman, 1988; Passonneau, 1988; Pulman, 1997; Chang et al., 1998), while others explore statistical learning of aspectual classes and features (Siegel and McKeown, 2000; Mathew and Katz, 2009; Friedrich and Palmer, 2014). Lexical resources have also been developed for aspectual annotation (Donatelli et al., 2018) and the count/ mass distinction (Schiehlen and Spranger, 2006; Kiss et al., 2017). 3.5 Metaphor The dimension of METAPHOR is broadly concerned with cross-domain comparison, in which speakers “conceptualize two distinct structures in relation to one another” (Langacker, 1993, p. 450). Metaphors have been analyzed as structured mappings that allow a target domain to be conceptualized in terms of a source domain (Lakoff and Johnson, 1980). Metaphors pe"
2020.acl-main.462,2020.acl-main.559,0,0.01838,". How well do such models capture the phenomena reviewed in §3, and where do they fall short? A fuller account of the constellation of factors involved in construal should also take seriously the grounded, situated nature of language use (Harnad, 1990; Kiros et al., 2018; Bender and Koller, 2020; Bisk et al., 2020). Frameworks motivated by the linguistic insights mentioned in §2 (such as the work on computational construction grammar referenced in §4) and by growing evidence of embodied simulations as the basis for meaning (Narayanan, 1999; Bergen and Chang, 2005; Feldman, 2006; Bergen, 2012; Tamari et al., 2020) are especially relevant lines of inquiry. Much work remains to flesh out the construal dimensions, operations and phenomena preliminarily identified in §3 and §4, especially in connecting to typological, sociolinguistic, developmental, and neural constraints on conceptualization. We believe a concerted effort across the language sciences would provide valuable guidance for developing better NL systems and resources. 7 Conclusion As the saying goes, the camera doesn’t lie—but it may tell us only a version of the truth. The same goes for language. Some of the phenomena we have described may see"
2020.acl-main.462,W11-0910,0,\N,Missing
2020.acl-main.462,W18-4912,1,\N,Missing
2020.acl-main.462,D18-1166,0,\N,Missing
2020.acl-main.462,2020.tacl-1.3,0,\N,Missing
2020.acl-main.462,2020.emnlp-main.703,0,\N,Missing
2020.acl-main.462,W19-3322,0,\N,Missing
2020.acl-main.696,W04-0103,0,0.0827145,"Missing"
2020.acl-main.696,P06-2114,0,0.08149,"Missing"
2020.coling-main.264,E17-2039,0,0.0668035,"Missing"
2020.coling-main.264,P98-1013,0,0.62291,"s. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in principle be applied to any language. This fact distinguishes UCCA and STREUSLE from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998; Fillmore and Baker, 2009) and the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005). The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses few lexicon-free roles, but its semantics is determined by a valency lexicon. The Parallel Meaning Bank (Abzianidze et al., 2017) uses lexicon-free5 VerbNet (Schuler, 2005) semantic roles. The STREUSLE tagset for preposition supersenses generalizes VerbNet’s role set to cover non-core arguments/adjuncts of verbs, as well as prepositional complements of nouns and adjectives."
2020.coling-main.264,D16-1134,1,0.684487,"ntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a situation mentioned in the sentence, typically involving a scene-"
2020.coling-main.264,K19-2007,0,0.0143559,"d for data-driven analysis and complement the rules. TUPA. This UCCA parser (Hershcovich et al., 2017) is based on a transition-based algorithm with a neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al., 2019). While Che et al. (2019) fine-tuned BERT (Devlin et al., 2019) for contextualized word representation, our delexicalized version replaces it with UD and STREUSLE features: POS tag, dependency relation, supersenses (scene role and function; see §2), the lexical category of the word or the MWE that the word is part of, and the BIO tag. These are concatenated to form word representations.10 5 Experiments Data. We use the Reviews section from UD 2.6 English_EWT (Zeman et al., 2020), with lexical semantic ann"
2020.coling-main.264,N18-2020,1,0.848296,"in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a situation mentioned in the sentence, typically involving a scene-evoking predicate, partic"
2020.coling-main.264,2020.dmr-1.5,1,0.734608,"bels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same supersense inventory and are identical for many tokens. The lexcat annotations (syntactic category of lexical unit) is a slight extension to the Universal POS tagset, adding categories for certain MWE subtypes, such as light verb constructions, following Walsh 1 For further details, see the extensive UCCA annotation manual: https://github.com/UniversalConceptualCognitiveAnnotation/docs/blob/master/guidelines.pdf 2 UCCA also supports implicit units which do not correspond to any tokens (Cui and Hershcovich, 2020), but these are excluded from parsing evaluation and we ignore them for purposes of this paper. 3 STREUSLE distinguishes strong MWEs, which are opaque (noncompositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations"
2020.coling-main.264,N19-1423,0,0.0113546,"neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al., 2019). While Che et al. (2019) fine-tuned BERT (Devlin et al., 2019) for contextualized word representation, our delexicalized version replaces it with UD and STREUSLE features: POS tag, dependency relation, supersenses (scene role and function; see §2), the lexical category of the word or the MWE that the word is part of, and the BIO tag. These are concatenated to form word representations.10 5 Experiments Data. We use the Reviews section from UD 2.6 English_EWT (Zeman et al., 2020), with lexical semantic annotations from STREUSLE 4.4 (Schneider and Smith, 2015; Schneider et al., 2018),11 and with UCCA graphs from UCCA_English-EWT v1.0.1 (Hershcovich et al.,"
2020.coling-main.264,W17-6811,0,0.0125142,"s, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with"
2020.coling-main.264,P17-1104,1,0.842047,"as the C. 4 Second Conversion Approach: Delexicalized Supervised UCCA Parsing Previous work tackled the UCCA parsing task using supervised learning. In order to complement and validate the analysis of the rule-based converter, we compare its findings to a delexicalized supervised parser, that can be seen as inducing a converter from data. By removing all word and lemma features from these parsers, and instead adding features based on gold UD and STREUSLE annotations, we obtain supervised “converters”, which can be used for data-driven analysis and complement the rules. TUPA. This UCCA parser (Hershcovich et al., 2017) is based on a transition-based algorithm with a neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al.,"
2020.coling-main.264,P18-1035,1,0.846519,"rties into account? What does this suggest about the potential for exploiting simpler or better-resourced linguistic representations for improved MR parsing? Intuitively, we ask whether: ? sentence-level MR = syntax + lexical semantics To address this question, we examine UCCA, a document-level MR often used for sentence-level semantics (see §2.1). Hershcovich et al. (2019) began to examine the relation of UCCA to syntax, contributing a corpus with gold standard UD and UCCA parses, heuristically aligning them, and quantifying the correlations between syntactic and semantic labels. Conversely, Hershcovich et al. (2018) provided some initial evidence that other MRs can be brought to bear on the UCCA parsing task via multitask learning, but left the details of the relationship between representations to latent (and opaque) parameters of neural models. In this paper, we aim to close the gap between the two previous investigations by (1) building an interpretable rule-based system to convert from shallower representations (syntax and lexical semantic units/tags) into UCCA, forcing us to be linguistically precise about what UCCA captures and how it “decomposes”; and (2) training top-performing supervised parsers"
2020.coling-main.264,N19-1047,1,0.900698,"rm linguistic understanding of MRs. In particular: are they merely a coarsening and rearranging of syntactic information, such as is encoded in Universal Dependencies (UD; Nivre et al., 2016, 2020)? To what extent do they take lexical semantic properties into account? What does this suggest about the potential for exploiting simpler or better-resourced linguistic representations for improved MR parsing? Intuitively, we ask whether: ? sentence-level MR = syntax + lexical semantics To address this question, we examine UCCA, a document-level MR often used for sentence-level semantics (see §2.1). Hershcovich et al. (2019) began to examine the relation of UCCA to syntax, contributing a corpus with gold standard UD and UCCA parses, heuristically aligning them, and quantifying the correlations between syntactic and semantic labels. Conversely, Hershcovich et al. (2018) provided some initial evidence that other MRs can be brought to bear on the UCCA parsing task via multitask learning, but left the details of the relationship between representations to latent (and opaque) parameters of neural models. In this paper, we aim to close the gap between the two previous investigations by (1) building an interpretable rul"
2020.coling-main.264,S17-1022,1,0.90147,"Missing"
2020.coling-main.264,2020.dmr-1.6,1,0.720944,"nal, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but co"
2020.coling-main.264,W04-2705,0,0.331073,"Missing"
2020.coling-main.264,L18-1537,0,0.0446629,"Missing"
2020.coling-main.264,2020.lrec-1.497,0,0.0283756,"Missing"
2020.coling-main.264,L16-1262,0,0.0457846,"Missing"
2020.coling-main.264,K19-2001,1,0.895831,"Missing"
2020.coling-main.264,J05-1004,0,0.278998,"upersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in principle be applied to any language. This fact distinguishes UCCA and STREUSLE from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998; Fillmore and Baker, 2009) and the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005). The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses few lexicon-free roles, but its semantics is determined by a valency lexicon. The Parallel Meaning Bank (Abzianidze et al., 2017) uses lexicon-free5 VerbNet (Schuler, 2005) semantic roles. The STREUSLE tagset for preposition supersenses generalizes VerbNet’s role set to cover non-core arguments/adjuncts of verbs, as well as prepositional complements of nouns and adjectives. Universal Decompositional Semantics (DeComp) defines semantic roles as bundles of lexicon-free features. Cross-linguistic applicability in"
2020.coling-main.264,2020.lrec-1.733,1,0.827363,"ntically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to"
2020.coling-main.264,D14-1162,0,0.0858457,"UD+STREUSLE (middle), and supervised parsers with word features (bottom). Evaluation. We use standard UCCA parsing evaluation, matching edges by the terminal yields of their endpoint units.13 Labeled precision, recall and F1-score consider the edge categories when matching edges. Where an edge has multiple categories, each of them is considered separately. 6 Results Table 2 shows the EWT Reviews dev scores. For comparison with parsers that have access to words, we also show the TUPA dev results from Hershcovich et al. (2019), who used syntactic features from the gold UD annotation and GloVe (Pennington et al., 2014);14 and the HIT-SCIR parser with BERT/GloVe, and with UD+STREUSLE features. Rules with gold UD and STREUSLE close the gap between the syntax-based converter and parsers with word information, reaching the same primary labeled F1 as TUPA with word features. This is surprising (since supervised parsers are known to usually outperform rule-based ones), and suggests that the training data (see table 1) was insufficient for the parser to learn a mapping as accurate as the complex conversion rules (described in §3). Enhancing GloVe-based HIT-SCIR with UD and STREUSLE yields similar results. However,"
2020.coling-main.264,picca-etal-2008-supersense,0,0.0245028,"s strong MWEs, which are opaque (noncompositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense feature"
2020.coling-main.264,K19-1017,1,0.839748,"excat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in"
2020.coling-main.264,P18-1018,1,0.932403,"es, such as the dotted edge from the possession scene unit to vehicle. 2.2 Universal Dependencies UD is a syntactic dependency scheme used in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. An example UD tree appears at the bottom of figure 1. 2.3 STREUSLE STREUSLE (Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions) is a corpus annotated comprehensively for several forms of lexical semantics (Schneider and Smith, 2015; Schneider et al., 2018). All kinds of multi-word expressions (MWEs) are annotated, giving each sentence a lexical semantic segmentation.3 Syntactic and semantic tags are then applied to individual units (single- and multi-word). The semantic tags are supersenses for noun, verb, and prepositional/possessive units. Preposition supersenses include two tiers of annotation: scene role labels represent the semantic role of the prepositional phrase marked by the preposition, and function labels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same supersense inventory and a"
2020.coling-main.264,N13-1076,1,0.760943,"mpositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, trainin"
2020.coling-main.264,N15-1177,1,0.875791,"s, which express reentrancies, such as the dotted edge from the possession scene unit to vehicle. 2.2 Universal Dependencies UD is a syntactic dependency scheme used in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. An example UD tree appears at the bottom of figure 1. 2.3 STREUSLE STREUSLE (Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions) is a corpus annotated comprehensively for several forms of lexical semantics (Schneider and Smith, 2015; Schneider et al., 2018). All kinds of multi-word expressions (MWEs) are annotated, giving each sentence a lexical semantic segmentation.3 Syntactic and semantic tags are then applied to individual units (single- and multi-word). The semantic tags are supersenses for noun, verb, and prepositional/possessive units. Preposition supersenses include two tiers of annotation: scene role labels represent the semantic role of the prepositional phrase marked by the preposition, and function labels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same s"
2020.coling-main.264,Q19-1027,0,0.0155495,"Time (T) and Quantifier (Q) expressions frequently coincide with certain syntactic categories such as adverbs and prepositions, and can typically be identified from corresponding supersenses, if available. The converter tends to err on the conservative side, falling back to Adverbials (D) and Elaborators (E) when it cannot find sufficient explicit semantic evidence. 7.3 Low Match—Divergences or Insufficient Information Noun compound interpretation. Lexical composition in noun compounds evokes various forms of event structures, which are underspecified by the meaning of the constituent words (Shwartz and Dagan, 2019). While often compounding is used for Elaboration, as in [E tap] [C water], it is not necessarily always the case. For example, in [C sea] [C bottom] both “sea"" and “bottom"" are Center, since they reflect part-whole relations. The modifier may also be a Participant in the scene evoked by the head, as in [A road] [P construction]. This is partially encoded in STREUSLE, as the fact that the MWE “road construction” has the N . EVENT supersense indicates that it is scene-evoking, but it still does not reveal the relationship between the constituent words. Adverbs and linkage. While many syntactic"
2020.coling-main.264,W15-3502,1,0.772929,"besides STREUSLE, UD also serves as the backbone of the DeComp scheme (White et al., 2016), and so information as to its semantic content is important there as well. Argument structural phenomena are at the heart of many MRs, which provide further motivation for empirical studies to the extent lexical semantics and syntax can encode them. 2.1 Universal Conceptual Cognitive Annotation Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available f"
2020.coling-main.264,P18-1016,1,0.84781,"gets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a s"
2020.coling-main.264,tsvetkov-etal-2014-augmenting-english,1,0.741527,"gular A or C. Scene-evoking adjectives. Inspecting the high-frequency confusions, adjectives stand out as persistent error inducers. Different classes of adjectives are handled differently in UCCA: e.g., while most adjectives are scene-evoking, pertainyms (academic), inherent-composition modifiers (sugary), and quantity modifiers (many) are not. Some adjectives are ambiguous: a legal practice may refer to a behavior that is legal as opposed to illegal, in which case it should be scene-evoking, or to a law office, in which case it should not. Enriching STREUSLE with supersenses for adjectives (Tsvetkov et al., 2014) might be fruitful for such distinctions. Even with lexical disambiguation, the scene attachment of the adjective may be ambiguous: e.g. a good chef probably means a chef who cooks well, so good should be an Adverbial in the scene evoked by chef—in contrast with a tall chef, where tall is not part of the cooking scene and instead should evoke a State. Predicative adjectives, and adjective modifiers in predicative NPs, are another source of difficulty, especially when they occur in fragments: sometimes the adjective is annotated as evoking the main scene, and sometimes not. Determining this req"
2020.coling-main.264,W18-4921,1,0.864317,"Missing"
2020.coling-main.264,D16-1177,0,0.0622494,"Missing"
2020.coling-main.264,D18-1194,0,0.0488351,"Missing"
2020.coling-main.420,W13-2322,1,0.847637,"problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems. 1 Introduction Abstract Meaning Representation, or AMR (Banarescu et al., 2013), is a representation of the meaning of a sentence as a rooted, labeled, directed acyclic graph. For example, (l / label-01 :ARG0 (c / country :wiki ""Georgia_(country)"" :name (n / name :op1 ""Georgia"")) :ARG1 (s / support-01 :ARG0 (c2 / country :wiki ""Russia"" :name (n2 / name :op1 ""Russia""))) :ARG2 (a / act-02 :mod (a2 / annex-01))) represents the sentence “Georgia labeled Russia’s support an act of annexation.” AMR does not represent some morphological and syntactic details such as tense, number, definiteness, and word order; thus, this same AMR could also represent alternate phrasings such as"
2020.coling-main.420,W05-0909,0,0.834162,"greeing on which system was the best. Concerns have also been raised about the suitability of BLEU for NLG in general; for example, Reiter (2018) found that BLEU has generally poor correlations with human judgments for NLG. Novikova et al. (2017) compared many metrics to human judgments on NLG from meaning representations and concluded that use of reference-based metrics relies on an invalid assumption that references are correct and complete enough to be used as a gold standard. Some recent AMR generation papers have reported other automatic metrics alongside BLEU. Many have reported METEOR (Banerjee and Lavie, 2005), and a few have included TER (Snover et al., 2006) and, more recently, CHRF++ (Popovic, 2017) and BERTScore (Zhang et al., 2020). However, it is unclear how accurately any of these metrics capture the relative performance of AMR generation systems. Human Evaluation: Prior to this work, the only human evaluation comparing several AMR generation systems was the SemEval-2017 AMR shared task, which used a ranking-based evaluation of five systems (May and Priyadarshi, 2017). All of these systems perform far below the current state-of-the-art, making a new evaluation necessary. While most AMR gene"
2020.coling-main.420,N19-1223,0,0.0254961,"graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al., 2019), and a densely-connected graph convolutional network (Guo et al., 2019). Recent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts within a sequence-to-sequence transformer model (Zhu et al., 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al., 2020). While neural approaches have achieved state-of-the-art BLEU scores, a few recent works have instead approached AMR generation through more rule-based methods. Manning (2019) constrains their system with rules, supplemented by simple statistical models, to avoid certain types of errors, such as hallucinations, that are possible in neural systems. Lapalme (2019) create a fully rule-based generation system to help humans check their AMR annotations. 3 Methodology We conduct a human evaluation of several AMR generation systems. §3.1 discusses th"
2020.coling-main.420,N19-1366,0,0.0190586,"Recent Advances in AMR Generation Shortly after the 2017 shared task, Konstas et al. (2017) made significant advances to the field with a neural sequence-to-sequence approach, mitigating the limitations of the small amount of AMR-annotated 4774 data by augmenting training data with a jointly-trained parser. Later work (Song et al., 2018) builds on this approach but uses a graph-to-sequence model to preserve information from the structure of the AMR. Several recent papers have explored variations on a graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al., 2019), and a densely-connected graph convolutional network (Guo et al., 2019). Recent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts within a sequence-to-sequence transformer model (Zhu et al., 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al., 2020). While neural approaches have achieved state-o"
2020.coling-main.420,W14-3348,0,0.131443,"Missing"
2020.coling-main.420,N16-1087,0,0.236524,"Missing"
2020.coling-main.420,Q19-1019,0,0.201358,"rt, making a new evaluation necessary. While most AMR generation papers have reported no human evaluation of their systems, a few have conducted smaller-scale evaluations. Ribeiro et al. (2019) conducted a Mechanical Turk evaluation to compare their best graph encoder model with a sequence-to-sequence baseline, finding that their model performs better on both meaning similarity between the generated sentence and the gold reference, and readability of the generated sentence. Mager et al. (2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment. For the three systems included in both our evaluation and theirs, the relative results are comparable; Mager et al. (2020) find their own system to be better than all three. Lapalme (2019) also conducted a small human evaluation in which annotators chose the best output out of three options: their own system, ISI (Pourdamghani et al., 2016), and JAMR (Flanigan et al., 2016). They find that their rule-based system is on par with ISI and much better than JAMR, despite having a much lower BLEU. Beyond AM"
2020.coling-main.420,P17-1014,0,0.0496726,"of one with 25); more thorough instructions were given, with examples; and wording was changed from “sentence” to “utterance” to reflect that some are not full sentences in a grammatical sense. 3.3 Main Evaluation The main evaluation was larger than the pilot, and evaluated more recent systems, most of which are of a markedly higher quality than those in the pilot. We contacted the authors of several recent papers on AMR-to-English generation to obtain their system’s output for use in the evaluation, and included all 4776 systems for which we obtained usable data in time to begin evaluation: Konstas et al. (2017), Guo et al. (2019), Manning (2019), Ribeiro et al. (2019), and Zhu et al. (2019). These systems are all described in §2.2. The Konstas and Zhu systems both approach AMR generation as a sequence-to-sequence task: Konstas is pretrained on ‘silver’ data from a jointly-trained AMR parser to mitigate the limitations of the small amount of gold AMR data, while Zhu uses a transformer-based model. Guo and Ribeiro both use graph-tosequence models: Guo uses densely-connected graph convolutional networks to model the graph structure, while Ribeiro uses a dual representation to capture both bottom-up and"
2020.coling-main.420,C18-1101,0,0.0148943,""" :name (n2 / name :op1 ""Russia""))) :ARG2 (a / act-02 :mod (a2 / annex-01))) represents the sentence “Georgia labeled Russia’s support an act of annexation.” AMR does not represent some morphological and syntactic details such as tense, number, definiteness, and word order; thus, this same AMR could also represent alternate phrasings such as “Russia’s support is being labeled an act of annexation by Georgia.” AMR generation is the task of generating a sentence in natural language (in this case, English) from an AMR graph. This has applications to a range of NLP tasks, including summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Like other Natural Language Generation (NLG) tasks, this is difficult to evaluate due to the range of possible valid sentences corresponding to any single AMR. Currently, AMR generation systems are often evaluated only with automatic metrics that compare a generated sentence to a single human-authored reference; for AMR, this is the sentence from which the AMR graph was created. However, there is evidence that these metrics may not be a good representation of human judgments for AMR generation (May and Priyadarshi, 2017) and NLG in general (see §2."
2020.coling-main.420,N19-3009,1,0.905566,"19), and a densely-connected graph convolutional network (Guo et al., 2019). Recent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts within a sequence-to-sequence transformer model (Zhu et al., 2019), generating syntactic constituency trees as an intermediate step before generating surface structure (Cao and Clark, 2019), and fine-tuning GPT-2 on AMR (Mager et al., 2020). While neural approaches have achieved state-of-the-art BLEU scores, a few recent works have instead approached AMR generation through more rule-based methods. Manning (2019) constrains their system with rules, supplemented by simple statistical models, to avoid certain types of errors, such as hallucinations, that are possible in neural systems. Lapalme (2019) create a fully rule-based generation system to help humans check their AMR annotations. 3 Methodology We conduct a human evaluation of several AMR generation systems. §3.1 discusses the general survey design, while §3.2 discusses details of the pilot survey, which validates the methodology by applying it to data from the SemEval evaluation, and §3.3 discusses the evaluation of more recent systems. Figure 1:"
2020.coling-main.420,S17-2090,0,0.242614,"NLP tasks, including summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Like other Natural Language Generation (NLG) tasks, this is difficult to evaluate due to the range of possible valid sentences corresponding to any single AMR. Currently, AMR generation systems are often evaluated only with automatic metrics that compare a generated sentence to a single human-authored reference; for AMR, this is the sentence from which the AMR graph was created. However, there is evidence that these metrics may not be a good representation of human judgments for AMR generation (May and Priyadarshi, 2017) and NLG in general (see §2.1). Thus, in this work, we present a new human evaluation of several recent AMR generation systems, most of which had not previously been manually evaluated. Our methodology (§3) differs in several ways from previous evaluations of AMR generation, including separate direct assessment of fluency and adequacy; and asking annotators to evaluate sentences without comparison to a reference, in order to avoid biasing them toward a particular wording. We analyze (§4) what our results show about the relative quality of the systems and how this compares to their scores from"
2020.coling-main.420,D17-1238,0,0.195424,"Missing"
2020.coling-main.420,P02-1040,0,0.111298,"ces which received low scores from annotators, identifying issues for future researchers to address including hallucination, anonymization, and repetition. 2 Background In §2.1 we discuss previous work on evaluation of AMR generation and related NLG tasks, both with automatic metrics and human evaluation. In §2.2 we survey recent work in AMR generation, including describing the systems which we evaluate. 2.1 Evaluation of AMR Generation Automatic Metrics: The vast majority of AMR generation papers measure their performance only with automatic metrics. The most common of these metrics is BLEU (Papineni et al., 2002), which is typically used to determine the state of the art. However, it is unclear whether BLEU is a reliable metric to compare AMR generation systems: May and Priyadarshi (2017) found that BLEU disagreed with human judgments on the ranking of five AMR generation systems, including disagreeing on which system was the best. Concerns have also been raised about the suitability of BLEU for NLG in general; for example, Reiter (2018) found that BLEU has generally poor correlations with human judgments for NLG. Novikova et al. (2017) compared many metrics to human judgments on NLG from meaning repr"
2020.coling-main.420,W17-4770,0,0.0418418,"Missing"
2020.coling-main.420,W18-6319,0,0.0571871,"Missing"
2020.coling-main.420,W16-6603,0,0.0876278,"Missing"
2020.coling-main.420,J18-3002,0,0.0511248,"Missing"
2020.coling-main.420,D19-1314,0,0.322069,"ng et al., 2020). However, it is unclear how accurately any of these metrics capture the relative performance of AMR generation systems. Human Evaluation: Prior to this work, the only human evaluation comparing several AMR generation systems was the SemEval-2017 AMR shared task, which used a ranking-based evaluation of five systems (May and Priyadarshi, 2017). All of these systems perform far below the current state-of-the-art, making a new evaluation necessary. While most AMR generation papers have reported no human evaluation of their systems, a few have conducted smaller-scale evaluations. Ribeiro et al. (2019) conducted a Mechanical Turk evaluation to compare their best graph encoder model with a sequence-to-sequence baseline, finding that their model performs better on both meaning similarity between the generated sentence and the gold reference, and readability of the generated sentence. Mager et al. (2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment. For the three systems included in both our evaluation and theirs, the relative resu"
2020.coling-main.420,2006.amta-papers.25,0,0.590164,"been raised about the suitability of BLEU for NLG in general; for example, Reiter (2018) found that BLEU has generally poor correlations with human judgments for NLG. Novikova et al. (2017) compared many metrics to human judgments on NLG from meaning representations and concluded that use of reference-based metrics relies on an invalid assumption that references are correct and complete enough to be used as a gold standard. Some recent AMR generation papers have reported other automatic metrics alongside BLEU. Many have reported METEOR (Banerjee and Lavie, 2005), and a few have included TER (Snover et al., 2006) and, more recently, CHRF++ (Popovic, 2017) and BERTScore (Zhang et al., 2020). However, it is unclear how accurately any of these metrics capture the relative performance of AMR generation systems. Human Evaluation: Prior to this work, the only human evaluation comparing several AMR generation systems was the SemEval-2017 AMR shared task, which used a ranking-based evaluation of five systems (May and Priyadarshi, 2017). All of these systems perform far below the current state-of-the-art, making a new evaluation necessary. While most AMR generation papers have reported no human evaluation of"
2020.coling-main.420,Q19-1002,0,0.0137449,"/ act-02 :mod (a2 / annex-01))) represents the sentence “Georgia labeled Russia’s support an act of annexation.” AMR does not represent some morphological and syntactic details such as tense, number, definiteness, and word order; thus, this same AMR could also represent alternate phrasings such as “Russia’s support is being labeled an act of annexation by Georgia.” AMR generation is the task of generating a sentence in natural language (in this case, English) from an AMR graph. This has applications to a range of NLP tasks, including summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Like other Natural Language Generation (NLG) tasks, this is difficult to evaluate due to the range of possible valid sentences corresponding to any single AMR. Currently, AMR generation systems are often evaluated only with automatic metrics that compare a generated sentence to a single human-authored reference; for AMR, this is the sentence from which the AMR graph was created. However, there is evidence that these metrics may not be a good representation of human judgments for AMR generation (May and Priyadarshi, 2017) and NLG in general (see §2.1). Thus, in this work, we present a new hum"
2020.coling-main.420,P18-1150,0,0.0270604,", many studies have found that these metrics are not a reliable proxy for human judgments. One example of the use of human evaluation is the Conference on Machine Translation (WMT), which runs an annual evaluation of machine translation systems (e.g. Barrault et al., 2019). 2.2 Recent Advances in AMR Generation Shortly after the 2017 shared task, Konstas et al. (2017) made significant advances to the field with a neural sequence-to-sequence approach, mitigating the limitations of the small amount of AMR-annotated 4774 data by augmenting training data with a jointly-trained parser. Later work (Song et al., 2018) builds on this approach but uses a graph-to-sequence model to preserve information from the structure of the AMR. Several recent papers have explored variations on a graphto-sequence approach: improvements in encoding reentrancies and long-range dependencies (Damonte and Cohen, 2019), a dual graph encoder that captures top-down and bottom-up representations of graph structure (Ribeiro et al., 2019), and a densely-connected graph convolutional network (Guo et al., 2019). Recent sequence-to-sequence approaches include using structure-aware self-attention to capture relations between concepts wi"
2020.coling-main.420,D19-1548,0,0.599192,"hile most AMR generation papers have reported no human evaluation of their systems, a few have conducted smaller-scale evaluations. Ribeiro et al. (2019) conducted a Mechanical Turk evaluation to compare their best graph encoder model with a sequence-to-sequence baseline, finding that their model performs better on both meaning similarity between the generated sentence and the gold reference, and readability of the generated sentence. Mager et al. (2020) carry out a human evaluation of overall quality, comparing their GPT-2-based system to three others (Guo et al., 2019; Ribeiro et al., 2019; Zhu et al., 2019), all of which are also evaluated in our experiment. For the three systems included in both our evaluation and theirs, the relative results are comparable; Mager et al. (2020) find their own system to be better than all three. Lapalme (2019) also conducted a small human evaluation in which annotators chose the best output out of three options: their own system, ISI (Pourdamghani et al., 2016), and JAMR (Flanigan et al., 2016). They find that their rule-based system is on par with ISI and much better than JAMR, despite having a much lower BLEU. Beyond AMR generation, other NLG tasks are also of"
2020.coling-main.420,W15-4708,0,\N,Missing
2020.coling-main.420,W16-2301,0,\N,Missing
2020.coling-tutorials.1,P13-1023,1,0.776661,"nd supports rapid annotation. The tutorial will provide a detailed introduction to the UCCA annotation guidelines, design philosophy and the available resources; and a comparison to other meaning representations. It will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains. 1 Introduction Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013), abbreviated as “UCCA”, is a symbolic meaning representation (MR) that supports human annotation of text with broad coverage. While several meaning representation schemes share this goal (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotate"
2020.coling-tutorials.1,P17-1008,1,0.85136,"will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains. 1 Introduction Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013), abbreviated as “UCCA”, is a symbolic meaning representation (MR) that supports human annotation of text with broad coverage. While several meaning representation schemes share this goal (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotated on several corpora of different genres and languages,1 as summarized in table 1. Pilot studies have been conducted in additional languages. A web-based annotation system is available (Abend et al., 2017). In UCCA,"
2020.coling-tutorials.1,P17-4019,1,0.922155,"l (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotated on several corpora of different genres and languages,1 as summarized in table 1. Pilot studies have been conducted in additional languages. A web-based annotation system is available (Abend et al., 2017). In UCCA, an analysis of a text passage is a directed acyclic graph over semantic elements called units. The principal kind of unit is a scene, which describes an action, movement or state, and is similar to FrameNet’s notion of a frame. Figure 1 contains three scenes, evoked, respectively, by the verb took, the noun phrase a repair, and the possessive our. Several elements are exemplified, including participants, secondary relations, and scene linkage. The graph is anchored in the text tokens (the leaves generally correspond to one or more tokens), and relations between units are indicated b"
2020.coling-tutorials.1,D19-3009,0,0.0678743,"abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda"
2020.coling-tutorials.1,2020.conll-shared.7,1,0.892951,"gners understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will i"
2020.coling-tutorials.1,K19-2008,0,0.0203831,"ulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow resea"
2020.coling-tutorials.1,P98-1013,0,0.725327,"t al., 2019; Oepen et al., 2020) show that multi-task meaning representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to"
2020.coling-tutorials.1,W13-2322,1,0.6552,"ing representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018"
2020.coling-tutorials.1,D16-1134,1,0.883248,"e 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al"
2020.coling-tutorials.1,L16-1601,0,0.0227815,"s not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, whi"
2020.coling-tutorials.1,P18-1035,1,0.942057,"aning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of the various approaches taken by existing parsers, and prepare attendees to work on UCCA parsing themselves. Furthermore, UCCA parsing has been shown to benefit from multi-task learning (Caruana, 1997) with 2 other meaning representations (Hershcovich et al., 2018), although results from the CoNLL 2019 and CoNLL 2020 shared tasks (Oepen et al., 2019; Oepen et al., 2020) show that multi-task meaning representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes"
2020.coling-tutorials.1,N19-1047,1,0.913183,"Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of the various approaches taken by existing parsers, and prepare attendees to work on UCCA parsing themselves. Furth"
2020.coling-tutorials.1,Q16-1023,0,0.0330143,"d typology. The necessary background will be provided as part of the tutorial. However, participants are expected to know about basic data structures such as trees and graphs. For the parsing section, prior knowledge is assumed about common machine learning techniques, including supervised learning and neural networks. 3.2 Reading list The following are recommended to read before the tutorial, as they provide background and frame the context in which the tutorial materials lie: 1. Chapter 3 of Dixon (2005) contains an introduction to some basic concepts in semantics on which UCCA is based. 2. Kiperwasser and Goldberg (2016) present a transition-based parser using an architecture on which TUPA, the first UCCA parser, is based (Hershcovich et al., 2017). 3. Peng et al. (2017) performed multi-task learning for meaning representation parsing, inspiring work on cross-framework parsing for UCCA (Hershcovich et al., 2018). 4. Abend and Rappoport (2017) compare and constrast several meaning representations according to various aspects. 5. Deng and Xue (2017) investigate translation divergences using a hierarchical alignment, and discuss bridging them with cross-lingual semantic representations. 6. Croft et al. (2017) li"
2020.coling-tutorials.1,P19-4002,0,0.0205062,", basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui and Hershcovich, 2020). 5. Relation to other representations (15m). Comparison to other meaning representations (Abend and Rappoport, 2017; Koller et al., 2019; Hershcovich et al., 2020) and to UD (Hershcovich et al., 2019a). 6. Parsing (25m). TUPA (Hershcovich et al., 2017; Hershcovich et al., 2018; Hershcovich and Arviv, 2019; Arviv et al., 2020), SemEval 2019 Task 1 (Hershcovich et al., 2019b; Jiang et al., 2019), CoNLL 2019 and CoNLL 2020 Shared Tasks (Oepen et al., 2019; Oepen et al., 2020), and more recent parsers (Zhang et al., 2019a). 7. Monolingual tasks and evaluation (20m). Sentence simplification (Sulem et al., 2018b), evaluation of sentence simplification (Sulem et al., 2018a; Alva-Manchego et al., 2019) and grammatical error correction"
2020.coling-tutorials.1,K19-2011,0,0.0153953,"l., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and m"
2020.coling-tutorials.1,K19-2010,0,0.0187105,"Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested"
2020.coling-tutorials.1,W16-1702,0,0.0189137,"great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic dist"
2020.coling-tutorials.1,K19-2004,0,0.017117,"l., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively"
2020.coling-tutorials.1,S19-2012,0,0.0169137,"eserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as w"
2020.coling-tutorials.1,W17-4769,0,0.0403872,"Missing"
2020.coling-tutorials.1,S19-2015,0,0.0117123,"LP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-ling"
2020.coling-tutorials.1,2020.lrec-1.497,0,0.0612167,"Missing"
2020.coling-tutorials.1,K19-2001,1,0.891885,"Missing"
2020.coling-tutorials.1,2020.conll-shared.1,1,0.842632,"Missing"
2020.coling-tutorials.1,J05-1004,0,0.324702,"al will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues"
2020.coling-tutorials.1,P17-1186,0,0.0196926,"d graphs. For the parsing section, prior knowledge is assumed about common machine learning techniques, including supervised learning and neural networks. 3.2 Reading list The following are recommended to read before the tutorial, as they provide background and frame the context in which the tutorial materials lie: 1. Chapter 3 of Dixon (2005) contains an introduction to some basic concepts in semantics on which UCCA is based. 2. Kiperwasser and Goldberg (2016) present a transition-based parser using an architecture on which TUPA, the first UCCA parser, is based (Hershcovich et al., 2017). 3. Peng et al. (2017) performed multi-task learning for meaning representation parsing, inspiring work on cross-framework parsing for UCCA (Hershcovich et al., 2018). 4. Abend and Rappoport (2017) compare and constrast several meaning representations according to various aspects. 5. Deng and Xue (2017) investigate translation divergences using a hierarchical alignment, and discuss bridging them with cross-lingual semantic representations. 6. Croft et al. (2017) list typologically-informed design criteria for Universal Dependencies (Nivre et al., 2020), which are also relevant for other structural representations i"
2020.coling-tutorials.1,K19-1017,1,0.843837,"heir corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic distinctions. We will give an overview of the recently proposed extensions (to support coreference) and joint parsing experiments (Prange et al., 2019a; Prange et al., 2019b). 3 Agenda The planned division of time is as follows: 1. Bird’s eye view (45m). Design philosophy, notion of scenes, basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui"
2020.coling-tutorials.1,W19-3319,1,0.835934,"heir corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic distinctions. We will give an overview of the recently proposed extensions (to support coreference) and joint parsing experiments (Prange et al., 2019a; Prange et al., 2019b). 3 Agenda The planned division of time is as follows: 1. Bird’s eye view (45m). Design philosophy, notion of scenes, basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui"
2020.coling-tutorials.1,S19-2016,0,0.0133934,"(Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representati"
2020.coling-tutorials.1,2020.conll-shared.5,0,0.0188687,"t abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of"
2020.coling-tutorials.1,2020.conll-shared.0,0,0.254101,"Missing"
2020.coling-tutorials.1,N15-4003,1,0.751125,"Missing"
2020.coling-tutorials.1,K19-2012,0,0.0140622,"020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing"
2020.coling-tutorials.1,W15-3502,1,0.92171,"Table 1: Data statistics for existing UCCA corpora. H A A D P took our A P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019"
2020.coling-tutorials.1,N18-1063,1,0.89855,"P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 20"
2020.coling-tutorials.1,P18-1016,1,0.908942,"P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 20"
2020.coling-tutorials.1,2020.starsem-1.6,1,0.83547,"F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019"
2020.coling-tutorials.1,S19-2014,0,0.0231412,"UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross"
2020.coling-tutorials.1,S19-2013,0,0.0142518,"(Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL"
2020.coling-tutorials.1,D16-1177,0,0.0645357,"Missing"
2020.coling-tutorials.1,2020.wmt-1.104,0,0.0835123,"scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Stra"
2020.coling-tutorials.1,S19-2017,0,0.0155621,"ulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et"
2020.coling-tutorials.1,D18-1194,0,0.0382982,"Missing"
2020.coling-tutorials.1,D19-1392,0,0.0259621,"Missing"
2020.coling-tutorials.1,K19-2014,0,0.10178,"and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et"
2020.dmr-1.6,W13-2322,1,0.898194,"construal within the SNACS framework. The light boxes cluster adpositional use 빌은 운동장을 걸었다. by their scene roles. The lines (Bill field-ACC walked) indicate the connections variLocus Theme ous construals have via function labels—what the adposition lexically encodes. Some 카페에서 지나가는 기차 소리를 들었다. of these construals are found He heard the passing train from the cafe. in both languages, others only Locus Source in one or the other. 빌은 운동장에서 걸었다. Bill walked in the field. Locus Locus Locus are omitted from the annotation entirely (as prepositions solely marking roles are omitted from English AMR; Banarescu et al., 2013). SNACS is thus the most promising basis for a semantic framework with which to annotate Korean adpositions. The goal is to pave way for a full-scope, comprehensive treatment of the major semantic dimension of Korean postpositions. To the best of our knowledge, this work is the first to annotate a Korean corpus specifically targeting postpositions. Moreover, this work represents the first Korean application of a lexically-agnostic semantic analysis which cross-cuts adpositional types. 2.3 SNACS Framework The Semantic Network of Adposition and Case Supersenses (SNACS; Schneider et al., 2018, 20"
2020.dmr-1.6,L18-1242,1,0.900523,"Missing"
2020.dmr-1.6,2020.emnlp-main.121,0,0.0327758,"n S TIMULUS↝T OPIC versus Korean’s choice of an E XPLANATION modifier to describe the reason behind the pilot’s mood (caused by the emotion). These divergences are natural variations based on linguistic choice and are complicated by the fact that both English and Korean texts are translations of the original French novella. Bridging such translation divergences (Dorr, 1994; Deng and Xue, 2017) would require a richer modeling of causality and representations that will allow for deeper inferences about the divergent categories (Vyas et al., 2018; Hershcovich et al., 2019; Nikolaev et al., 2020; Briakou and Carpuat, 2020). We do not have a ready proposal to offer for bridging such differences through the SNACS framework. However, investigating further into nuanced semantics like casuality or force dynamics (Croft, 2015, 2012) that would aid generalizations certainly remains a compelling area of future research. 6 Conclusion & Future Work In this work, we have presented the first annotated corpus of Korean preposition supersenses and included a detailed comparison of a subset of the data with a parallel English corpus. We find that, overall, Korean and English adpositions cover similar semantic ground, making E"
2020.dmr-1.6,W19-3314,0,0.0209307,"the POS category of NOUN is assigned to the full (noun + postposition) lexical unit (Oh et al., 2020; Chun et al., 2018). The status of postpositions as functional categories also plays into the lack of specific attention in computational semantic resources. For example, while the labeling of Korean PropBank (Palmer et al., 2006) arguments is to some extent guided by the semantics of the postposition (e.g., a nominative marker might suggest ARG0 and a locative marker, ARGM-LOC), the labels are annotated at the lexical and phrasal level centering on nominal elements. In the case of Korean AMR (Choe et al., 2019), postpositions 2 Here is a list of gloss abbreviations used in this paper: accusative (ACC), dative (DAT), focus (FOC), genitive (GEN), nominative (NOM), question (Q), speculation mood (SPEC) and information topic (TOP). 54 Goal Bill enrolled in the program. Goal Locus 빌은 병원으로 갔다. Bill went to the hospital. Goal Goal 빌은 서점을 갔다. (Bill store-ACC went) Goal Theme Bill listened to the music. Stimulus Goal 빌은 음악을 들었다. (Bill-ACC music listened) Stimulus Theme Stimulus Bill suffers from back pain. Stimulus Source Figure 1: Conceptual diagram illustrating selected adposition usages and how they are r"
2020.dmr-1.6,W11-3801,0,0.0234444,"e, Dative, Genitive, Locative, Allative) for postpositions (e.g., Sohn, 2001), these josa categories, as this paper will show, are only partially adequate in the face of the full range of semantic behaviors we observe in the data. Semantically adequate and comprehensive annotation requires a richer palette of semantic labels that can apply broadly across the postpositional types, for which we turn to SNACS. Computational approaches and resource creation projects have also attempted to classify Korean postpositions, with a focus on morphosyntax (in morphological tagging and syntactic parsing) (Choi and Palmer, 2011; Hong, 2009; Han, 2005). The Penn Korean Treebank (Han et al., 2001), for example, recognizes four part-of-speech (POS) categories (case, adverbial, conjunctive, and auxiliary) to cover all postpositional morphemes, and the 21st Century Sejong Project (Park and Tyers, 2019; Kim, 2006) retains a slightly larger inventory of nine POS tags, generally corresponding to the grammatical categories found in the traditional literature. More recently, the Korean Universal Dependency project guidelines do not directly address the individual postpositions since Korean postpositions are considered sub-lex"
2020.dmr-1.6,L18-1347,1,0.85055,"tegories (case, adverbial, conjunctive, and auxiliary) to cover all postpositional morphemes, and the 21st Century Sejong Project (Park and Tyers, 2019; Kim, 2006) retains a slightly larger inventory of nine POS tags, generally corresponding to the grammatical categories found in the traditional literature. More recently, the Korean Universal Dependency project guidelines do not directly address the individual postpositions since Korean postpositions are considered sub-lexical units. Instead, the POS category of NOUN is assigned to the full (noun + postposition) lexical unit (Oh et al., 2020; Chun et al., 2018). The status of postpositions as functional categories also plays into the lack of specific attention in computational semantic resources. For example, while the labeling of Korean PropBank (Palmer et al., 2006) arguments is to some extent guided by the semantics of the postposition (e.g., a nominative marker might suggest ARG0 and a locative marker, ARGM-LOC), the labels are annotated at the lexical and phrasal level centering on nominal elements. In the case of Korean AMR (Choe et al., 2019), postpositions 2 Here is a list of gloss abbreviations used in this paper: accusative (ACC), dative ("
2020.dmr-1.6,J17-3002,0,0.0194093,"emantics of the head predicate are parallel, but the difference comes from a collocational difference based on which adposition the verb prefers. In the example, English chooses to talk about the topic of the pilot’s emotion S TIMULUS↝T OPIC versus Korean’s choice of an E XPLANATION modifier to describe the reason behind the pilot’s mood (caused by the emotion). These divergences are natural variations based on linguistic choice and are complicated by the fact that both English and Korean texts are translations of the original French novella. Bridging such translation divergences (Dorr, 1994; Deng and Xue, 2017) would require a richer modeling of causality and representations that will allow for deeper inferences about the divergent categories (Vyas et al., 2018; Hershcovich et al., 2019; Nikolaev et al., 2020; Briakou and Carpuat, 2020). We do not have a ready proposal to offer for bridging such differences through the SNACS framework. However, investigating further into nuanced semantics like casuality or force dynamics (Croft, 2015, 2012) that would aid generalizations certainly remains a compelling area of future research. 6 Conclusion & Future Work In this work, we have presented the first annot"
2020.dmr-1.6,J94-4004,0,0.189013,"ple 5, the semantics of the head predicate are parallel, but the difference comes from a collocational difference based on which adposition the verb prefers. In the example, English chooses to talk about the topic of the pilot’s emotion S TIMULUS↝T OPIC versus Korean’s choice of an E XPLANATION modifier to describe the reason behind the pilot’s mood (caused by the emotion). These divergences are natural variations based on linguistic choice and are complicated by the fact that both English and Korean texts are translations of the original French novella. Bridging such translation divergences (Dorr, 1994; Deng and Xue, 2017) would require a richer modeling of causality and representations that will allow for deeper inferences about the divergent categories (Vyas et al., 2018; Hershcovich et al., 2019; Nikolaev et al., 2020; Briakou and Carpuat, 2020). We do not have a ready proposal to offer for bridging such differences through the SNACS framework. However, investigating further into nuanced semantics like casuality or force dynamics (Croft, 2015, 2012) that would aid generalizations certainly remains a compelling area of future research. 6 Conclusion & Future Work In this work, we have pres"
2020.dmr-1.6,Y02-1007,1,0.327548,"001), these josa categories, as this paper will show, are only partially adequate in the face of the full range of semantic behaviors we observe in the data. Semantically adequate and comprehensive annotation requires a richer palette of semantic labels that can apply broadly across the postpositional types, for which we turn to SNACS. Computational approaches and resource creation projects have also attempted to classify Korean postpositions, with a focus on morphosyntax (in morphological tagging and syntactic parsing) (Choi and Palmer, 2011; Hong, 2009; Han, 2005). The Penn Korean Treebank (Han et al., 2001), for example, recognizes four part-of-speech (POS) categories (case, adverbial, conjunctive, and auxiliary) to cover all postpositional morphemes, and the 21st Century Sejong Project (Park and Tyers, 2019; Kim, 2006) retains a slightly larger inventory of nine POS tags, generally corresponding to the grammatical categories found in the traditional literature. More recently, the Korean Universal Dependency project guidelines do not directly address the individual postpositions since Korean postpositions are considered sub-lexical units. Instead, the POS category of NOUN is assigned to the full"
2020.dmr-1.6,N19-1047,0,0.113701,"ses to talk about the topic of the pilot’s emotion S TIMULUS↝T OPIC versus Korean’s choice of an E XPLANATION modifier to describe the reason behind the pilot’s mood (caused by the emotion). These divergences are natural variations based on linguistic choice and are complicated by the fact that both English and Korean texts are translations of the original French novella. Bridging such translation divergences (Dorr, 1994; Deng and Xue, 2017) would require a richer modeling of causality and representations that will allow for deeper inferences about the divergent categories (Vyas et al., 2018; Hershcovich et al., 2019; Nikolaev et al., 2020; Briakou and Carpuat, 2020). We do not have a ready proposal to offer for bridging such differences through the SNACS framework. However, investigating further into nuanced semantics like casuality or force dynamics (Croft, 2015, 2012) that would aid generalizations certainly remains a compelling area of future research. 6 Conclusion & Future Work In this work, we have presented the first annotated corpus of Korean preposition supersenses and included a detailed comparison of a subset of the data with a parallel English corpus. We find that, overall, Korean and English"
2020.dmr-1.6,S17-1022,1,0.579352,"Missing"
2020.dmr-1.6,P14-1120,0,0.0164622,"roadly capture prepositional semantics without particular reference to any lexicon. The current version of the scheme defines 50 such supersenses for event participant roles (inspired by traditional thematic roles: AGENT, T HEME, R ECIPIENT, etc.), circumstantial roles (e.g. T IME, M ANNER), and roles describing relationships between entities (e.g. P OSSESSOR, W HOLE, Q UANTITY VALUE). Annotating adposition uses in context serves to disambiguate them—e.g. “the wheel of the car” (W HOLE) versus “the destruction of the city” (T HEME). Unlike dictionary senses (cf. Litkowski and Hargraves, 2005; Litkowski, 2014), the supersenses transcend lexical types in order to group together different adpositions with related meanings: thus W HOLE applies to both “the wheel of the car” and “the paint on the car”. While the semantic criteria aim to be language-agnostic, the details of how to apply these labels to disambiguate adposition tokens in text—including specific criteria for which tokens to annotate, and how to deal with various language-specific constructions—need to be developed on a per-language basis. Extensive guidelines and multiple annotated corpora (web reviews; The Little Prince) are available for"
2020.dmr-1.6,2020.iwpt-1.13,1,0.732407,"f-speech (POS) categories (case, adverbial, conjunctive, and auxiliary) to cover all postpositional morphemes, and the 21st Century Sejong Project (Park and Tyers, 2019; Kim, 2006) retains a slightly larger inventory of nine POS tags, generally corresponding to the grammatical categories found in the traditional literature. More recently, the Korean Universal Dependency project guidelines do not directly address the individual postpositions since Korean postpositions are considered sub-lexical units. Instead, the POS category of NOUN is assigned to the full (noun + postposition) lexical unit (Oh et al., 2020; Chun et al., 2018). The status of postpositions as functional categories also plays into the lack of specific attention in computational semantic resources. For example, while the labeling of Korean PropBank (Palmer et al., 2006) arguments is to some extent guided by the semantics of the postposition (e.g., a nominative marker might suggest ARG0 and a locative marker, ARGM-LOC), the labels are annotated at the lexical and phrasal level centering on nominal elements. In the case of Korean AMR (Choe et al., 2019), postpositions 2 Here is a list of gloss abbreviations used in this paper: accusa"
2020.dmr-1.6,W19-4022,0,0.0191971,"tation requires a richer palette of semantic labels that can apply broadly across the postpositional types, for which we turn to SNACS. Computational approaches and resource creation projects have also attempted to classify Korean postpositions, with a focus on morphosyntax (in morphological tagging and syntactic parsing) (Choi and Palmer, 2011; Hong, 2009; Han, 2005). The Penn Korean Treebank (Han et al., 2001), for example, recognizes four part-of-speech (POS) categories (case, adverbial, conjunctive, and auxiliary) to cover all postpositional morphemes, and the 21st Century Sejong Project (Park and Tyers, 2019; Kim, 2006) retains a slightly larger inventory of nine POS tags, generally corresponding to the grammatical categories found in the traditional literature. More recently, the Korean Universal Dependency project guidelines do not directly address the individual postpositions since Korean postpositions are considered sub-lexical units. Instead, the POS category of NOUN is assigned to the full (noun + postposition) lexical unit (Oh et al., 2020; Chun et al., 2018). The status of postpositions as functional categories also plays into the lack of specific attention in computational semantic resou"
2020.dmr-1.6,2020.lrec-1.733,1,0.555528,"of an existing annotation schema, SNACS (§2.3), which is specifically geared towards adpositional semantics. The expanded schema details 54 semantic and pragmatic categories called supersenses that resolve major ambiguities and generalize across adpositional types. Although the SNACS framework was built based on English preposition senses, the authors claim that the semantically coarse-grained and lexically-agnostic characteristics of the supersenses are wellsuited to their adoption for other languages (Hwang et al., 2017). The schema has been so far applied successfully to Mandarin Chinese (Peng et al., 2020). We now apply it to Korean in order to further test claims of cross-linguistic extensibility. Notably, SNACS has yet to be tested on a highly agglutinative language like Korean, whose adpositions (josa, §2.1) are bound morphemes suffixed on nominals, rather than independent lexical items like in English and Chinese. Korean adpositions are also peculiar in that some participate in case marking; we annotate postpositional nominative and accusative markers within the purview of SNACS. Our contributions are three-fold: (1) we show that SNACS can be applied to Korean by adapting the SNACS hierarch"
2020.dmr-1.6,P18-1018,1,0.899804,"R; Banarescu et al., 2013). SNACS is thus the most promising basis for a semantic framework with which to annotate Korean adpositions. The goal is to pave way for a full-scope, comprehensive treatment of the major semantic dimension of Korean postpositions. To the best of our knowledge, this work is the first to annotate a Korean corpus specifically targeting postpositions. Moreover, this work represents the first Korean application of a lexically-agnostic semantic analysis which cross-cuts adpositional types. 2.3 SNACS Framework The Semantic Network of Adposition and Case Supersenses (SNACS; Schneider et al., 2018, 2020) is a framework for annotating adpositions with coarse-grained semantic classes called supersenses that broadly capture prepositional semantics without particular reference to any lexicon. The current version of the scheme defines 50 such supersenses for event participant roles (inspired by traditional thematic roles: AGENT, T HEME, R ECIPIENT, etc.), circumstantial roles (e.g. T IME, M ANNER), and roles describing relationships between entities (e.g. P OSSESSOR, W HOLE, Q UANTITY VALUE). Annotating adposition uses in context serves to disambiguate them—e.g. “the wheel of the car” (W HO"
2020.dmr-1.6,W19-3316,1,0.783139,"IC (see example 3). In an intransitive event, where the NOM marks the patient argument, the function label of T HEME is assigned (4). For cases where the predicate assigns to NOM and ACC semantics that is different to that of their prototypical use, we represent the semantics assigned by the predicate as the scene role (e.g., O RIGINATOR of a communication event in 5 and L OCUS of an action in 6), while the function is the role associated most directly with the case marking (e.g. T HEME for ACC). Our decisions are fully compatible with the treatment of English subjects and objects proposed in Shalev et al. (2019) (though available English SNACS corpora do not yet contain such annotations). (3) 빌이/AGENT 사과를/T HEME 먹었다 Bill-NOM apple-ACC ate “Bill ate an apple” (5) 빌이/O RIGINATOR↝AGENT 대답했다 Bill-NOM answered “Bill answered” (4) 해가/T HEME 일찍 떴다 sun-NOM early rose “the sun rose early” (6) 빌이/AGENT 공원을/L OCUS↝T HEME 걸었다 Bill-NOM park-ACC walked “Bill took a walk in the park” Contextual Postpositions. English SNACS has strictly focused on the annotation of semantic relations, excluding discourse connectives like “according_to him” or “as_for me” from annotation. We extend this treatment to two Korean discou"
2020.dmr-1.6,N18-1136,0,0.0137333,"ample, English chooses to talk about the topic of the pilot’s emotion S TIMULUS↝T OPIC versus Korean’s choice of an E XPLANATION modifier to describe the reason behind the pilot’s mood (caused by the emotion). These divergences are natural variations based on linguistic choice and are complicated by the fact that both English and Korean texts are translations of the original French novella. Bridging such translation divergences (Dorr, 1994; Deng and Xue, 2017) would require a richer modeling of causality and representations that will allow for deeper inferences about the divergent categories (Vyas et al., 2018; Hershcovich et al., 2019; Nikolaev et al., 2020; Briakou and Carpuat, 2020). We do not have a ready proposal to offer for bridging such differences through the SNACS framework. However, investigating further into nuanced semantics like casuality or force dynamics (Croft, 2015, 2012) that would aid generalizations certainly remains a compelling area of future research. 6 Conclusion & Future Work In this work, we have presented the first annotated corpus of Korean preposition supersenses and included a detailed comparison of a subset of the data with a parallel English corpus. We find that, ov"
2020.law-1.11,biemann-2012-turk,0,0.0329449,"rk has been a popular method for scaling up linguistic annotation. Early studies on its efficacy for semantic annotations like word sense disambiguation, textual entailment, and word similarity (Snow et al., 2008) and psycholinguistic studies and judgment elicitation (Munro et al., 2010) have shown that crowdsourced annotations can be as good as if not even better than annotations produced using traditional methods. Several studies have focused specifically on word sense annotations for content-words like nouns, adjectives, and verbs (Rumshisky, 2011; Jurgens, 2013; Biemann and Nygaard, 2010; Biemann, 2012; Tsvetkov et al., 2014), but function words have received less attention. To our knowledge, the only work carried out specifically on prepositional sense annotation using a non-traditional annotation methodology is due to Tratz (2011, §4.2), who describes a process by which existing prepositional sense annotations were refined by three annotators, two of which have unspecified levels of linguistic or other competencies. We conjecture that one reason why content-words have been favored over function-words for crowdsourced annotation is that their comparatively less abstract meanings make reaso"
2020.law-1.11,2020.lrec-1.541,0,0.0207435,"ver function-words for crowdsourced annotation is that their comparatively less abstract meanings make reasoning about their semantics more approachable to linguistically naïve crowdworkers, simplifying task design, while for function words, it can be difficult to tap into crowdworkers’ intuitions without changing the task considerably. Some work has investigated gamification with the hope that bringing gamelike elements would allow crowdworkers to produce good annotations without a traditional training process, in some cases achieving performance on par with or better than expert annotation (Fort et al., 2020; Hartshorne et al., 2014; Schneider et al., 2014). Independent from the matter of whether to crowdsource or gamify, some have modified their annotation schemes with an eye explicitly to annotation expense (in terms of time or money). In the QA-SRL annotation scheme proposed by He et al. (2015), plain-language questions are used to describe the predicate-argument structures of verbs instead of formalisms such as frames or predicates, rendering it theory- and formalism-neutral and easier to explain to non-expert workers. Our work is different from the work we have reviewed here in that we have"
2020.law-1.11,P14-2065,0,0.0265504,"for crowdsourced annotation is that their comparatively less abstract meanings make reasoning about their semantics more approachable to linguistically naïve crowdworkers, simplifying task design, while for function words, it can be difficult to tap into crowdworkers’ intuitions without changing the task considerably. Some work has investigated gamification with the hope that bringing gamelike elements would allow crowdworkers to produce good annotations without a traditional training process, in some cases achieving performance on par with or better than expert annotation (Fort et al., 2020; Hartshorne et al., 2014; Schneider et al., 2014). Independent from the matter of whether to crowdsource or gamify, some have modified their annotation schemes with an eye explicitly to annotation expense (in terms of time or money). In the QA-SRL annotation scheme proposed by He et al. (2015), plain-language questions are used to describe the predicate-argument structures of verbs instead of formalisms such as frames or predicates, rendering it theory- and formalism-neutral and easier to explain to non-expert workers. Our work is different from the work we have reviewed here in that we have attempted to have partici"
2020.law-1.11,D15-1076,0,0.0310107,"ns without changing the task considerably. Some work has investigated gamification with the hope that bringing gamelike elements would allow crowdworkers to produce good annotations without a traditional training process, in some cases achieving performance on par with or better than expert annotation (Fort et al., 2020; Hartshorne et al., 2014; Schneider et al., 2014). Independent from the matter of whether to crowdsource or gamify, some have modified their annotation schemes with an eye explicitly to annotation expense (in terms of time or money). In the QA-SRL annotation scheme proposed by He et al. (2015), plain-language questions are used to describe the predicate-argument structures of verbs instead of formalisms such as frames or predicates, rendering it theory- and formalism-neutral and easier to explain to non-expert workers. Our work is different from the work we have reviewed here in that we have attempted to have participants solve a task that is not the same task we would have given to an expert annotator. Pursuing such a proxy task, as we have termed it, introduces the challenge of turning proxy data into gold data, but reduces the need for worker training. Proxy tasks have been succ"
2020.law-1.11,N13-1062,0,0.0165096,"Work Crowdsourcing on Amazon Mechanical Turk has been a popular method for scaling up linguistic annotation. Early studies on its efficacy for semantic annotations like word sense disambiguation, textual entailment, and word similarity (Snow et al., 2008) and psycholinguistic studies and judgment elicitation (Munro et al., 2010) have shown that crowdsourced annotations can be as good as if not even better than annotations produced using traditional methods. Several studies have focused specifically on word sense annotations for content-words like nouns, adjectives, and verbs (Rumshisky, 2011; Jurgens, 2013; Biemann and Nygaard, 2010; Biemann, 2012; Tsvetkov et al., 2014), but function words have received less attention. To our knowledge, the only work carried out specifically on prepositional sense annotation using a non-traditional annotation methodology is due to Tratz (2011, §4.2), who describes a process by which existing prepositional sense annotations were refined by three annotators, two of which have unspecified levels of linguistic or other competencies. We conjecture that one reason why content-words have been favored over function-words for crowdsourced annotation is that their compa"
2020.law-1.11,W16-2505,0,0.016858,"ce. sim is used to compare it to every instance in L, and the top k most similar inst/ances in L are retrieved with their labels, ⟨s1 ,t1 ,ℓ1 ⟩,...,⟨sk ,tk ,ℓk ⟩. We call these retrieved instances the target’s neighbors. Neighbors may optionally be filtered, e.g. to ensure that no label ℓ is represented more than once among ℓ1 ,...,ℓk . The target sentence s is presented to crowdworkers along with s1 ,...,sk from the neighbors, with the target preposition indicated in each, and crowdworkers are asked to select any neighbors for which the 4 This is unrelated to the term “proxy task” as used by Mostafazadeh et al. (2016), where it is used to refer to intrinsic evaluations for word embeddings. 5 We deliberately do not mention a specific metric or representation here, since there are many ways to implement this design. As we describe in §4.3, we use cosine distance between supersense membership softmax vectors from a supersense tagger for our pilots in this work, though one could imagine other implementations, such as Euclidean distance between raw or fine-tuned BERT embeddings. 118 throughout for [Omit] Other: they fixed my garage doors >> in << literally less than an hour . at Search Method Trial around 6/14/"
2020.law-1.11,W10-0719,0,0.199344,"y our discussion of the prepositional supersense tagging task and speak of it as if it consisted of assigning a single label (the scene role and function tags, concatenated). 2 117 The 14th Linguistic Annotation Workshop, pages 117–126 Barcelona, Spain (Online), December 12, 2020. 3 Two Task Designs Our ultimate goal is to obtain supersense labels for prepositions in context from crowdsourced data. One possible technique would be to provide definitions and canonical examples of each label, or subsets of labels, and ask the crowdworker which label most closely applies to the annotation target (Munro et al., 2010). Another tactic would be to decompose our labels into more readily intuitive semantic features (Reisinger et al., 2015). But given the extensive semantic range of the many prepositions we seek to annotate, both of these approaches seem difficult to achieve with crowdworkers. Instead, we explore what we term proxy tasks:4 rather than teach and elicit supersenses (or semantic features associated with supersenses) directly, we elicit judgments of surface substitution/similarity, as has been done by previous work on word sense crowdsourcing (§5). This approach leverages current annotated data in"
2020.law-1.11,2020.lrec-1.733,1,0.824383,"btaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations. 1 Introduction Prepositions are highly ambiguous function words which can express a wide variety of relationships (Litkowski and Hargraves, 2005; Tratz, 2011). Supersenses have been proposed as an analytic framework for studying their lexical semantics, but extant gold-annotated corpora (e.g. Schneider et al., 2018; Peng et al., 2020) are small because preposition supersense annotation is a relatively complex annotation task that requires substantial training and time. We ask whether preposition supersense annotation could be made cheaper and quicker with crowdsourced labor. This will require “sensible” annotation tasks accessible to non-experts. In this work we present two possible designs for proxy tasks for crowdsourcing from which supersense labels can be recovered indirectly. These designs involve in-context substitution and similarity judgments. Based on four in-house pilot experiments, we conclude that both designs"
2020.law-1.11,W11-0409,0,0.0278355,"e. 123 5 Related Work Crowdsourcing on Amazon Mechanical Turk has been a popular method for scaling up linguistic annotation. Early studies on its efficacy for semantic annotations like word sense disambiguation, textual entailment, and word similarity (Snow et al., 2008) and psycholinguistic studies and judgment elicitation (Munro et al., 2010) have shown that crowdsourced annotations can be as good as if not even better than annotations produced using traditional methods. Several studies have focused specifically on word sense annotations for content-words like nouns, adjectives, and verbs (Rumshisky, 2011; Jurgens, 2013; Biemann and Nygaard, 2010; Biemann, 2012; Tsvetkov et al., 2014), but function words have received less attention. To our knowledge, the only work carried out specifically on prepositional sense annotation using a non-traditional annotation methodology is due to Tratz (2011, §4.2), who describes a process by which existing prepositional sense annotations were refined by three annotators, two of which have unspecified levels of linguistic or other competencies. We conjecture that one reason why content-words have been favored over function-words for crowdsourced annotation is t"
2020.law-1.11,P18-1018,1,0.791242,"o sensible methods for obtaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations. 1 Introduction Prepositions are highly ambiguous function words which can express a wide variety of relationships (Litkowski and Hargraves, 2005; Tratz, 2011). Supersenses have been proposed as an analytic framework for studying their lexical semantics, but extant gold-annotated corpora (e.g. Schneider et al., 2018; Peng et al., 2020) are small because preposition supersense annotation is a relatively complex annotation task that requires substantial training and time. We ask whether preposition supersense annotation could be made cheaper and quicker with crowdsourced labor. This will require “sensible” annotation tasks accessible to non-experts. In this work we present two possible designs for proxy tasks for crowdsourcing from which supersense labels can be recovered indirectly. These designs involve in-context substitution and similarity judgments. Based on four in-house pilot experiments, we conclud"
2020.law-1.11,schneider-etal-2014-comprehensive,1,0.814444,"on is that their comparatively less abstract meanings make reasoning about their semantics more approachable to linguistically naïve crowdworkers, simplifying task design, while for function words, it can be difficult to tap into crowdworkers’ intuitions without changing the task considerably. Some work has investigated gamification with the hope that bringing gamelike elements would allow crowdworkers to produce good annotations without a traditional training process, in some cases achieving performance on par with or better than expert annotation (Fort et al., 2020; Hartshorne et al., 2014; Schneider et al., 2014). Independent from the matter of whether to crowdsource or gamify, some have modified their annotation schemes with an eye explicitly to annotation expense (in terms of time or money). In the QA-SRL annotation scheme proposed by He et al. (2015), plain-language questions are used to describe the predicate-argument structures of verbs instead of formalisms such as frames or predicates, rendering it theory- and formalism-neutral and easier to explain to non-expert workers. Our work is different from the work we have reviewed here in that we have attempted to have participants solve a task that i"
2020.law-1.11,D08-1027,0,0.33932,"Missing"
2020.law-1.11,tsvetkov-etal-2014-augmenting-english,1,0.837746,"opular method for scaling up linguistic annotation. Early studies on its efficacy for semantic annotations like word sense disambiguation, textual entailment, and word similarity (Snow et al., 2008) and psycholinguistic studies and judgment elicitation (Munro et al., 2010) have shown that crowdsourced annotations can be as good as if not even better than annotations produced using traditional methods. Several studies have focused specifically on word sense annotations for content-words like nouns, adjectives, and verbs (Rumshisky, 2011; Jurgens, 2013; Biemann and Nygaard, 2010; Biemann, 2012; Tsvetkov et al., 2014), but function words have received less attention. To our knowledge, the only work carried out specifically on prepositional sense annotation using a non-traditional annotation methodology is due to Tratz (2011, §4.2), who describes a process by which existing prepositional sense annotations were refined by three annotators, two of which have unspecified levels of linguistic or other competencies. We conjecture that one reason why content-words have been favored over function-words for crowdsourced annotation is that their comparatively less abstract meanings make reasoning about their semanti"
2020.law-1.12,W08-2227,0,0.0321146,"Missing"
2020.law-1.12,P14-1120,0,0.0119809,"preposition semantics (Schneider et al., 2018). SNACS includes 50 broadcoverage semantic labels called supersenses, which is organized into three broad branches reflecting event participant roles (e.g., AGENT, T HEME, R ECIPIENT), roles relating to the circumstance of an event (e.g., T IME, L OCATION, G OAL) and relational roles between two entities (e.g., I DENTITY, P OSSESSION). A supersense label, thus, indicates the semantic relationship between the constituent object or the governing head of the preposition. Unlike prior dictionary-based efforts in representing postpositional semantics (Litkowski, 2014; Litkowski and Hargraves, 2005), SNACS labels the prepositions within its context (e.g., “cat on/L OCUS the mat” vs. “found the cat in/L OCUS the box”) irrespective of the lexical type the target represents. SNACS also utilizes the construal analysis, a mechanism that allows annotators to assign a preposition with two labels instead of one in a systematic manner. All prepositions are labeled at both the scene role and the function levels, where the scene role specifies the preposition’s role with respect to the scene set by the governing head (typically a verb) and function label indicates th"
2020.law-1.12,J05-1004,0,0.155653,"aces an additional constraint: they must resolve the problem while making the schema accessible to annotators for the production of consistent annotations. Generally, semantic resources have maintained the balance in one of two ways: by creating many fine-grained labels that are systematically organized into hierarchies or ontologies, or by resorting to very small number of distinct labels and making them conditional on the relation they annotate. FrameNet (Ruppenhofer et al., 2016) and the TRIPS ontology (Allen et al., 2008) exemplify the former approach, while PropBank’s numbered arguments (Palmer et al., 2005) illustrate the latter one. This paper focuses on the SNACS framework of Schneider et al. (2018)—a hierarchy of 50 semantic labels that seeks to characterize the semantic space of prepositions. Like most resources, SNACS falls somewhere in between the two extremes described above. What is unique about this scheme is that it tries to be as economical as possible with regards to the number of semantic types of prepositions it accepts into the hierarchy. However, it does so while being lexically agnostic of the identity of its syntactic governor (e.g., the governing verb). As a balancing mechanis"
2020.law-1.12,P18-1018,1,0.910899,"le to annotators for the production of consistent annotations. Generally, semantic resources have maintained the balance in one of two ways: by creating many fine-grained labels that are systematically organized into hierarchies or ontologies, or by resorting to very small number of distinct labels and making them conditional on the relation they annotate. FrameNet (Ruppenhofer et al., 2016) and the TRIPS ontology (Allen et al., 2008) exemplify the former approach, while PropBank’s numbered arguments (Palmer et al., 2005) illustrate the latter one. This paper focuses on the SNACS framework of Schneider et al. (2018)—a hierarchy of 50 semantic labels that seeks to characterize the semantic space of prepositions. Like most resources, SNACS falls somewhere in between the two extremes described above. What is unique about this scheme is that it tries to be as economical as possible with regards to the number of semantic types of prepositions it accepts into the hierarchy. However, it does so while being lexically agnostic of the identity of its syntactic governor (e.g., the governing verb). As a balancing mechanism between specialization and generalization of categories, it employs construals, a two-level an"
2020.lrec-1.733,W07-1604,0,0.0518592,"a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext. Keywords: adpositions, supersenses, Mandarin Chinese, corpus, annotation 1. Introduction Adpositions (i.e. prepositions and postpositions) include some of the most frequent words in languages like Chinese and English, and help convey a myriad of semantic relations of space, time, causality, possession, and other domains of meaning. They are also a persistent thorn in the side of second language learners owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c,"
2020.lrec-1.733,hashemi-hwa-2014-comparison,0,0.0250065,"iosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empiric"
2020.lrec-1.733,2009.eamt-1.9,0,0.0890222,"Missing"
2020.lrec-1.733,W13-2322,1,0.88058,"with the main predicate of the clause and introduce an NP argument to the clause (Li and Thompson, 1974) as in (4). These tokens are referred to as coverbs. In some cases, coverbs can also occur as the main predicate. For example, the coverb zài heads the predicate phrase in (5). (4) (5) t¯a zài:L OCUS xuéshù 3 SG P:at academia shàng:T OPIC↝L OCUS yˇousuˇozuòwéi. LC :on-top-of successful ‘He succeeded in academia.’ 4.1. Preprocessing We use the same Chinese translation of The Little Prince as the Chinese AMR corpus (Li et al., 2016), which is also sentence-aligned with the English AMR corpus (Banarescu et al., 2013). These bitext annotations in multiple languages and annotation semantic frameworks can facilitate crossframework comparisons. Prior to supersense annotation, we conducted the following preprocessing steps in order to identify the adposition targets that merit supersense annotation. nˇı yào de yáng jiù zài lˇımiàn. 2 SG want DE sheep RES at inside ‘The sheep you wanted is in the box.’ (zh_lpp_1943.92) In this project, we only annotate coverbs when they do not function as the main predicate in the sentence, echoing the view that coverbs modify events introduced by the predicates, rather than es"
2020.lrec-1.733,W09-2110,0,0.0111112,"poses of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (§3), we apply the SNACS supersenses to a t"
2020.lrec-1.733,C16-1085,0,0.0224686,"s many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (§3), we apply the SNACS supersenses to a translation of The Li"
2020.lrec-1.733,S17-1022,1,0.640534,"Missing"
2020.lrec-1.733,W16-1702,0,0.339033,"st adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, Lo"
2020.lrec-1.733,W03-1712,0,0.329789,"ogicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, LocationIni, Kernel word) whereas the prepositions and postpositions were respectively labeled with syntactic relations Preposition and LocationPreposition.4 Similarly, in Semantic Dependency Relations (SDR, Che et al. 2012, 2016), prepositions a"
2020.lrec-1.733,P14-1120,0,0.165666,"Petit Prince by Antoine de St. Exupéry, published in 1943 and subsequently translated into numerous languages. 2 corpus, and compare to adposition behavior in a separate English corpus (see §5). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (§6). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpo"
2020.lrec-1.733,K18-2016,0,0.0405455,"Missing"
2020.lrec-1.733,W16-1712,1,0.855068,"guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate"
2020.lrec-1.733,P18-1018,1,0.830137,"reover, there is a dearth of annotated corpora for investigating the cross-linguistic variation of adposition semantics, or for building multilingual disambiguation systems. This paper presents a corpus in which all adpositions have been semantically annotated in Mandarin Chinese; to the best of our knowledge, this is the first Chinese corpus to be broadly annotated with adposition semantics. Our approach adapts a framework that defined a general set of supersenses according to ostensibly language-independent semantic criteria, though its development focused primarily on English prepositions (Schneider et al., 2018). We find that the supersense categories are well-suited to Chinese adpositions despite syntactic differences from English. On a Mandarin translation of The Little Prince, we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext. Keywords: adpositions, supersenses, Mandarin Chinese, corpus, annotation 1. Introduction Adpositions (i.e. prepositions and postpositions) include some of the most frequent words in languages like Chinese and English, and help convey a myriad of semantic relations of space, time, causality, possession, and other dom"
2020.lrec-1.733,N15-1177,1,0.916719,"Missing"
2020.lrec-1.733,W15-1612,1,0.846978,"toine de St. Exupéry, published in 1943 and subsequently translated into numerous languages. 2 corpus, and compare to adposition behavior in a separate English corpus (see §5). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (§6). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.3 2. Related Work To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014). Schneider et al. (2015) proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the"
2020.lrec-1.733,S07-1005,0,0.147385,"Missing"
2020.lrec-1.733,W12-0514,0,0.0309314,"cond language learners owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annota"
2020.lrec-1.733,N04-1032,0,0.11466,"fit the needs of the scene in actual language use. (3) I care about:S TIMULUS↝T OPIC you. For instance, (3) blends the domains of emotion (principally 4 Though named LocationPreposition in Li et al. (2003), these adpositions actually occur postnominally, equivalent to localizers in this paper. 5 Throughout this paper, adposition tokens under discussion are bolded and labeled. Causer Agent StartTime EndTime Co-Agent Theme Configuration Identity Species Gestalt Possessor Frequency Co-Theme Duration Topic Characteristic Stimulus Possession Experiencer PartPortion Interval Locus Source Goal Path Sun and Jurafsky (2004) compared PropBank parsing performance on Chinese and English, and showed that four Chinese prepositions (zài, yú, bˇı, and duì) are among the top 20 lexicalized syntactic head words in Chinese PropBank, bridging the connections between verbs and their arguments. The high frequency of prepositions as head words in PropBank reflects their importance in context. However, very few annotation scheme attempted to directly label the semantics of these adposition words. Chinese Knowledge Information Processing Group (CKIP) (1993) is the most relevant adposition annotation effort, categorizing Chinese"
2020.lrec-1.733,L16-1262,0,0.134227,"Missing"
2020.lrec-1.733,C08-1109,0,0.0251851,"has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in"
2020.lrec-1.733,W15-4923,0,0.0309798,"Missing"
2020.lrec-1.733,2014.amta-researchers.21,0,0.0298506,"s owing to their extreme idiosyncrasy (Chodorow et al., 2007; Lorincz and Gordon, 2012). For instance, the English word in has no exact parallel in another language; rather, for purposes of translation, its many different usages cluster differently depending on the second language. Semantically annotated corpora of adpositions in multiple languages, including parallel data, would facilitate broader empirical study of adposition variation than is possible today, and could also contribute to NLP applications such as machine translation (Li et al., 2005; Agirre et al., 2009; Shilon et al., 2012; Weller et al., 2014, 2015; Hashemi and Hwa, 2014; Popovi´c, 2017) and grammatical error correction (Chodorow et al., 2007; Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; Hermet and Alain, 2009; Huang et al., 2016; Graën and Schneider, 2017). This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted Schneider et al.’s (2018) Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see §2.2) to Chinese.1 Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been"
2020.lrec-1.733,J08-2004,0,0.0466704,"entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative element. Li et al. (2003) annotated the Tsinghua Corpus (Zhang, 1999) from People’s Daily where the content words were 5986 3 https://github.com/nert-nlp/Chinese-SNACS/ selected as the headwords, i.e., the object is the headword of the prepositional phrase. In these prepositional phrases, the nominal headwords were labeled with one of the 59 semantic relations (e.g. Location, LocationIni, Kernel word) whereas the prepositio"
2020.lrec-1.733,xue-etal-2014-interlingua,0,0.0669534,"Missing"
2020.lrec-1.733,Y98-1003,0,0.164563,"would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see §2.2). Previous SNACS annotation efforts have been mostly focused on English— particularly STREUSLE (Schneider et al., 2016, 2018), the semantically annotated corpus of reviews from the English Web Treebank (EWT; Bies et al., 2012). We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince. 2.1. Chinese Adpositions and Roles In the computational literature for Chinese, apart from some focused studies (e.g., Yang and Kuo (1998) on logicalsemantic representation of temporal adpositions), there has been little work addressing adpositions specifically. Most previous semantic projects for Mandarin Chinese focused on content words and did not directly annotate the semantic relations signaled by functions words such as prepositions (Xue et al., 2014; Hao et al., 2007; You and Liu, 2005; Li et al., 2016). For example, in Chinese PropBank, Xue (2008) argued that the head word and its part of speech are clearly informative for labeling the semantic role of a phrase, but the preposition is not always the most informative elem"
2020.lrec-1.733,C08-1022,0,\N,Missing
2020.lrec-1.733,W17-0303,0,\N,Missing
2020.lrec-1.733,S16-1167,0,\N,Missing
2020.lrec-1.733,S12-1050,0,\N,Missing
2021.acl-long.257,2020.emnlp-main.123,0,0.0167767,"TAMR is evaluated on a subset of 91 sentences. Chen and Palmer (2017) perform unsupervised EM alignment between AMR nodes and tokens, taking advantage of a Universal Dependencies (UD) syntactic parse as well as named entity and semantic role features. Szubert et al. (2018) and Chu and Kurohashi (2016) both produce hierachical (nested) alignments between AMR and a syntactic parse. Szubert et al. use a rule-based algorithm to align AMR subgraphs with UD subtrees. Chu and Kurohashi use a supervised algorithm to align AMR subgraphs with constituency parse subtrees. Word Embeddings. Additionally, Anchiêta and Pardo (2020) use an alignment method designed to work well in low-resource settings using pretrained word embeddings for tokens and nodes. Graph Distance. Wang and Xue (2017) use an HMM-based aligner to align tokens and nodes. They include in their aligner a calculation of graph distance as a locality constraint on predicted alignments. This is similar to our use of projection distance as described in §5. Drawbacks of Current Alignments. Alignment methods vary in terms of components of the AMR that are candidates for alignment. Most systems either align nodes (e.g., ISI) or connected subgraphs (e.g., JAMR"
2021.acl-long.257,W13-2322,1,0.725981,"nces. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation. 1 Introduction Research with the Abstract Meaning Representation (AMR; Banarescu et al., 2013), a broadcoverage semantic annotation framework in which sentences are paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accurac"
2021.acl-long.257,R19-1014,0,0.0111871,"reen (duplicate subgraphs), and orange (relations). Relations that also participate in reentrancy alignments are bolded. tation conventions can be opaque with respect to the words or surface structure of the sentence, e.g., by unifying coreferent mentions and making explicit certain elided or pragmatically inferable concepts and relations. Previous efforts toward general tools for AMR alignment have considered mapping tokens, spans, or syntactic units to nodes, edges, or subgraphs (§2). Other approaches to AMR alignment have targeted specific compositional formalisms (Groschwitz et al., 2018; Beschke, 2019; Blodgett and Schneider, 2019). We advocate here for a definition of alignment that is principled—achieving full coverage of the graph structure—while being framework-neutral and easy-to-understand, by aligning graph substructures to shallow token spans on the form side, rather than using syntactic parses. We do use structural considerations to constrain alignments on the meaning side, but by using spans on the form side, we ensure the definition of the alignment search space is not at the mercy of error-prone parsers. Definitions. Given a tokenized sentence w and its corresponding AMR graph"
2021.acl-long.257,W19-0405,1,0.848718,"subgraphs), and orange (relations). Relations that also participate in reentrancy alignments are bolded. tation conventions can be opaque with respect to the words or surface structure of the sentence, e.g., by unifying coreferent mentions and making explicit certain elided or pragmatically inferable concepts and relations. Previous efforts toward general tools for AMR alignment have considered mapping tokens, spans, or syntactic units to nodes, edges, or subgraphs (§2). Other approaches to AMR alignment have targeted specific compositional formalisms (Groschwitz et al., 2018; Beschke, 2019; Blodgett and Schneider, 2019). We advocate here for a definition of alignment that is principled—achieving full coverage of the graph structure—while being framework-neutral and easy-to-understand, by aligning graph substructures to shallow token spans on the form side, rather than using syntactic parses. We do use structural considerations to constrain alignments on the meaning side, but by using spans on the form side, we ensure the definition of the alignment search space is not at the mercy of error-prone parsers. Definitions. Given a tokenized sentence w and its corresponding AMR graph G, a complete alignment assumes"
2021.acl-long.257,C88-1016,0,0.0586509,"(Flanigan et al., 2014, 2016) aligns token spans to subgraphs using iterative application of an ordered list of 14 rules which include exact and fuzzy matching. JAMR alignments form a connected subgraph of the AMR by the nature of the rules being applied. A disadvantage of JAMR is that it lacks a method for resolving ambiguities, such as repeated tokens, or of learning novel alignment patterns. ISI. The ISI system (Pourdamghani et al., 2014) produces alignments between tokens and nodes and between tokens and relations via an ExpectationMaximization (EM) algorithm in the style of IBM Model 2 (Brown et al., 1988). First, the AMR is linearized; then EM is applied using a symmetrized scoring function of the form P(a ∣ t) + P(t ∣ a), where a is any node or edge in the linearized AMR and t is any token in the sentence. Graph connectedness is not enforced for the elements aligning to a given token. Compared to JAMR, ISI produces more novel alignment patterns, but also struggles with rare strings such as dates and names, where a rule-based approach is more appropriate. Extensions and Combinations. TAMR (Tuned Abstract Meaning Representation; Liu et al., 2018) uses the JAMR alignment rules, along with two ot"
2021.acl-long.257,2020.acl-main.119,0,0.0120268,"ed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accuracy of parsing algorithms. Where parsers use latent alignments (e.g., Lyu and Titov, 2018; Cai and Lam, 2020), explicit alignments can still facilitate evaluation and error analysis. Moreover, AMR-to-text generation research and applications using AMR stand to benefit from accurate, human-interpretable alignments. We present Linguistically Enriched AMR (LEAMR) alignment, which achieves full graph coverage via four distinct types of aligned structures: subgraphs, relations, reentrancies, and duplicate subgraphs arising from ellipsis. This formulation lends itself to unsupervised learning of alignment models. Advantages of our algorithm and released alignments include: (1) much improved coverage over p"
2021.acl-long.257,E17-1053,0,0.0182005,"e parse that is most similar to the gold AMR. Some AMR parsers (Naseem et al., 2019; Fernandez Astudillo et al., 2020) use alignments which are a union of alignments produced by the JAMR and ISI systems. The unioned alignments achieve greater coverage, improving parser performance. Syntax-based. Several alignment systems attempt to incorporate syntax into AMR alignments. JAMR ISI TAMR∗ nodes 91.1 78.7 94.9 edges ✗ 9.8 ✗ reentrancies ✗ ✗ ✗ Table 1: Coverage and types of previous alignment systems. Scores are evaluated on 200 gold test sentences. ∗ TAMR is evaluated on a subset of 91 sentences. Chen and Palmer (2017) perform unsupervised EM alignment between AMR nodes and tokens, taking advantage of a Universal Dependencies (UD) syntactic parse as well as named entity and semantic role features. Szubert et al. (2018) and Chu and Kurohashi (2016) both produce hierachical (nested) alignments between AMR and a syntactic parse. Szubert et al. use a rule-based algorithm to align AMR subgraphs with UD subtrees. Chu and Kurohashi use a supervised algorithm to align AMR subgraphs with constituency parse subtrees. Word Embeddings. Additionally, Anchiêta and Pardo (2020) use an alignment method designed to work wel"
2021.acl-long.257,2020.findings-emnlp.89,1,0.720091,"g to a given token. Compared to JAMR, ISI produces more novel alignment patterns, but also struggles with rare strings such as dates and names, where a rule-based approach is more appropriate. Extensions and Combinations. TAMR (Tuned Abstract Meaning Representation; Liu et al., 2018) uses the JAMR alignment rules, along with two others, to produce a set of candidate alignments for the sentence. Then, the alignments are “tuned” with a parser oracle to select the candidates that correspond to the oracle parse that is most similar to the gold AMR. Some AMR parsers (Naseem et al., 2019; Fernandez Astudillo et al., 2020) use alignments which are a union of alignments produced by the JAMR and ISI systems. The unioned alignments achieve greater coverage, improving parser performance. Syntax-based. Several alignment systems attempt to incorporate syntax into AMR alignments. JAMR ISI TAMR∗ nodes 91.1 78.7 94.9 edges ✗ 9.8 ✗ reentrancies ✗ ✗ ✗ Table 1: Coverage and types of previous alignment systems. Scores are evaluated on 200 gold test sentences. ∗ TAMR is evaluated on a subset of 91 sentences. Chen and Palmer (2017) perform unsupervised EM alignment between AMR nodes and tokens, taking advantage of a Universal"
2021.acl-long.257,S16-1186,0,0.0231381,"ence on Natural Language Processing, pages 3310–3321 August 1–6, 2021. ©2021 Association for Computational Linguistics multiple nodes while preserving AMR structure. Previous systems use various strategies for aligning. They also have differing approaches to what types of substructures of AMR are aligned—whether they are nodes, subgraphs, or relations—and what they are aligned to—whether individual tokens, token spans, or syntactic parses. Two main alignment strategies remain dominant, though they may be combined or extended in various ways: rule-based strategies as in Flanigan et al. (2014), Flanigan et al. (2016), Liu et al. (2018), and Szubert et al. (2018), and statistical strategies using ExpectationMaximization as in Pourdamghani et al. (2014). JAMR. The JAMR system (Flanigan et al., 2014, 2016) aligns token spans to subgraphs using iterative application of an ordered list of 14 rules which include exact and fuzzy matching. JAMR alignments form a connected subgraph of the AMR by the nature of the rules being applied. A disadvantage of JAMR is that it lacks a method for resolving ambiguities, such as repeated tokens, or of learning novel alignment patterns. ISI. The ISI system (Pourdamghani et al.,"
2021.acl-long.257,P14-1134,0,0.188648,"ternational Joint Conference on Natural Language Processing, pages 3310–3321 August 1–6, 2021. ©2021 Association for Computational Linguistics multiple nodes while preserving AMR structure. Previous systems use various strategies for aligning. They also have differing approaches to what types of substructures of AMR are aligned—whether they are nodes, subgraphs, or relations—and what they are aligned to—whether individual tokens, token spans, or syntactic parses. Two main alignment strategies remain dominant, though they may be combined or extended in various ways: rule-based strategies as in Flanigan et al. (2014), Flanigan et al. (2016), Liu et al. (2018), and Szubert et al. (2018), and statistical strategies using ExpectationMaximization as in Pourdamghani et al. (2014). JAMR. The JAMR system (Flanigan et al., 2014, 2016) aligns token spans to subgraphs using iterative application of an ordered list of 14 rules which include exact and fuzzy matching. JAMR alignments form a connected subgraph of the AMR by the nature of the rules being applied. A disadvantage of JAMR is that it lacks a method for resolving ambiguities, such as repeated tokens, or of learning novel alignment patterns. ISI. The ISI syst"
2021.acl-long.257,P18-1170,0,0.03947,"Missing"
2021.acl-long.257,D18-1264,0,0.233505,"e in research on AMR parsing, generation, and evaluation. 1 Introduction Research with the Abstract Meaning Representation (AMR; Banarescu et al., 2013), a broadcoverage semantic annotation framework in which sentences are paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accuracy of parsing algorithms. Where parsers use latent alignments (e.g., Lyu and Titov, 2018; Cai and Lam, 2020), explicit alignments can still facilitate evaluation and error analysis. Moreover, AMR-to-text generation research and applications using AMR stand to benefit from accurate, human-interpretable alignments. We present Linguistically Enriched AMR (LEAMR) alignment, which achieves full graph coverage via four distinct types of aligned structures: subgraphs,"
2021.acl-long.257,P18-1037,0,0.0159249,"re paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accuracy of parsing algorithms. Where parsers use latent alignments (e.g., Lyu and Titov, 2018; Cai and Lam, 2020), explicit alignments can still facilitate evaluation and error analysis. Moreover, AMR-to-text generation research and applications using AMR stand to benefit from accurate, human-interpretable alignments. We present Linguistically Enriched AMR (LEAMR) alignment, which achieves full graph coverage via four distinct types of aligned structures: subgraphs, relations, reentrancies, and duplicate subgraphs arising from ellipsis. This formulation lends itself to unsupervised learning of alignment models. Advantages of our algorithm and released alignments include: (1) much impr"
2021.acl-long.257,P19-1451,0,0.0499955,"ts and aligner for use in research on AMR parsing, generation, and evaluation. 1 Introduction Research with the Abstract Meaning Representation (AMR; Banarescu et al., 2013), a broadcoverage semantic annotation framework in which sentences are paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accuracy of parsing algorithms. Where parsers use latent alignments (e.g., Lyu and Titov, 2018; Cai and Lam, 2020), explicit alignments can still facilitate evaluation and error analysis. Moreover, AMR-to-text generation research and applications using AMR stand to benefit from accurate, human-interpretable alignments. We present Linguistically Enriched AMR (LEAMR) alignment, which achieves full graph coverage via four distinct types of aligned stru"
2021.acl-long.257,D14-1048,0,0.131375,"ll release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation. 1 Introduction Research with the Abstract Meaning Representation (AMR; Banarescu et al., 2013), a broadcoverage semantic annotation framework in which sentences are paired with directed acyclic graphs, must contend with the lack of gold-standard alignments between words and semantic units in the English data. A variety of rule-based and statistical algorithms have sought to fill this void, with improvements in alignment accuracy often translating into improvements in AMR parsing accuracy (Pourdamghani et al., 2014; Naseem et al., 2019; Liu et al., 2018). Yet current alignment algorithms still suffer from limited coverage and less-than-ideal accuracy, constraining the design and accuracy of parsing algorithms. Where parsers use latent alignments (e.g., Lyu and Titov, 2018; Cai and Lam, 2020), explicit alignments can still facilitate evaluation and error analysis. Moreover, AMR-to-text generation research and applications using AMR stand to benefit from accurate, human-interpretable alignments. We present Linguistically Enriched AMR (LEAMR) alignment, which achieves full graph coverage via four distinct"
2021.acl-long.257,2020.acl-demos.14,0,0.0181546,"t with an extra variable for the reentrancy type: score(⟨r,s,type⟩) = Palign (r,type ∣ s;θ6 ) ⋅ Pdist (d1 ;θ7 ) ⋅ Pdist (d2 ;θ8 ) (7) where r is the role label of the reentrant edge. Legal Candidates. There are 8 reentrancy types (§3.4). For each type, a rule-based test determines if a span and edge are permitted to be aligned. The 8 tests use part of speech, the structure of the AMR, and subgraph and relation alignments. A span may be aligned (rarely) to multiple reentrancies, but these alignments are scored separately. 6 Experimental Setup Sentences are preprocessed with the Stanza library (Qi et al., 2020) to obtain lemmas, part-of-speech tags, and named entities. We identify token spans using a combination of named entities and a fixed list of multiword expressions (details are given in appendix B). Coreference information, which is used to identify legal candidates in the reentrancy alignment phase, is obtained using NeuralCoref. 8 Lemmas are used in each alignment phase to normalize representation of spans, while parts of speech and coreference are used to restrict legal candidates in the relation and reentrancy alignment phases. We tune hyperparameters, including penalties for duplicate ali"
2021.acl-long.257,P18-1018,1,0.886793,"Missing"
2021.acl-long.257,N15-1177,1,0.765063,"Missing"
2021.acl-long.257,2020.findings-emnlp.199,0,0.0267812,"lude in their aligner a calculation of graph distance as a locality constraint on predicted alignments. This is similar to our use of projection distance as described in §5. Drawbacks of Current Alignments. Alignment methods vary in terms of components of the AMR that are candidates for alignment. Most systems either align nodes (e.g., ISI) or connected subgraphs (e.g., JAMR), with incomplete coverage. Most current systems do not align relations to tokens or spans, and those that do (such as ISI) do so with low coverage and performance. None of the current systems align reentrancies, although Szubert et al. (2020) developed a rule-based set of heuristics for identifying reentrancy types. Table 1 summarizes the coverage and variety of prominent alignment systems. 3 An All-Inclusive Formulation of AMR Alignment Aligning AMRs to English sentences is a vexing problem not only because the English training data lacks gold alignments, but also because AMRs— unlike many semantic representations—are not designed with a derivational process of form–function subunits in mind. Rather, each AMR graph represents the full-sentence meaning, and AMR anno3311 (w / want-01 :ARG0 (p / person :ARG0-of (s / study-01) :ARG1-"
2021.acl-long.257,N18-1106,1,0.903998,"Missing"
2021.acl-long.257,D17-1129,0,0.0198824,"ependencies (UD) syntactic parse as well as named entity and semantic role features. Szubert et al. (2018) and Chu and Kurohashi (2016) both produce hierachical (nested) alignments between AMR and a syntactic parse. Szubert et al. use a rule-based algorithm to align AMR subgraphs with UD subtrees. Chu and Kurohashi use a supervised algorithm to align AMR subgraphs with constituency parse subtrees. Word Embeddings. Additionally, Anchiêta and Pardo (2020) use an alignment method designed to work well in low-resource settings using pretrained word embeddings for tokens and nodes. Graph Distance. Wang and Xue (2017) use an HMM-based aligner to align tokens and nodes. They include in their aligner a calculation of graph distance as a locality constraint on predicted alignments. This is similar to our use of projection distance as described in §5. Drawbacks of Current Alignments. Alignment methods vary in terms of components of the AMR that are candidates for alignment. Most systems either align nodes (e.g., ISI) or connected subgraphs (e.g., JAMR), with incomplete coverage. Most current systems do not align relations to tokens or spans, and those that do (such as ISI) do so with low coverage and performan"
2021.blackboxnlp-1.43,N06-2015,0,0.474268,"ot easy to understand exactly which aspects of cial resource for work on word senses, providlinguistic form and meaning contextualized word ing a fine-grained and comprehensive inventory embeddings are able to capture. of words and their senses for English. Several In response to these difficulties, much work has been done attempting to interpret CWE models, large annotated corpora have been constructed using WordNet senses, including SemCor (Miller most notably in the field of BERTology. Over 100 et al., 1993; Landes et al., 1998) and OntoNotes BERTological studies have been published since (Hovy et al., 2006). More recently, WSD systems BERT was introduced in 2018, covering a diverse 1 range of linguistic phenomena (Rogers et al., 2020) All code for this work is available at https://github. such as POS tags, constituency, and event factuality com/lgessler/bert-has-uncommon-sense. 539 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 539–547 Online, November 11, 2021. ©2021 Association for Computational Linguistics have been able to achieve human-like performance on WSD tasks, but performance on rare senses remains comparatively poor (Blevin"
2021.blackboxnlp-1.43,2021.emnlp-main.806,1,0.743306,"cheme. Tayyar Madabushi et al. (2020) train a BERT variant where the next sentence prediction task has been replaced with a same construction prediction task and find mixed results on downstream tasks. Levine et al. (2020) train a BERT variant by adding a new supersense prediction task, wherein the masked token’s WordNet supersense is to be predicted, and find performance gains on a variety of meaning-related tasks, which shows that BERT’s representations do not perfectly capture word senses. Some recent approaches have developed specialized methods for exploring CWE models’ embedding spaces. Karidi et al. (2021) present an iterative method for surveying the “topography” of BERT’s embedding space, finding that word senses often form cohesive regions. 3 CWE Similarity Ranking “database” corpus D, and a smaller “query” corpus Q. We will use sentences from Q to rank sentences in D, as detailed below. 1. Select a query sentence from Q consisting of (q) (q) (q) tokens w1 ,...,wn , where a token wi has been designated as the target token and has sense si . 2. Find embeddings for the query sentence, (q) (q) (q) (q) h1 ,...,hn = f (w1 ,...,wn ). (d) 3. For every instance w j with its context (d) (d) w1 ,...,w"
2021.blackboxnlp-1.43,P14-1120,0,0.063883,"Missing"
2021.blackboxnlp-1.43,N19-1112,0,0.0605812,"and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations. 1 Introduction (Liu et al., 2019a). The methods developed in this area are usually applicable to other CWE models. An important question for CWE models is how well they can represent rare word senses. Word sense disambiguation systems have been observed to be most lacking in the long tail of rare word senses (Blevins and Zettlemoyer, 2020; Blevins et al., 2021), and sense-awareness is of fundamental importance for many NLP applications. Thus a better understanding of how well CWE models understand senses, and especially rare senses, would aid the interpretation of NLP systems. To address this question, we perform an evaluati"
2021.blackboxnlp-1.43,N19-1225,0,0.154682,"and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations. 1 Introduction (Liu et al., 2019a). The methods developed in this area are usually applicable to other CWE models. An important question for CWE models is how well they can represent rare word senses. Word sense disambiguation systems have been observed to be most lacking in the long tail of rare word senses (Blevins and Zettlemoyer, 2020; Blevins et al., 2021), and sense-awareness is of fundamental importance for many NLP applications. Thus a better understanding of how well CWE models understand senses, and especially rare senses, would aid the interpretation of NLP systems. To address this question, we perform an evaluati"
2021.blackboxnlp-1.43,2021.ccl-1.108,0,0.0812182,"Missing"
2021.blackboxnlp-1.43,H93-1061,0,0.372364,"Missing"
2021.blackboxnlp-1.43,2020.tacl-1.54,0,0.0261468,"alized word ing a fine-grained and comprehensive inventory embeddings are able to capture. of words and their senses for English. Several In response to these difficulties, much work has been done attempting to interpret CWE models, large annotated corpora have been constructed using WordNet senses, including SemCor (Miller most notably in the field of BERTology. Over 100 et al., 1993; Landes et al., 1998) and OntoNotes BERTological studies have been published since (Hovy et al., 2006). More recently, WSD systems BERT was introduced in 2018, covering a diverse 1 range of linguistic phenomena (Rogers et al., 2020) All code for this work is available at https://github. such as POS tags, constituency, and event factuality com/lgessler/bert-has-uncommon-sense. 539 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 539–547 Online, November 11, 2021. ©2021 Association for Computational Linguistics have been able to achieve human-like performance on WSD tasks, but performance on rare senses remains comparatively poor (Blevins and Zettlemoyer, 2020), leading to the construction of corpora tailored for assessing systems on rare senses (Blevins et al., 20"
2021.blackboxnlp-1.43,D15-1036,0,0.0370764,"Wolf et al., 2020). Corpora Our two corpora are OntoNotes 5.0 (Hovy et al., 2006), which has sense annotations for nouns and verbs,3 and the Pattern Dictionary of English Prepositions (PDEP) corpus (Litkowski, 2014), which has sense annotations for prepositions.4 For OntoNotes, we discard instances labeled with “none-of-the-above” senses, as we expect them to be heterogeneous. For PDEP, we 2 Indeed, the basic method of finding embeddings that are nearest to a target embedding has been widely used both before and after the rise of CWEs (cf. Wiedemann et al. 2019, which we describe in §2.2, and Schnabel et al. 2015). 3 Specifically, we use the OntoNotes English noun and verb sense groupings, whose sense inventory was formulated by merging WordNet senses for each lemma until an acceptable level of interannotator agreement was reached. Our preliminary experiments with WordNet senses in another corpus, SemCor (Langone et al., 2004), were difficult to interpret because annotations often seemed to be inconsistent. 4 We use a copy of PDEP obtained from a SQL dump dated 2019-04-19. Original data is available with our code. We formulate a query-by-example task in which a word in sentence context is used to query"
2021.blackboxnlp-1.43,P18-1018,1,0.897615,"Missing"
2021.blackboxnlp-1.43,N15-1177,1,0.817589,"e overtaken by noise. For OntoNotes, ∣D∣ = 229,989, ∣Q∣ = 50,395. For PDEP, ∣D∣ = 33,090, ∣Q∣ = 8,020. Inoculation by Fine-Tuning In addition to the base models, we also evaluate versions of the models that have been fine-tuned with a small number of instances from another dataset, a method called inoculation by fine-tuning (Richardson et al., 2020; Liu et al., 2019b). Inoculation by fine-tuning allows model to surface more domain-relevant information in its output embeddings, while using only a small amount of data so as to avoid teaching the model anything entirely new. We use STREUSLE 4.4 (Schneider and Smith, 2015; Schneider et al., 2018) to fine-tune, sampling supersense annotations of single-word nouns, verbs and prepositions in equal numbers for total counts of 100, 250, 500, 1000, and 2500. See Appendix A for full details. Layer Choice In this work, we use embeddings from the last layer of every model we assess. Preliminary experiments showed that many models show no improvement past the middle layers, and using the last layer is also of interest because many systems freeze their CWE models’ weights and use embeddings from their final layers. Lemma Restriction In all cases, neighbors are restricted"
2021.blackboxnlp-1.43,2020.coling-main.355,0,0.0307635,"kNN classifier: the predicted sense of a word is the one most represented among by the word’s k nearest neighbors (via cosine distance). Despite the simplicity of this WSD system, it is able to achieve results that are competitive with state-of-the-art systems, even achieving new SOTA scores on the SensEval-2 and SensEval-3 datasets. Reif et al. (2019) construct a similar WSD system, relying not on kNN but closest sense-centroids (centroids are constructed using a labeled training set) to decide on a label. Other studies have approached the question by modifying BERT’s training scheme. Tayyar Madabushi et al. (2020) train a BERT variant where the next sentence prediction task has been replaced with a same construction prediction task and find mixed results on downstream tasks. Levine et al. (2020) train a BERT variant by adding a new supersense prediction task, wherein the masked token’s WordNet supersense is to be predicted, and find performance gains on a variety of meaning-related tasks, which shows that BERT’s representations do not perfectly capture word senses. Some recent approaches have developed specialized methods for exploring CWE models’ embedding spaces. Karidi et al. (2021) present an itera"
2021.emnlp-main.806,2020.acl-main.385,0,0.0328306,"nd to distinct concepts. Our experiments indicate a substantial regularity in the BERT-space. We see regions in the space that correspond to distinct senses. These regions can be recovered using our technique; for example, by sampling points around a pseudoword and looking at the points in the BERT-space which decode to it. Moreover, we see that between sense-regions there are often “voids” in the space that do not correspond to any intelligible sense. 2 Analyzing Contextual Representations resentations is directly analyzing the attention weights and activation patterns (Brunner et al., 2020; Abnar and Zuidema, 2020). Criticism against some instances of this approach is found in Jain and Wallace (2019), who claimed that attention weights are less transparent than is often stipulated. The shortcomings of probing. Some recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Re"
2021.emnlp-main.806,P17-1080,0,0.0652387,"Missing"
2021.emnlp-main.806,D09-1046,0,0.041057,"e the pseudoword-space for its own sake—pseudowords are a tool to shed light on the geometry and behavior of the BERT-space—our experiments with pseudowords and artificially perturbed pseudowords reveal that the pseudowordspace contains regions that are semantically coherent as inputs to BERT. Prospects for the MaPP technique. Our dataset is manually curated to control for specific linguistic phenomena. We expect that pseudoword may be less semantically targeted if learned with larger contexts that create more opportunities for confounds. We note also that senses are not necessarily discrete (Erk and McCarthy, 2009), and it would be worthwhile to explore how graded semantic distinctions are represented, as well as underspecified meanings. We are also interested in exploring how BERT represents tokens in sentences that permit multiple plausible interpretations. The MaPP technique can be applied to investigate the properties of other CR models as well, as it requires only that the model be a differentiable function from input token embeddings to contextualized embeddings. 7 Conclusion there is substantial regularity in the BERT-space, with regions that correspond to distinct senses. Moreover, we found evid"
2021.emnlp-main.806,D19-1006,0,0.0180079,"extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous"
2021.emnlp-main.806,2021.acl-long.540,0,0.0409096,"ng from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word sense"
2021.emnlp-main.806,S19-1026,0,0.0545581,"Missing"
2021.emnlp-main.806,N19-1112,0,0.173523,"ts (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use them as inputs for masked prediction of w"
2021.emnlp-main.806,D19-1007,0,0.0629295,"Missing"
2021.emnlp-main.806,2020.emnlp-main.552,0,0.0385908,"Missing"
2021.emnlp-main.806,2020.scil-1.35,0,0.0838615,"Missing"
2021.emnlp-main.806,2020.acl-main.420,0,0.0125787,"ted 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze"
2021.emnlp-main.806,2020.tacl-1.54,0,0.0446319,"od as used in the specialization experiments (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use t"
2021.emnlp-main.806,P18-1018,1,0.796855,"he ambiguous word “for” has a PURPOSE sense, strongly signaled by “reading”. All sentences were reviewed by a linguist to maximize naturalness and minimize ambiguity. The dataset consists of 3 portions, each used in different experiments. We describe each portion adjacent to the relevant experiment. Relational words as a test case. We chose to focus our analysis on the ambiguity of relational words in English, specifically prepositions and verbs. Relational words present an interesting test case: many are highly ambiguous and encode basic semantic distinctions, such as space, time and manner (Schneider et al., 2018). We do not attempt to cover all possible senses of the selected words; instead, we have constructed our dataset to illustrate just a few clear contrasts (see further discussion in appendix A.4). 5 Experiments Query The dinner is on Monday. Top 5 predictions z fire 7 offer 7 sale 7 Friday 3 hold 7 z∗ Sunday 3 Saturday 3 Thursday 3 Tuesday 3 Friday 3 The clip is z minute 7 year 7 second 7 day 7 week 7 about a queen. z∗ woman 3 girl 3 man 3 child 3 boy 3 Table 2: Specialization examples where the pseudoword z∗ learned from the query sentence corresponds to a different sense from BERT’s static wo"
2021.emnlp-main.806,N19-1162,0,0.0216687,"present MaPP to study this hypothesis, and a continuous function also allows us to invert it, in doing so introduce the concept of pseudowords. and obtain a point in the inverse image z∗ of BERT This concept opens additional research questions. by solving an optimization problem. We note that ∗ d viewing the BERT space as a continuous space, e.g., Specialization. Let z ∈ R be a pseudoword obfor purposes of mapping between it and other con- tained by solving eq. (1) for a sentence s with a focus token t and cue token at position j, holding tinuous spaces, is an increasingly common practice ∗ (Schuster et al., 2019; Gauthier and Levy, 2019); a sense η. Does z yield a sense distribution (determined by its slot fillers in the jth position) that see further discussion in appendix A.4. concentrates on η? That is, does a pseudoword In our experiments (§5), the pseudowords will help us explore the geometry of the BERT-space, decode to a specific sense of the focus token? by traveling across it in a “continuous” way— Generalization. Is it possible to transplant a pseudoword into a sentence where the context something that is not possible to do with the BERT around the focus token is different, and still obtain"
2021.emnlp-main.806,J98-1004,0,0.882729,"Missing"
2021.emnlp-main.806,2021.naacl-main.8,1,0.826036,"Missing"
2021.emnlp-main.806,2021.acl-long.550,0,0.0392035,"ns ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as inputs to fying the most suitable meaning from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However,"
2021.emnlp-main.806,2020.emnlp-main.14,0,0.0239961,"Missing"
2021.emnlp-main.806,2020.acl-main.383,0,0.0254198,"recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Relying on classifiers to interpret representations might be problematic; they add additional confounds to the interpretability of the results, and different representations may need different classifiers (Wu et al., 2020; Zhou and Srikumar, 2021). Another critique concerns the difference between correlation and causation (Feder et al., 2021): classifier-based probes may rely on shallow correlations in the training set, thus reflecting data artifacts that are irrelevant to the studied distinction. Probing representations. Deciphering the inforWord Sense Disambiguation. Word Sense Dismation encoded in contextualized representations ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as i"
2021.emnlp-main.806,2021.eacl-main.297,0,0.0142132,"eudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous space. Our proposed methodology thus allows for a more direct inspection of “gaps” between embedded tokens, that does n"
2021.eval4nlp-1.12,W13-2322,1,0.718615,"related fields. In this paper we examine the potential and limitations of one such approach: us- the details that are not included. ing semantic parsing to compare a generated senWe examine the hypothesis that we can meatence to a meaning representation from which it sure the semantic adequacy of a sentence genwas generated, in order to measure semantic ade- erated from an AMR by performing the reverse quacy. We focus on generation of English text from operation—namely, parsing the generated sentence Abstract Meaning Representation graphs (“AMRs”; into AMR—and measuring the similarity of the Banarescu et al., 2013). Figure 1 shows an exam- parsed AMR graph to the original. In essence, this ple of an AMR, which represents the meaning of a idea exploits complementarity of English-to-AMR sentence. AMR does not represent certain morpho- parsers and AMR-to-English generators being evallogical and syntactic details such as tense, number, uated. Assuming an accurate parse, we would ex114 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 114–122 c November 10, 2021. 2021 Association for Computational Linguistics Var r a g p p′ Description Reference sentence Gold"
2021.eval4nlp-1.12,W05-0909,0,0.328904,"rics. mantically related one, or to mitigate the effects of certain parser errors. Thus, we also experiment with computing the S2 match similarity of parsed sentences to the original AMRs. 5 5.3 Results The primary statistic of interest for this study is the sentence-level correlation between a proposed metric and human judgments, particularly those for adequacy. We measure this with Spearman’s Rho correlation. Following Manning et al. (2020), we compare several popular reference-based metrics; table 2 reports the correlations for the 5 metrics they used: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), ChrF++ (Popovi´c, 2017), and BERTScore (Zhang et al., 2020). We add the results of one newer metric, BLEURT (Sellam et al., 2020). Of these, BLEURT performs the best by this measure with a correlation of 0.69. BLEU, the most popular metric for this task, has a correlation of 0.52. Table 3 shows the correlation with adequacy for each variant of the parser-based metric, combining the three AMR parsers and three similarity metrics used. Notably, even the highest correlations here underperform those achieved by BLEU, METEOR, BERTScore, and BLEURT. As expected, the corr"
2021.eval4nlp-1.12,2020.acl-main.119,0,0.0432026,"proach with improved parsing, we conduct an additional study using manually-corrected AMR parses; we find that this substantially improves the quality of the metric (§6). 3 Background The evaluation method we analyze in this paper is closely related to the MFb metric suggested by Opitz and Frank (2021), which combines a measure of meaning preservation, M, with a language model–based measure of grammatical form, F. Their meaning preservation metric, M, assigns a score to a generated sentence by parsing it into AMR and computing the parse’s similarity to the gold AMR. They use the AMR parser by Cai and Lam (2020) and the S2 match similarity metric; we experiment with these as well as other options for both parser and metric. While they perform a number of pilot experiments to test the robustness of MFb , such as its performance with different parsers, Opitz and Frank do not test the correlation of their metric with human judgments; thus, the work presented here adds to our understanding of the validity of this type of metric as a proxy for human evaluation. As a baseline, we also compare the results of parsing-based evaluation with several referencebased metrics, including those that have traditionall"
2021.eval4nlp-1.12,P13-2131,0,0.0217882,"tions on the automatic version of the parsing based metric; that is, the use of similarity metrics comparing the automatic parse p to the gold AMR a. We experiment with different AMR parsers (§5.1) and variations on the Smatch similarity metric (§5.2) and measure the correlation to human judgments of adequacy (§5.3). The second piece needed for parsing-based evaluation is a way to quantify the similarity of an AMR parse to the original AMR. The standard metric for comparing two AMRs— such as to evaluate the quality of an AMR parser or inter-annotator-agreement between human parses— is Smatch (Cai and Knight, 2013). The Smatch score compares triples between two AMR graphs, where each triple is an edge of the graph (a semantic relationship) combined with each of the nodes it connects. For a given pair of AMRs, the Smatch score is the maximum F1-score of triples which can be obtained with a one-to-one mapping of variables between the two graphs. 4 We also experimented with a small variation on the original Smatch. Smatch computes the similarity between two different AMR graphs based on inferred alignments between the two graphs’ concepts. Since checking all possible mappings is computationally intractable"
2021.eval4nlp-1.12,S16-1186,0,0.0385729,"Missing"
2021.eval4nlp-1.12,P14-1134,0,0.0284599,"es the concept of embedding-based semantic gradable semantic similarity by allowing for soft matches between concepts. While the primary advantage of this variant is for tasks with more variation in wording, such as measuring the similarity of paraphrases, it could also be advantageous in our setting—for example, to penalize AMR generation systems that represent a concept with the wrong word less if it is a se5.1 Parsers We compare gold AMRs to AMR parses of the generated sentences. This includes using three different automatic English-to-AMR parsers, described below. JAMR. The JAMR parser 1 (Flanigan et al., 2014, 2016) is an early AMR parser; we use it as a baseline to compare against the more recent, higher-accuracy parsers. The JAMR parser uses a semi-Markov model to identify concepts, followed by a graph variant on Maximum Spanning Tree algorithms to identify the relations between concepts. We used the 2016 version, which achieved a Smatch score of 67 on the LDC2015E86 dataset. LYU -T ITOV. While most AMR parsers first train an aligner to align AMR nodes with words in a sentence prior to training the parser itself, Lyu and Titov (2018) 2 treat alignments as latent variables in a joint probabilisti"
2021.eval4nlp-1.12,P18-1037,0,0.0179554,"ish-to-AMR parsers, described below. JAMR. The JAMR parser 1 (Flanigan et al., 2014, 2016) is an early AMR parser; we use it as a baseline to compare against the more recent, higher-accuracy parsers. The JAMR parser uses a semi-Markov model to identify concepts, followed by a graph variant on Maximum Spanning Tree algorithms to identify the relations between concepts. We used the 2016 version, which achieved a Smatch score of 67 on the LDC2015E86 dataset. LYU -T ITOV. While most AMR parsers first train an aligner to align AMR nodes with words in a sentence prior to training the parser itself, Lyu and Titov (2018) 2 treat alignments as latent variables in a joint probabilistic model for identifying concepts, relations, and alignments. This parser achieved a Smatch score of 73.7 on LDC2015E86 and 74.4 on LDC2016E25, which at the time was state-of-the-art. C AI -L AM. Cai and Lam (2020) 3 was the state of the art in AMR parsing as of 2020, with a Smatch score of 80.2 on LDC2017T10. This transformerbased parser uses iterative inference to determine which part of the input sentence to parse and where to add it to the output graph, without requiring explicit alignments. Parser performance. We evaluate each"
2021.eval4nlp-1.12,2020.coling-main.420,1,0.926628,"parsed from g Manually-corrected version of automatic parses p Table 1: Summary of notation used in this paper, with a description of each type of sentence and AMR used. pect this to be a good measure of the adequacy of the generated sentence, since a sentence that accurately expresses the meaning in the original AMR should have the same AMR. We further formalize this approach in §2. As discussed in §3, this method has also been suggested by Opitz and Frank (2021); we contribute new analyses of its validity, in particular by measuring its correlation with human adequacy judgments collected by Manning et al. (2020). We find that errors made by an automatic AMR parser substantially limit the quality of parsing-based evaluation as a proxy for human evaluation, resulting in a lower correlation with adequacy scores than many reference-based metrics (§5). To approximate an upper bound for the potential of this evaluation approach with improved parsing, we conduct an additional study using manually-corrected AMR parses; we find that this substantially improves the quality of the metric (§6). 3 Background The evaluation method we analyze in this paper is closely related to the MFb metric suggested by Opitz and"
2021.eval4nlp-1.12,2021.eacl-main.129,0,0.0335078,"omputational Linguistics Var r a g p p′ Description Reference sentence Gold AMR, created from r Sentence automatically generated from a AMR automatically parsed from g Manually-corrected version of automatic parses p Table 1: Summary of notation used in this paper, with a description of each type of sentence and AMR used. pect this to be a good measure of the adequacy of the generated sentence, since a sentence that accurately expresses the meaning in the original AMR should have the same AMR. We further formalize this approach in §2. As discussed in §3, this method has also been suggested by Opitz and Frank (2021); we contribute new analyses of its validity, in particular by measuring its correlation with human adequacy judgments collected by Manning et al. (2020). We find that errors made by an automatic AMR parser substantially limit the quality of parsing-based evaluation as a proxy for human evaluation, resulting in a lower correlation with adequacy scores than many reference-based metrics (§5). To approximate an upper bound for the potential of this evaluation approach with improved parsing, we conduct an additional study using manually-corrected AMR parses; we find that this substantially improve"
2021.eval4nlp-1.12,2020.tacl-1.34,0,0.0174815,"lt is four random restarts. This means that Smatch scores are nondeterministic; when running twice on the same pair of AMRs, we sometimes got different scores. To mitigate this effect, we made two changes: First, we increased the number of restarts to 100 to increase the chance that the best mapping would be found, while still maintaining a reasonable runtime. Second, we seeded the random function in the Smatch script to make the results reproducible. In table 3, we refer to the default Smatch as ‘Smatch4 ’, while the variation with a seed and 100 restarts is ‘Smatch100 +seed’. More recently, Opitz et al. (2020) analyzed both Smatch and an alternative metric, SemBleu (Song and Gildea, 2019), and proposed a new variant of Smatch, S2 match, which conforms to desirable principles better than either previous metric. In particular, S2 match introduces the concept of embedding-based semantic gradable semantic similarity by allowing for soft matches between concepts. While the primary advantage of this variant is for tasks with more variation in wording, such as measuring the similarity of paraphrases, it could also be advantageous in our setting—for example, to penalize AMR generation systems that represen"
2021.eval4nlp-1.12,P02-1040,0,0.111386,"udgments for reference-based metrics. mantically related one, or to mitigate the effects of certain parser errors. Thus, we also experiment with computing the S2 match similarity of parsed sentences to the original AMRs. 5 5.3 Results The primary statistic of interest for this study is the sentence-level correlation between a proposed metric and human judgments, particularly those for adequacy. We measure this with Spearman’s Rho correlation. Following Manning et al. (2020), we compare several popular reference-based metrics; table 2 reports the correlations for the 5 metrics they used: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), ChrF++ (Popovi´c, 2017), and BERTScore (Zhang et al., 2020). We add the results of one newer metric, BLEURT (Sellam et al., 2020). Of these, BLEURT performs the best by this measure with a correlation of 0.69. BLEU, the most popular metric for this task, has a correlation of 0.52. Table 3 shows the correlation with adequacy for each variant of the parser-based metric, combining the three AMR parsers and three similarity metrics used. Notably, even the highest correlations here underperform those achieved by BLEU, METEOR, BERTScore"
2021.eval4nlp-1.12,W17-4770,0,0.022909,"Missing"
2021.eval4nlp-1.12,2020.acl-main.704,0,0.0248429,"ces to the original AMRs. 5 5.3 Results The primary statistic of interest for this study is the sentence-level correlation between a proposed metric and human judgments, particularly those for adequacy. We measure this with Spearman’s Rho correlation. Following Manning et al. (2020), we compare several popular reference-based metrics; table 2 reports the correlations for the 5 metrics they used: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), ChrF++ (Popovi´c, 2017), and BERTScore (Zhang et al., 2020). We add the results of one newer metric, BLEURT (Sellam et al., 2020). Of these, BLEURT performs the best by this measure with a correlation of 0.69. BLEU, the most popular metric for this task, has a correlation of 0.52. Table 3 shows the correlation with adequacy for each variant of the parser-based metric, combining the three AMR parsers and three similarity metrics used. Notably, even the highest correlations here underperform those achieved by BLEU, METEOR, BERTScore, and BLEURT. As expected, the correlation increases with parser quality, indicating that parsers that have higher accuracy on human-authored sentences also do better with generated sentences."
2021.eval4nlp-1.12,2006.amta-papers.25,0,0.138036,"to mitigate the effects of certain parser errors. Thus, we also experiment with computing the S2 match similarity of parsed sentences to the original AMRs. 5 5.3 Results The primary statistic of interest for this study is the sentence-level correlation between a proposed metric and human judgments, particularly those for adequacy. We measure this with Spearman’s Rho correlation. Following Manning et al. (2020), we compare several popular reference-based metrics; table 2 reports the correlations for the 5 metrics they used: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), ChrF++ (Popovi´c, 2017), and BERTScore (Zhang et al., 2020). We add the results of one newer metric, BLEURT (Sellam et al., 2020). Of these, BLEURT performs the best by this measure with a correlation of 0.69. BLEU, the most popular metric for this task, has a correlation of 0.52. Table 3 shows the correlation with adequacy for each variant of the parser-based metric, combining the three AMR parsers and three similarity metrics used. Notably, even the highest correlations here underperform those achieved by BLEU, METEOR, BERTScore, and BLEURT. As expected, the correlation increases with pars"
2021.eval4nlp-1.12,P19-1446,0,0.0205762,"c; when running twice on the same pair of AMRs, we sometimes got different scores. To mitigate this effect, we made two changes: First, we increased the number of restarts to 100 to increase the chance that the best mapping would be found, while still maintaining a reasonable runtime. Second, we seeded the random function in the Smatch script to make the results reproducible. In table 3, we refer to the default Smatch as ‘Smatch4 ’, while the variation with a seed and 100 restarts is ‘Smatch100 +seed’. More recently, Opitz et al. (2020) analyzed both Smatch and an alternative metric, SemBleu (Song and Gildea, 2019), and proposed a new variant of Smatch, S2 match, which conforms to desirable principles better than either previous metric. In particular, S2 match introduces the concept of embedding-based semantic gradable semantic similarity by allowing for soft matches between concepts. While the primary advantage of this variant is for tasks with more variation in wording, such as measuring the similarity of paraphrases, it could also be advantageous in our setting—for example, to penalize AMR generation systems that represent a concept with the wrong word less if it is a se5.1 Parsers We compare gold AM"
2021.findings-emnlp.423,J99-2004,0,0.441201,"Missing"
2021.findings-emnlp.423,W02-2203,0,0.272676,"confidence scores with SMCE (overall error) and GMCE (pergroup error). 4.1 Taggers We consider two supervised tagging tasks trained and evaluated on different English datasets: CCG supertagging—a syntactic task with a large amount of training data and a high-accuracy model, and Lexical Semantic Recognition—a semantic task with less data and a lower-accuracy model. 4.1.1 CCG Supertagging CCG is a lexicalized grammar formalism that is frequently used for syntactic and semantic parsing. CCG supertagging is the task of labeling each token with a complex, structured label that belies its function (Clark, 2002; Bangalore and Joshi, 2010). Ban4 galore and Joshi (1999) describe supertagging as Patel et al. (2021) explored a similar idea in one of their experiments on digit recognition: digits with similar class “almost parsing”, because a sequence of supertags priors were grouped together manually for recalibration. Howmaps a sentence to a small set of possible parses— ever, Patel et al. did not propose a general grouping technique, nor did they address large sparse tagsets as we do here. the CCGBank (Hockenmaier and Steedman, 2007) 4923 Figure 1: Illustration of tag frequency grouping (TFG) with 45"
2021.findings-emnlp.423,N19-1423,0,0.0200814,"Missing"
2021.findings-emnlp.423,J07-3004,0,0.0130795,"e task of labeling each token with a complex, structured label that belies its function (Clark, 2002; Bangalore and Joshi, 2010). Ban4 galore and Joshi (1999) describe supertagging as Patel et al. (2021) explored a similar idea in one of their experiments on digit recognition: digits with similar class “almost parsing”, because a sequence of supertags priors were grouped together manually for recalibration. Howmaps a sentence to a small set of possible parses— ever, Patel et al. did not propose a general grouping technique, nor did they address large sparse tagsets as we do here. the CCGBank (Hockenmaier and Steedman, 2007) 4923 Figure 1: Illustration of tag frequency grouping (TFG) with 45 training instances and G = 3 tag groups. Each shape represents a gold tag from the training data. Tags are sorted by frequency. Starting with the most frequent tag, groups are formed by iteratively adding all instances of a tag until the size of the group equals or exceeds the number of training instances divided by the number of groups. When TFG is used for recalibration (as opposed to just evaluation), a separate recalibration model is learned for each group. dataset has over 1,200 unique CCG labels. By convention, the mode"
2021.findings-emnlp.423,2020.acl-main.188,0,0.0199045,"inning), the calibrated score is the average output of the scaling function on the development scores in the bin. In our experiments with scaling binning, we use isotonic regression as the scaling function. 2.3 Related Work Zadrozny and Elkan (2002) initially proposed the one-vs.-rest approach for multiclass probabilities. Kuleshov and Liang (2015) recognize the sparsity problem and suggest reducing multiclass calibration of structured prediction to targeted “events of interest” and training a binary forecaster to learn calibrated probabilities of the event happening. This work is extended by Jagannatha and Yu (2020), who treat a sequence of tags as a compositional model output and develop a forecaster based on gradient boosted decision trees. They achieve reductions in expected calibration error and a slight increase in model performance after reranking. Reranking refers to the process of normalizing calibrated scores and reordering them. With most recalibration techniques, it is rare for the ranking to be affected, and with some techniques like isotonic regression, the ranking of calibrated confidence scores will always match the uncalibrated ones. an important component of calibration, they don’t tell"
2021.findings-emnlp.423,2021.mwe-1.6,1,0.755524,"na follow power law distributions and thus feature a long tail of individually rare events, which, as we will show, makes it nontrivial to measure calibration error with existing methods, including marginal calibration error (MCE), which requires sufficient samples of each class to produce a reliable estimate (Kumar et al., 2019). We evaluate two English sentence taggers1 with closed sets of 100s of tags that disambiguate word tokens: a Combinatory Categorial Grammar (CCG) syntactic supertagger with 426 tags (Prange et al., 2021), and a Lexical Semantic Recognition (LSR) tagger with 598 tags (Liu et al., 2021). Our main contributions are the following: • We posit that evaluation of calibration should go beyond a model’s highest-confidence prediction, extending the arguments of Nixon et al. (2020), with a particular focus on sparse tagsets. • We propose tag frequency grouping (TFG), a novel technique for evaluating and recalibrating groups of similarly frequent tags in a sparse tagging space. An advantage of probabilistic models is that, in addition to providing a prediction, they also quantify uncertainty. Knowing how certain a model is about a particular prediction can be crucial when using its ou"
2021.findings-emnlp.423,D15-1182,0,0.0495419,"Missing"
2021.findings-emnlp.423,P18-1018,1,0.851687,"Missing"
2021.findings-emnlp.423,N15-1177,1,0.882985,"Missing"
2021.law-1.10,P13-1023,0,0.431894,"(§2), and fills the gap of Adverbials, as subtyping Adverbials in particular can refine UCCA annotation to be more informative and precise (§3). After providing the motivation to study Adverbials, we propose a subcategorization scheme with explanations (§4). 1 Finally we present pilot results of annotation on UCCA data and IAA measures (§5), 2 followed by discussions of difficulties, complexities, and limitations (§6). Introduction The UCCA representation scheme is designed to be a multi-layered, cross-linguistically portable and stable semantic annotation scheme based on linguistic typology (Abend and Rappoport, 2013; Dixon, 2010). Among its 14 coarse-grained semantic categories, Adverbial serves as the label for modifiers to a scene’s main predicate. Despite the syntactic-sounding name, Adverbial is a semantic category that cuts across a range of syntactic constructions that provide scene-modifying meanings. These meanings fall into drastically different semantic categories—negations, manner PPs, modals, adverbs are all candidates for the Adverbial category—that change the sentence’s meaning in different ways. This paper proposes to refine UCCA Adverbials, in order to better understand the semantic funct"
2021.law-1.10,W13-2322,1,0.723133,"oundaries between these categories. 6.2 Relation to Other Representations Clarifying foundational layer decisions. We find two cases where other foundational layer categories and D are not reliably distinguished. First, we find that D-Aspectual and Time (T) overlap: both have been applied to instances of frequency and duration, e.g. (59, 60). Empirically, DAspectual also includes event quantification (61), change-of-state verbs (62), as well as temporally unspecific indicators of recency (or urgency, or lateness) (63). AMR. In Abstract Meaning Representation (AMR) and its proposed extensions (Banarescu et al., 2013; Donatelli et al., 2018; Bonial et al., 2018), a sentence is labeled with relations and concepts in a rooted, directed, acyclic graph.7 The current work has overlaps with AMR concepts—at different levels of the AMR graph—some of which can even be said to be close mappings, but overall our proposed scheme is designed to be compatible with UCCA’s layered annotation structure rather than already-fine-grained AMR graphs. For example, in (66) and (67) we juxtapose the AMR and D-refined UCCA annotations of a sentence, respectively. While D - POSSIBILITY maps to the AMR concept possible-01 and D - N"
2021.law-1.10,L18-1266,1,0.890729,"Missing"
2021.law-1.10,K19-1017,1,0.85041,"(P), core argument units called Participants (A), which may be either non-scenes (i.e., roughly, ‘entities’) or scenes themselves, and modifiers such as Time (T) or Adverbial (D) units (Abend et al., 2020). Adverbials can be understood as denoting secondary predications over a scene’s main State or Process relation. Other extensions to UCCA. Building upon the UCCA foundational layer, there have been several efforts and proposals to refine annotations of specific constructions such as implicit arguments (Cui and Hershcovich, 2020), numeric expressions (Cui and Hershcovich, 2021), coreference (Prange et al., 2019b), and adpositional phrases (Prange et al., 2019a; Shalev et al., 2019). The latter class in particular partially overlaps with the Adverbial category, as discussed in §6. This paper will accompany these annotation refinements, with a special focus on the Adverbials. 3 the meaning of the rest of the sentence is interpreted. Because Adverbials (D) cover wide semantic ground, refining the semantic annotation can thus be more informative, along similar lines as refining Participants with semantic roles (Prange et al., 2019a; Shalev et al., 2019). Foundational layer annotations are inconsistent."
2021.law-1.10,W19-3319,1,0.852663,"(P), core argument units called Participants (A), which may be either non-scenes (i.e., roughly, ‘entities’) or scenes themselves, and modifiers such as Time (T) or Adverbial (D) units (Abend et al., 2020). Adverbials can be understood as denoting secondary predications over a scene’s main State or Process relation. Other extensions to UCCA. Building upon the UCCA foundational layer, there have been several efforts and proposals to refine annotations of specific constructions such as implicit arguments (Cui and Hershcovich, 2020), numeric expressions (Cui and Hershcovich, 2021), coreference (Prange et al., 2019b), and adpositional phrases (Prange et al., 2019a; Shalev et al., 2019). The latter class in particular partially overlaps with the Adverbial category, as discussed in §6. This paper will accompany these annotation refinements, with a special focus on the Adverbials. 3 the meaning of the rest of the sentence is interpreted. Because Adverbials (D) cover wide semantic ground, refining the semantic annotation can thus be more informative, along similar lines as refining Participants with semantic roles (Prange et al., 2019a; Shalev et al., 2019). Foundational layer annotations are inconsistent."
2021.law-1.10,P18-1018,1,0.888765,"Missing"
2021.law-1.10,W19-3316,1,0.840001,"non-scenes (i.e., roughly, ‘entities’) or scenes themselves, and modifiers such as Time (T) or Adverbial (D) units (Abend et al., 2020). Adverbials can be understood as denoting secondary predications over a scene’s main State or Process relation. Other extensions to UCCA. Building upon the UCCA foundational layer, there have been several efforts and proposals to refine annotations of specific constructions such as implicit arguments (Cui and Hershcovich, 2020), numeric expressions (Cui and Hershcovich, 2021), coreference (Prange et al., 2019b), and adpositional phrases (Prange et al., 2019a; Shalev et al., 2019). The latter class in particular partially overlaps with the Adverbial category, as discussed in §6. This paper will accompany these annotation refinements, with a special focus on the Adverbials. 3 the meaning of the rest of the sentence is interpreted. Because Adverbials (D) cover wide semantic ground, refining the semantic annotation can thus be more informative, along similar lines as refining Participants with semantic roles (Prange et al., 2019a; Shalev et al., 2019). Foundational layer annotations are inconsistent. Beyond the domain of Adverbial-proper, another key motivation of D-refin"
2021.law-1.6,2020.framenet-1.11,0,0.0251174,"than the source text—through an inclusion relation symbol (Bos, 2014). Work on the Parallel Meaning Bank also includes a comparison of translations as being meaning-preserving or not, and these discrepancies as being largely due to: human annotation error, syntactic differences in definite articles, translation of proper names, or non-literal translations (van Noord et al., 2018). Alignment for multilingual meaning representations has also been studied in relation to FrameNet, including recent work looking to unify a Multilingual FrameNet with alignments between all dozen FrameNet languages (Baker and Lorenzi, 2020). The Prague Czech-English Dependency Treebank (PCEDT) similarly considers the adaptation of an English framework to a non-English extension, focusing on dependency annotation for the Czech translations of the Penn Treebank. PCEDT highlights some of the issues that arise out of automatically transferring annotation schema to another language. Ultimately, the authors find that the annotation schema are not sufficiently fine-grained to provide for a seamless conversion from annotation in one language to annotation in the other, and the difficulty of developing an annotation schema capable of thi"
2021.law-1.6,W13-2322,1,0.890384,"; for example, it has been shown that translation divergences have a measurable impact on machine translation (Vyas et al., 2018). This information on the types and causes of divergence enables these parallel texts to be more fully utilized in crosslingual natural language processing tasks. Specifically, different types of semantic divergences impact the performance of neural machine translation systems differently (Briakou and Carpuat, 2021), which motivates work to categorize and describe divergences in parallel texts. Taking advantage of the fact that Abstract Meaning Representation (AMR) (Banarescu et al., 2013, 2019) resources and tools have been developed for a variety of languages (§2.3) and given the 56 Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 56–65 November, 2021. ©2021 Association for Computational Linguistics cross-lingual Spanish-English analysis of AMR pairs, as well as promote expansion of this work to other languages. 1 2 2020). Similarly, an attempt to classify these divergences automatically includes the hierarchical alignment scheme of Chinese and English parse trees which enables the identificati"
2021.law-1.6,2020.emnlp-main.195,0,0.0236944,"CA visualizes and automatically aligns AMRs, including two AMRs of a sentence and its translation, designed to facilitate research into cross-lingual AMRs (Saphra and Lopez, 2015). We take this as inspiration to use AMR pairs as a starting point for divergence classification between parallel texts. Cross-lingual approaches to AMR parsing explore transfer learning techniques to generate parallel AMR annotations in multiple languages, and suggest that AMR can serve as a cross-lingual semantic representation capable of overcoming linguistic differences (Damonte and Cohen, 2018; Zhu et al., 2019; Blloshmi et al., 2020). Additional work has explored whether structural differences across cross-lingual Chinese/English and English/Czech AMR pairs are due to syntactic idiosyncrasies (Xue et al., 2014), which can be of use in machine translation (Song et al., 2019; Nguyen et al., 2021). 3 Non-Core Role Differences Switch Arg and Non-Core Different NonCore Role Chosen Arg Differences Added/ Omitted Arg Different Arg Chosen Figure 2: Types of structural divergence. Cause of Divergence Semantic Divergence Annotation Divergence Syntactic Divergence Figure 3: Causes of structural divergence. divergence as well as the"
2021.law-1.6,W13-0101,0,0.0319165,"gual AMR pairs to identify sentences as being divergent or equivalent. 5 Related Work on Cross-lingual Meaning Representations Related work has been done on cross-lingual adaptations of other meaning representations, including the development and analysis of cross-lingual semantic annotation schemes (Van Gysel et al., 2019). The Universal Dependencies framework has also been useful in the analysis of cross-lingual syntactic divergences (Nikolaev et al., 2020). Universal Conceptual Cognitive Annotation (UCCA) annotates grammatical meaning while abstracting away from the syntax of the language (Abend and Rappoport, 2013). The Uniform Meaning Representation framework extends AMR with a focus on quantification and scope, as well as uniformity across languages (Van Gysel et al., 2021). The Parallel Meaning Bank is a corpus of paral62 lel texts with corresponding linguistic annotations and Discourse Representation Structure annotations projected from English (Abzianidze et al., 2017). Prior work towards the production of parallel meaning banks focused on the alignment of informative translations—translations where more details are included in the target translation than the source text—through an inclusion relati"
2021.law-1.6,2021.acl-long.562,0,0.0199715,"istic considerations, or simply the translator’s preference for idiomatic phrasing. Identifying translation divergences may enable more nuanced use of parallel text in applications; for example, it has been shown that translation divergences have a measurable impact on machine translation (Vyas et al., 2018). This information on the types and causes of divergence enables these parallel texts to be more fully utilized in crosslingual natural language processing tasks. Specifically, different types of semantic divergences impact the performance of neural machine translation systems differently (Briakou and Carpuat, 2021), which motivates work to categorize and describe divergences in parallel texts. Taking advantage of the fact that Abstract Meaning Representation (AMR) (Banarescu et al., 2013, 2019) resources and tools have been developed for a variety of languages (§2.3) and given the 56 Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 56–65 November, 2021. ©2021 Association for Computational Linguistics cross-lingual Spanish-English analysis of AMR pairs, as well as promote expansion of this work to other languages. 1 2 2020)"
2021.law-1.6,P13-2131,0,0.0355796,"s it does not represent morphology, articles, or tense, but does require a fair amount of training. Inter-annotator agreement is measured using Smatch, which calculates semantic overlap English: Which is your planet? (p / planet :poss (y / you) :domain (a / amr-unknown)) Spanish: ¿ De qué planeta eres ? Literal translation: What planet are you from? (s / ser-de-91 :ARG1 (t / tú) :ARG2 (p / planeta :campo (a / amr-desconocido))) 1 The dataset for this work can be found at: https:// github.com/shirawein/spanish-english-amr-corpus Figure 1: Example of a focus,sem divergence. 57 between two AMRs (Cai and Knight, 2013). The root of the AMR is the focus, an attribute marked with :argN is a core argument role (where N is some number ≥0), and any other attribute (e.g., :opN, :domain, :manner) is a non-core role. 2.3 Structural Divergence Different Focus Use of AMR in Many Languages and Cross-linguistically Added/Omitted Non-Core Role Abstract Meaning Representation is not an interlingua, but has since been studied in multiple languages and cross-linguistically (Xue et al., 2014; Li et al., 2016; Migueles-Abraira, 2017; Linh and Nguyen, 2019; Sobrevilla Cabezudo and Pardo, 2019; Choe et al., 2019). AMR annotati"
2021.law-1.6,E17-2039,0,0.05122,"Missing"
2021.law-1.6,W17-3209,0,0.0239626,"your planet?” is aligned to “¿De qué planeta eres?” which literally translates to “What planet are you from?” The Spanish sentence is less awkward than the original English sentence, and more explicitly asks about planet of origin, as opposed to ownership of a planet. This is a semantic divergence (due to translation choice), resulting in the Spanish AMR having a different focus (root). Divergences have been explored with respect to synonymy (Gaillard et al., 2010) and diachronically (Montariol and Allauzen, 2021). Other approaches have addressed whether and how given sentence pairs diverge. Carpuat et al. (2017) identify divergences in parallel corpora using a cross-lingual textual entailment system to identify less equivalent sentence pairs. Relatedly, work has been done to identify semantic divergences in parallel texts, classifying sentences as being divergent or non-divergent (Vyas et al., 2018). Prior work which both categorically annotated and modeled semantic divergences includes the REFreSD dataset of English-French sentence pairs annotated with three types of divergences (subtree deletion, phrase replacement, and lexical substitution) based on a tree model (Briakou and Carpuat, 2.2 AMR for C"
2021.law-1.6,2020.emnlp-main.123,0,0.025141,"and Cross-linguistically Added/Omitted Non-Core Role Abstract Meaning Representation is not an interlingua, but has since been studied in multiple languages and cross-linguistically (Xue et al., 2014; Li et al., 2016; Migueles-Abraira, 2017; Linh and Nguyen, 2019; Sobrevilla Cabezudo and Pardo, 2019; Choe et al., 2019). AMR annotations of The Little Prince have been presented in English, Chinese (Li et al., 2016), Spanish (Migueles-Abraira, 2017), and Vietnamese (Linh and Nguyen, 2019). Additional work has been done to adapt AMR annotation (Sobrevilla Cabezudo and Pardo, 2019) and alignment (Anchiêta and Pardo, 2020) to Portuguese, and Korean (Choe et al., 2019). Cross-lingual studies of AMR primarily compare the structures between two AMRs representative of a parallel text, which may partially be due to the fact that AMR is not designed to be interlingual (Xue et al., 2014). AMRICA visualizes and automatically aligns AMRs, including two AMRs of a sentence and its translation, designed to facilitate research into cross-lingual AMRs (Saphra and Lopez, 2015). We take this as inspiration to use AMR pairs as a starting point for divergence classification between parallel texts. Cross-lingual approaches to AMR"
2021.law-1.6,W19-3317,0,0.0344955,"rpus Figure 1: Example of a focus,sem divergence. 57 between two AMRs (Cai and Knight, 2013). The root of the AMR is the focus, an attribute marked with :argN is a core argument role (where N is some number ≥0), and any other attribute (e.g., :opN, :domain, :manner) is a non-core role. 2.3 Structural Divergence Different Focus Use of AMR in Many Languages and Cross-linguistically Added/Omitted Non-Core Role Abstract Meaning Representation is not an interlingua, but has since been studied in multiple languages and cross-linguistically (Xue et al., 2014; Li et al., 2016; Migueles-Abraira, 2017; Linh and Nguyen, 2019; Sobrevilla Cabezudo and Pardo, 2019; Choe et al., 2019). AMR annotations of The Little Prince have been presented in English, Chinese (Li et al., 2016), Spanish (Migueles-Abraira, 2017), and Vietnamese (Linh and Nguyen, 2019). Additional work has been done to adapt AMR annotation (Sobrevilla Cabezudo and Pardo, 2019) and alignment (Anchiêta and Pardo, 2020) to Portuguese, and Korean (Choe et al., 2019). Cross-lingual studies of AMR primarily compare the structures between two AMRs representative of a parallel text, which may partially be due to the fact that AMR is not designed to be interli"
2021.law-1.6,L18-1486,0,0.0377574,"Missing"
2021.law-1.6,2021.acl-long.100,0,0.0628177,"Missing"
2021.law-1.6,N18-1104,0,0.019912,"to be interlingual (Xue et al., 2014). AMRICA visualizes and automatically aligns AMRs, including two AMRs of a sentence and its translation, designed to facilitate research into cross-lingual AMRs (Saphra and Lopez, 2015). We take this as inspiration to use AMR pairs as a starting point for divergence classification between parallel texts. Cross-lingual approaches to AMR parsing explore transfer learning techniques to generate parallel AMR annotations in multiple languages, and suggest that AMR can serve as a cross-lingual semantic representation capable of overcoming linguistic differences (Damonte and Cohen, 2018; Zhu et al., 2019; Blloshmi et al., 2020). Additional work has explored whether structural differences across cross-lingual Chinese/English and English/Czech AMR pairs are due to syntactic idiosyncrasies (Xue et al., 2014), which can be of use in machine translation (Song et al., 2019; Nguyen et al., 2021). 3 Non-Core Role Differences Switch Arg and Non-Core Different NonCore Role Chosen Arg Differences Added/ Omitted Arg Different Arg Chosen Figure 2: Types of structural divergence. Cause of Divergence Semantic Divergence Annotation Divergence Syntactic Divergence Figure 3: Causes of structu"
2021.law-1.6,J17-3002,0,0.0233139,"r a variety of languages (§2.3) and given the 56 Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 56–65 November, 2021. ©2021 Association for Computational Linguistics cross-lingual Spanish-English analysis of AMR pairs, as well as promote expansion of this work to other languages. 1 2 2020). Similarly, an attempt to classify these divergences automatically includes the hierarchical alignment scheme of Chinese and English parse trees which enables the identification and quantification of translation divergences (Deng and Xue, 2017). These divergences have also been considered for downstream tasks. Prior work includes fine-tuning approach to account for non-literal translations in the pre-training of cross-lingual language models (Zhai et al., 2020). Background and Related Work 2.1 Semantic Divergences Translation divergences occur when translation from one language to another results in a different meaning or structure (Dorr, 1994). These translation divergences can appear due to translation choices or to syntactic differences between the languages (Dorr, 1990; Dorr and Voss, 1993). The implications of these translation"
2021.law-1.6,2020.acl-main.109,0,0.0114874,"f divergence) and columns (causes of divergence) are found in clear cells to the right and bottom, respectively. the semantic information captured by cross-lingual AMR pairs to identify sentences as being divergent or equivalent. 5 Related Work on Cross-lingual Meaning Representations Related work has been done on cross-lingual adaptations of other meaning representations, including the development and analysis of cross-lingual semantic annotation schemes (Van Gysel et al., 2019). The Universal Dependencies framework has also been useful in the analysis of cross-lingual syntactic divergences (Nikolaev et al., 2020). Universal Conceptual Cognitive Annotation (UCCA) annotates grammatical meaning while abstracting away from the syntax of the language (Abend and Rappoport, 2013). The Uniform Meaning Representation framework extends AMR with a focus on quantification and scope, as well as uniformity across languages (Van Gysel et al., 2021). The Parallel Meaning Bank is a corpus of paral62 lel texts with corresponding linguistic annotations and Discourse Representation Structure annotations projected from English (Abzianidze et al., 2017). Prior work towards the production of parallel meaning banks focused o"
2021.law-1.6,P90-1017,0,0.34707,"ried and widespread, challenging approaches that rely on parallel text. To annotate translation divergences, we propose a schema grounded in the Abstract Meaning Representation (AMR), a sentence-level semantic framework instantiated for a number of languages. By comparing parallel AMR graphs, we can identify specific points of divergence. Each divergence is labeled with both a type and a cause. We release a small corpus of annotated English-Spanish data, and analyze the annotations in our corpus. 1 Introduction A variety of factors come into play in translating from one language into another (Dorr, 1990; Dorr and Voss, 1993). The resulting parallel texts are not always completely equivalent in meaning. Differences, or divergences, between the source and target may reflect lexical or grammatical differences between the two languages, stylistic considerations, or simply the translator’s preference for idiomatic phrasing. Identifying translation divergences may enable more nuanced use of parallel text in applications; for example, it has been shown that translation divergences have a measurable impact on machine translation (Vyas et al., 2018). This information on the types and causes of diverg"
2021.law-1.6,N15-3008,0,0.0200571,"), and Vietnamese (Linh and Nguyen, 2019). Additional work has been done to adapt AMR annotation (Sobrevilla Cabezudo and Pardo, 2019) and alignment (Anchiêta and Pardo, 2020) to Portuguese, and Korean (Choe et al., 2019). Cross-lingual studies of AMR primarily compare the structures between two AMRs representative of a parallel text, which may partially be due to the fact that AMR is not designed to be interlingual (Xue et al., 2014). AMRICA visualizes and automatically aligns AMRs, including two AMRs of a sentence and its translation, designed to facilitate research into cross-lingual AMRs (Saphra and Lopez, 2015). We take this as inspiration to use AMR pairs as a starting point for divergence classification between parallel texts. Cross-lingual approaches to AMR parsing explore transfer learning techniques to generate parallel AMR annotations in multiple languages, and suggest that AMR can serve as a cross-lingual semantic representation capable of overcoming linguistic differences (Damonte and Cohen, 2018; Zhu et al., 2019; Blloshmi et al., 2020). Additional work has explored whether structural differences across cross-lingual Chinese/English and English/Czech AMR pairs are due to syntactic idiosyncr"
2021.law-1.6,J94-4004,0,0.656158,"divergences automatically includes the hierarchical alignment scheme of Chinese and English parse trees which enables the identification and quantification of translation divergences (Deng and Xue, 2017). These divergences have also been considered for downstream tasks. Prior work includes fine-tuning approach to account for non-literal translations in the pre-training of cross-lingual language models (Zhai et al., 2020). Background and Related Work 2.1 Semantic Divergences Translation divergences occur when translation from one language to another results in a different meaning or structure (Dorr, 1994). These translation divergences can appear due to translation choices or to syntactic differences between the languages (Dorr, 1990; Dorr and Voss, 1993). The implications of these translation divergences include difficulties when using parallel texts for downstream tasks, because it can be difficult to identify why or how parallel sentences differ. For example, a parallel corpus, such as a work of fiction, likely contains some non-literal translations. When training a machine translation system on this parallel corpus, these divergences present a problem if looking to produce as literal a tra"
2021.law-1.6,Y10-1094,0,0.0201391,"blem if looking to produce as literal a translation as possible. An example of this from our corpus is shown in figure 1. The English sentence “Which is your planet?” is aligned to “¿De qué planeta eres?” which literally translates to “What planet are you from?” The Spanish sentence is less awkward than the original English sentence, and more explicitly asks about planet of origin, as opposed to ownership of a planet. This is a semantic divergence (due to translation choice), resulting in the Spanish AMR having a different focus (root). Divergences have been explored with respect to synonymy (Gaillard et al., 2010) and diachronically (Montariol and Allauzen, 2021). Other approaches have addressed whether and how given sentence pairs diverge. Carpuat et al. (2017) identify divergences in parallel corpora using a cross-lingual textual entailment system to identify less equivalent sentence pairs. Relatedly, work has been done to identify semantic divergences in parallel texts, classifying sentences as being divergent or non-divergent (Vyas et al., 2018). Prior work which both categorically annotated and modeled semantic divergences includes the REFreSD dataset of English-French sentence pairs annotated wit"
2021.law-1.6,W19-4028,0,0.0229415,"sem divergence. 57 between two AMRs (Cai and Knight, 2013). The root of the AMR is the focus, an attribute marked with :argN is a core argument role (where N is some number ≥0), and any other attribute (e.g., :opN, :domain, :manner) is a non-core role. 2.3 Structural Divergence Different Focus Use of AMR in Many Languages and Cross-linguistically Added/Omitted Non-Core Role Abstract Meaning Representation is not an interlingua, but has since been studied in multiple languages and cross-linguistically (Xue et al., 2014; Li et al., 2016; Migueles-Abraira, 2017; Linh and Nguyen, 2019; Sobrevilla Cabezudo and Pardo, 2019; Choe et al., 2019). AMR annotations of The Little Prince have been presented in English, Chinese (Li et al., 2016), Spanish (Migueles-Abraira, 2017), and Vietnamese (Linh and Nguyen, 2019). Additional work has been done to adapt AMR annotation (Sobrevilla Cabezudo and Pardo, 2019) and alignment (Anchiêta and Pardo, 2020) to Portuguese, and Korean (Choe et al., 2019). Cross-lingual studies of AMR primarily compare the structures between two AMRs representative of a parallel text, which may partially be due to the fact that AMR is not designed to be interlingual (Xue et al., 2014). AMRICA visu"
2021.law-1.6,Q19-1002,0,0.129061,"tructure of an AMR graph, whether it be the label or role) between multilingual AMR pairs serve as a reflection of semantic differences between a sentence and its translation. We develop and present a categorization schema to identify both the type and the cause of the divergence as being due to semantic divergence, annotation divergence, or syntactic divergence; this schema annotates both the type and cause of a structural divergence, to (1) make the data more adaptable to cross-lingual NLP applications, (2) identify non-literal translations, (3) make AMR more crosslinguistically consistent (Song et al., 2019), and (4) investigate the ways in which annotation, semantics, and syntax play a role in cross-lingual AMR parsing (Damonte, 2019). We then annotate a set of 50 parallel EnglishSpanish AMRs annotations from The Little Prince (Migueles-Abraira, 2017) using our divergence schema and make these annotations available online. Using this small set of gold annotated data, we are able to explore the comprehensiveness and meaningfulness of this annotation schema. Our contributions include: • A novel annotation schema for the classification of semantic and translation divergences between cross-lingual p"
2021.law-1.6,W16-1702,0,0.0179113,"thub.com/shirawein/spanish-english-amr-corpus Figure 1: Example of a focus,sem divergence. 57 between two AMRs (Cai and Knight, 2013). The root of the AMR is the focus, an attribute marked with :argN is a core argument role (where N is some number ≥0), and any other attribute (e.g., :opN, :domain, :manner) is a non-core role. 2.3 Structural Divergence Different Focus Use of AMR in Many Languages and Cross-linguistically Added/Omitted Non-Core Role Abstract Meaning Representation is not an interlingua, but has since been studied in multiple languages and cross-linguistically (Xue et al., 2014; Li et al., 2016; Migueles-Abraira, 2017; Linh and Nguyen, 2019; Sobrevilla Cabezudo and Pardo, 2019; Choe et al., 2019). AMR annotations of The Little Prince have been presented in English, Chinese (Li et al., 2016), Spanish (Migueles-Abraira, 2017), and Vietnamese (Linh and Nguyen, 2019). Additional work has been done to adapt AMR annotation (Sobrevilla Cabezudo and Pardo, 2019) and alignment (Anchiêta and Pardo, 2020) to Portuguese, and Korean (Choe et al., 2019). Cross-lingual studies of AMR primarily compare the structures between two AMRs representative of a parallel text, which may partially be due to"
2021.law-1.6,W19-3301,0,0.0539781,"Missing"
2021.law-1.6,L18-1267,0,0.0450605,"Missing"
2021.law-1.6,N18-1136,0,0.0501066,"Missing"
2021.law-1.6,xue-etal-2014-interlingua,0,0.0473574,"Missing"
2021.law-1.6,2020.coling-main.522,0,0.0123805,"r Computational Linguistics cross-lingual Spanish-English analysis of AMR pairs, as well as promote expansion of this work to other languages. 1 2 2020). Similarly, an attempt to classify these divergences automatically includes the hierarchical alignment scheme of Chinese and English parse trees which enables the identification and quantification of translation divergences (Deng and Xue, 2017). These divergences have also been considered for downstream tasks. Prior work includes fine-tuning approach to account for non-literal translations in the pre-training of cross-lingual language models (Zhai et al., 2020). Background and Related Work 2.1 Semantic Divergences Translation divergences occur when translation from one language to another results in a different meaning or structure (Dorr, 1994). These translation divergences can appear due to translation choices or to syntactic differences between the languages (Dorr, 1990; Dorr and Voss, 1993). The implications of these translation divergences include difficulties when using parallel texts for downstream tasks, because it can be difficult to identify why or how parallel sentences differ. For example, a parallel corpus, such as a work of fiction, li"
2021.law-1.6,W19-3320,0,0.0153105,"t al., 2014). AMRICA visualizes and automatically aligns AMRs, including two AMRs of a sentence and its translation, designed to facilitate research into cross-lingual AMRs (Saphra and Lopez, 2015). We take this as inspiration to use AMR pairs as a starting point for divergence classification between parallel texts. Cross-lingual approaches to AMR parsing explore transfer learning techniques to generate parallel AMR annotations in multiple languages, and suggest that AMR can serve as a cross-lingual semantic representation capable of overcoming linguistic differences (Damonte and Cohen, 2018; Zhu et al., 2019; Blloshmi et al., 2020). Additional work has explored whether structural differences across cross-lingual Chinese/English and English/Czech AMR pairs are due to syntactic idiosyncrasies (Xue et al., 2014), which can be of use in machine translation (Song et al., 2019; Nguyen et al., 2021). 3 Non-Core Role Differences Switch Arg and Non-Core Different NonCore Role Chosen Arg Differences Added/ Omitted Arg Different Arg Chosen Figure 2: Types of structural divergence. Cause of Divergence Semantic Divergence Annotation Divergence Syntactic Divergence Figure 3: Causes of structural divergence. di"
2021.mwe-1.6,S17-1022,1,0.905383,"Missing"
2021.mwe-1.6,W17-6901,0,0.02388,"“be”, it can take either the AUX or V lexcats. The POS and lemma constraints are only applied during evaluation; to avoid relying on gold POS/lemma annotations at test time we use an offthe-shelf system (Qi et al., 2018). We/O-PRON took/B-V.VPC.full-v.Motion our/o-PRON.POSS vehicle/o-N-n.ARTIFACT in/I_ for/O-P-p.Purpose a/O-DET repair/O-N-n.ACT to/O-P-p.Theme the/O-DET air/B-N-n.ARTIFACT conditioning/I_ Figure 2: Serialization as token-level tags for the example sentence from figure 1. 2.2 Modeling Related Frameworks The Universal Semantic Tagset takes a similar approach (Bjerva et al., 2016; Abzianidze and Bos, 2017; Abdou et al., 2018), and defines a crosslinguistic inventory of semantic classes for content and function words, which is designed as a substrate for compositional semantics, and does not have a trivial mapping to STREUSLE categories. However, two shared task datasets consist of subsets of the categories used for STREUSLE annotations, on text from different sources. PARSEME Verbal MWEs. The first such dataset is the English test set for the PARSEME 1.1 Shared Task (Ramisch et al., 2018), which covers several genres (including literature and several web genres) and is annotated only for verba"
2021.mwe-1.6,S16-1144,0,0.0253229,"Missing"
2021.mwe-1.6,E17-2026,0,0.0243443,"ambiguation of noun, verb, and preposition expressions; this task subsumes and unifies the previous PARSEME and DiMSUM evaluations. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel. NL is supported by an NSF Graduate Research Fellows"
2021.mwe-1.6,N19-1112,1,0.923816,"ompare to the results in Schneider et al. (2018), who performed MWE identification and supersense labeling for prepositions only. Note that Schneider et al. (2018) used version 4.0 of the STREUSLE corpus, which is slightly different from the version we use (some of the SNACS annotations have been revised). However, our baseline tagger, even with GloVe embeddings, outperforms Schneider et al. (2018) on that subset. Using BERT embeddings with constraints POS tags and lemmas improves performance substantially; on preposition supersense tagging, it even outperforms using gold POS tags and lemmas. Liu et al. (2019) also found that BERT embeddings improved SNACS labeling on STREUSLE 4.0, although they study a simplified setting (gold preposition identification, 8 A small fraction of sentences in the PARSEME test set (194/3965) are EWT reviews sentences that also appear in STREUSLE’s dev set. The rest of the PARSEME test set contains other web and non-web genres (Walsh et al., 2018), and thus it is mostly out-of-domain relative to STREUSLE. None of the PARSEME training set overlaps with STREUSLE. 52 I have a new born daughter and she helped me with a lot O-PRON O-V-v.stative O-DET O-ADJ I_ O-N-n.PERSON O-"
2021.mwe-1.6,C16-1333,0,0.0296049,"AUX and its lemma is “be”, it can take either the AUX or V lexcats. The POS and lemma constraints are only applied during evaluation; to avoid relying on gold POS/lemma annotations at test time we use an offthe-shelf system (Qi et al., 2018). We/O-PRON took/B-V.VPC.full-v.Motion our/o-PRON.POSS vehicle/o-N-n.ARTIFACT in/I_ for/O-P-p.Purpose a/O-DET repair/O-N-n.ACT to/O-P-p.Theme the/O-DET air/B-N-n.ARTIFACT conditioning/I_ Figure 2: Serialization as token-level tags for the example sentence from figure 1. 2.2 Modeling Related Frameworks The Universal Semantic Tagset takes a similar approach (Bjerva et al., 2016; Abzianidze and Bos, 2017; Abdou et al., 2018), and defines a crosslinguistic inventory of semantic classes for content and function words, which is designed as a substrate for compositional semantics, and does not have a trivial mapping to STREUSLE categories. However, two shared task datasets consist of subsets of the categories used for STREUSLE annotations, on text from different sources. PARSEME Verbal MWEs. The first such dataset is the English test set for the PARSEME 1.1 Shared Task (Ramisch et al., 2018), which covers several genres (including literature and several web genres) and i"
2021.mwe-1.6,W06-1670,0,0.207898,"the DiMSUM shared task on MWEs and noun/verb supersenses (Schneider et al., 2016)? Results show our LSR model based on STREUSLE is general enough to capture different types of analysis consistently, and suggest an integrated full-sentence tagging framework is valuable for explicit modeling of lexical semantics in NLP.1 Introduction 2 Many NLP tasks traditionally approached as tagging focus on lexical semantic behavior—they aim to identify and categorize lexical semantic units in running text using a general set of labels. Two examples are supersense tagging of nouns and verbs as formulated by Ciaramita and Altun (2006), and verbal multiword expression (MWE) identification and classification in the multilingual PARSEME shared tasks (Savary et al., 2017; Ramisch et al., 2018, 2020). By analogy with named entity recognition, we can use the term lexical semantic recognition (LSR) for such chunking-and-labeling tasks that apply to lexical meaning generally, not just entities. This disambiguation can serve as a foundational layer of analysis for downstream applications in natural language processing, and provides an initial level of organization for compiling lexical resources, such as semantic nets and thesauri."
2021.mwe-1.6,J17-4005,0,0.0251262,"Missing"
2021.mwe-1.6,W17-1706,0,0.0201885,"Missing"
2021.mwe-1.6,N19-1423,0,0.0334889,"that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area. In this paper, we tackle a more inclusive LSR task of lexical semantic segmentation and disambiguation. The STREUSLE corpus (see §2) contains comprehensive annotations of MWEs (along with their holistic syntactic status) and noun, verb, and preposition/possessive supersenses. We train a neural CRF tagger (Lafferty et al., 2001) using BERT embeddings (Devlin et al., 2019) and find that it obtains strong results as a first baseline for this task in its full form. In addition, we ask: Does a tagger trained on STREUSLE generalize to evaluations like the PARSEME shared task on verbal MWEs (Ramisch et al., 2018) and the DiMSUM shared task on MWEs and noun/verb supersenses (Schneider et al., 2016)? Results show our LSR model based on STREUSLE is general enough to capture different types of analysis consistently, and suggest an integrated full-sentence tagging framework is valuable for explicit modeling of lexical semantics in NLP.1 Introduction 2 Many NLP tasks trad"
2021.mwe-1.6,2020.lrec-1.497,0,0.0510942,"Missing"
2021.mwe-1.6,W09-2903,0,0.0309119,"ed by the STREUSLE corpus, which involves joint MWE identification and coarse-grained (supersense) disambiguation of noun, verb, and preposition expressions; this task subsumes and unifies the previous PARSEME and DiMSUM evaluations. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Bin"
2021.mwe-1.6,D14-1162,0,0.0865821,"yle annotations can be evaluated for verbal MWE identification and subtyping by simply discarding the supersenses and the non-verbal MWEs and lexcats from the output. 3.1 Experiments We train the tagger on version 4.3 of the English STREUSLE corpus and evaluate on the STREUSLE, English PARSEME, and DiMSUM test sets (§2). The latter two are (zero-shot) out-ofdomain test sets; the tagger is not retrained on the associated shared task training data. We also compare to a model with static word representations by replacing BERT with the concatenation of 300-dimensional pretrained GloVe embeddings (Pennington et al., 2014) and the output of a character-level convolutional neural netDiMSUM. The second shared task dataset is DiMSUM (Schneider et al., 2016), which was annotated in three genres—TrustPilot web reviews, TED talk transcripts, and tweets—echoing the annotation style of STREUSLE when it contained only MWEs and noun and verb supersenses. DiMSUM does not contain prepositional/possessive supersenses or lexcats. It also lacks weak MWEs. should apply; whereas the correct lexcat for a single-word verb is plain V. In practice this is not a problem. 7 For prepositions and possessives, the supersense is either a"
2021.mwe-1.6,W18-6008,0,0.0127232,"due to the verbal lexcats, which distinguish between single-word and strong multiword expressions (see Appendix A): if a B-* or I~-* tag is followed by a gap, there is no local indication of whether the expression will be strong or weak (strength is indicated only after the gap). If the expression being started is strong, then one of the verbal MWE subtypes (V.VID, etc.) 3 The gap in a discontinuous MWE may contain singleword and/or other multiword expressions, provided that those embedded MWEs do not themselves contain gaps. 4 This is also done in other resources (e.g., Shigeto et al., 2013; Gerdes et al., 2018). 50 3 beginning of an MWE whose lexcat is N and supersense is N . ARTIFACT. I_ and i_ tags never contain lexcat or supersense information as they continue a lexical unit, whereas O, B, I~, o, b, and i~ always do. Figure 2 illustrates the full tagging. All told, STREUSLE has 601 complete tags. We develop and evaluate a strong neural sequence tagger on the full task of lexical semantic recognition with MWEs and noun/verb/preposition/possessive supersenses to assess the performance of modern techniques on the full joint tagging task. Our tagger feeds pre-trained BERT representations (Devlin et a"
2021.mwe-1.6,picca-etal-2008-supersense,0,0.102901,"Missing"
2021.mwe-1.6,K18-2016,0,0.0570454,"Missing"
2021.mwe-1.6,C16-1256,0,0.0164816,"the third sentence, the model constrained only by POS and lemma is inclined toward the literal meaning of “rip”, whereas the MWE-constrained model recovers the gappy verbparticle construction “rip off”. Naturally, in other sentences, the MWE-constrained model sometimes suffers from false positive or false negative MWEs, but always produces a coherent segmentation. 4 MWEs and noun and verb supersenses with featurebased sequence models. Richardson (2017) trained such a model on STREUSLE 3.0 as a noun, verb, and preposition supersense tagger (without modeling MWEs). For preposition supersenses, Gonen and Goldberg (2016) incorporated multilingual cues; Schneider et al. (2018) experimented with feature-based and neural classifiers; and Liu et al. (2019), modeling supersense disambiguation of single-word prepositions only, found pretretrained contextual embeddings to be much more effective even with simple linear probing models. 5 Conclusion We study the lexical semantic recognition task defined by the STREUSLE corpus, which involves joint MWE identification and coarse-grained (supersense) disambiguation of noun, verb, and preposition expressions; this task subsumes and unifies the previous PARSEME and DiMSUM e"
2021.mwe-1.6,K15-1009,1,0.80019,"tion and coarse-grained (supersense) disambiguation of noun, verb, and preposition expressions; this task subsumes and unifies the previous PARSEME and DiMSUM evaluations. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel. NL"
2021.mwe-1.6,Q14-1016,1,0.921676,"tion is available. 2.1 1 STREUSLE Annotation Layers STREUSLE comprises the entire 55K-word Reviews section of the English Web Treebank (Bies et al., 2012), for which there are gold Universal Dependencies (UD; Nivre et al., 2020) graphs, and adopts the same train/dev/test split. The lexical-level annotations do not make use of the UD parse directly, but there are constraints on compatibility between lexical categories and UPOS tags (see §3). Multiword expressions (MWEs; Baldwin and Kim, 2010) are expressed as groupings of two or more tokens into idiomatic or collocational units. As detailed by Schneider et al. (2014a,b), these units may be contiguous or gappy (discontinuous).3 Each unit is marked with a binary strength value: idiomatic/noncompositional expressions are strong; collocations that are nevertheless semantically compositional, like “highly recommended”, are weak. We use the term lexical unit for any expression that is either a strong MWE grouping of multiple tokens, or a token that does not belong to a strong MWE. Every token in the sentence thus belongs to exactly one lexical unit. The other layers of semantic annotation augment lexical units, and weak MWEs are groupings of (entire) lexical u"
2021.mwe-1.6,P18-1018,1,0.835007,"ogy with named entity recognition, we can use the term lexical semantic recognition (LSR) for such chunking-and-labeling tasks that apply to lexical meaning generally, not just entities. This disambiguation can serve as a foundational layer of analysis for downstream applications in natural language processing, and provides an initial level of organization for compiling lexical resources, such as semantic nets and thesauri. LSR Tagging Frameworks Our tagger is based on STREUSLE (SupersenseTagged Repository of English with a Unified Semantics for Lexical Expressions; Schneider and Smith, 2015; Schneider et al., 2018),2 a corpus of web reviews annotated comprehensively for lexical semantic units and supersense labels. Specifically, there are three annotation layers: multiword expressions, lexical categories, and supersenses. The supersenses apply to noun, verb, and prepositional/possessive units. Figure 1 shows an example. Many of the component annotations have been applied to other languages: verbal multiword expressions (Savary et al., 2017; Ramisch et al., 2018), noun and verb supersenses (e.g., Picca et al., 1 Code, pretrained models, and model and scorer output (all train/dev/test splits) can be found"
2021.mwe-1.6,N13-1076,1,0.78466,"usle 49 Proceedings of the 17th Workshop on Multiword Expressions, pages 49–56 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics STREUSLE SS v.Motion p.Possessor n.Artifact LexCat PRON V.VPC.full PRON.POSS N MWE We took our p.Purpose P vehicle in for DET a n.Act N p.Theme P n.Artifact DET repair to the N air conditioning Figure 1: Example annotated sentence from the STREUSLE training set. The (strong) multiword expressions “took. . . in” and “air conditioning” each receive a single lexcat and supersense. UD syntax is not shown. 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and adposition supersenses (Hwang et al., 2017; Zhu et al., 2019). In this paper we focus on English, where comprehensive annotation is available. 2.1 1 STREUSLE Annotation Layers STREUSLE comprises the entire 55K-word Reviews section of the English Web Treebank (Bies et al., 2012), for which there are gold Universal Dependencies (UD; Nivre et al., 2020) graphs, and adopts the same train/dev/test split. The lexical-level annotations do not make use of the UD parse directly, but there are constraints on compatibility between lexical categories and"
2021.mwe-1.6,schneider-etal-2014-comprehensive,1,0.855525,"tion is available. 2.1 1 STREUSLE Annotation Layers STREUSLE comprises the entire 55K-word Reviews section of the English Web Treebank (Bies et al., 2012), for which there are gold Universal Dependencies (UD; Nivre et al., 2020) graphs, and adopts the same train/dev/test split. The lexical-level annotations do not make use of the UD parse directly, but there are constraints on compatibility between lexical categories and UPOS tags (see §3). Multiword expressions (MWEs; Baldwin and Kim, 2010) are expressed as groupings of two or more tokens into idiomatic or collocational units. As detailed by Schneider et al. (2014a,b), these units may be contiguous or gappy (discontinuous).3 Each unit is marked with a binary strength value: idiomatic/noncompositional expressions are strong; collocations that are nevertheless semantically compositional, like “highly recommended”, are weak. We use the term lexical unit for any expression that is either a strong MWE grouping of multiple tokens, or a token that does not belong to a strong MWE. Every token in the sentence thus belongs to exactly one lexical unit. The other layers of semantic annotation augment lexical units, and weak MWEs are groupings of (entire) lexical u"
2021.mwe-1.6,N15-1177,1,0.964791,"t al., 2018, 2020). By analogy with named entity recognition, we can use the term lexical semantic recognition (LSR) for such chunking-and-labeling tasks that apply to lexical meaning generally, not just entities. This disambiguation can serve as a foundational layer of analysis for downstream applications in natural language processing, and provides an initial level of organization for compiling lexical resources, such as semantic nets and thesauri. LSR Tagging Frameworks Our tagger is based on STREUSLE (SupersenseTagged Repository of English with a Unified Semantics for Lexical Expressions; Schneider and Smith, 2015; Schneider et al., 2018),2 a corpus of web reviews annotated comprehensively for lexical semantic units and supersense labels. Specifically, there are three annotation layers: multiword expressions, lexical categories, and supersenses. The supersenses apply to noun, verb, and prepositional/possessive units. Figure 1 shows an example. Many of the component annotations have been applied to other languages: verbal multiword expressions (Savary et al., 2017; Ramisch et al., 2018), noun and verb supersenses (e.g., Picca et al., 1 Code, pretrained models, and model and scorer output (all train/dev/"
2021.mwe-1.6,W97-0811,0,0.377391,"nifies the previous PARSEME and DiMSUM evaluations. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel. NL is supported by an NSF Graduate Research Fellowship under grant number DGE-1656518. 53 References Oliver Hellwig. 2017. Coa"
2021.mwe-1.6,W13-1021,0,0.0106521,"y a slight limitation due to the verbal lexcats, which distinguish between single-word and strong multiword expressions (see Appendix A): if a B-* or I~-* tag is followed by a gap, there is no local indication of whether the expression will be strong or weak (strength is indicated only after the gap). If the expression being started is strong, then one of the verbal MWE subtypes (V.VID, etc.) 3 The gap in a discontinuous MWE may contain singleword and/or other multiword expressions, provided that those embedded MWEs do not themselves contain gaps. 4 This is also done in other resources (e.g., Shigeto et al., 2013; Gerdes et al., 2018). 50 3 beginning of an MWE whose lexcat is N and supersense is N . ARTIFACT. I_ and i_ tags never contain lexcat or supersense information as they continue a lexical unit, whereas O, B, I~, o, b, and i~ always do. Figure 2 illustrates the full tagging. All told, STREUSLE has 601 complete tags. We develop and evaluate a strong neural sequence tagger on the full task of lexical semantic recognition with MWEs and noun/verb/preposition/possessive supersenses to assess the performance of modern techniques on the full joint tagging task. Our tagger feeds pre-trained BERT repres"
2021.mwe-1.6,Q19-1027,0,0.018382,"and preposition expressions; this task subsumes and unifies the previous PARSEME and DiMSUM evaluations. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel. NL is supported by an NSF Graduate Research Fellowship under grant number DGE"
2021.mwe-1.6,W19-5119,0,0.0440212,"Missing"
2021.mwe-1.6,R11-1040,0,0.0421345,"s. We develop a strong baseline neural sequence model, and see encouraging results on the task. Furthermore, zero-shot out-of-domain evaluation of our baselines on partial versions of the task yields scores comparable to the fully-supervised in-domain state of the art. Related Work Acknowledgments The computational study of MWEs has a long history (Sag et al., 2002; Diab and Bhutada, 2009; Baldwin and Kim, 2010; Ramisch, 2015; Qu et al., 2015; Constant et al., 2017; Bingel and Søgaard, 2017; Shwartz and Dagan, 2019), as does supersense tagging (Segond et al., 1997; Ciaramita and Altun, 2006). Vincze et al. (2011) developed a sequence tagger for both MWEs and named entities in English. Schneider and Smith (2015); Schneider et al. (2016) featured joint tagging of We are grateful to anonymous reviewers as well as members of the NERT lab for their feedback on this work. This research was supported in part by NSF award IIS-1812778 and grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel. NL is supported by an NSF Graduate Research Fellowship under grant number DGE-1656518. 53 References Oliver Hellwig. 2017. Coarse semantic classification of rare nouns using cr"
2021.mwe-1.6,W18-4921,1,0.849527,"rforms Schneider et al. (2018) on that subset. Using BERT embeddings with constraints POS tags and lemmas improves performance substantially; on preposition supersense tagging, it even outperforms using gold POS tags and lemmas. Liu et al. (2019) also found that BERT embeddings improved SNACS labeling on STREUSLE 4.0, although they study a simplified setting (gold preposition identification, 8 A small fraction of sentences in the PARSEME test set (194/3965) are EWT reviews sentences that also appear in STREUSLE’s dev set. The rest of the PARSEME test set contains other web and non-web genres (Walsh et al., 2018), and thus it is mostly out-of-domain relative to STREUSLE. None of the PARSEME training set overlaps with STREUSLE. 52 I have a new born daughter and she helped me with a lot O-PRON O-V-v.stative O-DET O-ADJ I_ O-N-n.PERSON O-CCONJ O-PRON O-V-v.social o-PRON O-P-p.Theme B-DET I_ O-PRON O-V-v.stative O-DET B-ADJ I_ O-N-n.PERSON O-CCONJ O-PRON O-V-v.social O-PRON O-P-p.Theme B-DET I_ Go down 1 block B-V.VPC.semi-v.motion I_ O-NUM O-N-n.COGNITION O-V-v.motion O-P-p.Direction O-NUM O-N-n.LOCATION O-V-v.motion O-P-p.Direction O-NUM O-N-n.RELATION to Super 8 . O-P-p.Goal B-N-n.LOCATION O-NUM O-PUNC"
2021.scil-1.34,P10-1022,0,0.0423098,"n space before adding it to the word’s contextualized encoding. Data. A limitation of standard CCG evaluation datasets is that they contain very few tokens of categories seen less than 10 times in training. Thus, scores computed over these small samples may not reliably estimate the models’ generalization capacity. To correct for this, we investigate what happens if the models are trained on sentences containing exclusively the higher-frequency (≥10) categories, and evaluated only on sentences with at least one rare category. We split the (English) CCG Rebank training set (WSJ sections 02–21; Honnibal et al., 2010) in this way. Baselines. We compare our TreeRNN and AddrMLP models to the following baselines: 1) Nonconstructive (MLP): We compute the output probabilities for complete categories directly from the encoder’s hidden state. 2) Sequential: Kogkalidis et al. (2019) construct type-logical supertags by generating for each sentence a single sequence of atomic types and functors. We adapt their implementation of the sequence-to-sequence Transformer model (Vaswani et al., 2017) to our problem (“K+19”). We also implement a simplified version of Bhargava and Penn’s (2020) tagger, where each word’s super"
2021.scil-1.34,W19-4314,0,0.0978195,"ble supertags; in practice, they follow a power law distribution. CCG treebanks contain numerous rare supertags, including several that occur only in the test sets. Still others can be expected to occur in a much larger corpus. This long tail of the distribution is particularly challenging for taggers, due to its sparseness and relatively high complexity of categories. In most previous work, CCG supertaggers have skirted this problem by treating categories as a fixed set of opaque labels (fig. 1b) and ignoring those occurring fewer than a certain threshold (following Clark, 2002). Conversely, Kogkalidis et al. (2019) and Bhargava and Penn (2020) have recently proposed different methods of constructive supertagging, where supertags are constructed as sequences ... wk-1 wk wk+1 ... ... wk-1 Encoder wk wk+1 ... Encoder depth ult res 1 arg (b)  1 lt u es r NP ar g 10 S 2 lt u es r NP 11 ar g ∅ ∅ 110 111 ... wk-1 101 100 3 (SNP)/NP / 0 wk+1 ... Encoder ∅ ∅ ∅ ∅ 1000 1001 1010 1011 (a) wk ... /  S NP NP ... (c) Figure 1: Schematic of our tree-structured supertagger (left) in contrast with unstructured (top right) and sequential (bottom right) models. of minimal pieces and there is no constraint that predicted"
2021.scil-1.56,W10-3216,0,0.0405385,"lso performing the action unaided (AGENT). SNACS currently cannot resolve the conflict between these two equally valid functions; we currently label (2b) as O RIGINATOR↝R ECIPIENT in 456 door [with a (4) us-ne m¯alik=se darv¯az¯a 3 SG.ERG owner.OBL=INS door.NOM khulv¯ay¯a open.CAUS.PRF ‘She made [the landlord]:? open the door.’ Much like an obligated agent, the impelled agent takes part in two events, exhibiting properties of both AGENT and T HEME. Furthermore, an impelled agent can control I NSTRUMENTs of its own, and there cannot be two participants in the scene with the same semantic role (Begum and Sharma, 2010). For SNACS, Shalev et al. (2019) mentioned similar issues in English. This construction was rare in our corpus, but we find the best solution for this is a new label for animate and ambiguously volitional counterparts to I NSTRUMENT in the SNACS hierarchy, much like the distinction between inanimate C AUSER and animate AGENT. Emphatic particles Following work on SNACS for Korean, which created a new label F OCUS for “postpositions that indicate the focus of a sentence (FOC), contributing information such as contrastiveness, likelihood, or value judgements” (Hwang et al., 2020), we found that"
2021.scil-1.56,W09-3036,0,0.0604896,"ific case markers and adpositions for particular semantic roles is idiosyncratic to every language. Hindi–Urdu has a case-marking system along with a large postposition inventory. Idiosyncratic bundling of case and adpositional relations poses problems in many natural language processing tasks for Hindi, such as machine translation (Ratnam et al. 2018, Jha 2017, Ramanathan et al. 2009, Rao et al. 1998) and semantic role labelling (Pal and Sharma 2019, Gupta 2019). Many models for these tasks rely on human-annotated corpora as training data, such as the one created for the Hindi–Urdu PropBank (Bhatt et al., 2009), and by Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a linguistic perspective, in comparing and categorizing the encoding of such relations across languages. There is a lack of corpora in South Asian languages for such tasks. Even Hindi, despite being a resource-rich language, is limited in available labelled data (Joshi et al., 2020). This extended abstract reports on in-progress annotation of case markers and adpositions in a Hindi corpus, employing the cross-lingual SNACS scheme (Semantic Network of Adposition and Case Supersenses; Schneide"
2021.scil-1.56,S17-1022,1,0.879869,"Missing"
2021.scil-1.56,2020.dmr-1.6,1,0.787277,"nherent lexical meaning (function) and another label for the predicate-licensed semantic relation (scene role) of a token. This is expressed as S CENE ROLE↝F UNCTION, and is useful for disambiguating the use of different encodings of the same semantic relation such as “she is looking at me” (S TIMULUS↝D IRECTION) and “he is listening to me” (S TIMULUS↝G OAL). When the scene role and function are identical, a single label is given. SNACS, thus far, has been used to annotate the English STREUSLE corpus (Schneider and Smith, 2015), The Little Prince in English and translations of it into Korean (Hwang et al., 2020), Mandarin (Peng et al., 2020; Zhu et al., 2018) and German, with ongoing annotation efforts in Finnish, Latin, and Gujarati and past work on French and Hebrew. There has also been annotation of L2 English (Kranzlein et al., 2020). This effort is accompanied by the release of language-specific guidelines (based on Schneider et al., 2020) that aid in annotator training. 4 Applying SNACS to Hindi–Urdu Several linguistic features of Hindi–Urdu adposition and case semantics posed difficulties in annotating. Some are examined below. The annotation process itself relied on grammatical analyses of Hi"
2021.scil-1.56,P09-1090,0,0.0569284,"uments they apply to and the action of a verb. Adpositions (prepositions, postpositions, and circumpositions) further express a range of semantic relations, including space, time, possession, properties, and comparison. The use of specific case markers and adpositions for particular semantic roles is idiosyncratic to every language. Hindi–Urdu has a case-marking system along with a large postposition inventory. Idiosyncratic bundling of case and adpositional relations poses problems in many natural language processing tasks for Hindi, such as machine translation (Ratnam et al. 2018, Jha 2017, Ramanathan et al. 2009, Rao et al. 1998) and semantic role labelling (Pal and Sharma 2019, Gupta 2019). Many models for these tasks rely on human-annotated corpora as training data, such as the one created for the Hindi–Urdu PropBank (Bhatt et al., 2009), and by Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a linguistic perspective, in comparing and categorizing the encoding of such relations across languages. There is a lack of corpora in South Asian languages for such tasks. Even Hindi, despite being a resource-rich language, is limited in available labelled data ("
2021.scil-1.56,2020.acl-main.560,0,0.0193831,", Rao et al. 1998) and semantic role labelling (Pal and Sharma 2019, Gupta 2019). Many models for these tasks rely on human-annotated corpora as training data, such as the one created for the Hindi–Urdu PropBank (Bhatt et al., 2009), and by Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a linguistic perspective, in comparing and categorizing the encoding of such relations across languages. There is a lack of corpora in South Asian languages for such tasks. Even Hindi, despite being a resource-rich language, is limited in available labelled data (Joshi et al., 2020). This extended abstract reports on in-progress annotation of case markers and adpositions in a Hindi corpus, employing the cross-lingual SNACS scheme (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018, 2020). The guidelines we are developing also apply to Urdu, since the grammatical base of Hindi and Urdu is largely the same. 2 Corpus The corpus was the entirety of the The Little Prince.1 Annotation was done by two highly proficient Hindi speakers (one native), and guidelines were developed simultaneously. Table 1 contains statistics about the corpus, and table 2 giv"
2021.scil-1.56,P18-1018,1,0.92311,"., 2009), and by Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a linguistic perspective, in comparing and categorizing the encoding of such relations across languages. There is a lack of corpora in South Asian languages for such tasks. Even Hindi, despite being a resource-rich language, is limited in available labelled data (Joshi et al., 2020). This extended abstract reports on in-progress annotation of case markers and adpositions in a Hindi corpus, employing the cross-lingual SNACS scheme (Semantic Network of Adposition and Case Supersenses; Schneider et al., 2018, 2020). The guidelines we are developing also apply to Urdu, since the grammatical base of Hindi and Urdu is largely the same. 2 Corpus The corpus was the entirety of the The Little Prince.1 Annotation was done by two highly proficient Hindi speakers (one native), and guidelines were developed simultaneously. Table 1 contains statistics about the corpus, and table 2 gives proportions for each label and target. Adjudication of annotator disagreements is ongoing and is expected to be completed by February 2021. Annotation targets Following Masica’s (1993) analysis of Indo-Aryan languages, we an"
2021.scil-1.56,2020.law-1.10,1,0.732997,"ings of the same semantic relation such as “she is looking at me” (S TIMULUS↝D IRECTION) and “he is listening to me” (S TIMULUS↝G OAL). When the scene role and function are identical, a single label is given. SNACS, thus far, has been used to annotate the English STREUSLE corpus (Schneider and Smith, 2015), The Little Prince in English and translations of it into Korean (Hwang et al., 2020), Mandarin (Peng et al., 2020; Zhu et al., 2018) and German, with ongoing annotation efforts in Finnish, Latin, and Gujarati and past work on French and Hebrew. There has also been annotation of L2 English (Kranzlein et al., 2020). This effort is accompanied by the release of language-specific guidelines (based on Schneider et al., 2020) that aid in annotator training. 4 Applying SNACS to Hindi–Urdu Several linguistic features of Hindi–Urdu adposition and case semantics posed difficulties in annotating. Some are examined below. The annotation process itself relied on grammatical analyses of Hindi such as Koul (2008), dictionaries (McGregor, 1993; Dasa, 1965–1975), and native speaker judgements. Functions for case markers Case markers encode little lexical content relative to adpositions. Table 2 shows the dominance of"
2021.scil-1.56,N15-1177,1,0.810178,"ss, given the productivity of the oblique genitive ke as a postposition former. 455 role or inherent lexical meaning (function) and another label for the predicate-licensed semantic relation (scene role) of a token. This is expressed as S CENE ROLE↝F UNCTION, and is useful for disambiguating the use of different encodings of the same semantic relation such as “she is looking at me” (S TIMULUS↝D IRECTION) and “he is listening to me” (S TIMULUS↝G OAL). When the scene role and function are identical, a single label is given. SNACS, thus far, has been used to annotate the English STREUSLE corpus (Schneider and Smith, 2015), The Little Prince in English and translations of it into Korean (Hwang et al., 2020), Mandarin (Peng et al., 2020; Zhu et al., 2018) and German, with ongoing annotation efforts in Finnish, Latin, and Gujarati and past work on French and Hebrew. There has also been annotation of L2 English (Kranzlein et al., 2020). This effort is accompanied by the release of language-specific guidelines (based on Schneider et al., 2020) that aid in annotator training. 4 Applying SNACS to Hindi–Urdu Several linguistic features of Hindi–Urdu adposition and case semantics posed difficulties in annotating. Some"
2021.scil-1.56,W19-3316,1,0.847837,"GENT). SNACS currently cannot resolve the conflict between these two equally valid functions; we currently label (2b) as O RIGINATOR↝R ECIPIENT in 456 door [with a (4) us-ne m¯alik=se darv¯az¯a 3 SG.ERG owner.OBL=INS door.NOM khulv¯ay¯a open.CAUS.PRF ‘She made [the landlord]:? open the door.’ Much like an obligated agent, the impelled agent takes part in two events, exhibiting properties of both AGENT and T HEME. Furthermore, an impelled agent can control I NSTRUMENTs of its own, and there cannot be two participants in the scene with the same semantic role (Begum and Sharma, 2010). For SNACS, Shalev et al. (2019) mentioned similar issues in English. This construction was rare in our corpus, but we find the best solution for this is a new label for animate and ambiguously volitional counterparts to I NSTRUMENT in the SNACS hierarchy, much like the distinction between inanimate C AUSER and animate AGENT. Emphatic particles Following work on SNACS for Korean, which created a new label F OCUS for “postpositions that indicate the focus of a sentence (FOC), contributing information such as contrastiveness, likelihood, or value judgements” (Hwang et al., 2020), we found that the Hindi emphatic particles h¯ı"
2021.scil-1.56,W19-4020,0,0.0234929,"ns, postpositions, and circumpositions) further express a range of semantic relations, including space, time, possession, properties, and comparison. The use of specific case markers and adpositions for particular semantic roles is idiosyncratic to every language. Hindi–Urdu has a case-marking system along with a large postposition inventory. Idiosyncratic bundling of case and adpositional relations poses problems in many natural language processing tasks for Hindi, such as machine translation (Ratnam et al. 2018, Jha 2017, Ramanathan et al. 2009, Rao et al. 1998) and semantic role labelling (Pal and Sharma 2019, Gupta 2019). Many models for these tasks rely on human-annotated corpora as training data, such as the one created for the Hindi–Urdu PropBank (Bhatt et al., 2009), and by Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a linguistic perspective, in comparing and categorizing the encoding of such relations across languages. There is a lack of corpora in South Asian languages for such tasks. Even Hindi, despite being a resource-rich language, is limited in available labelled data (Joshi et al., 2020). This extended abstract reports on in-progress"
2021.scil-1.56,2020.lrec-1.733,1,0.845959,"on) and another label for the predicate-licensed semantic relation (scene role) of a token. This is expressed as S CENE ROLE↝F UNCTION, and is useful for disambiguating the use of different encodings of the same semantic relation such as “she is looking at me” (S TIMULUS↝D IRECTION) and “he is listening to me” (S TIMULUS↝G OAL). When the scene role and function are identical, a single label is given. SNACS, thus far, has been used to annotate the English STREUSLE corpus (Schneider and Smith, 2015), The Little Prince in English and translations of it into Korean (Hwang et al., 2020), Mandarin (Peng et al., 2020; Zhu et al., 2018) and German, with ongoing annotation efforts in Finnish, Latin, and Gujarati and past work on French and Hebrew. There has also been annotation of L2 English (Kranzlein et al., 2020). This effort is accompanied by the release of language-specific guidelines (based on Schneider et al., 2020) that aid in annotator training. 4 Applying SNACS to Hindi–Urdu Several linguistic features of Hindi–Urdu adposition and case semantics posed difficulties in annotating. Some are examined below. The annotation process itself relied on grammatical analyses of Hindi such as Koul (2008), dict"
2021.scil-1.58,W06-2106,0,0.175049,"Missing"
2021.scil-1.58,P18-1018,1,0.753519,"lection (Pilot Part 2) I would appreciate reviews from anyone who has worked with me before &gt;&gt; in << the mental health setting . Luke Gessler Shira Wein Nathan Schneider Georgetown University {lg876, sw1158, nathan.schneider}@georgetown.edu at around with within during through throughout for [Omit] Other: Introduction1 Prepositions are highly ambiguous function words which can express a wide variety of relationships (Litkowski and Hargraves, 2006; Tratz, 2011). Supersenses have been proposed as an analytic framework for studying their lexical semantics, but extant gold-annotated corpora (e.g. Schneider et al., 2018) are small because preposition supersense annotation is a complex annotation task that requires much training and time. Here, we present two proxy task designs for crowdsourcing from which supersense labels can be sensed – that is, recovered indirectly. These designs involve in-context substitution and similarity judgments. Based on four in-house pilot experiments, we conclude that both designs are promising methods for building a large preposition supersense–annotated corpus, and that they differ in difficulty for the annotators and for the researchers. Prepositional Supersenses Prepositions"
2021.teachingnlp-1.11,W08-0201,0,0.061637,"have strong programming skills, such as Computer Science graduate students, can begin their NLP journey in this same ENLP course, which has projects emphasizing collaboration between students of different backgrounds; 4 as discussed in Fosler-Lussier (2008), cross-disciplinary collaborations are helpful to establish respect between students from different fields and mitigate the challenges of disparate backgrounds. Many other NLP courses, such as those focusing on Dialogue Systems and Machine Translation, are also crosslisted between the Linguistics and CS departments, which, as noted in e.g. Baldridge and Erk (2008), helps these courses reach a wider audience. Teaching NLP concepts alongside basic programming skills has been a significant challenge. INLP requires no prior programming experience, but students who enter the course with none sometimes struggle to grasp programming concepts at the speed they are taught, and many students rely on significant support from teaching assistants to successfully complete the course’s programming assignments. Our experience has taught us that frequent contact and check-ins initiated by teaching assistants are very important for catching students who may fall behind"
2021.teachingnlp-1.11,W08-0205,0,0.0408726,"these various backgrounds. Linguistics students with little or no prior programming experience are introduced to basic Python and NLP foundations in an Introduction to NLP (INLP) course; 3 they can then further develop their programming skills with Computational Linguistics with Advanced Python before taking Empirical Methods in NLP (ENLP). Students who already have strong programming skills, such as Computer Science graduate students, can begin their NLP journey in this same ENLP course, which has projects emphasizing collaboration between students of different backgrounds; 4 as discussed in Fosler-Lussier (2008), cross-disciplinary collaborations are helpful to establish respect between students from different fields and mitigate the challenges of disparate backgrounds. Many other NLP courses, such as those focusing on Dialogue Systems and Machine Translation, are also crosslisted between the Linguistics and CS departments, which, as noted in e.g. Baldridge and Erk (2008), helps these courses reach a wider audience. Teaching NLP concepts alongside basic programming skills has been a significant challenge. INLP requires no prior programming experience, but students who enter the course with none somet"
2021.teachingnlp-1.11,J14-4007,0,0.0132726,"a POS tagging in-class exercise in ENLP, and annotation in three different semantic representations in Meaning Representations. Others include error analysis of NLP systems, such as a comparison of the output from statistical and neural translation systems in Machine Translation. The Discourse Modeling course teaches discourse parsing frameworks and algorithms, including introducing students to topics in annotating Rhetorical Structure Theory (Mann and Thompson, 1988), Segmented Discourse Representation Theory (SDRT, Asher and Lascarides 2003) and the Penn Discourse Treebank framework (PDTB, Prasad et al. 2014). Balancing Skills Taught Along with coming from different academic backgrounds, we acknowledge that students studying NLP have a variety of goals: for example, they may wish to pursue NLP in academia or industry, or they may be interested in using computational methods for linguistics, or other Digital Humanities or Social Science fields. To support these varying goals, we endeavor to teach a balance of different skills and perspectives on NLP. While some courses emphasize algorithms, others focus more on computational representations of language, on creating and using resources such as corpo"
C14-1100,bhatia-etal-2014-unified,1,0.727454,"es or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in Chinese (a language without articles), where the existential construction can be used to express indefinite subjects and the ba- construction can be used to express definite direct objects (Chen, 2004). Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites expressing these functions. We refer to these as communicative functions of definiteness, following Bhatia et al. (2014). Croft (2003, pp. 6–7) shows that even when two languages have access to the same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French translations (both languages use definite as well as indefinite articles) such as: (1) He showed extreme care. (unmarked) Il montra un soin extrême. (indef.) (2) I love artichokes and asparagus. (unmarked) J’aime les artichauts et les asperges. (def.) (3) His brother became a soldier. (indef.) Son frère est devenu"
C14-1100,D13-1174,1,0.849382,"ness. After preprocessing the text with a dependency parser and coreference resolver, which is described in §6.1, we extract several kinds of percepts for each NP. 4.2.1 Basic Words of interest. These are the head within the NP, all of its dependents, and its governor (external to the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing the dependency path upward from the head. For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a 3 Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection. As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are excluded from regularization. 5 See Theorem 1.2 in Breiman (2001) for details. 4 1063 binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we have additional features specific to the first and the last one. Moreover, to better capture tense, aspect and modality, we collect the attached verb’s auxiliaries. We also make note of the negative particle (with"
C14-1100,P05-1066,0,0.0178084,"rom some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000),"
C14-1100,C88-1044,0,0.577504,"to predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listen"
C14-1100,2007.mtsummit-papers.29,0,0.0509283,"ic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identifica"
C14-1100,T87-1035,0,0.537601,"aced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they are spoken. In co"
C14-1100,C10-1068,0,0.0128025,"se, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that re"
C14-1100,P06-1077,0,0.0109354,"matical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While defini"
C14-1100,W00-0708,0,0.0373875,"(Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distingu"
C14-1100,C02-1139,0,0.0446422,"dded or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share"
C14-1100,C00-2162,0,0.0170846,"s made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of"
C14-1100,W12-3807,1,0.89797,"Missing"
C14-1100,N13-1071,0,0.0612652,"Missing"
C14-1100,D10-1032,0,0.0256045,"gh we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in multiple communicative functions for each grammatical construction. Other attempts have also been made to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation, Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by prepositions. 8 Conclusion We have presented a data-driven approach to modeling the relationship between universal communicative functions associated with (in)definiteness and their lexical/grammatical realization in a particular language. Our feature-rich classifiers can give insights into this relationship as well as predict communicative functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear classifier com"
C14-1100,P10-1005,0,0.108554,"tediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling o"
C14-1100,N10-1018,0,0.0157261,"(b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which"
C14-1100,P13-1045,0,0.0784612,"Missing"
C14-1100,W13-2234,1,0.798697,"s input to (or jointly with) the coreference task. Applications such as information extraction and dialogue processing could be expected to benefit not only from coreference information, but also from some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of"
C14-1100,P02-1039,0,0.00839924,"to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme predictio"
C14-1100,2007.iwslt-1.3,0,0.0123911,"rmation (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly"
D14-1108,N13-1037,0,0.0101107,"t al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007), and the expense of creating Penn Treebank–style annotations (Marcus et al., 1993). This paper presents T WEEBOPARSER, the first syntactic dependency parser designed explicitly for English tweets. We developed this parser following current best practices in empirical NLP: we annotate a corpus (T WEEBANK) and train the parameters of a statistical parsing algorithm. Our research contributions include: • a survey of key challenges posed by syntactic analysis of tweets (by humans or machines) and decisions motivated"
D14-1108,C96-1058,0,0.325768,"is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word w"
D14-1108,J92-4003,0,0.124367,"x p or xg , and transitions that consider unselected tokens as children, are eliminated. In order to allow the scores to depend on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is t"
D14-1108,N13-1070,0,0.00887336,"$a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key differences from YM are in coordination structures (discussed in §3.2; YM makes the first conjunct the head) and possessive structures, in which the possessor is the child of the clitic, which is the child of the semantic head, e.g., the > king > ’s > horses. 3.4 Intrinsic Quality Our approach to developing this initi"
D14-1108,W06-2920,0,0.0727057,"Missing"
D14-1108,N09-1037,0,0.0604952,"Missing"
D14-1108,P14-1070,0,0.0175782,"c function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy on developing and respecting conventions (or makin"
D14-1108,D07-1101,0,0.0132253,"jectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasib"
D14-1108,W02-1001,0,0.20767,"Missing"
D14-1108,W11-0809,0,0.0137621,"Missing"
D14-1108,P12-1022,0,0.0128621,"the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy"
D14-1108,W08-1301,0,0.0281126,"Missing"
D14-1108,P11-2008,1,0.814385,"Missing"
D14-1108,N13-1013,0,0.0174979,"ally modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k annotations in th"
D14-1108,J98-4004,0,0.0947122,"Colors highlight token selection (gray; §2.1), multiword expressions (blue; §2.2), multiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence s"
D14-1108,P08-1068,0,0.0484886,"end on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from"
D14-1108,D08-1017,1,0.807895,"erienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k"
D14-1108,D11-1022,1,0.315387,", which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs in"
D14-1108,P09-1039,1,0.732309,"ultiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets,"
D14-1108,D10-1004,1,0.488796,"Missing"
D14-1108,P05-1012,0,0.0428376,"errors included, for example, an incorrect dependency relation between an auxiliary verb and the main verb (like ima > [have to]). Minor errors included an incorrect attachment between two modifiers of the same head, as in the > only > [grocery store]—the correct annotation would have two attachments to a single head, i.e. the > [grocery store] < only (or equivalent). feature vector representation g is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald a"
D14-1108,H05-1066,0,0.195682,"Missing"
D14-1108,P10-1001,0,0.00934067,"approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have"
D14-1108,W07-2216,0,0.0321536,"al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all tree"
D14-1108,D10-1125,0,0.0181274,"Missing"
D14-1108,P14-5021,1,0.819279,"Missing"
D14-1108,P08-1108,0,0.0093581,"losely as possible.7 2. An experienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our"
D14-1108,C14-1177,0,0.0252561,"Missing"
D14-1108,N13-1039,1,0.599524,"Missing"
D14-1108,P14-2128,0,0.0325991,"Missing"
D14-1108,J93-2004,0,0.0527718,"tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein,"
D14-1108,P13-2109,1,0.553015,"Missing"
D14-1108,P92-1017,0,0.491182,"s whose roots are marked by the ** symbol. Schneider et al.’s GFL offers some additional features, only some of which we made use of in this project. One important feature allows an annotator to leave the parse underspecified in some ways. We allowed our annotators to make use of this feature; however, we excluded from our training and testing data any parse that was incomplete (i.e., any parse that contained multiple disconnected fragments with no explicit root, excluding unselected tokens). Learning to parse from incomplete annotations is a fascinating topic explored in the past (Hwa, 2001; Pereira and Schabes, 1992) and, in the case of tweets, left for future work. An important feature of GFL that we did use is special notation for coordination structures. For the coordination structure in Figure 1, for example, the notation is: $a :: {♥ want} :: the attachments specially for postprocessing, following Schneider et al. (2013). In our evaluation (§5), these are treated like other attachments. {&} where $a creates a new node in the parse tree as it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordi"
D14-1108,A97-1004,0,0.0375716,"der TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets, however, often contain multiple sentences or fragments, which we call “utterances,” each with its own syntactic root disconnected from the others. The selected tokens in Figure 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-struc"
D14-1108,W06-1616,0,0.0166238,"ion of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first col"
D14-1108,D11-1141,0,0.491647,"f traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on twee"
D14-1108,Q14-1016,1,0.746015,"tags) average accuracy in the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators s"
D14-1108,W13-2307,1,0.848519,"Missing"
D14-1108,D08-1016,0,0.00740608,"ing) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adap"
D14-1108,P14-2068,0,0.0232384,"; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007),"
D14-1108,P10-1040,0,0.0319044,"and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from the Penn Treebank. Indeed, Foster et a"
D14-1108,P07-1031,0,0.0119923,"re 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-structure annotations, as is typically done using the Penn Treebank, is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the an"
D14-1108,D13-1015,0,0.0116876,", is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the annotation process (§3.2), important convention choices (§3.3), and measures of quality (§3.4). 3.1 Data Selection We added manual dependency parses to 929 tweets (12,318 tokens) drawn from the POS-tagged Twitter corpus of Owoputi et al. (2013), which are tokenized and contain manually annotated POS tags. Owoputi et al.’s data consists of two parts. The first, originally annotated by Gimpel et al. (2011), consists of tweets sampled from a particular day, October 27, 2010—t"
D14-1108,W03-3023,0,0.0804945,"s it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled dependency parses collapsing nodes like $a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key di"
D14-1108,N09-1057,0,\N,Missing
D14-1108,I11-1100,0,\N,Missing
D14-1108,D07-1112,0,\N,Missing
D14-1108,J13-1009,0,\N,Missing
D17-2001,N15-4006,1,0.892691,"Missing"
D17-2001,P98-1013,0,0.592409,"annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt. 1 2 Introduction Installation Instructions for installing NLTK are found at nltk.org. NLTK is cross-platform and supports Python 2.7 as well as Python 3.x environments. It is bundled in the Anaconda and Enthought Canopy Python distributions for data scientists.3 In a working NLTK installation (version 3.2.2 or later), one merely has to invoke a method to download the FrameNet data:4,5 For over a decade, the Berkeley FrameNet (henceforth, simply “FrameNet”) project (Baker et al., 1998) has been documenting the vocabulary of contemporary English with respect to the theory of frame semantics (Fillmore, 1982). A freely available, linguistically-rich resource, FrameNet now covers over 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexical annotations in sentences drawn from corpora. The resource has formed a basis for much research in natural language processing—most notably, a tradition of semantic role labeling that continues to this day (Gildea and Jurafsky, 2002; Baker et al., 2007; Das et al., 2014; FitzGerald et al., 2015; Roth and Lapata, 2015, inter alia). De"
D17-2001,D15-1112,0,0.0490693,"et (henceforth, simply “FrameNet”) project (Baker et al., 1998) has been documenting the vocabulary of contemporary English with respect to the theory of frame semantics (Fillmore, 1982). A freely available, linguistically-rich resource, FrameNet now covers over 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexical annotations in sentences drawn from corpora. The resource has formed a basis for much research in natural language processing—most notably, a tradition of semantic role labeling that continues to this day (Gildea and Jurafsky, 2002; Baker et al., 2007; Das et al., 2014; FitzGerald et al., 2015; Roth and Lapata, 2015, inter alia). Despite the importance of FrameNet, computational users are often frustrated by the complexity of its custom XML format. Whereas much of the resource is browsable on the web (http://framenet. icsi.berkeley.edu/), certain details of the linguistic descriptions and annotations languish in obscurity as they are not exposed by the HTML views of the data.1 The few open source APIs for &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download(&apos;framenet_v17&apos;) berkeley.edu/fndrupal/FrameGrapher, if the user knows to look there.) In the interest of space, our API does not show them in the"
D17-2001,J02-3001,0,0.154054,"load the FrameNet data:4,5 For over a decade, the Berkeley FrameNet (henceforth, simply “FrameNet”) project (Baker et al., 1998) has been documenting the vocabulary of contemporary English with respect to the theory of frame semantics (Fillmore, 1982). A freely available, linguistically-rich resource, FrameNet now covers over 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexical annotations in sentences drawn from corpora. The resource has formed a basis for much research in natural language processing—most notably, a tradition of semantic role labeling that continues to this day (Gildea and Jurafsky, 2002; Baker et al., 2007; Das et al., 2014; FitzGerald et al., 2015; Roth and Lapata, 2015, inter alia). Despite the importance of FrameNet, computational users are often frustrated by the complexity of its custom XML format. Whereas much of the resource is browsable on the web (http://framenet. icsi.berkeley.edu/), certain details of the linguistic descriptions and annotations languish in obscurity as they are not exposed by the HTML views of the data.1 The few open source APIs for &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download(&apos;framenet_v17&apos;) berkeley.edu/fndrupal/FrameGrapher, if the user knows to look ther"
D17-2001,C16-2033,0,0.0410447,"Missing"
D17-2001,Q15-1032,0,0.0611871,"rameNet”) project (Baker et al., 1998) has been documenting the vocabulary of contemporary English with respect to the theory of frame semantics (Fillmore, 1982). A freely available, linguistically-rich resource, FrameNet now covers over 1,000 semantic frames, 10,000 lexical senses, and 100,000 lexical annotations in sentences drawn from corpora. The resource has formed a basis for much research in natural language processing—most notably, a tradition of semantic role labeling that continues to this day (Gildea and Jurafsky, 2002; Baker et al., 2007; Das et al., 2014; FitzGerald et al., 2015; Roth and Lapata, 2015, inter alia). Despite the importance of FrameNet, computational users are often frustrated by the complexity of its custom XML format. Whereas much of the resource is browsable on the web (http://framenet. icsi.berkeley.edu/), certain details of the linguistic descriptions and annotations languish in obscurity as they are not exposed by the HTML views of the data.1 The few open source APIs for &gt;&gt;&gt; import nltk &gt;&gt;&gt; nltk.download(&apos;framenet_v17&apos;) berkeley.edu/fndrupal/FrameGrapher, if the user knows to look there.) In the interest of space, our API does not show them in the frame display, but the"
E12-1017,W10-2417,0,0.263206,"development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wikipedia.10 A descriptive list of our features is available in the supplementary document. We use a first-order structured perceptron; none of our features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram hO, Ii is disallowed). Training this model on ACE and ANER data achie"
E12-1017,attia-etal-2010-automatically,0,0.305231,"Missing"
E12-1017,W03-2201,0,0.00816761,"but not entirely, consistent with each other in their creation of custom categories. Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of (Sekine et al., 2002), which speaks to the reasonableness of both sets of categories—and by extension, our open-ended annotation process. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5 When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories only suggested by the other annotator were ignored. 164 NEs from English articles to their Arabic counterparts (Hassan et al., 2007), automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf. Frietag, 2004), or using non-canonical classes to improve the (author-specified) article categories in Wikipedia. Hereafter, we merge all article-specific categories with"
E12-1017,W09-3302,0,0.174637,"hard Stallman Amman were reserved Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System for training annotators, Claudio Filippone (PER) àñJ.ÊJ ¯ ñK XñÊ¿; Linux (SOFTWARE) ºJJ Ë; Spanish Gulf War for esti KðQK.; nuclear and League (CHAMPIONSHIPS) úGAJ.B@ ø PðYË@; proton (PARTICLE) àñ mating inter-annotator   agreement.  ; Real Zaragoza (ORG) é¢¯Qå ÈAK P radiation (GENERIC - MISC) ø ðñJË@ ¨AªB@ History Science Sports Technology appropriate entity classes will vary widely by domain; occurrence rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Se"
E12-1017,D08-1030,0,0.302426,"Missing"
E12-1017,W03-1022,0,0.0325535,"elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to pro"
E12-1017,W03-0407,0,0.0451401,"recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) simila"
E12-1017,W02-1001,0,0.00590103,"test data. A larger set of Arabic Wikipedia articles, selected on the basis of quality heuristics, serves as unlabeled data for semisupervised learning. Our out-of-domain labeled NE data is drawn from the ANER (Benajiba et al., 2007) and ACE-2005 (Walker et al., 2006) newswire corpora. Entity types in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal"
E12-1017,D07-1074,0,0.00392108,"ance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain da"
E12-1017,P07-1033,0,0.00539308,"Missing"
E12-1017,farber-etal-2008-improving,0,0.399022,"the choice of this value (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial dive"
E12-1017,D10-1033,0,0.0133877,"mains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them"
E12-1017,W04-3234,0,0.0317638,"Missing"
E12-1017,N10-1112,1,0.0770113,"2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of iterations T 0 Output: w w ← L(hhx(n) , y (n) iiN n=1 ) for t = 1 to T 0 do for j = 1 to J do ¯ (j) , y) yˆ(j) ← arg maxy w&gt; g(x ¯ (j) , yˆ(j) iiJj=1 ) w ← L(hhx(n) , y (n) iiN n=1 ∪ hhx y0 which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = hy1 , y2 , . . . , yM i, Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O ∧ yˆi 6= O for the ith word), recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-t"
E12-1017,W11-0411,0,0.0136423,"ews text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (bu"
E12-1017,P05-1071,0,0.00777494,"ature weights (model parameters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will b"
E12-1017,N06-2015,0,0.019601,"a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER ( SON ), ORG ( ANIZATION ), and LOC ( ATION ) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2 OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics Table 1: Translated titles of Arabic Wikipedia arti´"
E12-1017,P11-1147,0,0.0170901,"ng this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (includin"
E12-1017,N06-1010,0,0.0244745,"t entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who r"
E12-1017,D07-1073,0,0.00752362,"t only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedi"
E12-1017,P11-2016,0,0.0185145,"English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Arabic Wikipedia articles v"
E12-1017,W04-3250,0,0.00512011,"sed model as the initial labeler for recall-oriented self-training. • ROP/ROP (the “double ROP” condition): recalloriented supervised model as the initial labeler for recall-oriented self-training. Note that the two ROPs can use different cost parameters. For evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities. This is measured by per-entity precision, recall, and F1 .13 To measure statistical significance of differences between models we use Gimpel and Smith’s (2010) implementation of the paired bootstrap resampler of (Koehn, 2004), taking 10,000 samples for each comparison. 5.1 Baseline Our baseline is the perceptron, trained on the POL entity boundaries in the ACE+ANER corpus (reg/none).14 Development data was used to select the number of iterations (10). We performed 3-fold cross-validation on the ACE data and found wide variance in the in-domain entity detection performance of this model: fold 1 fold 2 fold 3 average P 70.43 87.48 65.09 74.33 R 63.08 81.13 51.13 65.11 F1 66.55 84.18 57.27 69.33 (Fold 1 corresponds to the ACE test set described in table 4.) We also trained the model to perform POL detection and class"
E12-1017,N06-1020,0,0.0445779,"and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tr"
E12-1017,W04-2405,0,0.0438164,"= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the"
E12-1017,N06-2024,0,0.386018,"earning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique was to directly tune the weight of a single feature—the feature marking O (nonentity tokens); a lower weight for this feature will incur a greater penalty for predicting O. Below we demonstrate that our method, which is less coarse, is more successful in our setting.12 In our experiments we will show that injecting “arrogance” into the learner via the recall-oriented loss function substantially improves recall, especially for non-POL entities (§5.3). 4.2 Self-Training and Semisupervised Learning As we will show exper"
E12-1017,nezda-etal-2006-world,0,0.220858,"e—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly. testing examples, but not as training data. In §4 we will discuss our semisupervised approach to learning, which leverages ACE and ANER data as an annotated training corpus. 2.1 Annotation Strategy We conducted a small annotation project on Arabic Wikipedia articles. Two college-educated native Arabic speakers annotated about 3,000 sentences from 31 articles. We identified four topical areas of interest—history, technology, scien"
E12-1017,E09-1070,0,0.00793652,"glish and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The d"
E12-1017,D10-1069,0,0.00615818,"stinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled"
E12-1017,W09-1119,0,0.00861532,"9/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step:   yˆ ← arg max w&gt; g(x, y 0 ) + c(y, y 0 ) , (2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of it"
E12-1017,P08-2030,0,0.0266617,"meters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorpo"
E12-1017,sekine-etal-2002-extended,0,0.045577,"nce rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it"
E12-1017,W04-1221,0,0.0307168,"9). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also"
E12-1017,W11-0902,0,0.0289153,"Missing"
E12-1017,D09-1158,0,0.0127085,"ic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—"
E12-1017,W03-1708,0,0.0109477,"ndantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annota"
E12-1017,N04-1001,0,\N,Missing
J13-2006,E03-1005,0,0.0840901,"Missing"
J13-2006,W02-1503,0,0.0511658,"snan 2000), HeadDriven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal metaframework allows computational linguists to formalize their hypotheses and intuitions about a language’s grammatical behavior and then explore how these representational choices affect the processing of natural language utterances. Many of the aforementioned approaches have engendered large-scale platforms that can be used and reused to provide formal description of grammars for different languages, such as Par-Gram for LFG (Butt et al. 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger, and Oepen 2002). FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG treats constructions as the basic units of grammatical organization in language. The constructions are viewed as learned associations between form (e.g., sounds, morphemes, syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG does not impose a strict separation between lexicon and grammar—indeed, it is perhaps best known as treat"
J14-1002,S07-1018,0,0.666381,"Missing"
J14-1002,boas-2002-bilingual,0,0.0274075,"experiments on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style r"
J14-1002,W04-2412,0,0.0162126,"Missing"
J14-1002,W05-0620,0,0.105832,"Missing"
J14-1002,D11-1003,0,0.0540671,"kled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out m"
J14-1002,S10-1059,1,0.89659,"y filled; in the SemEval 2007 development data, the average number of roles an evoked frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In 29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this work we do not distinguish different types of null instantiation. The interested reader may refer to Chen et al. (2010), who handle the different types of null instantions during argument identification. 31 Computational Linguistics Volume 40, Number 1 training, if a labeled argument is not a subtree of the dependency parse, we add its span to S.30 Let Ai denote the mapping of roles in Rfi to spans in S. Our model makes a prediction for each Ai (rk ) (for all roles rk ∈ Rfi ) using: Ai (rk ) ← argmax pψ (s |rk , fi , ti , x) s∈S (7) We use a conditional log-linear model over spans for each role of each evoked frame: exp ψ h(s, rk , fi , ti , x) pψ (Ai (rk ) = s |fi , ti , x) =  exp ψ h(s , rk , fi , ti , x"
J14-1002,S12-1029,1,0.386125,"Missing"
J14-1002,P11-1061,1,0.647271,"Missing"
J14-1002,N10-1138,1,0.945675,"sition algorithm (Section 7) that collectively predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error"
J14-1002,P11-1144,1,0.926493,"y predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition a"
J14-1002,N12-1086,1,0.870084,"ating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition argument identification algorithm, in contrast with the beam search"
J14-1002,P11-1043,0,0.0102251,"solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constraine"
J14-1002,D09-1003,0,0.0126412,"een verbs using a graph alignment method; this method represents sentences and their syntactic analysis as graphs and graph alignment is used to project annotations from seed examples to unlabeled sentences. This alignment problem is again modeled as a linear program. ¨ Furstenau and Lapata (2012) present an detailed expansion of the aforementioned papers. Although this line of work presents a novel direction in the area of SRL, the published approach does not yet deal with non-verbal predicates and does not evaluate the presented methods on the full text annotations of the FrameNet releases. Deschacht and Moens (2009) present a technique of incorporating additional information from unlabeled data by using a latent words language model. Latent variables are used to model the underlying representation of words, and parameters of this model 15 Computational Linguistics Volume 40, Number 1 are estimated using standard unsupervised methods. Next, the latent information is used as features for an SRL model. Improvements over supervised SRL techniques are observed with the augmentation of these extra features. The authors also compare ¨ their method with the aforementioned two methods of Furstenau and Lapata (200"
J14-1002,W03-1007,0,0.112387,"Missing"
J14-1002,C04-1134,0,0.0437403,"on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a fe"
J14-1002,P10-1160,0,0.00634321,"ables z are binary. Here, apart from the ILP formulation, we will consider the following relaxation of Equation (11), which replaces the binary constraint z ∈ {0, 1}d by a unit interval constraint z ∈ [0, 1]d , yielding a linear program: maximize  c(r, s) × zr,s r∈Rf s∈S with respect to such that z ∈ [0, 1]d Az ≤ b. (17) 42 We noticed that, in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010). 41 Computational Linguistics Volume 40, Number 1 There are several LP and ILP solvers available, and a great deal of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17)"
J14-1002,J02-3001,0,0.989881,"012; accepted for publication: 22 December 2012. doi:10.1162/COLI a 00163 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 1 1. Introduction FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore 1982), it suggests—but does not formally define—a semantic representation that blends representations familiar from word-sense disambiguation (Ide and V´eronis 1998) and semantic role labeling (SRL; Gildea and Jurafsky 2002). Given the limited size of available resources, accurately producing richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference. In this article, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in Figure 1. We aim to predict a frame-semantic representation with two statistical models rather than a collection of l"
J14-1002,S07-1003,0,0.00995843,"Missing"
J14-1002,W09-1201,0,0.0598567,"Missing"
J14-1002,J98-1001,0,0.0264943,"Missing"
J14-1002,S07-1048,0,0.102556,"s handling many more labels, and resulting in richer frame-semantic parses. Recent work in frame-semantic parsing—in which sentences may contain multiple frames which need to be recognized along with their arguments—was undertaken as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus 3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013. 14 Das et al. Frame-Semantic Parsing containing a little more than 2,000 sentences with full text annotations. The LTH system of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the best performance in the SemEval 2007 task in terms of full frame-semantic parsing. Johansson and Nugues broke down the task as identifying targets that could evoke frames in a sentence, identifying the correct semantic frame for a target, and finally determining the arguments that fill the semantic roles of a frame. They used a series of SVMs to classify the frames for a given target, associating unseen lexical items to frames and identifying and classifying token spans as various semantic roles. Both the full text annotation corpus as well"
J14-1002,D08-1008,0,0.0243643,"Missing"
J14-1002,kingsbury-palmer-2002-treebank,0,0.713501,"ly discuss work done on PropBank-style semantic role labeling, following which we will concentrate on the more relevant problem of frame-semantic structure extraction. Next, we review previous work that has used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior work on joint structure prediction relevant to frame-semantic parsing. 2.1 Semantic Role Labeling Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there has been a great deal of computational work using predicate-argument structures for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed by CoNLL shared tasks on semantic role labeling (Carreras and M`arquez 2004, 2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank. PropBank annotations are closely tied to syntax, because the data set consists of the 1 See http://www.ark.cs.cmu.edu/SEMAFOR. 11 Computational Linguistics Volume 40, Number 1 (a) (b) Figure 2 (a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank predicate-argument structures. The verbs created and pushed serve as predicates in this sentence. Dotted arrows connect each predicate to its semantic"
J14-1002,D10-1125,0,0.0072494,", as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`"
J14-1002,N10-1137,0,0.00679874,"d Lapata (2009a, 2009b) and show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most"
J14-1002,D11-1122,0,0.0108005,"Missing"
J14-1002,P93-1016,0,0.0449607,"e constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We scanned the exemplar sentences in FrameNet 1.5 and the training section of the full text annotations and gathered a distribution over frames for each LU appearing in FrameNet data. For a pair of LUs, we measured"
J14-1002,C94-1079,0,0.14144,"Missing"
J14-1002,P98-2127,0,0.0298059,"ed learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We sca"
J14-1002,S07-1005,0,0.0401753,"Missing"
J14-1002,J93-2004,0,0.0478041,"Missing"
J14-1002,J08-2001,0,0.0552021,"Missing"
J14-1002,D11-1022,1,0.788077,"Missing"
J14-1002,P09-1039,1,0.427643,"Missing"
J14-1002,D10-1004,1,0.864365,"r knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more ac"
J14-1002,P09-1003,0,0.0644821,"Missing"
J14-1002,P05-1012,0,0.0484643,"Missing"
J14-1002,W04-2705,0,0.72468,".v, ... Inheritance relation Causative_of relation Excludes relation Figure 3 Partial illustration of frames, roles, and lexical units related to the C AUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame’s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). C AUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here. Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) contains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs, and prepositions among its lexical units. Finally, FrameNet frames organize predicates according to semantic principles, both by allowing related terms to evoke a common frame (e.g., push.V, raise.V, and growth.N for C AUSE CHANGE POSITION ON A SCALE) and by defining frames and their roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Sect"
J14-1002,C04-1100,0,0.0140332,"n. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information a"
J14-1002,H05-1108,0,0.0829981,"Missing"
J14-1002,D08-1048,0,0.100472,"Missing"
J14-1002,N04-1030,0,0.00857398,"-DIR, and ARGM-TMP are shown in the figure. PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Mann"
J14-1002,J08-2005,0,0.157863,"Missing"
J14-1002,C04-1197,0,0.401501,"igure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Manning (2005), where global features looking at all arguments of a particular verb together are incorporated into a dynamic programming and reranking framework. The Computational Linguistics special issue on semantic role labeling (M`arquez et al. 2008) includes ot"
J14-1002,W96-0213,0,0.464731,"Missing"
J14-1002,W06-1616,0,0.0176524,"or semantic role labeling (M`arquez et al. 2008). To our knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since th"
J14-1002,W04-2401,0,0.0218668,"nvestigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing ("
J14-1002,P11-1008,0,0.0341732,") proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constrained problems, for which su"
J14-1002,D10-1001,0,0.0101386,"Missing"
J14-1002,N03-1028,0,0.0145026,"labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or in our case, syntactically disambiguated 27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not observed to fire in the training data) has been observed to give performance improvements in NLP problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010). 27 Computational Linguistics Volume 40, Number 1 predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We c"
J14-1002,D07-1002,0,0.273124,"ught to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information as well as the seeds are"
J14-1002,W04-2008,0,0.0362235,"r roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson, Levy, and Manning (2003) used a generative model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pado´ (2006) introduced the Shalmaneser tool, which uses naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet, containing around 300 frames and fewer than 500 unique semantic roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role types—thus handling ma"
J14-1002,D08-1016,0,0.00944644,"Missing"
J14-1002,D08-1114,0,0.00506907,"en shown to perform better than several other semi-supervised algorithms ¨ on benchmark data sets (Chapelle, Scholkopf, and Zien 2006, chapter 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or i"
J14-1002,D10-1017,0,0.0238482,"Missing"
J14-1002,P03-1002,0,0.0695193,"rained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and"
J14-1002,E12-1003,0,0.0172185,"show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most high-performance SRL systems that"
J14-1002,P05-1073,0,0.0166888,"Missing"
J14-1002,P10-1040,0,0.0142959,"Missing"
J14-1002,W04-3212,0,0.126024,"Missing"
J14-1002,N07-1069,0,0.0626288,"Missing"
J14-1002,erk-pado-2006-shalmaneser,0,\N,Missing
J14-1002,W08-2121,0,\N,Missing
J14-1002,E09-1026,0,\N,Missing
J14-1002,D09-1002,0,\N,Missing
J14-1002,P11-1048,0,\N,Missing
J14-1002,J12-1005,0,\N,Missing
J14-1002,C98-2122,0,\N,Missing
J14-1002,P10-2069,0,\N,Missing
K15-1009,P14-2133,0,0.0119464,"results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequ"
K15-1009,D14-1012,0,0.011555,"rns word embeddings and applied it to POS tagging, Chunking, NER and semantic role labelling. When they combined word embeddings with hand-crafted features (e.g., word suffixes for POS tagging; gazetteers for NER) and applied other tricks like cascading and classifier combination, they achieved state-of-theart performance. Similarly, Turian et al. (2010) evaluated three different word representations on NER and Chunking, and concluded that unsupervised word representations improved NER and Chunking. They also found that combining different word representations can further improve performance. Guo et al. (2014) also explored different ways of using word embeddings for NER. Owoputi et al. (2013) and Schneider et al. (2014a) found that B ROWN clustering enhances Twitter POS tagging and MWE, respectively. Compared to previous work, we consider more word representations including the most recent work and evaluate them on more sequence labelling tasks, 6 Conclusions We have performed an extensive extrinsic evaluation of four word embedding methods under fixed experimental conditions, and evaluated their applicability to four sequence labelling tasks: POS tagging, Chunking, NER and MWE identification. We"
K15-1009,P14-2131,0,0.150071,"Missing"
K15-1009,P11-2125,0,0.0499902,"Missing"
K15-1009,P14-1023,0,0.0950679,"o achieve competitive results, and that word embeddings lead to improvements over out-of-vocabulary words and also out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. 1 Introduction Recently, distributed word representations have grown to become a mainstay of natural language processing (NLP), and have been shown to have empirical utility in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and eval"
K15-1009,S13-1005,0,0.0137913,"Missing"
K15-1009,J92-4003,0,0.179922,"asks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“S KIP - GRAM”) (Mikolov et al., 2013b), and Global vectors (“G LOVE”) (Pennington et al., 2014). Al"
K15-1009,P09-1056,0,0.0429599,"Missing"
K15-1009,N13-1039,1,0.0552629,"Missing"
K15-1009,P08-1068,0,0.114458,"Missing"
K15-1009,D10-1125,0,0.0312081,"Missing"
K15-1009,D14-1162,0,0.127711,"ty in a myriad of tasks (Collobert and Weston, 2008; Turian et al., 2010; Baroni et al., 2014; Andreas and Klein, 2014). The underlying idea behind distributed word representations is simple: to map each word w in vocabulary V onto a continuous-valued vector of dimensionality d  |V |. Words that are similar (e.g., 83 Proceedings of the 19th Conference on Computational Language Learning, pages 83–93, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics sion (MWE) identification.1 We build on previous empirical studies (Collobert et al., 2011; Turian et al., 2010; Pennington et al., 2014) in considering a broader range of word embedding approaches and evaluating them over more sequence labelling tasks. In addition, we explore the following research questions: RQ1: are word embeddings better than baseline approaches of one-hot unigram2 features and Brown clusters? RQ2: do word embeddings require less training data (i.e., generalise better) than one-hot unigram features? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrica"
K15-1009,P93-1024,0,0.70936,"act of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”) (Mikolov et al., 2013a), the continuous skip-gram model (“S KIP - GRAM”) (Mikolov et al., 2013b), and Gl"
K15-1009,Q15-1016,0,0.069416,"Missing"
K15-1009,P09-1116,0,0.00595057,"s? If so, to what degree can word embeddings reduce the amount of labelled data? RQ3: what is the impact of updating word embeddings in sequence labelling tasks, both empirically over the target task and geometrically over the vectors? RQ4: what is the impact of these word embeddings (with and without updating) on both OOV items (relative to the training data) and out-of-domain data? RQ5: overall, are some word embeddings better than others in a sequence labelling context? 2 2.1 Cluster-based representation methods build clusters of words by applying either soft or hard clustering algorithms (Lin and Wu, 2009; Li and McCallum, 2005). Some of them also rely on a co-occurrence matrix of words (Pereira et al., 1993). The Brown clustering algorithm (Brown et al., 1992) is the best-known method in this category. Distributed representation methods usually map words into dense, low-dimensional, continuous-valued vectors, with x ∈ Rd , where d is referred to as the word dimension. 2.2 Selected Word Representations Over a range of sequence labelling tasks, we evaluate four methods for inducing word representations: Brown clustering (Brown et al., 1992) (“B ROWN ”), the continuous bag-of-words model (“CBOW”"
K15-1009,Q14-1016,1,0.26664,"e relationship between w and its local contexts of use, either by predicting w based on its local context, or using w to predict the context words. Other than B ROWN , which utilises a cluster-based representation, all the other methods employ a distributed representation. The starting point for CBOW and S KIP - GRAM is to employ softmax to predict word occurrence: ! Tv exp(vw ctx(w) ) J(w, ctx(w)) = − log P T j∈V exp(vj vctx(w) ) 1 MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2 Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = |V |), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. 3 The word embedding approach proposed in Collobert et al. (2011) is not considered because it was found to be inferior to our four target word embedding approaches in previous work. 84 Data set UMBC (Han et al., 2013) One Billion (Chelba et al., 2013) English Wikipedia where vctx(w) denotes the distributed representation of th"
K15-1009,schneider-etal-2014-comprehensive,1,0.242578,"e relationship between w and its local contexts of use, either by predicting w based on its local context, or using w to predict the context words. Other than B ROWN , which utilises a cluster-based representation, all the other methods employ a distributed representation. The starting point for CBOW and S KIP - GRAM is to employ softmax to predict word occurrence: ! Tv exp(vw ctx(w) ) J(w, ctx(w)) = − log P T j∈V exp(vj vctx(w) ) 1 MWEs are lexicalized combinations of two or more simplex words that are exceptional enough to be considered as single units in the lexicon (Baldwin and Kim, 2010; Schneider et al., 2014a), e.g., pick up or part of speech. 2 Word vectors with one-hot representation are binary vectors with a single dimension per word in the vocabulary (i.e., d = |V |), with the single dimension corresponding to the target word set to 1 and all other dimensions set to 0. 3 The word embedding approach proposed in Collobert et al. (2011) is not considered because it was found to be inferior to our four target word embedding approaches in previous work. 84 Data set UMBC (Han et al., 2013) One Billion (Chelba et al., 2013) English Wikipedia where vctx(w) denotes the distributed representation of th"
K15-1009,J93-2004,0,0.0551578,"wo predictions, because we want to evaluate all word representations with the same type of model — a first-order graph transformer. In training the distributed word representations, we consider two settings: (1) the word representations are fixed during sequence model training; and (2) the graph transformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English W"
K15-1009,N03-1028,0,0.0860405,"Missing"
K15-1009,D11-1118,0,0.00833748,"Missing"
K15-1009,W00-0726,0,0.138209,"sformer updated the tokenlevel word representations during training. As outlined in Table 2, for each sequence labelling task, we experiment over the de facto corpus, based on pre-existing training–dev–test splits where available:4 POS tagging: the Wall Street Journal portion of the Penn Treebank (Marcus et al. (1993): “WSJ”) with Penn POS tags Chunking: the Wall Street Journal portion of the Penn Treebank (“WSJ”), converted into IOBstyle full-text chunks using the CoNLL conversion scripts for training and dev, and the WSJ-derived CoNLL-2000 full text chunking test data for testing (Tjong Kim Sang and Buchholz, 2000) NER: the English portion of the CoNLL-2003 English Named Entity Recognition data set, for which the source data was taken from Reuters newswire articles (Tjong Kim Sang and De Meulder (2003): “Reuters”) MWE: the MWE dataset of Schneider et al. (2014b), over a portion of text from the English Web Treebank5 (“EWT”) For all tasks other than MWE,6 we additionally have an out-of-domain test set, in order to evaluate the out-of-domain robustness of the different word representations, with and without updating. These datasets are as follows: POS tagging: the English Web Treebank with Penn POS tags ("
K15-1009,N03-1033,0,0.0101606,"Missing"
K15-1009,P10-1040,0,0.526824,"nately, it has been shown that they can be “pre-trained” from unlabelled text data using various algorithms to model the distributional hypothesis (i.e., that words which occur in similar contexts tend to be semantically similar). Pre-training methods have been refined considerably in recent years, and scaled up to increasingly large corpora. As with other machine learning methods, it is well known that the quality of the pre-trained word embeddings depends heavily on factors including parameter optimisation, the size of the training data, and the fit with the target application. For example, Turian et al. (2010) showed that the optimal dimensionality for word embeddings is taskspecific. One factor which has received relatively little attention in NLP is the effect of “updating” the pre-trained word embeddings as part of the task-specific training, based on self-taught learning (Raina et al., 2007). Updating leads to word representations that are task-specific, but often at the cost of over-fitting low-frequency and OOV words. In this paper, we perform an extensive evaluation of four recently proposed word embedding approaches under fixed experimental conditions, applied to four sequence labelling tas"
K15-1009,D11-1116,0,\N,Missing
K15-1009,D14-1082,0,\N,Missing
K19-1017,P13-1023,1,0.875202,"á et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of semantic units and relations, with typologically-based criteria for marking predicate-argument structures, based on “Basic Linguistic Theory”, an established framework for typological description (Dixon, 2010/2012). On the other hand, a recent approach to annotation of English prepositions and possessives (SNACS; Schneider et al., 2018) provides an inventory of labels that characterize semantic relations. UCCA and SNACS follow similar design principles: they are both language-neutral, with general-purpose coarse-grained labels rather than lexically-specific se"
K19-1017,P17-1008,1,0.850498,"that hold across paraphrasing or translation: for example, semantic dependency relations capturing predicateargument structures or other types of semantic relations that can be annotated within sentences (e.g., Böhmová et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of semantic units and relations, with typologically-based criteria for marking predicate-argument structures, based on “Basic Linguistic Theory”, an established framework for typological description (Dixon, 2010/2012). On the other hand, a recent approach to annotation of English prepositions and possessives (SNACS; Schneider et al., 2018) provides a"
K19-1017,W13-2322,1,0.949289,"structure from the prepositions on which they were originally annotated. The following UCCA categories are abbreviated: A = Participant, R = Relator, H = Parallel scene, Q = Quantifier, Fxn = Function. Introduction A common thread in many approaches to meaning representation is the idea that abstract structures can describe semantic invariants that hold across paraphrasing or translation: for example, semantic dependency relations capturing predicateargument structures or other types of semantic relations that can be annotated within sentences (e.g., Böhmová et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of"
K19-1017,D16-1134,1,0.803322,"Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the root) has a single incoming primary edge, and may also have incoming reentrant remote edges to express shared argumenthood. The primary edges of a UCCA structure thus form a tree"
K19-1017,N18-2020,1,0.865565,"cessfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the root) has a single incoming primary edge, and may also have incoming reentrant remote edges to express shared argumenthood. The primary edges of a UCCA structure thus form a tree, which along with the remote edges, forms a DA"
K19-1017,D09-1047,0,0.03106,"/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the tw"
K19-1017,N19-1423,0,0.00703687,"has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexical semantic information, confirming our hypothesis that UCCA and SNACS are complementary and compatible. Based on preliminary re"
K19-1017,C16-1256,0,0.0136147,"instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings ("
K19-1017,N18-1082,0,0.0187017,"here is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We hav"
K19-1017,P17-1104,1,0.949322,"other. That is, we expect that knowing what semantic role is signaled by an adposition informs the underlying semantic structure of the sentence, and vice versa. In order to test this hypothesis, we use our annotated corpus to parse into the integrated representation. We consider several different ways of orchestrating the prediction of the foundational UCCA structure and the prediction of SNACS roles: modeling SNACS and UCCA in (i) a pipeline, (ii) a multitask setup with shared parameters, (iii) a single joint model. 4.1 Baseline: TUPA We choose the neural transition-based graph parser TUPA (Hershcovich et al., 2017, 2018) as a strong baseline for UCCA parsing. It was the official baseline in the recent SemEval shared task on UCCA parsing (Hershcovich et al., 2019b). TUPA’s transition system is defined to address 178 Avg Test F-Score 70 65 60 55 50 45 40 35 30 25 20 baseline dep MTL/ter Refined:exact pipeline joint/rel indep MTL joint/ter Refined:SNACS dep MTL/rel Refined:UCCA Refined:unlabeled Full Figure 2: Average F1-score on the test set over 5 random restarts with error bars indicating standard deviation. ter stands for terminal-level and rel for relation-level SNACS refinement (prediction or featur"
K19-1017,P18-1035,1,0.685576,"ation. ter stands for terminal-level and rel for relation-level SNACS refinement (prediction or features). the different formal structural phenomena exhibited by UCCA structures, notably reentrancies and discontiguous units. There are transitions for creating nonterminal nodes, and for attaching terminal and nonterminal nodes to a primary or remote (reentrant) parent with an UCCA category label on the edge. The transition system is general enough to be able to tackle parsing into a variety of formalisms, including SDP (Oepen et al., 2015) and a simplified form of AMR (Banarescu et al., 2013); Hershcovich et al. (2018) take advantage of this flexibility in their multitask learning framework. TUPA’s learning architecture largely follows that of Kiperwasser and Goldberg (2016). It encodes the parser’s configuration (buffer, stack and intermediate graph structure) using BiLSTMs, and predicts the next transition using an MLP, stacked on top of them. Token-based features, including POS tags, dependency parses, as well as NER and orthographic features, are embedded in the BiLSTM. Another set of features, taking into account the partially constructed graph and previously predicted transition types, is fed into the"
K19-1017,N19-1047,1,0.837606,"imensions of canonical adpositional phrase constructions. The adposition is bolded, the semantic head is italicized, and the semantic dependent is [ bracketed ]. Rows indicate whether the semantic head is nominal or verbal, while columns differentiate between scene-evoking and nonscene-evoking heads. Scene-evokers are underlined. 3.1 Data We use the STREUSLE 4.0 corpus (Schneider and Smith, 2015; Schneider et al., 2018), which covers the reviews section from the English_EWT treebank of UD 2.3, and lexical semantic annotations for the same text.6 The same corpus has been annotated with UCCA by Hershcovich et al. (2019a). We use the standard train/dev/test split for this dataset (table 2). §3.3 shows the distribution of linguistic phenomena at issue here. 3.2 Procedure Given an UCCA graph with annotated terminals, the integration routine projects a SNACS annotation of a token onto the appropriate edge representing a semantic relation. This is illustrated by dashed arrows in figure 1. The procedure starts with a single terminal node, traversing the graph upwards until it finds an edge that satisfies the criteria given by the rules. The rules concern canonical prepositional phrase modifiers, plus a variety of"
K19-1017,N06-2015,0,0.105766,"est nor the largest model, suggesting that the particular linguistic signals and the method of using them have a genuine effect on performance. 6 Related Work The benefits of integrating lexical analysis and sentence semantic or syntactic structure have been pursued by a vast body of work over the years. Compositional approaches to the syntax-semantics interface, such as CCG (Steedman, 2000) and HPSG (Pollard and Sag, 1994), usually integrate the lexicon at the leaves of the syntactic parse, but propagate grammatically-relevant features up the tree. A different approach is taken by OntoNotes (Hovy et al., 2006), which consists of a number of separate, albeit linked tiers, including syntactic and argument structure, but also the lexical tiers of word senses and coreference. Role semantics frequently features in structured semantic schemes. Some approaches, such as PropBank and AMR (Palmer et al., 2005; Banarescu et al., 2013), follow a lexical approach. The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al."
K19-1017,S17-1022,1,0.88944,"Missing"
K19-1017,S19-2002,0,0.0124667,"and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both"
K19-1017,Q16-1023,0,0.242456,"bited by UCCA structures, notably reentrancies and discontiguous units. There are transitions for creating nonterminal nodes, and for attaching terminal and nonterminal nodes to a primary or remote (reentrant) parent with an UCCA category label on the edge. The transition system is general enough to be able to tackle parsing into a variety of formalisms, including SDP (Oepen et al., 2015) and a simplified form of AMR (Banarescu et al., 2013); Hershcovich et al. (2018) take advantage of this flexibility in their multitask learning framework. TUPA’s learning architecture largely follows that of Kiperwasser and Goldberg (2016). It encodes the parser’s configuration (buffer, stack and intermediate graph structure) using BiLSTMs, and predicts the next transition using an MLP, stacked on top of them. Token-based features, including POS tags, dependency parses, as well as NER and orthographic features, are embedded in the BiLSTM. Another set of features, taking into account the partially constructed graph and previously predicted transition types, is fed into the MLP directly. 4.2 Pipeline We extend TUPA by providing the SNACS label as a feature on the adposition token.9 This is added in preprocessing in the same way a"
K19-1017,S07-1005,0,0.0584806,", but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in"
K19-1017,N19-1112,0,0.0135803,"h coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexi"
K19-1017,W04-2609,0,0.0977459,"ollow a lexical approach. The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be e"
K19-1017,S15-2153,0,0.202558,"Missing"
K19-1017,W03-0411,0,0.223052,"Missing"
K19-1017,J09-2002,0,0.0857217,"Missing"
K19-1017,J05-1004,0,0.796137,"the intended roles to a human reader. 2.2 SNACS SNACS is an inventory of 50 roles/relations used to disambiguate adpositions and possessives in multiple languages, including English (Schneider et al., 2018, 2019), Mandarin Chinese (Zhu et al., 2019), and to a lesser extent, Korean, Hindi, and Hebrew (Hwang et al., 2017). Many of the SNACS labels, such as AGENT, T HEME, and T OPIC, are derived from VerbNet’s (Kipper et al., 2008) core roles of predicates. (Others, such as Q UANTITY and W HOLE, are for entity modification.) But unlike VerbNet, FrameNet (Fillmore and Baker, 2009), and PropBank (Palmer et al., 2005), SNACS does not require a language-specific predicate lexicon (hence Schneider et al. (2018) use the term “supersenses”, which we adopt in the remainder of this paper)—and is therefore compatible with UCCA’s design principle of crosslinguistic applicability.5 Currently, SNACS labels are applied directly to lexical items, without marking up underlying structure on either the subword (morphological) or the sentence-structure level. 3 Automatically Integrating Semantic Roles with UCCA With the benefit of a jointly annotated corpus, we examined the data and determined that the proper placement of"
K19-1017,P18-1018,1,0.0571484,"b Prange Nathan Schneider Georgetown University Omri Abend The Hebrew University of Jerusalem jakob@cs.georgetown.edu nathan.schneider@georgetown.edu oabend@cs.huji.ac.il N A TY TI N R Q er nt Fxn Ce r Cente R NA TIO UA AL A Process O A LA ∣Q G 1 Linker A∣ Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions, as formulated by Schneider et al. (2018), is complementary and suitable for integration within UCCA. We show empirically for English that the schemes, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task. XP ces s H Pro H∣E Abstract I went to ohm after reading some of the reviews Figure 1: Semantic parse illustrating the integrated representation proposed here. Solid edges are the UCCA parse’s primary edges, and the dotted edge is a remote edge. Dashed arrows show"
K19-1017,N15-1177,1,0.884414,"Missing"
K19-1017,W19-3316,1,0.823685,"atz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with"
K19-1017,D11-1012,0,0.0274562,"at we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work."
K19-1017,Q13-1019,0,0.0221331,"ics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT conte"
K19-1017,W15-3502,1,0.789181,"rser code: https://github.com/ integration routine and evaluation scripts are being released as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016;"
K19-1017,N18-1202,0,0.0115588,". Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexical semantic information, confirming our hypothesis that UCCA and SNACS are complementary and compatible. B"
K19-1017,N18-1063,1,0.864593,"d as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acy"
K19-1017,P18-1016,1,0.812053,"d as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acy"
K19-1017,W19-3319,1,0.586466,"al layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the"
K19-1017,P13-1037,0,0.0246997,"ank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019"
K19-1017,D16-1177,0,0.21101,"Missing"
K19-1017,S14-2008,0,\N,Missing
K19-1017,D11-1116,0,\N,Missing
K19-1017,D18-1412,0,\N,Missing
K19-1017,S19-2001,1,\N,Missing
L16-1629,P14-2064,0,0.0465076,"Missing"
L16-1629,W06-1670,0,0.0628994,"ecifically belongs to the verb (e.g. ‘listen to’, ‘look for’) and nomi3986 1 2 http://www.cs.cmu.edu/˜ark/LexSem/ http://www.inf.u-szeged.hu/rgai/mwe nal compounds that include more than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proport"
L16-1629,dickinson-lee-2008-detecting,0,0.27069,"erent labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annota"
L16-1629,A00-2020,0,0.712067,"t have been labeled differently. A prominent family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring me"
L16-1629,S14-1001,0,0.0251135,"biguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfavorable evidence (e.g., the number of times M occurs not annotated as an MWE). Weighted Discrepancy Ranking for MWE Annotations As already noted, M ∈ M is the set of tokens that have been annotated at least once in the STREUSLE corpus for a given sequ"
L16-1629,P10-2014,0,0.124836,"considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on co"
L16-1629,E09-1060,0,0.0698249,"ar instances that have been labeled differently. A prominent family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-bas"
L16-1629,C02-1101,0,0.391767,"ction in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annotation, we leave to future work the possibility of exploiting context to detect inconsistencies, though the benefits of doing so may be limited for our small corpora. Other approaches have taken advantage of multiple annotations from different annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014). Our methods only consider one annotation per sentence, and therefore do not depend on"
L16-1629,Q14-1025,0,0.0579799,"ing context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annotation, we leave to future work the possibility of exploiting context to detect inconsistencies, though the benefits of doing so may be limited for our small corpora. Other approaches have taken advantage of multiple annotations from different annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014). Our methods only consider one annotation per sentence, and therefore do not depend on information which is available only for some corpora. 7. Conclusion Since inconsistency detection in semantic annotation is a largely unexplored topic and semantic inconsistencies are more difficult to grasp than syntactic inconsistencies, 3989 we explore two ranking-based methods to approach this task. We apply both methods to annotations of multiword expressions and supersense labels on different data sets. Overall, the proposed ranking methods are successful in detecting inconsistency candidates with hig"
L16-1629,P14-2083,0,0.0266289,"ncies can be found in any manually-annotated corpus due to underspecified or even missing guidelines, ambiguity, insufficient annotator expertise, and/or human errors. A consistent corpus is not only a high-quality lexical resource, but a foundation for robust automatic language processing via supervised learning. Since inconsistencies in the training corpus can lead to low performance, consistent annotation is of practical as well as theoretical benefit. Annotation inconsistencies can be subclassified into annotation errors and hard cases, although they cannot always be easily distinguished (Plank et al., 2014). An annotation error is an instance that is annotated incorrectly according to the guidelines. Annotation errors have a correct answer. To give an example from part-of-speech annotation, consider the phrase ‘a summer feeling’. The word ‘summer’ should be annotated as a common noun; marking it as a comparative adjective or a determiner would be an annotation error. Linguistically hard cases are instances that do not have one correct answer; they may have multiple correct answers or it may not be obvious what the label should be. Such annotations are not necessarily incorrect, but can be ambigu"
L16-1629,N15-1177,1,0.861515,"re than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfavorable evidence (e.g., the number of times M occurs n"
L16-1629,P12-2050,1,0.878986,".u-szeged.hu/rgai/mwe nal compounds that include more than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfa"
L16-1629,schneider-etal-2014-comprehensive,1,0.893213,"Missing"
L16-1629,W15-1612,1,0.80389,"ed and not annotated occurrences is an indication that the type is inconsistently annotated. This discrepancy is weighted (scaled) by the number of times the MWE was annotated to put more weight on frequent multiword expressions. With the above measure, we rank types in descending order. The hypothesis is that types with greater annotated-MWE frequency overall, and greater discrepancy between annotated and unannotated tokens, are more likely to contain inconsistently labeled tokens. 3.2.2. 3 Supersense inventories have also been proposed for adjectives and prepositions (Tsvetkov et al., 2014; Schneider et al., 2015), though supersense annotations are limited to nouns and verbs in the data sets we use. 4 https://github.com/coastalcph/ supersense-data-twitter 5 A new, larger corpus that includes STREUSLE was compiled for the DiMSUM 2016 shared task on MWE identification and supersense tagging (Schneider et al., 2016): https://github.com/dimsum16/dimsum-data Weighted Discrepancy Ranking for Supersense Annotations A similar weighted discrepancy ranking method can be defined for supersense labels. A type S ∈ S is defined as a word form—noun or verb— paired with a single POS tag. For instance, in the Twitter c"
L16-1629,tsvetkov-etal-2014-augmenting-english,1,0.864974,"frequencies of annotated and not annotated occurrences is an indication that the type is inconsistently annotated. This discrepancy is weighted (scaled) by the number of times the MWE was annotated to put more weight on frequent multiword expressions. With the above measure, we rank types in descending order. The hypothesis is that types with greater annotated-MWE frequency overall, and greater discrepancy between annotated and unannotated tokens, are more likely to contain inconsistently labeled tokens. 3.2.2. 3 Supersense inventories have also been proposed for adjectives and prepositions (Tsvetkov et al., 2014; Schneider et al., 2015), though supersense annotations are limited to nouns and verbs in the data sets we use. 4 https://github.com/coastalcph/ supersense-data-twitter 5 A new, larger corpus that includes STREUSLE was compiled for the DiMSUM 2016 shared task on MWE identification and supersense tagging (Schneider et al., 2016): https://github.com/dimsum16/dimsum-data Weighted Discrepancy Ranking for Supersense Annotations A similar weighted discrepancy ranking method can be defined for supersense labels. A type S ∈ S is defined as a word form—noun or verb— paired with a single POS tag. For i"
L16-1629,ule-simov-2004-unexpected,0,0.244919,"nt family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3),"
L16-1629,R11-1040,0,0.123599,"Missing"
L16-1629,S16-1084,1,\N,Missing
L18-1242,S17-1022,1,0.856092,"Missing"
L18-1242,W04-2609,0,0.0330951,"ENEFICIARY, O RG ROLE, R ECIPIENT, and perhaps most interestingly P OSSESSOR. Semantic differences in distribution are known to play a role in English’s genitive alternation studied in previous work (Shih et al., 2012; Wolk et al., 2013). Many other factors have been established as well. Our data augments the empirical record. 5.2. Comparison to previous schemes Previous annotation schemes for English possessive constructions have been based on attempts to characterize relations between nominals. Badulescu and Moldovan (2009, herafter “BM”) adapted one such set of 35 semantic categories (from Moldovan et al., 2004), resulting in 22 labels for English possessive constructions. Tratz and Hovy (2013, “TH”), building on prior work by BM and others, developed a set of 18 semantic categories specific to the s-genitive. They did so iteratively, adjusting the categories as needed to reduce disagreements between annotators. TH’s study and inventory were limited to the s-genitive (’s and possessive pronouns). Our approach, by contrast, uses an adposition annotation scheme as the point of departure. We find that indeed, many of the semantic relations expressed with prepositions like in, with, etc. can also be conv"
L18-1242,P13-1037,0,0.172645,"e: • Alienable possession: John’s computer. • Kinship: My sister was surprisingly late. • Part–whole relations: The car’s windshield. • Thematic roles: The boy’s murder was never reported. (This is actually ambiguous: The role that the boy fills with respect to the predicate murder could be either agent or patient, depending on whether he was the victim or perpetrator.) Previous work on annotating the semantics of possessive constructions has taken a sense disambiguation perspective, with semantic categories specific to relations between nominals (Badulescu and Moldovan, 2009) or s-genitives (Tratz and Hovy, 2013). In this paper, we show that a tagset for broad-coverage semantic annotation of prepositions and postpositions can be applied—mostly as is—to English possessive constructions. We use the adposition supersense inventory (Schneider et al., 2015, 2016, 2017), which was designed with adpositions (including of) in mind, to annotate all of-genitive and s-genitive tokens in a 55,000 word corpus of English web reviews (§3). In so doing, we demonstrate that the existing supersense categories are readily applicable to English possessives. The latest version consists of 50 general supersense categories"
L18-1242,W16-1712,1,0.831669,"Missing"
L18-1242,schneider-etal-2014-comprehensive,1,0.898431,"Missing"
L18-1242,W15-1612,1,0.838189,"lls with respect to the predicate murder could be either agent or patient, depending on whether he was the victim or perpetrator.) Previous work on annotating the semantics of possessive constructions has taken a sense disambiguation perspective, with semantic categories specific to relations between nominals (Badulescu and Moldovan, 2009) or s-genitives (Tratz and Hovy, 2013). In this paper, we show that a tagset for broad-coverage semantic annotation of prepositions and postpositions can be applied—mostly as is—to English possessive constructions. We use the adposition supersense inventory (Schneider et al., 2015, 2016, 2017), which was designed with adpositions (including of) in mind, to annotate all of-genitive and s-genitive tokens in a 55,000 word corpus of English web reviews (§3). In so doing, we demonstrate that the existing supersense categories are readily applicable to English possessives. The latest version consists of 50 general supersense categories including thematic role labels (AGENT, T HEME, R ECIPIENT, etc.) and relations that hold between entities (P OSSESSOR, W HOLE, S OCIAL R EL, etc.). §4 describes the supersenses that proved useful for possessive constructions. In §5 we examine"
L18-1266,K16-1007,0,0.0145068,"termed the Standard (the compared-to entity) in the Comparison frame is replaced by a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer,"
L18-1266,W13-2322,1,0.895126,"Missing"
L18-1266,L16-1628,1,0.836428,"andeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for blink list appropriate semant"
L18-1266,bonial-etal-2014-propbank,1,0.924991,"antic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the PropBank rolesets for bli"
L18-1266,P13-2131,1,0.826219,"Missing"
L18-1266,W17-0812,0,0.0128119,"a Comparison Set. Our treatment generalizes somewhat over the FrameNet treatment by exploiting a single roleset 3 For ARG0 and ARG1 only, an effort is made to map to Dowty’s prototypical agent and patient (Dowty, 1991), respectively. 1678 for all of these constructions (see Section 5.1). Our general roleset is very similar (in definition, albeit distinct in labeling) to that of Bakhshandeh and Allen (2015), who aim to predict the predicate-argument structure of comparison sentences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank"
L18-1266,W10-1810,1,0.796285,"ences to support semantic parsing (Bakhshandeh et al., 2016). Also related is the constructional annotation scheme of Dunietz et al. (2017), which targets causal language: in its current form their scheme includes the DegreeConsequence and The X-er, The Y-er constructions but excludes argument structure constructions like Caused Motion. PropBank has always included multi-word verbal predicates (e.g., eat up), and has recently been expanding its lexicon of rolesets and annotations to include light verb constructions and the degree and quantity-related constructions that will be discussed here (Hwang et al., 2010; Bonial et al., 2014; Bonial and Palmer, 2016). The expansion of the roleset lexicon was necessary not only for PropBank to provide construction-based annotations, but also because PropBank and AMR maintain a shared lexicon of rolesets that ensures symmetry between the two projects. 3. AMR Approach to Constructions In building the AMR corpus, we have run across a variety of cases where a predicate appeared to be in an atypical context: none of the senses listed in the lexicon of rolesets provided the appropriate role label choices for novel arguments encountered. For example, none of the Prop"
L18-1266,N15-1114,0,0.0551831,"meaning. Keywords: Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of"
L18-1266,W16-5706,1,0.830623,"Missing"
L18-1266,J05-1004,1,0.31696,"grammar approach, the construction itself can license the arguments. This distinction also affects what is thought to be stored in the lexicon and, perhaps most relevant to AMR, what would need to be represented in a computational lexicon: additional senses tied to a lexical predicate, or constructional entries. 2.2. Abstract Meaning Representation Annotation The AMR project annotations are completed on a sentenceby-sentence basis, where each sentence is represented by a rooted directed acyclic graph (DAG). See Figure 1. PENMAN notation. AMR concepts are either English words (boy), PropBank (Palmer et al., 2005) rolesets (want-01), or special keywords indicating generic entity types: date-entity, world-region, distance-quantity, etc. In addition to the PropBank lexicon of rolesets, which associate argument numbers (ARG 0–6) with predicate-specific3 semantic roles (e.g., ARG0=wanter in ex. 1), AMR uses approximately 100 relations of its own (e.g., :time, :age, :quantity, :destination, etc.). These AMR-specific relations can be thought of as a fine-grained inventory of modifier role labels. AMR abstracts away from language-specific, idiosyncratic facts, such that distinct syntactic realizations of the"
L18-1266,N15-1119,1,0.802771,"(Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics"
L18-1266,W16-6603,1,0.775868,": Semantics, Constructions, Meaning Representation 1. Introduction The Abstract Meaning Representation (AMR) project (Banarescu et al., 2013) has created a manually annotated “semantics bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). Howe"
L18-1266,W17-2315,1,0.813627,"cs bank” of text. A goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic facts; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. The purpose of these annotations is to support natural language processing (NLP) applications such as natural language understanding, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). With a growing body of over 70 research papers using AMR,1 the corpus is becoming a benchmark dataset. As a practical NLP resource, AMR annotation has focused on providing coverage for the more frequent and predictably patterned linguistic phenomena, and therefore has not necessarily provided adequate representations for some of the rarer structures found in the long tail of language (Zipf, 1949). However, as the project matures, we are aiming to expand the representation to go beyond capturing the semantics of purely compositional language to better capture the semantics"
N10-1138,S07-1018,0,0.696004,"though our models often involve strong independence assumptions, the probabilistic framework we adopt is highly amenable to future extension through new features, relaxed independence assumptions, and semisupervised learning. Some novel aspects of our current approach include a latent-variable model that permits disambiguation of words not in the FrameNet lexicon, a unified model for finding and labeling arguments, and a precision-boosting constraint that forbids arguments of the same predicate to overlap. Our parser achieves the best published results to date on the SemEval’07 FrameNet task (Baker et al., 2007). 2 Resources and Task We consider frame-semantic parsing resources. 2.1 FrameNet Lexicon The FrameNet lexicon is a taxonomy of manually identified general-purpose frames for English.1 Listed in the lexicon with each frame are several lemmas (with part of speech) that can denote the frame or some aspect of it—these are called lexical units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may see a target in new data that does not correspond to an LU for the frame it evokes. Each frame de"
N10-1138,boas-2002-bilingual,0,0.0220307,"7). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over t"
N10-1138,erk-pado-2006-shalmaneser,0,0.128834,"Missing"
N10-1138,W03-1007,0,0.0683486,"a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’"
N10-1138,C04-1134,0,0.0348662,"system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art a"
N10-1138,E09-1026,0,0.0300148,"Missing"
N10-1138,J02-3001,0,0.820262,"riefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, s"
N10-1138,S07-1048,0,0.547847,"easure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one. We present precision, recall, and F1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. More details can be found in Baker et al. (2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel’s randomized parsing evaluation comparator.3 2.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and Nugues (2007, hereafter J&N’07), the best system in the SemEval’07 shared task. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frameevoking words to cover unknown words, and then 3 http://www.cis.upenn.edu/˜dbikel/ software.html#comparator 950 TARGET I DENTIFICATION Our technique (§3) Baseline: J&N’07 P 89.92 87.87 R 70.79 67.11 F1 79.21 76.10 Table 3. Target identification results for our system and the baseline. Scores in bold denote significant improvements over the baseline (p &lt; 0.05). us"
N10-1138,D08-1008,0,0.010832,"dels trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame. These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification (§3), frame identification (§4, not unlike wordsense disambiguation), and argument identification (§5, not unlike semantic role labeling). The standard evaluation"
N10-1138,kingsbury-palmer-2002-treebank,0,0.137074,".57 68.46 49.68 42.82 46.00 58.08 38.76 46.49 51.59 35.44 42.01 partial frame matching P R F1 57.85 49.86 53.56 62.76 41.89 50.24 56.01 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and"
N10-1138,J93-2004,0,0.0372018,"1 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004"
N10-1138,J08-2001,0,0.0694048,"Missing"
N10-1138,P09-1003,0,0.164686,"ict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to quest"
N10-1138,P05-1012,0,0.00937302,"s. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by eac"
N10-1138,C04-1100,0,0.0279741,"ication features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard J"
N10-1138,H05-1108,0,0.0518989,"Missing"
N10-1138,D08-1048,0,0.0129921,"Missing"
N10-1138,W96-0213,0,0.500149,"ta to create a development set for tuning model hyperparameters. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the ar"
N10-1138,D07-1002,0,0.0551122,"ious 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reit"
N10-1138,W04-2008,0,0.148854,"(Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), perfo"
N10-1138,P03-1002,0,0.025474,"ught to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reiter for software, data, evaluation scripts, and methodological details. We thank the re"
N10-1138,D09-1029,0,0.0207559,"rames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on Se"
N13-1039,P11-1087,0,0.0199181,"of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” 7 http://www.ark.c"
N13-1039,D11-1052,0,0.403712,": u = “you”) and prepositions (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many"
N13-1039,J92-4003,0,0.437604,"Missing"
N13-1039,E03-1009,0,0.019282,"lusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the"
N13-1039,P11-1137,1,0.142556,"g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as"
N13-1039,N13-1037,0,0.383435,"rs corresponding to some of these words. This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. Introduction Online conversational text, typified by microblogs, chat, and text messages,1 is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text"
N13-1039,D11-1142,0,0.0448156,"g minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and §6.2. 6 Experiments We are primarily concerned with performance on our annotated datasets described"
N13-1039,P11-2008,1,0.727026,"Missing"
N13-1039,P11-1038,0,0.422239,"to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexic"
N13-1039,P08-1068,0,0.420847,"t for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha ha"
N13-1039,P11-1037,0,0.0938106,"e are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for b"
N13-1039,P12-3005,0,0.246426,"Missing"
N13-1039,J93-2004,0,0.061082,"Missing"
N13-1039,N10-1004,0,0.161793,"regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: • an open-source part-of-"
N13-1039,W96-0213,0,0.684593,"community: • an open-source part-of-speech tagger for online conversational text (§2); • unsupervised Twitter word clusters (§3); • an improved emoticon detector for conversational text (§4); 380 Proceedings of NAACL-HLT 2013, pages 380–390, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1 , and is parameterized by a multiclass logistic regression: p(yt = k |y t−1 , x, t; β) ∝   P (obs) (trans) exp βyt−1 ,k + j βj,k fj (x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K"
N13-1039,D11-1141,0,0.44769,"tactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results i"
N13-1039,P05-1044,1,0.328937,"demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about t"
N13-1039,N12-1052,0,0.100512,"s—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa aha"
N13-1039,P10-1040,0,0.242904,"analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha"
N13-1039,P02-1053,0,0.00928357,"y inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and"
N13-1039,petrov-etal-2012-universal,0,\N,Missing
N13-1076,N10-1083,0,0.0254529,"ected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupe"
N13-1076,D08-1092,0,0.0201754,"s starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This"
N13-1076,W10-2906,0,0.0157732,"5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not dep"
N13-1076,J07-2003,0,0.0858748,"Missing"
N13-1076,W06-1670,0,0.725634,"Missing"
N13-1076,W03-1022,0,0.189249,"applications communication ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. Introduction A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities. WordNet supersense tags denote coarse semantic classes, including person and artifact (for nouns) and motion and weather (for verbs); these categories can be taken as the top level of a taxonomy. Nominal supersense tagging (Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annota"
N13-1076,P11-1061,0,0.0261324,"e morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel"
N13-1076,P10-4002,1,0.808987,"Missing"
N13-1076,elkateb-etal-2006-building,0,0.0311891,"(Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective"
N13-1076,2005.eamt-1.15,0,0.0305449,"bout topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. 665 noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (even when the decoder uses large phrasal units for translation), allowing for more precise projections.11 We suspect this is especially important in light of findings that larger phrase sizes are indicative of better translations (Gamon et al., 2005), so these are exactly the cases where we expect the translations to be valuable. 5 Conclusion To our knowledge, this is the first study of automatic Arabic supersense tagging. We have shown empirically that an MT-in-the-middle technique is most effective of several approaches that do not require labeled training data. Analysis sheds light on several challenges that would need to be overcome for better Arabic lexical semantic tagging. Acknowledgments We thank Wajdi Zaghouani for the translation of the Arabic Wikipedia MT set, Francisco Guzman and Preslav Nakov for the output of QCRI’s MT syste"
N13-1076,P05-1071,0,0.0961947,"Missing"
N13-1076,N06-2015,0,0.0206598,"hunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English,"
N13-1076,P12-1073,0,0.0128156,"is; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the train"
N13-1076,N03-1017,0,0.00971947,"Missing"
N13-1076,P07-2045,1,0.0132485,"Missing"
N13-1076,J94-2001,0,0.08432,"nstructs the Arabic-to-English mapping {1→person11 , 4→location43 , {5, 6}→artifact76 }, resulting in the tagging shown in the bottom row. weighted to favor earlier senses (presumed by lexicographers to be more frequent) and then the supersense with the greatest aggregate weight is selected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with oth"
N13-1076,P07-1123,0,0.019554,"Missing"
N13-1076,H93-1061,0,0.0657483,"Missing"
N13-1076,E12-1017,1,0.866478,"Missing"
N13-1076,P02-1040,0,0.0859619,"Missing"
N13-1076,N12-1090,0,0.0532349,"ords that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpu"
N13-1076,P08-2030,0,0.0741844,"Missing"
N13-1076,P12-2050,1,0.801661,"toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and co"
N13-1076,W04-3207,1,0.770277,"ts with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in"
N13-1076,2006.amta-papers.25,0,0.0975624,"Missing"
N13-1076,N01-1026,0,0.0399772,"t sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely du"
N13-1076,H01-1035,0,0.0177244,"(for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch),"
N13-1076,D08-1063,0,0.11663,"0 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et"
N13-1076,W05-0909,0,\N,Missing
N15-1177,attardi-etal-2010-resource,0,0.388457,"Missing"
N15-1177,J92-4003,0,0.141282,"in our data; for experiments we only consider tags occurring in train, yielding ∣Y∣ = 146. We also run a condition where the supersense refinements are collapsed, i.e. Y consists of the 8 MWE tags. This allows us to measure the impact of the supersenses on MWE identification performance. 4.5 Features We constrast three feature sets for full supersense tagging: (a) Schneider et al.’s (2014a) basic MWE features, which include lemmas, POS tags, word shapes, and whether the token potentially matches entries in any of several multiword lexicons; (b) the basic MWE features plus the Brown clusters (Brown et al., 1992) used by Schneider et al. (2014a); and (c) the basic MWE features and Brown clusters, plus several new features shown in figure 4. Chiefly, these new features consult the supersenses of WordNet synsets associated with words in the sentence: the first WordNet supersense feature is inspired by Ciaramita and Altun (2006) and subsequent work on supersense tagging, while the has-supersense feature is novel. There is also a feature aimed at distinguishing auxiliary verbs from main verbs, and new capitalization features take into account the capitalization of the first word in the sentence and the ma"
N15-1177,W06-1670,0,0.720384,"synsets are associated with lexical entries, the supersense categories are unlexicalized. The N : PERSON category, for instance, contains synsets for both principal and student. A different sense of principal falls under N : POSSESSION. As far as we are aware, the supersenses were originally intended only as a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2"
N15-1177,W03-1022,0,0.128646,"e of the names overlap between the noun and verb inventories, but they are to be considered separate categories; hereafter, we will distinguish the noun and verb categories with prefixes, e.g. N : COGNITION vs. V: COGNITION. Though WordNet synsets are associated with lexical entries, the supersense categories are unlexicalized. The N : PERSON category, for instance, contains synsets for both principal and student. A different sense of principal falls under N : POSSESSION. As far as we are aware, the supersenses were originally intended only as a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their ow"
N15-1177,W02-1001,0,0.03984,": SOCIAL N : PERSON I B I ¯ I O B o Figure 3: Tagging for part of the lexical semantic analysis depicted in figure 1. Note that for nominal and verbal MWEs, the supersense label is only attached to the first tag of the expression. Though our focus in this paper is on English, automatic supersense tagging has also been explored in Italian (Picca et al., 2008, 2009; Attardi et al., 2010, 2013; Rossi et al., 2013), Chinese (Qiu et al., 2011), and Arabic (Schneider et al., 2013). 4.2 Model Like Ciaramita and Altun (2006) and Schneider et al. (2014a), we train a first-order structured perceptron (Collins, 2002) with averaging. This is a standard discriminative modeling setup, involving: a linear scoring function over features of input–output pairs; a Viterbi search to choose the highest-scoring valid output tag sequence given the input; and an online learning algorithm that makes M passes through the training data, searching for the best tagging given the current model and updating the parameters (linear feature weights) where the best tagging doesn’t match the gold tagging. With a first-order Markov assumption and tagset Y, the Viterbi search for a sentence x requires O(∣Y∣2 ⋅ ∣x∣) runtime. The dat"
N15-1177,W13-3511,0,0.0641405,"neralize beyond lexical items lead to better supersense labeling. The best model has access to supersense information in the WordNet lexicon; it is 4 F1 points better at choosing the correct class label than its nearest competitor, which relies on word clusters to abstract away from individual lexical items. Nouns, verbs, and auxiliaries all see improvements. We also inspect the learned parameters. The highest-weighted parameters suggest that the best model relies heavily on the supersense lookup features, whereas the second-best model—lacking those—in large part relies on Brown clusters (cf. Grave et al., 2013). The auxiliary verb vs. main verb feature in the best model is highly weighted as well, helping to distinguish between `a and V: STATIVE. Polysemy. We have motivated the task of supersense tagging in part as a coarse form of word sense disambiguation. Therefore, it is worth investigating how well the learned model manages to choose the correct supersense for nouns and verbs that are ambiguous in the data. A handful of lemmas in test have at least two different supersenses predicted several times; an examination of four such lemmas in table 3 shows that for three of them the tagging accuracy e"
N15-1177,S14-1001,0,0.419509,"semantic classes. Here we build on prior work with an inventory of semantic classes (for nouns and verbs) known as supersenses. The 41 supersenses resemble the types used for named entities (PERSON, LOCATION, etc.), but are more general, with semantic categories relevant to common nouns and verbs as well. As a result, their application to Because most supersense tagging studies have worked with data originally annotated for finegrained WordNet senses, then automatically mapped to supersenses, the resulting systems have been tied to the lexical coverage of WordNet. Schneider et al. (2012) and Johannsen et al. (2014) overcame this limitation in part by annotating supersenses directly in text; thus, nouns and verbs not in WordNet were not neglected. However, the issue of which units ought to receive supersenses has not been addressed satisfactorily. We argue that the semantically holistic nature of multiword expressions (MWEs) including idioms, light verb constructions, verb-particle constructions, and many compounds (Baldwin and Kim, 2010) means that they should be considered as units for manual and automatic supersense tagging. Below, we motivate the need for an integrated representation for broad-covera"
N15-1177,W03-1807,0,0.0578435,"ita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2005) did investigate MWEs in the context of a lexical tagger with a finer-grained taxonomy of semantic classes.) Consider these examples from online reviews: (1) IT IS NOT A HIGH END STEAK HOUSE (2) The white pages allowed me to get in touch with parents of my high school friends so that I could track people down one by one HIGH END functions as a unit to mean ‘sophisticated, expensive’. (It is not in WordNet, though NLTK (Bird et al., 2009) returns a synset’s lexicographer file. A subtle difference is that a special file called noun.Tops contains each noun supersense’s root synset (e.g., g"
N15-1177,W09-3531,0,0.17588,"Missing"
N15-1177,picca-etal-2008-supersense,0,0.894363,"a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2005) did investigate MWEs in the context of a lexical tagger with a finer-grained taxonomy of semantic classes.) Consider these examples from online reviews: (1) IT IS NOT A HIGH END STEAK HOUSE (2) The white pages allowed me to get in touch with parents of my high school friends so that I could track"
N15-1177,W95-0107,0,0.199567,"Missing"
N15-1177,Q14-1016,1,0.119996,"ANGE 274 fix day EMOTION 249 love experience PERCEPTION 143 see review CONSUMPTION 93 have price BODY 82 get. . . done quality CREATION 64 cook amount CONTACT 46 put dog COMPETITION 11 win hair WEATHER 0 — pain all 15 VSSTs 7806 flower portion N/A (see §3.2) oil `a 1191 have discomfort ` 821 anyone process `j 54 fried reason ∗ result COMMUNIC . is short for square COMMUNICATION tree stuff Table 1: Summary of noun and verb supersense categories. Each entry shows the label along with the count and most frequent lexical item in the STREUSLE corpus. enrich the MWE annotations of the CMWE corpus1 (Schneider et al., 2014b), are publicly released under the name STREUSLE.2 This includes new guidelines for verb supersense annotation. Our open-source tagger, implemented in Python, is available from that page as well. 2 Background: Supersense Tags WordNet’s supersense categories are the top-level hypernyms in the taxonomy (sometimes known as semantic fields) which are designed to be broad enough to encompass all nouns and verbs (Miller, 1990; Fellbaum, 1990).3 1 2 http://www.ark.cs.cmu.edu/LexSem/ Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions 3 WordNet synset entries were"
N15-1177,N13-1076,1,0.694134,": PERSON O O B N : GROUP I B N : COMMUNICATION I O V: COGNITION O friends so that I could track people down one by one ¯ ¯ N : PERSON O O O O V : SOCIAL N : PERSON I B I ¯ I O B o Figure 3: Tagging for part of the lexical semantic analysis depicted in figure 1. Note that for nominal and verbal MWEs, the supersense label is only attached to the first tag of the expression. Though our focus in this paper is on English, automatic supersense tagging has also been explored in Italian (Picca et al., 2008, 2009; Attardi et al., 2010, 2013; Rossi et al., 2013), Chinese (Qiu et al., 2011), and Arabic (Schneider et al., 2013). 4.2 Model Like Ciaramita and Altun (2006) and Schneider et al. (2014a), we train a first-order structured perceptron (Collins, 2002) with averaging. This is a standard discriminative modeling setup, involving: a linear scoring function over features of input–output pairs; a Viterbi search to choose the highest-scoring valid output tag sequence given the input; and an online learning algorithm that makes M passes through the training data, searching for the best tagging given the current model and updating the parameters (linear feature weights) where the best tagging doesn’t match the gold t"
N15-1177,P12-2050,1,0.898777,"nings is with coarse-grained semantic classes. Here we build on prior work with an inventory of semantic classes (for nouns and verbs) known as supersenses. The 41 supersenses resemble the types used for named entities (PERSON, LOCATION, etc.), but are more general, with semantic categories relevant to common nouns and verbs as well. As a result, their application to Because most supersense tagging studies have worked with data originally annotated for finegrained WordNet senses, then automatically mapped to supersenses, the resulting systems have been tied to the lexical coverage of WordNet. Schneider et al. (2012) and Johannsen et al. (2014) overcame this limitation in part by annotating supersenses directly in text; thus, nouns and verbs not in WordNet were not neglected. However, the issue of which units ought to receive supersenses has not been addressed satisfactorily. We argue that the semantically holistic nature of multiword expressions (MWEs) including idioms, light verb constructions, verb-particle constructions, and many compounds (Baldwin and Kim, 2010) means that they should be considered as units for manual and automatic supersense tagging. Below, we motivate the need for an integrated rep"
N15-1177,schneider-etal-2014-comprehensive,1,0.123901,"ANGE 274 fix day EMOTION 249 love experience PERCEPTION 143 see review CONSUMPTION 93 have price BODY 82 get. . . done quality CREATION 64 cook amount CONTACT 46 put dog COMPETITION 11 win hair WEATHER 0 — pain all 15 VSSTs 7806 flower portion N/A (see §3.2) oil `a 1191 have discomfort ` 821 anyone process `j 54 fried reason ∗ result COMMUNIC . is short for square COMMUNICATION tree stuff Table 1: Summary of noun and verb supersense categories. Each entry shows the label along with the count and most frequent lexical item in the STREUSLE corpus. enrich the MWE annotations of the CMWE corpus1 (Schneider et al., 2014b), are publicly released under the name STREUSLE.2 This includes new guidelines for verb supersense annotation. Our open-source tagger, implemented in Python, is available from that page as well. 2 Background: Supersense Tags WordNet’s supersense categories are the top-level hypernyms in the taxonomy (sometimes known as semantic fields) which are designed to be broad enough to encompass all nouns and verbs (Miller, 1990; Fellbaum, 1990).3 1 2 http://www.ark.cs.cmu.edu/LexSem/ Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions 3 WordNet synset entries were"
N15-1177,tsvetkov-etal-2014-augmenting-english,1,0.721284,"014b) multiword expression annotations. Schneider et al. (2012) offered a methodology for noun supersense annotation in Arabic Wikipedia, and predicted that it would port well to other languages and domains. Our experience with English web reviews has borne this out. We generally adhered to the same supersense annotation process (for nouns); the most important difference was that the data had already been annotated for MWEs, and supersense labels apply to any strong5 MWEs as a whole. 4 Future supersense annotation schemes for additional parts of speech could be assimilated into our framework. Tsvetkov et al. (2014) take a step in this direction for adjectives. 5 The CMWE corpus distinguishes strong and weak MWEs— essentially, the former are strongly entrenched and likely noncompositional, whereas weak MWEs are merely statistically collocated. See Schneider et al. (2014b) for details. Because they are deemed semantically compositional, weak MWEs do not receive a supersense as a whole. 1539 The same annotators had already done the MWE annotation; whenever they encountered an apparent mistake from an earlier stage (usually an oversight), they were encouraged to correct it. Our annotation interface supports"
N15-4006,W07-1424,1,\N,Missing
N15-4006,ferrandez-etal-2010-aligning,1,\N,Missing
N18-1088,D15-1041,1,0.919347,"overed by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we nonetheless designed a pipeline to parse raw tweets into Universal Dependencies. Our pipeline includes: a bidirectional LSTM (bi-LSTM) tokenizer, a word cluster–enhanced POS tagger (following Owoputi et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to"
N18-1088,D16-1211,1,0.907114,"Missing"
N18-1088,J92-4003,0,0.519758,"Missing"
N18-1088,de-marneffe-etal-2014-universal,0,0.0901755,"Missing"
N18-1088,R13-1026,0,0.167981,"Missing"
N18-1088,D16-1180,1,0.877777,"e updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Not"
N18-1088,K17-3002,0,0.103467,"ination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble int"
N18-1088,K17-3001,0,0.199799,"a tweet. We therefore treat the whole expression as non-syntactic, including assigning the other (X) part of speech to both RT and @coldplay, attaching the at-mention to RT with the discourse label and the colon to RT with the punct(uation) label, and attaching RT to the predicate of the following sentence. course while they used parataxis. In referential URLs, we use list (following the precedent of UD_English-EWT) while they used dep. Our choice of discourse for sentiment emoticons is inspired by the observation that emoticons are annotated as discourse by UD_English-EWT; Sanguinetti et al. (2017) used the same relation for the emoticons. Retweet constructions and truncated words were not explicitly touched by Sanguinetti et al. (2017). Judging from the released treebank8 , the RT marker, at-mention, and colon in the retweet construction are all attached to the predicate of the following sentence with dep, vocative:mention and punct. We expect that the official UD guidelines will eventually adopt standards for these constructions so the treebanks can be harmonized. Constructions handled by UD. A number of constructions that are especially common in tweets are handled by UD conventions:"
N18-1088,N13-1037,0,0.0477398,"Missing"
N18-1088,P16-1101,0,0.076965,"Missing"
N18-1088,P14-5010,0,0.00286153,"s the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3"
N18-1088,J93-2004,0,0.062664,"to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in both accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled"
N18-1088,W13-3711,0,0.0175481,"rimental results. Our preliminary results showed that our model trained on the combination of UD_English-EWT and T WEEBANK V 2 outperformed the one trained only on the UD_EnglishEWT or T WEEBANK V 2, consistent with previous work on dialect treebank parsing (Wang et al., 2017). So we trained our tokenizer on the training portion of T WEEBANK V 2 combined with the UD_English-EWT training set and tested on the T WEEBANK V 2 test set. We report F1 scores, combining precision and recall for token identification. Table 3 shows the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems a"
N18-1088,P11-2008,1,0.897627,"Missing"
N18-1088,C12-1059,0,0.0394777,"Missing"
N18-1088,N13-1039,1,0.950015,"Missing"
N18-1088,Q13-1033,0,0.0408296,"Missing"
N18-1088,D14-1162,0,0.0814147,"Missing"
N18-1088,D17-1256,0,0.437481,"Missing"
N18-1088,petrov-etal-2012-universal,0,0.0767282,"Missing"
N18-1088,P15-1119,1,0.752001,"ggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we"
N18-1088,D17-1035,0,0.0171475,"hich should help mitigate the challenge of spelling variation. We encourage the reader to refer their paper for more details about the model. In our initial experiments, we train our parser on the combination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowe"
N18-1088,D11-1141,0,0.495674,"Missing"
N18-1088,W17-6526,0,0.480518,"oth accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled dependencies to a portion of the data annotated with POS tags by Gimpel et al. (2011) and Owoputi et al. (2013) after rule-based tokenization (O’Connor et al., 2010). Kong et al. also contributed a system for parsing; we defer th"
N18-1088,D16-1139,0,0.0215692,"r comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Note that training a transition-based parser typically involves the transformation of the training data into a sequence of “oracle” state-action pairs. Let q(a |s) denote the distilled model’s probability of an action a given parser state s; let p(a |s) be the probability under the ensemble (i.e., the average of the 20 separately-trained ensemble members). To train the distilled model, we minimize the interpolation between their distillation loss and the conventional log loss: XX argminq α −p(a |si ) · log q(a |si ) Table 6: Dependency parser comparison on T WEE BANK V 2 test set,"
N18-1088,Q16-1023,0,0.108829,"Missing"
N18-1088,W13-2307,1,0.929727,"Missing"
N18-1088,D14-1108,1,0.404607,"uti et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in bot"
N18-1088,K17-3009,0,0.0304172,"oreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3 Table 5: Owoputi et al. (2013) POS tagging performance with automatic tokenization on the T WEEBANK V 2 test set. Experimental results. We tested the POS tagger"
N18-1088,P17-1159,0,0.100495,"d largely following conventions suggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines"
N18-1088,W03-3023,0,0.271111,"Missing"
N18-1088,L16-1262,0,\N,Missing
N18-1106,D15-1198,0,0.0279274,"lso pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location"
N18-1106,W13-2322,1,0.877311,"der Linguistics and Computer Science Georgetown University Washington, DC, USA {k.i.szubert@sms, alopez@inf}.ed.ac.uk nathan.schneider@georgetown.edu Abstract 1 cat My sun the c cat s sun i i Figure 1: “My cat lies in the sun.” An alignment between the dependency parse (left) and AMR (right). Nodes participating in lexical alignments are marked with boxes, but the links between them are not displayed. Structural alignments are colour-coded and linked by dotted lines. Sense numbers for concepts that are PropBank frames are omitted for brevity. Introduction Abstract Meaning Representation (AMR; Banarescu et al., 2013) is a popular framework for annotating whole sentence meaning. An AMR annotation is a directed, usually acyclic graph in which nodes represent entities and events, and edges represent relations between them, as on the right in figure 1.1 AMR annotations include no explicit mapping between elements of an AMR and the corresponding elements of the sentence that evoke them, and this presents a challenge to developers of machine learning systems that parse sentences to AMR or generate sentences from AMR, since they must 1 l lie lies Abstract Meaning Representation (AMR) annotations are often assume"
N18-1106,W15-0128,0,0.0385416,"9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure consists of a single node, we say its size is 0. The configuration of an alignment is then the pair of sizes for its AMR and DG sides; for example, an alignment with 1 AMR edge and"
N18-1106,P13-2131,0,0.116592,"Missing"
N18-1106,D14-1082,0,0.0382869,"Missing"
N18-1106,E17-1053,0,0.290687,". By principle of minimality we infer that some structural difference between the graphs prevented those relations from aligning individually. One measure of similarity between AMR and DG graphs is the configuration of the most complex subgraph alignment between them. Configuration a:b is higher than c:d if a + b > c + d. However, all configurations involving 0 are lower than those which do not. A maximum of 1:1 means the graphs have only node-to-node, node-to-edge, and edge-toedge alignments, rendering the graphs isomorphic (ignoring edge directions and unaligned nodes). In 11 In particular, Chen and Palmer (2017) align dependency paths to AMR edges. However, their evaluation only considers node-to-node alignment, and their code and data are not available for comparison at the time of this writing. 1174 named entities 2:0 3:1 4:2 1:1 5:2 other 112 44 7 6 4 20 total: 193 coordination semantic decomposition quantities & dates 2:2 3:4 3:3 4:3 3:2 other 1:0 2:0 2:1 4:1 3:1 other 2:1 3:0 1:0 3:2 8:2 other 30 14 13 5 5 50 117 32 14 11 6 6 15 84 15 5 4 3 1 0 28 other 0:0 1:1 1:2 1:0 2:2 other 1946 1002 220 42 42 13 3385 overall 0:0 1:1 1:2 2:0 2:2 other 1946 1046 244 127 83 361 3807 Table 2: Frequency of alig"
N18-1106,E17-1051,0,0.0396558,"inding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1. So edge alignments allow ISI to explain more of the AMR structure tha"
N18-1106,N16-1087,0,0.0731189,"Missing"
N18-1106,P14-1134,0,0.748519,"tations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic"
N18-1106,C14-1111,0,0.0588838,"Missing"
N18-1106,P17-1104,0,0.0252127,". sents complex configurations avg. max words config. 1:1 1:2 3:1 2:0 1:3 other 18 16 12 6 5 9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure consists of a single node, we say its size is 0. The configuration of an alignment is then th"
N18-1106,P17-1014,0,0.121621,"Missing"
N18-1106,P14-5010,0,0.00248589,"it, and show that this new task is much more difficult than lexical alignment (§4). We also show how our data can be used to analyze errors made by an AMR parser (§5). We make our annotated data and aligner freely available for further research.3 2 Aligning AMR to dependency syntax Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006). We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014). Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head, 3 https://github.com/ida-szubert/amr_ud resulting in dependency graphs (DGs).4 Our alignment guidelines generalize ideas present in the existing frameworks. We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa. For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraph"
N18-1106,D16-1183,0,0.0380923,"MR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1. So edge alignments allow ISI to explain more of"
N18-1106,S14-2008,0,0.0730881,"Missing"
N18-1106,E17-1035,0,0.0305119,"Missing"
N18-1106,D14-1048,0,0.320962,"should not be assumed that AMR and DG representations of a sentence are, or could trivially be made to be, isomorphic. It is worth noting that our analysis suggests that DG and AMR could be made more similar by applying simple transformations targeting problematic constructions like coordination and named entities. 4 Evaluation of automatic aligners We use our annotations to measure the accuracy of AMR aligners on specific phenomena that were inexpressible in previous annotation schemes. Our experiments evaluate the JAMR heuristic aligner (Flanigan et al., 2014), the ISI statistical aligner (Pourdamghani et al., 2014), and a heuristic rulebased aligner that we developed specifically for 12 An AMR concept evoked by a preposition usually dominates the structure (afterÐÐ→date-entityÐÐÐ→nineties), which is at odds with UD’s prepositions-as-case-markers polcase icy (ninetiesÐÐ→after). op1 decade structural alignment. 4.1 Rule-based aligner Our aligner operates in two passes: one for lexical alignment and one for structural alignment. Lexical alignment algorithm. AMR concepts are cognate with English words, so we align them by lexical similarity. This algorithm does not make use of the DG. Before alignment, we r"
N18-1106,W16-6603,0,0.116843,"Missing"
N18-1106,Q16-1010,0,0.0372812,"s. 10 http://universaldependencies.org/u/overview/ enhanced-syntax.html max # config. sents complex configurations avg. max words config. 1:1 1:2 3:1 2:0 1:3 other 18 16 12 6 5 9 8.7 13.1 13.4 5.8 13.2 15.2 total: 66 11.6 2:2 2:3 3:2 3:4 3:3 other # sents avg. words 21 14 13 12 10 64 12.9 16.0 16.8 20.3 19.1 20.9 134 18.0 Table 1: Number of sentences whose highest alignment configurations is max config. 2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017). However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style. Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence. Are AMRs and dependency graphs structurally similar? We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence. We define the size of a subgraph as the number of edges it contains. If a structure co"
N18-1106,L16-1376,0,0.0294195,"(§4). We also show how our data can be used to analyze errors made by an AMR parser (§5). We make our annotated data and aligner freely available for further research.3 2 Aligning AMR to dependency syntax Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006). We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014). Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head, 3 https://github.com/ida-szubert/amr_ud resulting in dependency graphs (DGs).4 Our alignment guidelines generalize ideas present in the existing frameworks. We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa. For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraphs in the dependency graph because of pronominal coreference. In the other direction,"
N18-1106,N15-1040,0,0.0450682,"syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax. For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus. first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2 This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment. Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment. In ISI alignments, edges often align to syntactic function words: for"
N18-1106,D16-1177,0,0.131476,"Missing"
P11-2008,C10-2005,0,0.591162,"toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T"
P11-2008,W10-0713,0,0.446157,"enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate t"
P11-2008,J93-2004,0,0.0664899,"erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. In this paper, we produce an English POS tagger that is designed especially for Twitter data. Our contributions are as follows: • we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we prov"
P11-2008,petrov-etal-2012-universal,1,0.297019,"Missing"
P11-2008,N10-1020,0,0.140487,"yD licenseN and& #2$ notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a"
P11-2008,N10-1100,0,0.180483,"notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour"
P11-2008,N03-1033,0,0.210842,"Missing"
P11-2008,P10-1040,0,0.263127,"butional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ USVT , where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. M ETAPH : Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7 1 α = 100 , C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8 Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (War"
P12-2050,attardi-etal-2010-resource,0,0.557965,"Missing"
P12-2050,P98-1013,0,0.0338496,"r require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (o"
P12-2050,W06-1670,0,0.786017,"Missing"
P12-2050,W03-1022,0,0.0596354,"(Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 N"
P12-2050,P05-1004,0,0.00884402,"al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work"
P12-2050,elkateb-etal-2006-building,0,0.102714,"Missing"
P12-2050,N06-2015,0,0.015385,"in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johns"
P12-2050,kingsbury-palmer-2002-treebank,0,0.0369554,"upersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and"
P12-2050,C02-1150,0,0.0134343,"SONs. This lumping property might be expected to give too much latitude to annotators; yet we find that in practice, it is possible to elicit reasonable inter-annotator agreement, even for a language other than English. We encapsulate our interpretation of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language; our annotators had no prior expertise in linguistics or linguistic annotation. Finally, we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering (Li and Roth, 2002) to animacy hierarchy representation for corpus linguistics (Zaenen et al., 2004). We believe the interpretation of the SSTs adopted here can serve as a single starting point for diverse resource engineering efforts and applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are"
P12-2050,H93-1061,0,0.614672,"applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are intended to be language-neutral. A more systematic breakdown, formulated as a 43-rule decision list, is included with the corpus.5 In developing these guidelines we consulted English WordNet (Fellbaum, 1998) and SemCor (Miller et al., 1993) for examples and synset definitions, occasionally making simplifying decisions where we found distinctions that seemed esoteric or internally inconsistent. Special cases (e.g., multiword expressions, anaphora, figurative 5 For example, one rule states that all man-made structures (buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs. 254 language) are addressed with additional rules. 3 Arabic Wikipedia Annotation The annotation in this work was on top of a small corpus of Arabic Wikipedia articles that had already been annotated for named entities (Mohit et al., 2012). Here we use t"
P12-2050,E12-1017,1,0.890745,"Missing"
P12-2050,passonneau-etal-2010-word,0,0.021609,"o a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for d"
P12-2050,picca-etal-2008-supersense,0,0.610731,"Missing"
P12-2050,W09-3531,0,0.371255,"two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work in supersense tagging used text with finegrained sense annotations that were then coarsened to SSTs. 4 The noun/verb d"
P12-2050,sekine-etal-2002-extended,0,0.0260826,"s paper describes coarse lexical semantic annotation of Arabic Wikipedia articles subject to these constraints. Traditional lexical semantic representations are either narrow in scope, like named entities,1 or make reference to a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr"
P12-2050,W04-0216,0,0.0213529,"Missing"
P12-2050,C98-1013,0,\N,Missing
P14-5021,diaz-de-ilarraza-etal-2004-abar,0,0.067333,"Missing"
P14-5021,J93-2004,0,0.0477748,"Missing"
P14-5021,N03-4009,0,0.0449942,"units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has text as the exclu7 Conclusion While the creation of high-quality, highly specified, syntactically annotated corpora is a goal that is out of reach for most languages and genres, GFL-Web facilitates a rapid annotation workflow within a simple framework for dependency syntax. More information on FUDG/GFL is available at http://www.ark.cs.cmu.edu/FUDG/, and the source code fo"
P14-5021,W13-2307,1,0.566479,"Missing"
P14-5021,E12-2021,0,0.0872351,"Missing"
P14-5021,P13-4001,0,0.0377836,"Missing"
P14-5021,P13-1023,0,0.0264748,"013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has te"
P14-5021,N13-1049,0,0.0175252,"UDG/GFL annotation. The simple interface provides instantaneous feedback on the wellformedness of a GFL annotation, and by wrapping Schneider et al.’s notation parsing and rendering software, gives a user-friendly visualization of the annotated sentence. The tool itself is lightweight, multi-user, and easily deployed with few software dependencies. Sentences are assigned to annotators via an administrative interface, which also records progress and provides for a text dump of 1 These can be especially effective when some details of the syntax can be predicted automatically with high accuracy (Alkuhlani et al., 2013). 121 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics (a) @Bryan_wright11 i lost all my contacts , smh . (b) Texas Rangers are in the World Series ! Rangers !!!!!!!!! http://fb.me/D2LsXBJx Go Figure 1: FUDG annotation graphs for two tweets. • Multiword units may be joined to form composite lexical nodes (e.g., World_Series in figure 1b). These nodes are not annotated with any internal syntactic structure. • Tokens that are used i"
P14-5021,W13-2322,1,0.771883,"GFL (Schneider et al., 2013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency anno"
P14-5021,2005.eamt-1.11,0,0.0485334,"Missing"
P14-5021,P11-2008,1,0.874453,"Missing"
P14-5021,P09-2056,0,0.0238252,"ies et al., 2012), the Penn Arabic Treebank (Maamouri et al., 2004), and the Prague dependency treebanks (Hajiˇc, 1998; ˇ Cmejrek et al., 2005), have relied on expert linguists to produce carefully-controlled annotated data. Because this process is costly, such annotation projects have been undertaken for only a handful of important languages. Therefore, developing syntactic resources for less-studied, lowerresource, or politically less important languages and genres will require alternative methods. To address this, simplified annotation schemes that trade cost for detail have been proposed (Habash and Roth, 2009).1 Although GFL offers a number of conveniences to annotators, the text-based UI is limiting: the existing tools require constant switching between a text editor and executing commands, and there are no tools for managing a large-scale annotation effort. Additionally, user interface research has found marked preferences for and better performance with graphical tools relative to text-based interfaces—particularly for less computer-savvy users (Staggers and Kobus, 2000). In this paper, we present the GFL-Web tool, a web-based interface for FUDG/GFL annotation. The simple interface provides inst"
P14-5021,vilnat-etal-2010-passage,0,\N,Missing
P15-2036,S07-1018,0,0.0348331,"nce relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentence"
P15-2036,P98-1013,0,0.803968,"TIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: ‡ t pan tici par ant r cip nce a"
P15-2036,W04-0817,0,0.0210726,"ING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually"
P15-2036,bonial-etal-2014-propbank,0,0.057978,"Missing"
P15-2036,W13-5503,0,0.0575005,"Missing"
P15-2036,J14-1002,1,0.781256,"ant, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annot"
P15-2036,S12-1029,1,0.891611,"model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at mo"
P15-2036,P11-1144,1,0.598328,"sk.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs an"
P15-2036,N12-1086,1,0.904091,"Missing"
P15-2036,P07-1033,0,0.0592643,"termined by frame identification). We use the heuristic procedure described by (Das et al., 2014) for extracting candidate argument spans for the predicate; call this spans(x, p, f ). spans always includes a special span denoting an empty or nonovert role, denoted ∅. For each candidate argument a ∈ spans(x, p, f ) and each role r, a binary feature vector φ (a,x, p, f ,r) is extracted. We use the feature extractors from (Das et al., 2014) as a baseline, adding additional ones in our experiments (§3.2– §3.4). Each a is given a real-valued score by a linear model: Domain Adaptation and Exemplars Daumé (2007) proposed a feature augmentation approach that is now widely used in supervised domain adaptation scenarios. We use a variant of this approach. Let Dex denote the exemplars training data, and Dft denote the full text training data. For every feature φ (a,x, p, f ,r) in the base model, we add a new feature φft (⋅) that fires only if φ (⋅) fires and x ∈ Dft . The intuition is that each base feature contributes both a “general” weight and a “domain-specific” weight to the model; thus, it can exhibit a general preference for specific roles, but this general preference can be fine-tuned for the dom"
P15-2036,S15-1005,0,0.0135716,"mance upon combining the best approaches. Both use full-text and exemplars for training; the first uses PropBank SRL as guide features, and the second adds hierarchy features. The best result is the 221 0.2 Acknowledgments 100 50 0 Test Examples 150 0.4 over, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneous annotations in different corpora will be necessary to build automatic semantic analyzers that are both accurate and robust. 0 200 400 600 800 1000 1200 1400 Frame Element, ordered by test set frequency 0.8 (a) Frequency of each role appearing in the test set. The authors are grateful to Dipanjan Das for his assistance, and to anonymous reviewers for their helpful feedback. This research has been supported by the"
P15-2036,W03-1007,0,0.023046,"annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annot"
P15-2036,J02-3001,0,0.941676,"A0 want ING is evoked by want, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We"
P15-2036,E12-1059,0,0.0207678,"Missing"
P15-2036,P14-1136,0,0.167275,"ts of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. We use the mode"
P15-2036,N06-2015,0,0.136256,"Missing"
P15-2036,S12-1016,0,0.0167416,"d T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel"
P15-2036,N13-1013,0,0.0347516,"r the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 200"
P15-2036,D14-1108,1,0.685009,"ularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonal"
P15-2036,C04-1179,0,0.0362007,"dicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train"
P15-2036,W04-0803,0,0.0508177,"espect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the docum"
P15-2036,D08-1017,1,0.640396,"case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the"
P15-2036,P09-1003,0,0.0182029,"ED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the C RIMINAL _ PROCESS frame groups together subframes A RREST, A RRAIGN MENT and T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been"
P15-2036,P08-1108,0,0.0186663,"Missing"
P15-2036,J05-1004,0,0.717357,"Missing"
P15-2036,P15-2067,0,0.0221741,"Missing"
P15-2036,J08-2005,0,0.587921,"Missing"
P15-2036,Q15-1003,0,0.23587,"domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of"
P15-2036,C98-1013,0,\N,Missing
P18-1018,P17-4019,1,0.730567,"ittle Prince, both to assess whether the scheme was applicable without major guidelines changes and to prepare the annotators for this genre. For the final annotation study, we chose chapters 4 and 5, in which 242 markables of 52 types were identified heuristically (§6.2). The types of, to, in, as, from, and for, as well as possessives, occurred at least 10 times. Annotators had the option to mark units as false positives using special labels (see §4) in addition to expressing uncertainty about the unit. For the annotation process, we adapted the open source web-based annotation tool UCCAApp (Abend et al., 2017) to our workflow, by extending it with a type-sensitive ranking module for the list of categories presented to the annotators. Annotators. Five annotators (A, B, C, D, E), all authors of this paper, took part in this study. All are computational linguistics researchers with advanced training in linguistics. Their involvement in the development of the scheme falls on a spectrum, with annotator A being the most active figure in guidelines development, and annotator E not being Labels involved in developing the guidelines and learning the scheme solely from reading the manual. Annotators A, B, an"
P18-1018,W17-6901,0,0.0631207,"OPIC ) flows from O RIG INATOR to R ECIPIENT , perhaps via an I NSTRU MENT . For AGENT , C O -AGENT , E XPERIENCER , O RIGINATOR, R ECIPIENT, B ENEFICIARY, P OS SESSOR, and S OCIAL R EL, the object of the preposition is prototypically animate. Because prepositions and possessives cover a vast swath of semantic space, limiting ourselves to 50 categories means we need to address a great many nonprototypical, borderline, and special cases. We have done so in a 75-page annotation manual with over 400 example sentences (Schneider et al., 2018). Finally, we note that the Universal Semantic Tagset (Abzianidze and Bos, 2017) defines a crosslinguistic inventory of semantic classes for content and function words. SNACS takes a similar approach to prepositions and possessives, which in Abzianidze and Bos’s (2017) specification are simply tagged REL, which does not disambiguate the nature of the relational meaning. Our categories can thus be understood as refinements to REL. 3.3 Adopting the Construal Analysis Hwang et al. (2017) have pointed out the perils of teasing apart and generalizing preposition semantics so that each use has a clear supersense label. One key challenge they identified is that the preposition i"
P18-1018,W13-2322,1,0.846996,"l., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitati"
P18-1018,C16-1256,0,0.420421,"t to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotation study or release a corpus to establish its feasibility empirically. We address that gap here. Second, 75 categories is an unwieldy number for both annotators and disambiguation systems. Some are quite specialized and extremely rare in STREUSLE 3.0, which causes data sparseness issues for supervised learning. In fact, the only published disambiguation system for preposition supersenses collapsed the distinctions to just 12 labels (Gonen and Goldberg, 2016). Hwang et al. (2017) remarked that solving the aforementioned problem could remove the need for many of the specialized categories and make the taxonomy more tractable for annotators and systems. We substantiate this here, defining a new hierarchy with just 50 categories (SNACS, §3) and providing disambiguation results for the full set of distinctions. Finally, given the semantic overlap of possessive case and the preposition of, we saw an opportunity to broaden the application of the scheme to include possessives. Our reannotated corpus, STREUSLE 4.0, thus has supersense annotations for over"
P18-1018,C10-2052,0,0.0584241,"Missing"
P18-1018,L18-1242,1,0.843466,". Three labels never appear in the annotated corpus: T EMPORAL from the C IRCUMSTANCE hierarchy, and PARTI CIPANT and C ONFIGURATION which are both the highest supersense in their respective hierarchies. While all remaining supersenses are attested as scene roles, there are some that never occur as functions, such as O RIGINATOR, which is most often realized as P OSSESSOR or S OURCE, and E XPERI ENCER . It is interesting to note that every subtype of C IRCUMSTANCE (except T EMPORAL) appears as both scene role and function, whereas many of the subtypes of the other two hierarchies are lim189 8 Blodgett and Schneider (2018) detail the extension of the scheme to possessives. 9 In the corpus, lexical expression tokens appear alongside a lexical category indicating which inventory of supersenses, if any, applies. SNACS-annotated units are those with ADP (adposition), PP, PRON . POSS (possessive pronoun), etc., whereas DISC (discourse) and CCONJ expressions do not receive any supersense. Refer to the STREUSLE README for details. ited to either role or function. This reflects our view that prepositions primarily capture circumstantial notions such as space and time, but have been extended to cover other semantic rela"
P18-1018,P11-2056,0,0.0371708,"Missing"
P18-1018,D09-1047,0,0.0768296,"a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to"
P18-1018,S17-1022,1,0.841894,"Missing"
P18-1018,P14-1120,0,0.460243,"omberg, 2010). Possessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on c"
P18-1018,L16-1630,0,0.0318812,"2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016)."
P18-1018,S07-1005,0,0.14038,"Missing"
P18-1018,J09-2002,0,0.169265,"Missing"
P18-1018,P14-5010,0,0.00262509,"tems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters. Gold tokenization was used throughout. Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation—i.e., tokens with special labels (see §4) were excluded. To test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0 (Manning et al., 2014).13 Named entity tags from the default 12-class CoreNLP model were used in all conditions. 6.2 Target Identification §3.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as 13 The CoreNLP parser was trained on all 5 genres of the English Web Treebank—i.e., a superset of our training set. Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested"
P18-1018,W04-2609,0,0.110573,"ll be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbas"
P18-1018,J05-1004,0,0.433013,"Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotate"
P18-1018,saint-dizier-2006-prepnet,0,0.045623,"can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple le"
P18-1018,W16-1712,1,0.912313,"Missing"
P18-1018,W15-1612,1,0.845402,"oridou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banaresc"
P18-1018,D11-1012,1,0.862736,"ternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009"
P18-1018,Q13-1019,1,0.934474,"ings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitations of our approach became clear to us over time. First, as pointed out by Hwang et al. (2017), the one-label-per-token assumption in STREUSLE is flawed because it in some cases puts into conflict the semantic role of the PP with respect to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotati"
P18-1018,N09-3017,0,0.0322753,"lations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, m"
P18-1018,P13-1037,0,0.0377858,"s—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are t"
P18-1018,S07-1051,0,0.0481426,"ssessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meanin"
P18-1018,L16-1262,0,\N,Missing
P18-1210,W17-0812,0,0.0231798,"rature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relations as holding over a text – for example, since as a subordinating conjunction may, in particular contexts, signal both a TEMPORAL relation and a CAUSAL relation, rather than just one or the other (Miltsakaki et al., 2005). We are aware of only two resources that allow more than one discourse relation to be annotated between two segments – the Penn Discourse TreeBank (PDTB; Prasad et al., 2008, 2014) and, more recently, the BECauSE Corpus 2.0 (Dunietz et al., 2017). The PDTB allows multiple discourse relations of the third and fourth types noted above. It also allows them to be annotated if there is no explicit connective between a pair of segments but annotators see more than one sense relation as linking them, as in the following variant of (4): (5) It’s too far to walk. Let’s take the bus. Here a RESULT relation can be associated with an implicit token of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the main cases in which PDTB annotates multiple relations. Relevant to this p"
P18-1210,P12-1007,0,0.0215427,"ken of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the main cases in which PDTB annotates multiple relations. Relevant to this paper, the PDTB does not annotate implicit conjunction relations where there is already an explicit discourse adverbial. Thus the PDTB would either ignore the implicit RESULT relation for (1) or (incorrectly) annotate instead in (1) as conveying both SUBSTITUTION and RESULT. Moreover, while the PDTB has been used in training many (but not all) discourse parsers (Marcu, 2000; Lin et al., 2014; Feng and Hirst, 2012; Xue et al., 2015, 2016; Ji and Eisenstein, (3) a. George Bush supports big business. b. He’s sure to veto House Bill 1711. 2014), discourse parsing has for the most part igAt the level of intentions, (3a) aims to provide EVI - nored its annotations of multiple concurrent relations between clauses, except in the case of distinct DENCE for the claim in (3b), while at an informational level, (3a) serves as the CAUSE of the situa- explicit connectives expressing distinct relations. Instead, they have arbitrarily taken just a single retion in (3b). RST would force annotators to choose lation to h"
P18-1210,J86-3001,0,0.79163,"ice, researchers working in the RST framework standardly produce a single analysis of a text, with a single relational labeling, selecting the analysis that is “most plausible in terms of the perceived goals of the writer” (Mann et al., 1989, pp. 34–35). If that single analysis is later mapped into a different structure to support further processing – e.g., a binary branching tree structure – the mapping does not change the chosen relational labeling. Multiple relations may additionally hold in theories of discourse coherence that posit multiple levels of text analysis. For example, following Grosz and Sidner (1986), Moore and Pollack (1992) characterized text as having both an informational structure (relating information conveyed by discourse segments) and an intentional structure (relating the functions of those segments with respect to what the speaker is trying to accomplish through the text). The kinds of relations at the two levels are different, as can be seen in the following example from (Moore and Pollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals"
P18-1210,P14-1002,0,0.0589878,"Missing"
P18-1210,prasad-etal-2008-penn,1,0.900776,"ollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relations as holding over a text – for example, since as a subordinating conjunction may, in particular contexts, signal both a TEMPORAL relation and a CAUSAL relation, rather than just one or the other (Miltsakaki et al., 2005). We are aware of only two resources that allow more than one discourse relation to be annotated between two segments – the Penn Discourse TreeBank (PDTB; Prasad et al., 2008, 2014) and, more recently, the BECauSE Corpus 2.0 (Dunietz et al., 2017). The PDTB allows multiple discourse relations of the third and fourth types noted above. It also allows them to be annotated if there is no explicit connective between a pair of segments but annotators see more than one sense relation as linking them, as in the following variant of (4): (5) It’s too far to walk. Let’s take the bus. Here a RESULT relation can be associated with an implicit token of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the"
P18-1210,J14-4007,1,0.889918,"Missing"
P18-1210,W15-2703,1,0.899234,"symptomatic of participant naïveté or bias, but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing. 1 Introduction A question that remains unresolved in work on discourse coherence is the nature and number of relations that can hold between clauses in a coherent text (Halliday and Hasan, 1976; Stede, 2012). Our earlier work (Rohde et al., 2015, 2016) showed that, in the presence of explicit discourse adverbials, people also infer additional discourse relations that they take to hold jointly with those associated with the adverbials. For example, in: best expressed how they took the segments to be related. Rohde et al. (2017) also asked participants to select any other conjunctions that they took to convey the same sense as their “best” choice. (More details of these experiments are given in Section 3.) All three studies showed participants selecting conjunctions whose sense differed from that of the explicit discourse adverbial. Bu"
P18-1210,W16-1707,1,0.547365,"junction(s) that best expressed how the two segments link together. The presentation of conjunction choices varied in order for each participant, but always consisted of AND, BECAUSE, BUT, OR, SO, NONE. While the task admittedly encourages participants to select one (or more) conjunctions, our prior work has shown that participants are very willing to use NONE if no conjunction is appropriate. We there4 Datasets fore take their insertion of a conjunction as their endorsement of the relation signaled by that conjunc- 4.1 In other words Dataset tion. To further control data quality, we included Rohde et al. (2016) report an OR∼SO response 6 catch trials with an expected correct conjunction split for in other words when participants could inlike “To be ______ not to be”. sert only their top choice of conjunction. Figure 1 Three of the explicit discourse adverbials that we chose are anaphoric: in other words, other- shows SO dominating participants’ choice in all cases, but OR showing up among their choices in wise, and instead (Webber et al., 2000). Unlike all but one passage (leftmost vertical bar). Addiconjunctions such as AND, BECAUSE, BUT, OR tionally, several passages elicited BUT as the top and SO"
P18-1210,W17-6814,1,0.933186,"on discourse coherence and lead to higher levels of performance in discourse parsing. 1 Introduction A question that remains unresolved in work on discourse coherence is the nature and number of relations that can hold between clauses in a coherent text (Halliday and Hasan, 1976; Stede, 2012). Our earlier work (Rohde et al., 2015, 2016) showed that, in the presence of explicit discourse adverbials, people also infer additional discourse relations that they take to hold jointly with those associated with the adverbials. For example, in: best expressed how they took the segments to be related. Rohde et al. (2017) also asked participants to select any other conjunctions that they took to convey the same sense as their “best” choice. (More details of these experiments are given in Section 3.) All three studies showed participants selecting conjunctions whose sense differed from that of the explicit discourse adverbial. But Rohde et al. (2015, 2016) also showed participants often selecting conjunctions that signal different coherence relations than those selected by other participants. And Rohde et al. (2017) showed participants often identifying very different conjunctions as conveying the same meaning."
P18-1210,W17-0803,0,0.0383961,"Missing"
P18-1210,W13-0124,1,0.798155,"re asked to select which of three options they took to be a valid paraphrase of the passage. Each use of otherwise was assigned a distinct paraphrase to link the left-hand and right-hand segments (LHS, RHS). 28 instead 21 14 7 0 none other so or but before because and Figure 3: Stacked bar chart for participants’ (N=28) conjunction insertions in instead passages (Rohde et al., 2016) We also allowed participants to choose a second paraphrase if they thought it appropriate. instead simply conveys that what follows is an alternative to an unrealised situation in the context (Prasad et al., 2008; Webber, 2013). The current experiment tests the hypothesis that this BUT∼SO split is a consequence of inference from properties of the segments themselves. To test this hypothesis, we created 16 minimal pairs of passages containing instead, one of which emphasized the information structural parotherwise none 28 allelism between the clauses, as in (13a), and another other variant (13b) that de-emphasized that par21 so by a allelism in favor of a causal link implied or 14 downward-entailing construction such as too X but (Webber, 2013). For each passage, half the particibefore 7 pants saw the parallelism var"
P18-1210,J92-4007,0,0.794365,"in the RST framework standardly produce a single analysis of a text, with a single relational labeling, selecting the analysis that is “most plausible in terms of the perceived goals of the writer” (Mann et al., 1989, pp. 34–35). If that single analysis is later mapped into a different structure to support further processing – e.g., a binary branching tree structure – the mapping does not change the chosen relational labeling. Multiple relations may additionally hold in theories of discourse coherence that posit multiple levels of text analysis. For example, following Grosz and Sidner (1986), Moore and Pollack (1992) characterized text as having both an informational structure (relating information conveyed by discourse segments) and an intentional structure (relating the functions of those segments with respect to what the speaker is trying to accomplish through the text). The kinds of relations at the two levels are different, as can be seen in the following example from (Moore and Pollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relatio"
Q14-1016,W13-1016,0,0.0856848,"Missing"
Q14-1016,I11-1130,0,0.0168818,"tic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Trad"
Q14-1016,W06-1620,0,0.105697,"rpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) shallow, gappy language model allows arbitrary token groupings within a sentence, whereas our model imposes projectivity and nesting constraints (§3). Blunsom and Baldwin (2006) present a sequence model for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures"
Q14-1016,J92-4003,0,0.037196,"ramming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters 199 on the 21-million-word Yelp Academic Dataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token"
Q14-1016,N10-1029,0,0.148488,"units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing c"
Q14-1016,W06-1670,0,0.0891639,"ng on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. G"
Q14-1016,W02-1001,0,0.0210912,"expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learning procedure, algorithm 1, generalizes the clas"
Q14-1016,W11-0809,0,0.0955959,"like their out-of-gap counterparts. Gappy, 2-level (8 tags). 8 tags are required to encode the 2-level scheme with gaps: {O o B b ¯ I¯ ı˜ I˜ ı}. Variants of the inside tag are marked for strength of the incoming link—this applies gap-externally (capitalized tags) and gap-internally (lowercase tags). If ¯ I or ˜ I immediately follows a gap, its diacritic reflects the strength of the gappy expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose a"
Q14-1016,P12-1022,0,0.521376,",y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged perceptron. (Adapted from Daumé, 2006, p. 19.) N experiments showed that a cost function penalizing all recall errors—i.e., with ρ⟦y∗ ≠ O ∧ y′ = O⟧ as the second term, as in Mohit et al.—tended to append additional tokens to high-confidence MWEs (such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2"
Q14-1016,W09-2903,0,0.21043,"edParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinctio"
Q14-1016,I13-1168,0,0.0572292,"in termine the number of training iterations (epochs) M places to eat in Baltimore (because eat in, meaning by early stopping—that is, after each iteration, we use ‘eat at home,’ is listed in WordNet). The supervised the model to decode the held-out data, and when that approach has learned not to trust WordNet too much accuracy ceases to improve, use the previous model. due to this sort of ambiguity. Downstream applicaThe two hyperparameters are the number of iterations tions that currently use lexicon matching for MWE and the value of the recall cost hyperparameter (ρ). identification (e.g., Ghoneim and Diab, 2013) likely Both are tuned via cross-validation on train; we use stand to benefit from our statistical approach. the multiple of 50 that maximizes average link-based F1 . The chosen values are shown in table 3. Experi- 6.2 How best to exploit MWE lexicons (type-level information)? ments were managed with the ducttape tool.18 For statistical tagging (right portion of table 2), using 6 Results more preexisting (out-of-domain) lexicons generally We experimentally address the following questions improves recall; precision also improves a bit. A lexicon of MWEs occurring in the non-held-out to probe an"
Q14-1016,W11-2165,1,0.894392,"Missing"
Q14-1016,W13-3511,0,0.0198865,"nd one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 cluste"
Q14-1016,D11-1067,0,0.0185646,"Missing"
Q14-1016,hajic-etal-2012-announcing,0,0.022231,"Missing"
Q14-1016,D08-1104,0,0.0477308,"such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many va"
Q14-1016,P08-1068,0,0.0247046,"f which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: http"
Q14-1016,J93-2004,0,0.0472323,"valuate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1 2 http://www.ark.cs.cmu.edu/LexSem/ Because we use treebank data, syntactic parses are available to assist in post hoc analysis. Syntactic information was not shown to annotators. # of constituent tokens 2 3"
Q14-1016,W97-0311,0,0.106691,"MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagge"
Q14-1016,H93-1061,0,0.216655,"such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); the named entities and other MWEs in the WSJ corpus on the English side of the CEDT (Hajiˇc et al., 2012); 10 The WordNet API in NLTK (Bird et al., 2009) was used for lemmatization. 11 http://en.wiktionary.org; data obtained from https://toolserver.org/~enwikt/definitions/ enwikt-defs-20130814-en.tsv.gz L OOKUP preexising lexicons none WordNet + SemCor 6 lexicons 10 lexicons entries 0 71k 420k 437k best con"
Q14-1016,N04-1043,0,0.0119357,"f word lemmas, some of which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) im"
Q14-1016,E12-1017,1,0.651813,"sion of the structured perceptron that is sensitive to different kinds of errors during training. When recall is the bigger obstacle, we can adopt the following cost function: given a sentence x, its gold labeling y∗ , and a candidate labeling y′ , ∗ ′ ∣y∗ ∣ cost(y ,y ,x) = ∑ c(y∗j ,y′j ) where j=1 ′ c(y∗ ,y′ ) = ⟦y∗ ≠ y ⟧ + ρ⟦y∗ ∈ {B, b} ∧ y′ ∈ {O, o}⟧ A single nonnegative hyperparameter, ρ, controls the tradeoff between recall and accuracy; higher ρ biases the model in favor of recall (possibly hurting accuracy and precision). This is a slight variant of the recall-oriented cost function of Mohit et al. (2012). The difference is that we only penalize beginning-of-expression recall errors. Preliminary 9 The 8-tag scheme licenses 42 tag bigrams: sequences such as B O and o ¯ ı are prohibited. There are also constraints on the allowed tags at the beginning and end of the sequence. 198 Input: data ⟨⟨x(n) ,y(n) ⟩⟩n=1 ; number of iterations M w←0 w←0 t ←1 for m = 1 to M do for n = 1 to N do ⟨x,y⟩ ← ⟨x(n) ,y(n) ⟩ yˆ ← argmaxy′ (w⊺ g(x,y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged per"
Q14-1016,W06-2405,0,0.0359021,"he sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking"
Q14-1016,C12-1127,0,0.308867,"WEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary as"
Q14-1016,N13-1039,1,0.165809,"Missing"
Q14-1016,W12-3311,0,0.0171944,"ction Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a"
Q14-1016,W12-3301,0,0.207661,"Missing"
Q14-1016,ramisch-etal-2010-mwetoolkit,0,0.0636479,"lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength distinction to separate highly idiomatic expressions from collocations. It is trained and evaluated on a corp"
Q14-1016,W95-0107,0,0.0576853,"O gappy, he was willing to budge a little on the price which means a lot to me . ¯ O ˜ ¯ ˜ ˜ 2-level O O O O B b ¯ ı I O O B I I I I O (O∣BI+ )+ ¯I ˜]+ )+ (O∣B[I (O∣B(o∣bi+ ∣I)∗ I+ )+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the"
Q14-1016,W09-1119,0,0.264965,"+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the standard contiguous chunking representation from Ramshaw and Marcus (1995) using the tags {O B I}. O is for tokens outside any chunk; B marks tokens beginning a chunk; and I marks o"
Q14-1016,D11-1141,0,0.0107976,"Missing"
Q14-1016,schneider-etal-2014-comprehensive,1,0.374841,"rpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the"
Q14-1016,N03-1033,0,0.00694519,"Missing"
Q14-1016,C10-2144,0,0.0511466,"non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allo"
Q14-1016,D11-1077,0,0.0260984,"cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength di"
Q14-1016,W11-0807,0,0.0247747,"ppy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Tre"
Q14-1016,S12-1010,0,0.0570731,"Missing"
Q14-1016,P10-1040,0,0.00760394,"iables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percy"
Q14-1016,2005.eamt-1.11,0,0.0139397,"Missing"
Q14-1016,M95-1005,0,0.0417178,"on1 the price which means4 a43 lot43 to4 me4 . Subscripts denote strong MW groups and superscripts weak MW groups; unmarked tokens serve as single-word expressions. The MW groups are thus {budge, on}, {a, little}, {a, lot}, and {means, {a, lot}, to, me}. As should be evident from the grammar, the projectivity and gap-nesting constraints apply here just as in the 1-level scheme. 3.2 Evaluation Matching criteria. Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature. The MUC criterion (Vilain et al., 1995) measures precision and recall great gateways never1 before1 , so23 far23 as23 Hudson knew2 , seen1 by Europeans was annotated in another corpus. 4 This was violated 6 times in our annotated data: modifiers within gaps are sometimes collocated with the gappy expression, as in on12 a12 tight1 budget12 and have12 little1 doubt12 . 196 of links in terms of groups (units) implied by the transitive closure over those links.5 It can be defined as follows: Let a Ð b denote a link between two elements in the gold standard, and aÐb ˆ denote a link in the system prediction. Let the ∗ operator denote the"
Q14-1016,vincze-2012-light,0,0.02716,"set for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant a"
Q14-1016,J13-1009,0,\N,Missing
Q14-1016,W13-1021,0,\N,Missing
S10-1059,N10-1138,1,0.246183,"rnegie Mellon University, Pittsburgh, PA 15213, USA {desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu Abstract we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. This paper describes the SEMAFOR system’s performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended system models null instantiations, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and F1 . 1 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were full"
S10-1059,S07-1018,0,\N,Missing
S10-1059,S10-1008,0,\N,Missing
S14-2027,J92-4003,0,0.0715867,"t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneff"
S14-2027,W08-1301,0,0.0702255,"Missing"
S14-2027,E14-1049,1,0.763349,"., nodes where there is at 178 least one outbound edge) are possible candidates to be “top”; the classifier probabilities are evaluated, and the highest-scoring node is chosen to be “top.” This is suboptimal, since some graphs have multiple tops (in PCEDT this is more common); but selection rules based on probability thresholds gave worse F1 performance on the dev set. For a given token t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent"
S14-2027,P14-1134,1,0.844279,"f length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision. Tree constraint: Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance. Negative Results We followed a forward-selection process during feature engineering. For each potential feature, we tested the current feature set versus the current feature set plus the new potential feature. If the new feature did not improve performance, we did not add it. We list in table 2 some"
S14-2027,D08-1008,0,0.0230174,"se multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision."
S14-2027,P13-2109,1,0.846354,"es and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotation was converted into"
S14-2027,E06-1011,0,0.0537858,"ur scores. 6 Table 2: Features and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotat"
S14-2027,S14-2008,0,0.0475934,"Missing"
S16-1084,attardi-etal-2010-resource,0,0.136968,"Missing"
S16-1084,S16-1143,0,0.0332991,"Missing"
S16-1084,S16-1142,0,0.0370432,"Missing"
S16-1084,J92-4003,0,0.211357,"Missing"
S16-1084,W13-0907,1,0.0637827,"Missing"
S16-1084,2012.eamt-1.60,0,0.0171689,"Missing"
S16-1084,P15-2079,1,0.0638888,"Missing"
S16-1084,W06-1670,0,0.0258617,"nal) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not suffer from these problems, as it uses a much smaller number of coarsegrained classes. However, these classes only apply to a subset of the nouns in a sentence and exclude verbs and adjectives. They therefore provide far from complete coverage in a corpus. Noun and verb supersenses (Ciaramita and Altun, 2006) offer a middle ground in granularity: they generalize named entity classes to cover all nouns (with 26 classes), but also cover verbs (15 classes)— see table 1—and provide a human-interpretable high-level clustering. WordNet supersenses for adjectives and adverbs nominally exist, but are based on morphosyntactic rather than semantic properties. There is, however, recent work on developing supersense taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION"
S16-1084,C10-2052,1,0.110925,"Missing"
S16-1084,W11-0809,0,0.114516,"ord expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized se"
S16-1084,S14-1001,1,0.837622,"V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idio"
S16-1084,S16-1140,0,0.0497829,"Missing"
S16-1084,D14-1108,1,0.144265,"Missing"
S16-1084,J93-2004,0,0.0679023,"Missing"
S16-1084,H93-1061,0,0.121824,"Missing"
S16-1084,2014.iwslt-papers.16,0,0.201582,"Missing"
S16-1084,N13-1039,1,0.0446902,"Missing"
S16-1084,W09-3531,0,0.0354613,"Missing"
S16-1084,E14-1078,1,0.043392,"Missing"
S16-1084,W12-3301,0,0.173236,"Missing"
S16-1084,W95-0107,0,0.102727,"Missing"
S16-1084,D11-1141,0,0.00536312,"Missing"
S16-1084,W15-1612,1,0.230436,"Missing"
S16-1084,S16-1141,0,0.125282,"Missing"
S16-1084,S10-1049,0,0.0152066,"evel organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION. Schneider and Smith (2015) develop this"
S16-1084,S16-1145,0,0.0357113,"Missing"
S16-1084,W13-0906,0,0.0274625,"Missing"
S16-1084,Q14-1016,1,0.485545,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,tsvetkov-etal-2014-augmenting-english,1,0.865215,"nse taxonomies for English adjectives and 547 N :T OPS N : ACT N : OBJECT N : PERSON V: COGNITION V: COMMUNICATION N : COMMUNICATION N : EVENT N : FEELING N : FOOD N : GROUP N : LOCATION N : RELATION N : SHAPE N : STATE N : SUBSTANCE N : TIME V: BODY V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation,"
S16-1084,N13-1076,1,0.85833,"Missing"
S16-1084,P12-2050,1,0.107223,"V: MOTION V: PERCEPTION V: POSSESSION V: SOCIAL V: STATIVE V: WEATHER N : ANIMAL N : ARTIFACT N : ATTRIBUTE N : BODY N : COGNITION N : MOTIVE N : PHENOMENON N : PLANT N : POSSESSION N : PROCESS N : QUANTITY V: CHANGE V: COMPETITION V: CONSUMPTION V: CONTACT V: CREATION V: EMOTION Table 1: The 41 noun and verb supersenses in WordNet. prepositions (Tsvetkov et al., 2014; Schneider et al., 2015). The inventory for nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should"
S16-1084,schneider-etal-2014-comprehensive,1,0.119259,"presentation is applicable to other languages: see §2. 546 Proceedings of SemEval-2016, pages 546–559, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics In these sentences, a lot, leaves. . . to be desired, Fuji Sushi, came up, made. . . order, and carry out are all multiword expressions (MWEs): their combined meanings can be thought of as “prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (vi"
S16-1084,N15-1177,1,0.219831,"rds. While the main corpus with WordNet senses, SemCor (Miller et al., 1993), does reflect several text genres, it is hard to expand SemCor-style annotations to new genres, such as social web text or transcribed speech. This severely limits the applicability of SemCor-based NLP tools and restricts opportunities for linguistic studies of lexical semantics in corpora. To address this limitation, in the DiMSUM 2016 shared task,1 we challenged participants to analyze the lexical semantics of English sentences with a tagset integrating multiword expressions and noun and verb supersenses (following Schneider and Smith, 2015), on multiple nontraditional genres of text. By moving away from fine-grained sense inventories and lexicalized, language-specific2 annotation, we take a step in the direction of broadcoverage, coarse-grained lexical semantic analysis. We believe this departure from the classical lexical semantics paradigm will ultimately prove fruitful for a variety of NLP applications in a variety of genres. The integrated lexical semantic representation (§2, §3) has been annotated in an extensive benchmark data set comprising several nontraditional domains (§4). Objective, controlled evaluation procedures ("
S16-1084,M95-1005,0,0.031558,"Missing"
S16-1084,W11-0817,0,0.0301138,"n with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and covering new domains and languages. Named entity recognition (NER) does not"
S16-1084,I13-1024,0,0.0698786,"prepackaged” in a single lexical expression that happens to be written with spaces. MWEs such as these have attracted a great deal of attention within computational semantics; see Baldwin and Kim (2010) for a review. Schneider et al. (2014b) introduced an English corpus resource annotated for heterogenous MWEs, suitable for training and evaluating generalpurpose MWE identification systems (Schneider et al., 2014a). Prior to that, most MWE evaluations were focused on particular constructions such as noun compounds (recently: Constant and Sigogne, 2011; Green et al., 2012; Ramisch et al., 2012; Vincze et al., 2013), though the corpus and identification system of Vincze et al. (2011) targets several kinds of MWEs. Importantly, the MWEs in Schneider et al.’s (2014b) corpus are not required to be contiguous, but may contain gaps (viz.: made. . . order). The corpus also contains qualitative labels indicating the strength of MWEs, either strong (mostly non-compositional) or weak (compositional but idiomatic). For simplicity we only include strong MWEs in this task. Supersenses. As noted above, relying on WordNet-like fine-grained, lexicalized senses creates problems for annotating at a large scale and coveri"
S16-1084,S07-1051,0,0.0267622,"nouns and verbs originates from the top-level organization of WordNet, but can be applied directly to annotate new data—including out-of-vocabulary words in English or other languages (Schneider et al., 2012; Johannsen et al., 2014). Similar to NER, supersense tagging approaches have generally used statistical sequence models and have been evaluated in English, Italian, Chinese, Arabic, and Danish.3 Features based on supersenses have been exploited in downstream semantics tasks such as preposition sense disambiguation, noun compound interpretation, question generation, and metaphor detection (Ye and Baldwin, 2007; Hovy et al., 2010; Tratz and Hovy, 2010; Heilman, 2011; Hovy et al., 2013; Tsvetkov et al., 2013). Relationship between MWEs and supersenses. We believe that MWEs and supersenses should be tightly coupled: idiomatic combinations such as MWEs are best labeled holistically, since their joint supersense category will often differ from that of the individual words. For example, spill the beans in its literal interpretation would receive supersenses V: CONTACT and N : FOOD , whereas the idiomatic interpretation, ‘divulge a secret’, is represented as an MWE holistically tagged as V: COMMUNICATION."
S16-1084,picca-etal-2008-supersense,0,\N,Missing
S16-1084,J13-1009,0,\N,Missing
S16-1084,W15-1806,1,\N,Missing
S16-1084,S16-1144,0,\N,Missing
S17-1022,N09-1057,0,0.0472258,"We also acknowledge that there is a level of construal contributed by the verb. For example, Alex in Alex sent the package to Pam can be AGENT or S OURCE depending whether the interpretation is focused on the agency of the argument or the spatial relation it has in reference to the action described by the verb. These verb-triggered construals have been previously explored, most notably by Jackendoff (1990). Perspective can also be evident in the choice of syntactic constructions, e.g., active vs. passive voice (I made a mistake versus Mistakes were made), which can be connected to sentiment (Greene and Resnik, 2009). We specifically focus on the construal that arises from the adposition in a given sentence. The preposition by gives the impression that the stimulus is responsible for triggering an instinctive fear reflex (i.e., C AUSER), while about portrays the thing feared as the content or T OPIC of thought.6 In some languages, the experiencer can be conceptualized as a recipient of the emotion or feeling, thus licensing dative marking.7 In the Hebrew example (8a), the experiencer of bodily perception is marked with the dative preposition l(e)- (Berman, 1982). Similarly, in Hindi, the dative postpostio"
S17-1022,W06-1670,0,0.0556475,"mar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inventory of categories through extensive deliberation involving the use of dictionaries, corpora, and pilot annotation experiments. They call the categories supersenses to emphasize their similarity to coarse-grained classifications of nouns and verbs that go by that name (Ciaramita and Altun, 2006; Schneider et al., 2012). The at examples in (1) are accompanied by the appropriate supersenses from the supersense scheme. Most supersenses resemble thematic roles (cf. Fillmore (1968)); a few others are needed to describe preposition-marked relations between entities. There are multiple English prepositions per supersense; e.g., “in the city” and “on the table” would join “at 123 Main St.” in being labeled as L OCATIONs. We understand the supersenses as prototype-based categories, and in some cases use heuristics like paraphrasability (“in order to” for P URPOSE) and WH-question words (“Why"
S17-1022,D09-1047,0,0.409241,"cs, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inve"
S17-1022,J05-1004,0,0.346456,"facilitate inter-annotator consistency: our experience thus far is that annotators benefit greatly from examples illustrating the possible supersenses that can be assigned to a preposition. If initial pilots are successful, we would then need to decide whether to annotate the role and function together or in separate stages. Because the function reflects one of the adposition’s prototypical senses, it may often be deterministic given the adposition and scene role, in which case we could focus annotators’ efforts on the scene roles. Existing annotations for lexical resources such as PropBank (Palmer et al., 2005), VerbNet (Palmer et al., 2017; Kipper et al., 2008), and FrameNet (Fillmore and Baker, 2009) might go a long way toward disambiguating the scene role, limiting the effort required from annotators. 6.5 Linguistic Utility of Annotated Data Assuming the above theoretical and practical concerns are surmountable, annotated corpora would facilitate empirical studies of the nature and limits of adposition/case construal within and across languages. For example: Is it the case that some of the supersense labels can only serve as scene roles, or only as functions? (A hypothesis is that PARTICI PANT su"
S17-1022,picca-etal-2008-supersense,0,0.466974,"icial to us; at the very least, it splits hairs in a way that would be difficult to explain to annotators. Below, we instead argue that the idea of construal/conceptualization offers a more principled answer; in our new analysis, the T OPIC suggested by about and the S TIMULUS suggested by cared can coexist. http://tiny.cc/prepwiki 180 3.2 Applying the Supersenses to Other Languages One of the premises of using unlexicalized supersenses was that the scheme would port well to other 4 http://tiny.cc/prepwiki/index.php/Category: SST-Topic languages (as the WordNet noun and verb supersenses have: Picca et al., 2008; Schneider et al., 2012, inter alia). To test this, we have begun applying the existing supersenses to three new languages, namely, Hebrew, Hindi, and Korean. Pilot annotation in these languages has echoed the fundamental problem discussed in the previous section. Consider the Hindi examples below. In (4a), the experiencer of an emotion is marked with a postposition kaa, the genitive case marker in Hindi. (4) a. [Hindi]: E XPERIENCER vs. P OSSESSOR bipaashaa kaa gussaa Bipasha GEN anger “Bipasha’s anger” 4.1 b. [Hindi]: E XPERIENCER bipaashaa bahut gussaa hui Bipasha very angry became “Bipash"
S17-1022,W16-1712,1,0.893848,"Missing"
S17-1022,P12-2050,1,0.931703,"ider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al. (2015) refined their inventory of categories through extensive deliberation involving the use of dictionaries, corpora, and pilot annotation experiments. They call the categories supersenses to emphasize their similarity to coarse-grained classifications of nouns and verbs that go by that name (Ciaramita and Altun, 2006; Schneider et al., 2012). The at examples in (1) are accompanied by the appropriate supersenses from the supersense scheme. Most supersenses resemble thematic roles (cf. Fillmore (1968)); a few others are needed to describe preposition-marked relations between entities. There are multiple English prepositions per supersense; e.g., “in the city” and “on the table” would join “at 123 Main St.” in being labeled as L OCATIONs. We understand the supersenses as prototype-based categories, and in some cases use heuristics like paraphrasability (“in order to” for P URPOSE) and WH-question words (“Why?” for P URPOSE and E XPL"
S17-1022,W15-1612,1,0.944534,"language—and discuss how this representation would allow for a simpler inventory of labels. 1 Introduction Prepositions and postpositions (collectively adpositions) are widespread in the world’s languages as grammatical markers expressing spatial, temporal, thematic,1 and other kinds of semantic relations. Unfortunately for semantic processing, a handful of high-frequency types carry an immense payload by way of extreme polysemy. Thus, disambiguation of adpositional meaning is crucial to piecing together the interpretation of a sentence (§2). A line of previous work (Srikumar and Roth, 2013a; Schneider et al., 2015, 2016, see §2) has developed a scheme for broad-coverage annotation 1 Nathan Schneider Georgetown University In the sense of thematic roles (agent, patient, etc.). of adpositions with an eye toward building automatic disambiguation systems. Their most recent proposal consists of an inventory of 75 categorical labels known as supersenses that characterize the polysemy of English prepositions in a lexicallyneutral and coarse-grained fashion. They envision disambiguation as assigning a single one of these supersenses to each preposition token. While formalizing disambiguation via singlelabel cla"
S17-1022,Q13-1019,1,0.952209,"ocessing of domaingeneral language—and discuss how this representation would allow for a simpler inventory of labels. 1 Introduction Prepositions and postpositions (collectively adpositions) are widespread in the world’s languages as grammatical markers expressing spatial, temporal, thematic,1 and other kinds of semantic relations. Unfortunately for semantic processing, a handful of high-frequency types carry an immense payload by way of extreme polysemy. Thus, disambiguation of adpositional meaning is crucial to piecing together the interpretation of a sentence (§2). A line of previous work (Srikumar and Roth, 2013a; Schneider et al., 2015, 2016, see §2) has developed a scheme for broad-coverage annotation 1 Nathan Schneider Georgetown University In the sense of thematic roles (agent, patient, etc.). of adpositions with an eye toward building automatic disambiguation systems. Their most recent proposal consists of an inventory of 75 categorical labels known as supersenses that characterize the polysemy of English prepositions in a lexicallyneutral and coarse-grained fashion. They envision disambiguation as assigning a single one of these supersenses to each preposition token. While formalizing disambigu"
S17-1022,N09-3017,0,0.0768667,"In cognitive linguistics, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized senses. Schneider et al."
S17-1022,S07-1051,0,0.0372896,"on semantics broadly. In cognitive linguistics, studies have examined abstract as well as concrete uses of English prepositions (e.g., Dirven, 1993; Lindstromberg, 2010). Notably, the polysemy of over and other prepositions has been explained in terms of sense networks encompassing core senses and motivated extensions (Brugman, 1981; Lakoff, 1987; Dewell, 1994; Tyler and Evans, 2001, 2003). The Preposition Project (TPP; Litkowski and Hargraves, 2005) broke ground in stimulating computational work on fine-grained word sense disambiguation of English prepositions (Litkowski and Hargraves, 2005; Ye and Baldwin, 2007; Tratz and Hovy, 2009; Dahlmeier et al., 2009). Typologists, meanwhile, have developed semantic maps of functions, where the nearness of two functions reflects their tendency to fall under the same adposition or case marker in many languages (Haspelmath, 2003; Wälchli, 2010). Preposition supersenses. Following Srikumar and Roth (2013b), Schneider et al. (2015) developed coarse-grained semantic categories of prepositions as a broader-coverage alternative to finegrained senses, using categories similar to those appearing in semantic maps (L OCATION, R ECIPIENT, etc.) rather than lexicalized sen"
schneider-etal-2014-comprehensive,W11-0807,0,\N,Missing
schneider-etal-2014-comprehensive,S12-1021,0,\N,Missing
schneider-etal-2014-comprehensive,S12-1010,0,\N,Missing
schneider-etal-2014-comprehensive,M95-1005,0,\N,Missing
schneider-etal-2014-comprehensive,W06-1670,0,\N,Missing
schneider-etal-2014-comprehensive,H93-1061,0,\N,Missing
schneider-etal-2014-comprehensive,D11-1067,0,\N,Missing
schneider-etal-2014-comprehensive,J13-1009,0,\N,Missing
schneider-etal-2014-comprehensive,N10-1029,0,\N,Missing
schneider-etal-2014-comprehensive,Q14-1016,1,\N,Missing
schneider-etal-2014-comprehensive,P12-1022,0,\N,Missing
schneider-etal-2014-comprehensive,vincze-2012-light,0,\N,Missing
schneider-etal-2014-comprehensive,C12-1127,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W10-0719,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,D08-1027,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,W06-1670,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,H93-1061,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,J12-3005,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P06-2072,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,P14-1024,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,P12-2050,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,N13-1132,1,\N,Missing
tsvetkov-etal-2014-augmenting-english,I08-2105,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,peters-peters-2000-treatment,0,\N,Missing
tsvetkov-etal-2014-augmenting-english,E14-1049,1,\N,Missing
W13-1736,J92-4003,0,0.123486,"Missing"
W13-1736,U07-1006,0,0.0307986,"fy that language. This task has a clear empirical motivation. Nonnative speakers make different errors when they write English, depending on their native language (Lado, 1957; Swan and Smith, 2001); understanding the different types of errors is a prerequisite for correcting them (Leacock et al., 2010), and systems such as the one we describe here can shed interesting light on such errors. Tutoring applications can use our system to identify the native language of students and offer better-targeted advice. Forensic linguistic applications are sometimes required to determine the L1 of authors (Estival et al., 2007b; Estival et al., 2007a). Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language. We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted. Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into “s"
W13-1736,C90-2036,0,0.0232636,"rb and Masuda, 2008). Since English’s orthography is largely phonemic—even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse l and r relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector (Kernighan, 1990) to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay. Restored tags We focus on three important token classes defined above: punctuation marks, function words and cohesive verbs. We first remove words in these classes from the texts, and then recover the most likely hidden tokens in a sequence of words, according to an n-gram language model"
W13-1736,P08-1068,0,0.0252399,"tions). To restore hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-clu"
W13-1736,P11-1132,1,0.750772,"e markers These are 40 function words (and short phrases) that have a strong discourse function in texts (however, because, in fact, etc.). Translators tend to spell out implicit utterances and render them explicitly in the target text (Blum-Kulka, 1986). We use the list of Volansky et al. (forthcoming). Cohesive verbs This is a list of manually compiled verbs that are used, like cohesive markers, to spell out implicit utterances (indicate, imply, contain, etc.). Function words Frequent tokens, which are mostly function words, have been used successfully for various text classification tasks. Koppel and Ordan (2011) define a list of 400 such words, of which we only use 100 (using the entire list was not significantly different). Note that pronouns are included in this list. Contextual function words To further capitalize on the ability of function words to discriminate, we define pairs consisting of a function word from the list mentioned above, along with the POS tag of its adjacent word. This feature captures patterns such as verbs and the preposition or particle immediately to their right, or nouns and the determiner that precedes them. We also define 3-grams consisting of one or two function words an"
W13-1736,N13-1039,1,0.79567,"Missing"
W13-1736,W13-1706,0,0.102937,"Missing"
W13-1736,N03-1033,0,0.010714,"etreault et al., 2013). The training data consists of 1000 essays from each native language. The essays are short, consisting of 10 to 20 sentences each. We used the provided splits of 900 documents for training and 100 for development. Each document is annotated with the author’s English proficiency level (low, medium, high) and an identification (1 to 8) of the essay prompt. All essays are tokenized and split into sentences. In table 1 we provide some statistics on the training corpora, listed by the authors’ proficiency level. All essays were tagged with the Stanford part-of-speech tagger (Toutanova et al., 2003). We did not parse the dataset. # Documents # Tokens # Types Low 1,069 245,130 13,110 Medium 5,366 1,819,407 37,393 High 3,456 1,388,260 28,329 Table 1: Training set statistics. 4 Model For our classification model we used the creg regression modeling framework to train a 11-class logistic regression classifier.1 We parameterize the classifier as a multiclass logistic regression: P exp j λ j h j (x, y) pλ (y |x) = , Zλ (x) where x are documents, h j (·) are real-valued feature functions of the document being classified, λ j are the corresponding weights, and y is one of the eleven L1 class lab"
W13-1736,W07-0602,0,0.263634,"Missing"
W13-1736,P10-1040,0,0.0107663,"hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-cluster http://www.ark.c"
W13-1736,U09-1008,0,0.0944702,"Missing"
W13-1736,D11-1148,0,0.0757912,"Missing"
W13-2307,2020.lrec-1.643,0,0.293721,"Missing"
W13-2307,P99-1010,0,0.418476,"fficiently mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Bo"
W13-2307,N06-1019,0,0.236476,"mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Both annotations and analys"
W13-2307,W07-2416,0,0.0563019,"ator labels are as in table 2. Per-annotator com (with lexical reconciliation) and inter-annotator softComPrec are aggregated over sentences by arithmetic mean. less burdensome, and the specialized annotations did prove complementary to each other.19 5.4 Treebank Comparison Though the annotators in our study were native speakers well acquainted with representations of English syntax, we sought to quantify their agreement with the expert treebankers who created the EWTB (the source of the Reviews sentences). We converted the EWTB’s constituent parses to dependencies via the PennConverter tool (Johansson and Nugues, 2007),20 then removed punctuation. Agreement with the converted treebank parses appears in the bottom two rows of table 3. Because the EWTB commits to a single analysis, precision scores are quite lopsided. Most of its attachments are consistent with our annotations (softComPrec &gt; 0.9), but these allow many additional analyses (hence the scores below 0.5). Annotator Specialization As an experiment in using underspecification for labor division, two of the annotators of Reviews data were assigned specific linguistic phenomena to focus on. Annotator “D” was tasked with the internal structure of base"
W13-2307,P06-2066,0,0.0538705,"nd compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced language"
W13-2307,W12-1706,0,0.033779,"When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced languages (to name two examples). Traditional syntactic annotation projects like the Penn Treebank (Marcus ∗ 2 A Dependency Grammar for Annotation Although depende"
W13-2307,J93-2004,0,0.0444543,"portion of the English Web Treebank 14 Malagasy is a VOS Austronesian language spoken by 15 million people, mostly in Madagascar. Kinyarwanda is an SVO Bantu language spoken by 12 million people mostly in Rwanda. All annotations were done by native speakers of English. The Kinyarwanda and Malagasy annotators had basic proficiency in these languages. 15 As a point of comparison, during the Penn Treebank project, annotators corrected the syntactic bracketings produced by a high-quality hand-written parser (Fidditch) and achieved a rate of only 375 tokens/hour using a specialized GUI interface (Marcus et al., 1993). 16 Included with the data and software release (footnote 1). 57 com thus reduces the commitment averages for each annotation—to a greater extent for annotator “A” (.96 in table 2 vs. .82 in table 3) because “A” marked more multiwords. An analysis fully compatible with both annotations exists for only 27/60 sentences; the finer-grained softComPrec measure (§4.2), however, offers insight into the balance between commitment and agreement. Qualitatively, we observe three leading causes of incompatibilities (disagreements): obvious annotator mistakes (such as the marked as a head); inconsistent h"
W13-2307,1993.iwpt-1.22,0,0.0410599,"lop algorithms to evaluate and compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and"
W13-2307,D07-1014,1,0.853574,"edges that are known to be incompatible with the annotation before searching for spanning trees. Our “upward-downward” method for constructing a graph of supported edges first enumerates a set of candidate top nodes for every fudge expression, then uses that information to infer a set of supported parents for every node.12 The supported edge graph then consists of vertices lexnodes(A) ∪ {root} and edges S 0 0 v∈lexnodes(A) {(v → v ) ∀ v ∈ suppParentsA (v)}. From this graph we can count all directed spanning trees in cubic time using Kirchhoff’s matrix tree theorem (Chaiken and Kleitman, 1978; Smith and Smith, 2007; Margoliash, 2010).13 If some lexical node has no supported parents, this reflects conflicting constraints in the annotation, and no spanning tree will be found. Promiscuity will tend to be higher for longer sentences. To control for this, we define a second quantity, the annotation’s commitment quotient (commitment being the opposite of promiscuity), 4.2 Inter-Annotator Agreement FUDG can encode flat groupings and coreference at the lexical level, as well as syntactic structure over lexical items. Inter-annotator agreement can be measured separately for each of these facets. Pilot annotator"
W13-2307,D08-1027,1,0.400347,"Missing"
W13-2307,N13-1039,1,0.775169,"Missing"
W13-2307,W13-2307,1,0.0512826,"Missing"
W13-2322,P13-1091,1,0.350566,"uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. (2012). Disjunctive AMR. AMR aims to canonicalize mul"
W13-2322,N12-1017,0,0.0148132,"Missing"
W13-2322,kingsbury-palmer-2002-treebank,1,0.410787,"ive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are wri"
W13-2322,martins-2012-le,0,0.0102149,"re able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tas"
W13-2322,W04-2705,0,0.0181197,"Missing"
W13-2322,W13-0101,0,0.0287813,"nces from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future"
W13-2322,P98-1013,0,0.597537,"Missing"
W13-2322,basile-etal-2012-developing,0,0.035603,". AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking tools (e.g., Basile et al. (2012b)), annotation guidelines,5 and sembanks that cover a wide range of genres, from news to fiction. UNL and AMR have both annotated many of the same sentences, providing the potential for direct comparison. We currently have a manually-constructed AMR bank of several thousand sentences, a subset of which can be freely downloaded,4 the rest being distributed via the LDC catalog. In initially developing AMR, the authors built consensus AMRs for: • 225 short sentences for tutorial purposes • 142 sentences of newswire (*) • 100 sentences of web data (*) Trained annotators at LDC then produced AMRs"
W13-2322,J05-1004,1,0.185231,"design an Abstract Meaning Representation (AMR) appropriate for sembanking. Our basic principles are: • AMRs are rooted, labeled graphs that are easy for people to read, and easy for programs to traverse. • AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences “he described her as a genius”, “his description of her: genius”, and “she was a genius, according to his description” are all assigned the same AMR. • AMR makes extensive use of PropBank framesets (Kingsbury and Palmer, 2002; Palmer et al., 2005). For example, we represent a phrase like “bond investor” using the frame “invest-01”, even though no verbs appear in the phrase. • AMR is agnostic about how we might want to derive meanings from strings, or viceversa. In translating sentences to AMR, we do not dictate a particular sequence of rule applications or provide alignments that reflect such rule sequences. This makes sembanking very fast, and it allows researchers to explore their own ideas about how strings We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings"
W13-2322,P02-1040,0,0.108678,"ted script.3 Smatch reports the semantic overlap between two AMRs by viewing each AMR as a conjunction of logical triples (see Figure 1). Smatch computes precision, recall, and F-score of one AMR’s triples against the other’s. To match up variables from two input AMRs, smatch needs to execute a brief search, looking for the variable mapping that yields the highest F-score. Smatch makes no reference to English strings or word indices, as we do not enforce any particular string-to-meaning derivation. Instead, we compare semantic representations directly, in the same way that the MT metric Bleu (Papineni et al., 2002) compares target strings without making reference to the source. For an initial IAA study, and prior to adjusting the AMR Editor to encourage consistency, 4 expert AMR annotators annotated 100 newswire sentences and 80 web text sentences. They then created consensus AMRs through discussion. The average annotator vs. consensus IAA (smatch) was 0.83 for newswire and 0.79 for web text. When newly trained annotators doubly annotated 382 web text sentences, their annotator vs. annotator IAA was 0.71. (m / marble :location (j / jar)) the marble in the jar ... (b / be-located-at-91 :arg1 (m / marble)"
W13-2322,W12-6207,1,0.412113,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W12-4209,1,0.36463,"glish WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific PropBank relations. UNL has a dedicated set of over 30 frequently used relations. Formalism. GMB meanings are written in DRT (Kamp et al., 2011), exploiting full first4 8 Future Work Sembanking. Our main goal is to continue sembanking. We would like to employ a large sembank to create shared tasks for natural language understanding and generation. These tasks may additionally drive interest in theoretical frameworks for probabilistically mapping between graphs and strings (Quernheim and Knight, 2012b; Quernheim and Knight, 2012a; Chiang et al., 2013). Applications. Just as syntactic parsing has found many unanticipated applications, we expect sembanks and statistical semantic processors to be used for many purposes. To get started, we are exploring the use of statistical NLU and NLG in 5 amr.isi.edu/download.html 184 UNL guidelines: www.undl.org/unlsys/unl/unl2005 a semantics-based machine translation (MT) system. In this system, we annotate bilingual Chinese/English data with AMR, then train components to map Chinese to AMR, and AMR to English. A prototype is described by Jones et al. ("
W13-2322,W13-0215,0,0.00777424,"ry))”, because “profess-01” 2 3 183 AMR Editor: amr.isi.edu/editor.html Smatch: amr.isi.edu/evaluation.html 6 Current AMR Bank order logic. GMB and ST both include universal quantification. Granularity. GMB and UCCA annotate short texts, so that the same entity can participate in events described in different sentences; other systems annotate individual sentences. Entities. AMR uses 80 entity types, while GMB uses 7. Manual versus automatic. AMR, UNL, and UCCA annotation is fully manual. GMB and ST produce meaning representations automatically, and these can be corrected by experts or crowds (Venhuizen et al., 2013). Derivations. AMR and UNL remain agnostic about the relation between strings and their meanings, considering this a topic of open research. ST and GMB annotate words and phrases directly, recording derivations as (for example) Montaguestyle compositional semantic rules operating on CCG parses. Top-down verus bottom-up. AMR annotators find it fast to construct meanings from the top down, starting with the main idea of the sentence (though the AMR Editor allows bottom-up construction). GMB and UCCA annotators work bottom-up. Editors, guidelines, genres. These projects have graphical sembanking"
W13-2322,N06-1056,0,0.0133733,"AMRs for: • 1546 sentences from the novel “The Little Prince” • 1328 sentences of web data • 1110 sentences of web data (*) • 926 sentences from Xinhua news (*) • 214 sentences from CCTV broadcast conversation (*) Collections marked with a star (*) are also in the OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011). Using the AMR Editor, annotators are able to translate a full sentence into AMR in 7-10 minutes and postedit an AMR in 1-3 minutes. 7 Related Work Researchers working on whole-sentence semantic parsing today typically use small, domain-specific sembanks like GeoQuery (Wong and Mooney, 2006). The need for larger, broad-coverage sembanks has sparked several projects, including the Groningen Meaning Bank (GMB) (Basile et al., 2012a), UCCA (Abend and Rappoport, 2013), the Semantic Treebank (ST) (Butler and Yoshimoto, 2012), the Prague Dependency Treebank (B¨ohmov´a et al., 2003), and UNL (Uchida et al., 1999; Uchida et al., 1996; Martins, 2012). Concepts. Most systems use English words as concepts. AMR uses PropBank frames (e.g., “describe-01”), and UNL uses English WordNet synsets (e.g., “200752493”). Relations. GMB uses VerbNet roles (Schuler, 2005), and AMR uses frame-specific Pr"
W13-2322,E12-2019,0,\N,Missing
W13-2322,C12-1083,1,\N,Missing
W13-2322,C98-1013,0,\N,Missing
W15-1612,J09-2001,0,0.0616612,"s were clustered automatically, then the clusters were manually refined and given names). Detailed in Srikumar and Roth (2013a), those categories cut across preposition types to combine related TPP senses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’H"
W15-1612,W13-2322,1,0.777114,"Missing"
W15-1612,bhatia-etal-2014-unified,0,0.0287958,"cial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. Th"
W15-1612,W07-1604,0,0.0343143,"unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comp"
W15-1612,W06-1670,0,0.146858,"and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numer"
W15-1612,W06-1207,0,0.0343708,"nses for better data-driven generalization. Cohen’s κ for inter-annotator agreement was 0.75, which is encouraging, though it is unclear whether the disagreements were due to systematic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Nei"
W15-1612,D09-1047,0,0.617638,"matic differences in interpretation of the scheme or to difficulty with rare preposition usages. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of"
W15-1612,J14-1002,1,0.642481,"more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the goal of describing and documenting the uses of individual prepositions in a lexical resource rather than labeling a corpus with free-text preposition annotations. We hope that the latter, token-driven approach will be taken for annotating text with preposition supersenses so that those annotations will be suitable for training statistical NLP systems. 2 Our Approach With the end of free-text semantic annotation in mind, we develop and document a preposition"
W15-1612,C04-1198,0,0.0252432,"es. We shall return to this scheme in §3 below. 1.3 Prepositions in NLP Despite a steady trickle of papers over the years (see Baldwin et al., 2009 for a review), there is no apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framewor"
W15-1612,C10-2052,0,0.261708,"cs in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspective—i.e. with the go"
W15-1612,P11-2056,0,0.150935,"Missing"
W15-1612,hwang-etal-2014-criteria,1,0.867098,"Missing"
W15-1612,W07-1601,0,0.0452238,"Missing"
W15-1612,P14-1120,0,0.245532,"Missing"
W15-1612,S07-1005,0,0.348619,"Missing"
W15-1612,H94-1020,0,0.121481,"Missing"
W15-1612,W10-1827,0,0.02899,"The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Saint-Dizier, 2006b) aimed to describe the semantics of prepositions across several languages; however, it seems not to have progressed beyond the preliminary stages. Thus far, our approach has focused on English, but aims to define supersense categories semantically rather than by language-specific criteria (e.g., syntactic tests) so as to encourage its adaptation to other languages in the future. 1.1 1.2 1 Background Linguistic Approaches Most studies of preposition semantics are limited to so"
W15-1612,W03-0411,0,0.252975,"Missing"
W15-1612,J09-2002,0,0.154496,"Missing"
W15-1612,D10-1032,0,0.0281672,"ns are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairl"
W15-1612,saint-dizier-2006-prepnet,0,0.462283,"3, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. The descriptive challenges raised by prepositions have not gone unnoticed in the literature; see, e.g., Saint-Dizier (2006a) for an assortment of syntactic and semantic issues. Here we touch on some of the lines of inquiry, resources, and NLP approaches to preposition semantics found in previous work. tention from theorists (Bowerman and Choi, 2001; Hagège, 2009; Regier, 1996; Xu and Kemp, 2010; Zelinsky-Wibbelt, 1993) but is of practical interest as well, especially when it comes to machine translation and second language acquisition. A corpus creation project for German preposition senses (Müller et al., 2010, 2011) is similar in spirit to the supersense approach taken below. Finally, the PrepNet resource (Sain"
W15-1612,P12-2050,1,0.404449,"; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki i"
W15-1612,N15-1177,1,0.172806,"wang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and"
W15-1612,Q13-1019,1,0.876864,"e descriptive annotation scheme for prepositions must deal with these messy facts. Following a brief discussion of existing approaches to preposition semantics (§1), this paper offers a new approach to characterizing their functions at a coarsegrained level. Our scheme is intended to apply to almost all preposition tokens, though some are excluded on the grounds that they belong to a larger multiword expression or are purely syntactic (§2). The rest of the paper is devoted to our coarse semantic categories, supersenses (§3).2 Many of these categories are based on previous proposals—primarily, Srikumar and Roth (2013a) (so-called preposition relations) and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also"
W15-1612,N09-3017,0,0.194973,"of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotated from a lexicographic, type-driven perspecti"
W15-1612,tsvetkov-etal-2014-augmenting-english,1,0.604089,"m into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to benefit from automatic disambiguation of preposition supersenses. 2 Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), definiteness (Bhatia et al., 2014, also hierarchical). 112 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps finegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples"
W15-1612,S07-1051,0,0.17518,"apparent consensus approach to the treatment of preposition semantics in NLP. Studies have examined preposition semantics within multiword expressions (Cook and Stevenson, 2006), in spatial relations (Hying, 114 2007), across languages (Saint-Dizier, 2006b), in nonnative writing (Chodorow et al., 2007), in semantic role labeling (Dahlmeier et al., 2009), in vector space models (Zwarts and Winter, 2000), and in discourse (Denand and Rolbert, 2004). Preposition sense disambiguation systems have been evaluated against one or more of the resources described in §1.2 (O’Hara and Wiebe, 2003, 2009; Ye and Baldwin, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Srikumar and Roth, 2013b). Unfortunately, all of these resources are problematic. Neither the PTB function tags nor the FrameNet roles were designed with prepositions in mind: the former set is probably not comprehensive enough to be a general-purpose account of prepositions, and the latter representation only makes sense in the broader analytical framework of frame semantics, which we believe should be treated as a separate task (Das et al., 2014). The Preposition Project data, though extensive, were selected and annotat"
W15-1612,P98-1013,0,\N,Missing
W15-1612,C98-1013,0,\N,Missing
W15-1618,E03-1068,0,0.0242274,"-established best practices notwithstanding— designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noah Smith, Bonnie Webber, and Archna Bhatia for their comments on a draft of this piece. References Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP, pages 9–16. Barcelona, Spain. Jason Baldridge and Alexis Palmer. 2009. How well does"
W15-1618,I11-1100,0,0.178691,"Missing"
W15-1618,N13-1014,0,0.025846,"ed a great many controlled annotation studies, whereas we were focused on producing as much useful data as possible given a limited budget. And of course, it’s possible that a more conventional approach to annotation with fewer annotators would have produced more useful data. In general, it has been my experience that—some well-established best practices notwithstanding— designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noa"
W15-1618,P11-2008,1,0.856508,"Missing"
W15-1618,N13-1132,0,0.0173158,"with fewer annotators would have produced more useful data. In general, it has been my experience that—some well-established best practices notwithstanding— designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noah Smith, Bonnie Webber, and Archna Bhatia for their comments on a draft of this piece. References Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin an"
W15-1618,P14-2062,0,0.0219445,"Missing"
W15-1618,P10-2014,0,0.016052,"designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noah Smith, Bonnie Webber, and Archna Bhatia for their comments on a draft of this piece. References Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP, pages 9–16. Barcelona, Spain. Jason Baldridge and Alexis Palmer. 2009. How well does active learning actually work? Time-based e"
W15-1618,D14-1108,1,0.892647,"Missing"
W15-1618,E09-1060,0,0.0229277,"otwithstanding— designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noah Smith, Bonnie Webber, and Archna Bhatia for their comments on a draft of this piece. References Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP, pages 9–16. Barcelona, Spain. Jason Baldridge and Alexis Palmer. 2009. How well does active learning"
W15-1618,J93-2004,0,0.0492526,"lz** roots Figure 1: FUDG GFL notation summary and two annotated Twitter examples. reliability will be low, and some categories may be highly sparse. Estimating these tradeoffs in a particular setting is a qualitative judgment call, so in lieu of a more concrete general principle, I will share some illustrative examples from my own experience. Twitter POS. Gimpel et al. (2011) introduced (and Owoputi et al., 2012 documented in greater detail) a coarse-grained POS tagset for English tweets. Given that the eventual goal was to build a syntactic parser, we considered extending the Penn Treebank (Marcus et al., 1993) tagset with a few additional tags for social media phenomena (such as emoticons and hashtags). However, we also wanted a “lightweight” tagset to facilitate rapid annotation, and did not feel that the fine-grained inflectional distinctions made in the PTB tags—VB, VBP, VBZ, VBG, VBD, and VBN indicating different forms of verbs, for instance—were an ideal use of annotators’ time. We ultimately decided to craft a tagset coarser grained than the 45 PTB categories, and similar to Petrov et al.’s (2011) “universal” set of 12 categories,2 but with additional categories suited to tweets: ! (interject"
W15-1618,P14-5021,1,0.899131,"Missing"
W15-1618,Q14-1025,0,0.0402001,"rs would have produced more useful data. In general, it has been my experience that—some well-established best practices notwithstanding— designing an annotation framework involves a mixture of guesswork, intuition, and trial and error. I hope future research will succeed at making this process more empirical and more predictable (see also Hovy and Lavid, 2010; Garrette and Baldridge, 2013). There is a great deal more to discover with regard to understanding the range of text varieties (Baldwin et al., 2013), building statistical models of annotator bias (Snow et al., 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014), automatically detecting inconsistencies in linguistic data (Dickinson and Meurers, 2003; Loftsson, 2009; Kato and Matsubara, 2010), and bringing extrinsic models into the annotation loop (Baldridge and Osborne, 2004; Baldridge and Palmer, 2009; Settles, 2012). Acknowledgments I would like to thank the LAW organizers for hosting a panel on this topic, and Noah Smith, Bonnie Webber, and Archna Bhatia for their comments on a draft of this piece. References Jason Baldridge and Miles Osborne. 2004. Active learning and the total cost of annotation. In Dekang Lin and Dekai Wu, editors, Proc. of EMN"
W15-1618,petrov-etal-2012-universal,0,0.0348011,"Missing"
W15-1618,D11-1141,0,0.0725962,"Missing"
W15-1618,W13-2307,1,0.923186,"Missing"
W15-1618,D08-1027,0,0.0176172,"Missing"
W15-1618,W04-3202,0,\N,Missing
W15-1618,D09-1031,0,\N,Missing
W15-1618,I13-1041,0,\N,Missing
W16-1707,J08-4004,0,0.373054,"Missing"
W16-1707,P87-1023,0,0.758335,"ng annotators what additional relation they infer (besides that associated with instead itself), one still needs to ask: • For clauses starting with discourse adverbials other than instead, is the relation signalled by the adverbial all there is, or can an additional relation be inferred with the previous text? In the former case, no additional annotation is required; in the latter, it is. Existing work highlights the importance of understanding discourse relations in context, showing a range of phenomena that are sensitive to the semantic connection that holds between two spans of discourse (Hirschberg and Litman, 1987; Kehler and Rohde, 2013). Such connections can be made explicit in text via an overt connective or marked syntax; otherwise they must be inferred. Various contextual cues have been identified that guide the 49 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 49–58, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics • If another relation can be inferred, can it be inferred deterministically based on the adverbial alone? If so, no additional work is required, as the relation can be annotated automatically. • If it can’t be inferred based on t"
W16-1707,N13-1132,0,0.0250313,"the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a bo"
W16-1707,N13-1000,0,0.223269,"Missing"
W16-1707,P12-3029,0,0.0207026,"adverbial was replaced with a slash. Each passage contained one of the following discourse adverbials after the gap: actually, after all, first of all, for example, for instance, however, in fact, in general, in other words, indeed, instead, nevertheless, nonetheless, on the one hand, on the other hand, otherwise, specifically, then, therefore, and thus. These represent a sampling of high-frequency adverbials, which belong to a variety of semantic classes and which showed a range of conjunction co-occurrence patterns in counts extracted from the Google Books Ngram Corpus (Michel et al., 2011; Lin et al., 2012). Half the target passages originally contained a conjunction before the adverbial. For those explicit passages, we excised the conjunction and replaced it with a gap. For excerpts that were originally implicit passages, we simply inserted a gap before the adverbial. For each of the 20 adverbials, participants saw 25 explicit passages and 25 implicit passages, with the exception of however, which appeared in 25 implicit passages and 1 explicit passage (due to the rarity of conjunctions that naturally occur directly before however). The distribution of original (author-chosen) conjunctions in t"
W16-1707,A00-3008,0,0.104179,"r and annotate its relation, if any, to the previous sentence. This reflected the common assumption, noted earlier, that the situation is “either/or” – if a discourse relation is marked, there is nothing to infer. With respect to research on explicit multiple cooccurring connectives, over 15 years ago, Webber et al. (1999) used them to argue that discourse spans could be related by both adjacency relations and anaphoric relations. Similary, in the context of Catalan and Spanish oral narrative, Cuenca and Marín (2009) used them to argue for different patterns and degrees of discourse cohesion. Oates (2000) considered how multiple discourse connectives should be used in Natural Language Generation, noting that the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for probl"
W16-1707,Q14-1025,0,0.10735,"they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train"
W16-1707,W05-0311,0,0.587643,"number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train engine. The next sentence specified what should then be done. Over half their participants interpreted the pronoun it in this next 50 sentence as referring to the boxcar, while others interpreted it to refer to the engine. But the situation associated with these two different interpretations was the same in both cases, since the engine"
W16-1707,miltsakaki-etal-2004-penn,1,0.830866,"Missing"
W16-1707,W15-2703,1,0.830948,"lds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to a larger set of 20 discourse adverbials by eliciting ≈28K conjunction completions via crowdsourcing. Our data replicate and extend Rohde et al.’s findings that discourse adverbials do indeed license inferred conjunctions. Further, the diverse patterns observed for the adverbials include cases in which more than one valid connection can be inferred, each one endorsed by a substantial number of participants; suc"
W16-1707,W13-0124,1,0.96323,"er}@ed.ac.uk, {anna.y.dickinson, chrisclark272}@gmail.com, nschneid@inf.ed.ac.uk, aplouis@essex.ac.uk Abstract establishment of discourse relations (Hirschberg and Litman, 1987; Kehler, 2002; Webber, 2013). When it comes to producing resources annotated with discourse relations—e.g., the Penn Discourse Treebank (PDTB; Prasad et al., 2008)—it is commonly assumed that at most a single discourse relation holds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to"
W16-1707,prasad-etal-2008-penn,1,\N,Missing
W16-1712,P08-1037,0,0.0242269,"of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clu"
W16-1712,bonial-etal-2014-propbank,1,0.91389,"Missing"
W16-1712,W07-1604,0,0.0246655,"his paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and ev"
W16-1712,W06-1670,0,0.0856608,"asses that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘ind"
W16-1712,D09-1047,0,0.77986,"ities, prepositions serve as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and"
W16-1712,hashemi-hwa-2014-comparison,0,0.105801,"tated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotate"
W16-1712,L16-1629,1,0.821862,"opBank expertise) checked the gold PropBank annotations, agreeing that 5 of the tokens were clearly incorrect. This analysis tells us that obvious errors with both types of annotation are indeed present in the corpus (11 tokens in the sample), adding some noise to the supersense–function tag correspondences. However, the outright errors are probably dwarfed by difficult/borderline cases for which the annotations are not entirely consistent throughout the corpus. For example, on time (i.e., ‘not late’) is variously annotated as S TATE, M ANNER, and T IME. Inconsistency detection methods (e.g., Hollenstein et al., 2016) may help identify these— though it remains to be seen whether methods developed for nouns and verbs would succeed on function words so polysemous as prepositions. Summary. The (mostly) clean correspondences of the supersenses to the independently annotated PropBank modifier labels speak to the linguistic validity of our supersense hierarchy. On the other hand, the confusion evident for the supersense labels corresponding to PropBank’s numbered arguments suggests further analysis and refinement is necessary for both annotation schemes. Some of these issues—especially correspondences between la"
W16-1712,C10-2052,0,0.267964,"arners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previ"
W16-1712,S14-1001,0,0.0160165,"al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English preposition"
W16-1712,P14-1120,0,0.236706,"s-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition token per sentence. By contrast, we will fully annotate documents for all preposition tokens. No interannotator agreement figures have been reported for the PDEP data to indicate its quality, or the overall difficulty of token annotation with TPP senses across a broad range of prepositions. 2.2 Supersenses From the literature on other kinds of supersenses, there is reason to believe that t"
W16-1712,S07-1005,0,0.110407,"Missing"
W16-1712,W03-0411,0,0.0816644,"Missing"
W16-1712,J05-1004,1,0.526768,"Missing"
W16-1712,picca-etal-2008-supersense,0,0.131684,"rdNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they gro"
W16-1712,N13-1076,1,0.852834,"for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotati"
W16-1712,P12-2050,1,0.867655,"v et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen"
W16-1712,schneider-etal-2014-comprehensive,1,0.808201,"with sparse training data. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infi"
W16-1712,N15-1177,1,0.85174,"ta. The Supersense Hierarchy Unlike the noun, verb, and adjective supersense schemes mentioned in §2.2, the preposition supersense inventory is hierarchical (as are Litkowski’s (2015) and Müller et al.’s (2012) inventories). The hierarchy, depicted in figure 2, encodes inheritance: 6 http://tiny.cc/prepwiki 101 3 3.1 Corpus Annotation Annotating Preposition Supersenses Source data. We fully annotated the R EVIEWS section of the English Web Treebank (Bies et al., 2012), chosen because it had previously been annotated for multiword expressions, noun and verb supersenses (Schneider et al., 2014; Schneider and Smith, 2015), and PropBank predicate-argument structures (§4). The corpus comprises 55,579 tokens organized into 3,812 sentences and 723 documents with gold tokenization and PTB-style POS tags. Identifying preposition tokens. TPP, and therefore PrepWiki, contains senses for canonical prepositions, i.e., those used transitively in the [PP P NP] construction. Taking inspiration from Pullum and Huddleston (2002), PrepWiki further assigns supersenses to spatiotemporal particle uses of out, up, away, together, etc., and subordinating uses of as, after, in, with, etc. (including infinitival to and infinitival-s"
W16-1712,W15-1612,1,0.871481,"own University Jena D. Hwang IHMC Vivek Srikumar University of Utah jhwang@ihmc.us svivek@cs.utah.edu nschneid@inf.ed.ac.uk Meredith Green Abhijit Suresh Kathryn Conger Tim O’Gorman University of Colorado at Boulder Martha Palmer {laura.green,abhijit.suresh,kathryn.conger,timothy.ogorman,martha.palmer}@colorado.edu Abstract (1) I have been going to/D ESTINATION the Wildwood_,_NJ for/D URATION over 30 years for/P URPOSE summer~vacations We present the first corpus annotated with preposition supersenses, unlexicalized categories for semantic functions that can be marked by English prepositions (Schneider et al., 2015). The preposition supersenses are organized hierarchically and designed to facilitate comprehensive manual annotation. Our dataset is publicly released on the web.1 1 (2) It is close to/L OCATION bus_lines for/D ESTINATION Opera_Plaza (3) I was looking~to/`i bring a customer to/D ESTINATION their lot to/P URPOSE buy a car Figure 1: Preposition supersenses illustrating the polysemy of to and for. Both can mark a D ESTINATION or P URPOSE, while there are other functions that do not overlap. The syntactic complement use of infinitival to is tagged as `i. The over token in (1) receives the label A"
W16-1712,W97-0811,0,0.211554,"itions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic function"
W16-1712,W12-0514,0,0.0797358,"ew corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requir"
W16-1712,D11-1012,1,0.950338,"e as essential linkers of meaning, and the few extremely frequent ones are exploited for many different functions (figure 1). For all their importance, however, prepositions have received relatively little attention in computational semantics, and the community has not yet arrived at a comprehensive and reliable scheme for annotating the semantics of prepositions in context (§2). We believe that such annotation of preposition functions is needed if preposition sense disambiguation systems are to be useful for downstream tasks—e.g., translation3 or semantic parsing (cf. Dahlmeier et al., 2009; Srikumar and Roth, 2011). This paper describes a new corpus, fully annotated with preposition supersenses (hierarchically 2 1 STREUSLE 3.0, available at http://www.cs.cmu.edu/ ~ark/LexSem/ 2 http://www.wordfrequency.info/free.asp?s=y 3 Background and Motivation Theoretical linguists have puzzled over questions such as how individual prepositions can acquire such a broad range of meanings and to what extent those meanings are systematically related (e.g., This work focuses on English, but adposition and case systems vary considerably across languages, challenging second language learners and machine translation system"
W16-1712,Q13-1019,1,0.913045,"translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English p"
W16-1712,N09-3017,0,0.440547,"ly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual preposition types. Most previous datasets of English preposition semantics at the token level (Litkowski and Hargraves, 2005, 2007; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Srikumar and Roth, 2013a) only cover high-frequency prepositions (the 34 represented in the SemEval-2007 shared task based on TPP, or a subset thereof).4 We sought a scheme that would facilitate comprehensive semantic annotation of all preposition tokens in a corpus, covering the full range of usages possible for all English preposition types. The recent TPP PDEP corpus (Litkowski, 2014, 2015) comes closer to this goal, as it consists of randomly sampled tokens for over 300 types. However, since sentences were sampled separately for each preposition, there is only one annotated preposition t"
W16-1712,D11-1116,0,0.0234141,"et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hovy (2011), and the “relations” of Srikumar and Roth (2013a) similarly label coarse-grained semantic functions of English prepositions; notably, they group senses from a lexicon rather than directly annotating tokens, and restrict each sense 5 php 100 http://www.clres.com/db/classes/ClassAnalysis. Superset Possessor Co-Agent Creator Whole Elements Instance Agent Species Causer Quantity Configuration Patient Accompanier Co-Patient Undergoer Reciprocation Purpose Theme Participant Experiencer Stimulus Via Place Value Path Manner Time Frequency Duration Temporal Circumstance Extent Location Beneficiary Ins"
W16-1712,D15-1243,0,0.0468006,"Missing"
W16-1712,W13-0906,0,0.0262783,"nemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al., 2012). Like with WordNet for nouns and verbs, the same argument holds for prepositions: TPPstyle sense annotation requires familiarity with a different set of (often highly nuanced) distinctions for each preposition type. For example, in has 15 different TPP senses, among them in 10(7a) ‘indicating the key in which a piece of music is written: Mozart’s Piano Concerto in E flat’. Supersenses have been exploited for a variety of tasks (e.g., Agirre et al., 2008; Tsvetkov et al., 2013, 2015), and full-sentence noun and verb taggers have been built for several languages (Segond et al., 1997; Johannsen et al., 2014; Picca et al., 2008; Martínez Alonso et al., 2015; Schneider et al., 2013, 2016). They are typically implemented as sequence taggers. In the present work, we extend a corpus that has already been hand-annotated with noun and verb supersenses, thus raising the possibility of systems that can learn all three kinds of supersenses jointly (cf. Srikumar and Roth, 2011). Though they go by other names, the TPP “classes” (Litkowski, 2015),5 the “clusters” of Tratz and Hov"
W16-1712,tsvetkov-etal-2014-augmenting-english,1,0.844664,", a disambiguation system trained on this dataset will therefore be biased and perform poorly on an ecologically valid sample of tokens. preposition supersenses (Schneider et al., 2015) will be more scalable and useful than senses. The term supersense has been applied to lexical semantic classes that label a large number of word types (i.e., they are unlexicalized). The best-known supersense scheme draws on two inventories—one for nouns and one for verbs—which originated as a high-level partitioning of senses in WordNet (Miller et al., 1990). A scheme for adjectives has been proposed as well (Tsvetkov et al., 2014). One argument advanced in favor of supersenses is that they provide a coarse level of generalization for essential contextual distinctions—such as artifact vs. person for chair, or temporal vs. locative in—without being so fine-grained that systems cannot learn them (Ciaramita and Altun, 2006). A similar argument applies for human learning as pertains to rapid, cost-effective, and open-vocabulary annotation of corpora: an inventory of dozens of categories (with mnemonic names) can be learned and applied to unlimited vocabulary without having to refer to dictionary definitions (Schneider et al"
W16-1712,S07-1051,0,0.3404,"ing second language learners and machine translation systems (Chodorow et al., 2007; Shilon et al., 2012; Hashemi and Hwa, 2014). 99 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 99–109, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; O’Dowd, 1998; Saint-Dizier and Ide, 2006; Lindstromberg, 2010). Prepositional polysemy has also been recognized as a challenge for AI (Herskovits, 1986) and natural language processing, motivating semantic disambiguation systems (O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; Hovy et al., 2010; Srikumar and Roth, 2013b). Training and evaluating these requires semantically annotated corpus data. Below, we comment briefly on existing resources and why (in our view) a new resource is needed to “road-test” an alternative, hopefully more scalable, semantic representation for prepositions. 2.1 Existing Preposition Corpora Beginning with the seminal resources from The Preposition Project (TPP; Litkowski and Hargraves, 2005), the computational study of preposition semantics has been fundamentally grounded in corpus-based lexicography centered around individual prepositio"
W16-1712,W15-1806,0,\N,Missing
W16-1712,S16-1084,1,\N,Missing
W17-6814,W15-2703,1,0.745006,"erent discourse connectives are used to realize particular types of coherence relations remains unresolved. While some connectives show nearly one-to-one mappings with individual coherence relations, other connectives permit much more flexible usage across contexts. One early enterprise targeting the above question was Alistair Knott’s systematic assessment of the conditions that permit one connective to substitute for another (Knott, 1996). Substitutability, along with categories of coherence relations, then predicts the behavior of individual connectives. Another such enterprise is our own (Rohde et al., 2015, 2016, 2017) on implicit connectives in the context of explicit discourse adverbials. Using naturally occurring passages, we have gathered judgments from multiple participants as to what connective, if any, they could insert into a particular passage immediately before an existing discourse adverbial, to make explicit the author’s intended message. For example, when shown the passage It’s too far to walk. Instead let’s take the bus., a participant might insert so to express what she takes to be the intended causal reading. Our findings show variation across participant responses. Such diverge"
W17-6814,W16-1707,1,0.859986,"Missing"
W18-4912,J86-2003,0,0.601705,"to refine both our annotation procedure and our overall labeling scheme. 3.3 Annotation Targets To keep the annotation task manageable, we only consider as sites for tense/aspect annotation those concepts in the AMR that correspond to finite predications in the syntax.9 This restriction is motivated by the understanding that finite clauses explicitly describe eventualities on the temporal dimension, and thus tense-aspect category values are relevant (Langacker, 1987). We understand eventualities to include all kinds of events: states, activities, achievements, accomplishments, and processes (Bach, 1986). Though labels themselves are annotated on the AMR concept that corresponds to the finite verb, they refer to the event structure denoted by the entire predication as evident in the frameset (which corresponds to the verb’s argument structure). Thus, while the sentence “I read a book last night while sitting” possesses only one annotation target (read-01), “I read a book last night while I was sitting” possesses two targets (read-01, sit-01). Note that a verb’s arguments may be relevant to the aspectual annotation on the annotation target: e.g., “I read a book last night” is telic whereas “I"
W18-4912,W13-2322,1,0.898502,"of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for NLP tasks and applications that require sophisticated reasoning about time and event structure. 1 Introduction The Abstract Meaning Representation (AMR) is a readable and compact framework for broad-coverage semantic annotation of English sentences (Banarescu et al., 2013).1 AMR aims to abstract away from syntactic idiosyncrasies such that sentences with the same basic meaning are represented by the same AMR graph. This paper extends existing AMR to include a coarse-grained representation of tense and aspect. Example (1) shows a sentence with its annotation from the current AMR corpus alongside our proposed additions for tense (in blue) and aspect (in purple): (1) “Your brother’s in the hospital and he’s not going to last the night.” CURRENT NEW (a / and :op1 (b / be-located-at-91 :ARG1 (p / person :ARG0-of (h / have-rel-role-91 :ARG1 (y / you) :ARG2 (b2 / brot"
W18-4912,P14-2085,0,0.0334218,"ent (identify it and anchor it in time); (ii) how to order events with respect to one another (considering both lexical and discourse properties of ordering); (iii) how to reason with contextually underspecified events (e.g. “last week” and “two weeks before”); and (iv) how to reason about the persistence of events (i.e. how long the event or its outcome lasts). TimeML precisely identifies events and their temporal relations within a text. This ability to reason with contextually underspecified events at the sentence level is a main concern of the current framework. Situation entity labeling. Friedrich and Palmer (2014) and Friedrich et al. (2016) present a method for automatically identifying the type of a situation entity (SE) using a system of seven SE types (Smith, 2008). The authors assume SEs to be expressed at the clause level: state, event, report, generic sentence, generalizing sentence, question, and imperative. The labeling of SE types is a non-trivial task even for humans, and the annotation trial encountered many borderline cases—an issue the current framework also deals with. The clause-level focus of annotation is the most similar to the current proposed framework, and the current framework ma"
W18-4912,P16-1166,0,0.379599,"n time); (ii) how to order events with respect to one another (considering both lexical and discourse properties of ordering); (iii) how to reason with contextually underspecified events (e.g. “last week” and “two weeks before”); and (iv) how to reason about the persistence of events (i.e. how long the event or its outcome lasts). TimeML precisely identifies events and their temporal relations within a text. This ability to reason with contextually underspecified events at the sentence level is a main concern of the current framework. Situation entity labeling. Friedrich and Palmer (2014) and Friedrich et al. (2016) present a method for automatically identifying the type of a situation entity (SE) using a system of seven SE types (Smith, 2008). The authors assume SEs to be expressed at the clause level: state, event, report, generic sentence, generalizing sentence, question, and imperative. The labeling of SE types is a non-trivial task even for humans, and the annotation trial encountered many borderline cases—an issue the current framework also deals with. The clause-level focus of annotation is the most similar to the current proposed framework, and the current framework makes reference to a few of th"
W18-4912,W16-1702,0,0.0994793,"ns, going deeper than grammatical tense/aspect categories (e.g. morphological past/present, progressive, perfect, etc.), which are polysemous. (iii) Our extensions are mostly categorical in nature (e.g. past/present/future time, static vs. dynamic, habitual vs. episodic) rather than a full calculus of timeline points and intervals. (iv) We focus on the semantic contrasts that are important to English grammar. Nevertheless, our design decisions are informed by reference to tense and aspect expression in other languages and the possibility of AMR being extended beyond English (Xue et al., 2014; Li et al., 2016; Migueles-Abraira et al., 2018). (v) Our current approach is limited to the semantics of finite verbs/predications. (vi) Our current approach is annotated at/below the level of the individual sentence. Additional temporal relations in discourse would need to be annotated at the document level. (vii) As AMR prioritizes broad-coverage corpus annotation, we have developed our scheme with reference to 99 corpus data. This has required us to consider tense/aspect as it intersects with other semantics and constructions, including narrative tense morphology, generics/habituals, conditionals, and mod"
W18-4912,L18-1486,0,0.060761,"Missing"
W18-4912,J88-2003,0,0.864643,"rk (§4), and we conclude with ongoing troubleshooting and future work to update the annotation scheme (§5). 2 The Case for Tense and Aspect Semantics in AMR Developing a framework for semantic representation of tense and aspect necessitates striking a balance between the elements that grammatically encode relevant information and the more flexible pragmatic effects that such information may have. There has been a vast amount of research on temporality and aspectuality, both theoretical (Reichenbach, 1947; Vendler, 1957; Comrie, 1976; Comrie, 1985; Langacker, 1982; Dowty, 1986; Hinrichs, 1986; Moens and Steedman, 1988; Klein, 1994; Chang, 1997; Chang et al., 1998; Partee, 1999; Allen et al., 2008; Croft, 2012) and applied to corpora (see §2.2). However, we wish to capture basic distinctions of practical value for NLP applications, and so we must distill this research into a small number of annotation categories. Here we present elements of existing AMR that relate to the new annotations; we then consider how previous and existing frameworks apart from AMR have attempted to define and specify the temporal nature of eventualities in text. These serve as points of comparison for how to integrate both theoreti"
W18-4912,W16-1007,0,0.0218449,") point. By decomposing complex events in a clause, the overall dynamic causal network of entities interacting over time as described in a text may be modeled. Though the RED framework is more fine-grained and discourse-oriented than the current proposed framework, the need to specify event-event relationships for a complete event structure of the sentence is an important takeaway. Challenging areas (event relations conveyed via presupposition, modality, idioms, and/or inference) overlap with challenges for the current guidelines, as well. Causal and Temporal Relation Scheme (CaTeRS). CaTeRS (Mostafazadeh et al., 2016) is a semantic annotation framework of event-event relations in commonsense stories designed to capture a comprehensive set of causal and temporal relations. The authors define an event to be any lexical entry under the following ontology types in the TRIPS ontology (Allen et al., 2008): Event-of-state; Event-of-change; Event-type; Physical-condition; Occurring; and Natural-phenomenon. Temporal relations are specified between events following the Interval Algebra of (Allen, 1984). Causal relations are further specified 7 Different languages face different hurdles: e.g., tense is not overt in C"
W18-4912,W16-5706,0,0.0433218,"Missing"
W18-4912,C18-1313,1,0.842596,"Missing"
W18-4912,J05-1004,0,0.281846,"b / be-located-at-91 :ARG1 (p / person :ARG0-of (h / have-rel-role-91 :ARG1 (y / you) :ARG2 (b2 / brother))) :ARG2 (h2 / hospital)) :op2 (l / last-01 :polarity - :stable - :time (n2 / now) :ongoing :time (a / after :op1 (n3 / now)) :ARG1 p :ARG2 (d / date-entity :dayperiod (n / night)))) In AMR, each concept (entity or predicate) is tied to a variable which uniquely identifies a graph node. In PENMAN notation (Matthiessen and Bateman, 1991), a slash links a variable to its concept, and names of relations/roles (edges in the graph) are preceded by a colon. PropBank framesets of semantic roles (Palmer et al., 2005) that account for argument structure play a central role in AMR design; sentential AMR additionally includes entity typing and wikification, as well as entity and event coreference within sentences.2 This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 http://amr.isi.edu/; data released at https://amr.isi.edu/download/amr-bank-struct-v1.6.txt (Little Prince) and https://catalog.ldc.upenn.edu/LDC2017T10 2 Multi-sentence AMR is an additional layer developed to indicate cross-sentence coreference link"
W18-4912,D10-1032,0,0.0297989,"sed by used to. as cause (before/overlaps); enable (before/overlaps); prevent (before/overlaps); and cause-to-end (before/overlaps/during). For example, “The [famine]e1 ended the [war]e2 ” relates e1 and e2 as cause-to-end (overlap); these relations are annotated at both the sentence and discourse levels. CaTeRs is noteworthy for the current paper because, while the ability to describe events as individual units may be relatively straightforward, the intersective temporal and causal relations between events may be less obvious. Tense Sense Disambiguation. The Tense Sense Disambiguation (TSD) (Reichart and Rappoport, 2010) task addresses the multiple possible semantic senses of English grammatical tense constructions, which are listed in a lexicon. For example, the ‘present simple’ construction has several possible meanings, including that of a future event at a definite time (viz. arrives in “My brother arrives in the evening”) and that of a repeated event (rises in “The sun rises in the East”). We share the philosophy of TSD that grammatical tense constructions are ambiguous, though our approach is less atomic in nature to make fewer distinctions and create semantic tense/aspect categories more amenable to an"
W18-4912,D14-1204,0,0.0218173,"es designed to capture a comprehensive set of causal and temporal relations. The authors define an event to be any lexical entry under the following ontology types in the TRIPS ontology (Allen et al., 2008): Event-of-state; Event-of-change; Event-type; Physical-condition; Occurring; and Natural-phenomenon. Temporal relations are specified between events following the Interval Algebra of (Allen, 1984). Causal relations are further specified 7 Different languages face different hurdles: e.g., tense is not overt in Chinese grammar, which has prompted work on disambiguation using contextual cues (Zhang and Xue, 2014). The approaches listed here assume different stances with regards to their cross-linguistic validity, for which the reader is directed to the original papers. 98 States :stable + Episodic :time ‚ now if salient He lives/lived/used to live in Paris. Habitual Dynamic Events :stable :time ‚ now He was/is living in Paris. :habitual +, and :time ‚ now if salient He is in Paris often. :time ‚ now, :ongoing +/-/?, and :complete +/- if telic and realized He went/is going/will go to Paris. He has been to Paris (ever, recently). He has been touring Paris for the past week. :habitual +, and :time ‚ now"
W18-4921,W07-2441,0,0.0451838,"e size of the corpus should allow for at least 3,500 MWE annotations 5. The language must be of sufficiently high quality There were several corpora considered for selection, including the DiMSUM corpus (Schneider et al., 2016), the UP/TAP corpus,3 Wikidata parallel text (Vrandeˇci´c and Krötzsch, 2014) and the Universal Dependencies (UD) treebanks.4 Three corpora from the UD treebanks for English were ultimately selected as a source of data, as they alone fulfilled the criteria mentioned above: text was selected from the English-EWT corpus (Silveira et al., 2014),5 the LinES parallel corpus (Ahrenberg, 2007) and the Parallel Universal Dependencies (PUD) treebank (Zeman et al., 2017).6 The files were extracted in CoNLL-U format and converted to FoLiA XML format (see section 3) for annotating. The training, development and testing datasets for each treebank were concatenated, and then split into files of 201 sentences for annotation. 3 Annotation During the data preparation period, annotators were trained in the use of the FoLiA Linguistic Annotation Tool (FLAT). FLAT is an open-source web-based environment,7 using the XML-based FoLiA format. In order to aid annotators in annotating only verbal MWE"
W18-4921,J17-4005,0,0.162863,"Missing"
W18-4921,W15-0905,0,0.0600086,"Missing"
W18-4921,N15-1177,1,0.831467,"ting multiword expressions, and were all native speakers of English. Four dialects of English were represented: Irish English, British English, American English and Canadian English. 3 Documentation for UP/TAP: https://www.l2f.inesc-id.pt/~thomas/metashare/report-UP-TAP.pdf Documentation for UD: http://universaldependencies.org 5 Originally sourced from the English Web Treebank (Bies et al., 2012) 6 Though not a part of the task dataset, we have also fully annotated the Reviews portion of the UD English-EWT corpus by adding VMWE types to the existing VMWEs in STREUSLE (Schneider et al., 2014; Schneider and Smith, 2015, https: //github.com/nert-gu/streusle/); they were previously uncategorized. STREUSLE as of version 4.1 comprises 3812 sentences and 871 VMWE instances (121 IAV, 12 LVC.cause, 123 LVC.full, 310 VID, 206 VPC.full, 99 VPC.semi). 4 7 http://flat.readthedocs.io/en/latest/ 194 Figure 1: Screenshot of the FLAT Platform 3.1 Categories of VMWE Seven categories of VMWE were used in the English annotation task: Verbal Idioms (VID), Verb-Particle Constructions (VPC.full and VPC.semi),8 Light-Verb Constructions (LVC.full and LVC.cause),8 MultiVerb Constructions (MVC) and Inherently Adpositional Verbs (IA"
W18-4921,schneider-etal-2014-comprehensive,1,0.86405,"th or interest in annotating multiword expressions, and were all native speakers of English. Four dialects of English were represented: Irish English, British English, American English and Canadian English. 3 Documentation for UP/TAP: https://www.l2f.inesc-id.pt/~thomas/metashare/report-UP-TAP.pdf Documentation for UD: http://universaldependencies.org 5 Originally sourced from the English Web Treebank (Bies et al., 2012) 6 Though not a part of the task dataset, we have also fully annotated the Reviews portion of the UD English-EWT corpus by adding VMWE types to the existing VMWEs in STREUSLE (Schneider et al., 2014; Schneider and Smith, 2015, https: //github.com/nert-gu/streusle/); they were previously uncategorized. STREUSLE as of version 4.1 comprises 3812 sentences and 871 VMWE instances (121 IAV, 12 LVC.cause, 123 LVC.full, 310 VID, 206 VPC.full, 99 VPC.semi). 4 7 http://flat.readthedocs.io/en/latest/ 194 Figure 1: Screenshot of the FLAT Platform 3.1 Categories of VMWE Seven categories of VMWE were used in the English annotation task: Verbal Idioms (VID), Verb-Particle Constructions (VPC.full and VPC.semi),8 Light-Verb Constructions (LVC.full and LVC.cause),8 MultiVerb Constructions (MVC) and Inhere"
W18-4921,silveira-etal-2014-gold,0,0.0775921,"Missing"
W18-4925,W17-1717,1,0.829766,"on in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our effo"
W18-4925,W17-1716,1,0.892903,"Missing"
W18-4925,P14-1070,1,0.86208,"mon guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The fi"
W18-4925,F12-2024,1,0.868442,"om the Croatian version of the SETimes corpora: mostly running text but also selected fragments, such as introductory blurbs and image descriptions characteristic of newswire text. The English corpus consists of 7,437 sentences taken from three of the UD: the Gold Standard Universal Dependencies Corpus for English, the LinES parallel corpus and the Parallel Universal Dependencies treebank. The Farsi corpus is built on top of the MULTEXT-East corpora (QasemiZadeh and Rahimi, 2006) and VMWE annotations are added to a portion of Orwell’s 1984 novel. The French corpus contains the Sequoia corpus (Candito and Seddah, 2012) converted to UD, the GDS French UD treebank, the French part of the Partut corpus, and part of the Parallel UD (PUD) corpus. The German corpus contains shuffled sentences crawled from online news, reviews and wikis, derived from the WMT16 shared task data (Bojar et al., 2016), and Universal Dependencies v2.0. The Greek corpus comprises Wikipedia articles and newswire texts from various on-line newspaper editions and news portals. The Hebrew corpus contains news and articles from Arutz 7 and HaAretz news websites, collected by the MILA Knowledge Center for Processing Hebrew. The Hindi corpus r"
W18-4925,P16-1016,0,0.0692628,"actices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generat"
W18-4925,J17-4005,1,0.881403,"Missing"
W18-4925,N09-1037,0,0.0413995,"anks for 15 languages, collaboratively documented according to common guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural netwo"
W18-4925,D11-1067,0,0.0237076,"Missing"
W18-4925,J13-1009,0,0.0221261,"Missing"
W18-4925,W17-1707,0,0.0299396,"en et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation guidelines were gradually enhanced, so as"
W18-4925,C14-1177,0,0.023678,"Missing"
W18-4925,W14-0406,0,0.112167,"Missing"
W18-4925,W17-1715,0,0.019845,"ore integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation"
W18-4925,P15-1108,1,0.836029,"f MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary e"
W18-4925,W17-1706,0,0.0136581,"ing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better"
W18-4925,L16-1262,0,0.0499042,"Missing"
W18-4925,S16-1084,1,0.93644,"participating systems, their methods and obtained results are also presented and analysed. 1 Introduction Across languages, multiword expressions (MWEs) are widely recognized as a significant challenge for natural language processing (NLP) (Sag et al., 2002; Baldwin and Kim, 2010). An international and highly multilingual research community, forged via regular workshops and initiatives such as the PARSEME network (Savary et al., 2015), has rallied around the goals of characterizing MWEs in lexicons, grammars and corpora and enabling systems to process them. Recent shared tasks, namely DiMSUM (Schneider et al., 2016) and the first edition of the PARSEME Shared Task on automatic identification of verbal multiword expressions in 2017 (Savary et al., 2017), have helped drive MWE research forward, yielding new corpora and testbeds for MWEs identification systems. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 222 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 222–240 Santa Fe, New Mexico, USA, August 25-26, 2018. This paper describ"
W18-4925,W17-1705,1,0.741206,"ome popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWE"
W18-4925,I13-1024,1,0.84782,"ns, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on lan"
W18-4925,C16-1042,1,0.834941,"guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous ver"
W18-4925,W10-3705,0,0.0133537,"rd to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new"
W18-4925,W14-0804,0,0.0128345,"ovide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not cov"
W19-0405,L18-1157,0,0.0190504,"its semantics, there has also been initial work using CCG to build parsers for Abstract Meaning Representation (AMR; Banarescu et al., 2013), a standard with which a large “sembank” of English sentences1 has been manually annotated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-based or transition-based parsing methods (e.g., Flanigan et al., 2014; Wang et al., 2016; Lyu and Titov, 2018) to transform the sentence string or syntactic parse into a semantic graph via a learned statistical model, without any explicit characterization of the syntax-semantics interface. There is good reason to apply CCG to the AMR parsing task: apart from transparency of the syntax-semantics interface, state-of-the-art AMR parsers are known to be weak at reentrancy (e.g., Lyu and"
W19-0405,D15-1198,0,0.109535,"e. There is good reason to apply CCG to the AMR parsing task: apart from transparency of the syntax-semantics interface, state-of-the-art AMR parsers are known to be weak at reentrancy (e.g., Lyu and Titov, 2018), which presumably can be partially attributed to syntactic reentrancy in control constructions, for example. Prior work applying CCG to AMR parsing has begun to address this, but some of the important mechanisms that make CCG a linguistically powerful and robust theory have yet to be incorporated into these approaches. In this paper, we build on a core insight of previous work (e.g., Artzi et al., 2015; Beschke and Menzel, 2018) that AMR fragments can be directly represented as the semantics of CCG lexical entries. With appropriate definitions of the lexical items and combinatorial rules of CCG, the compositionality of CCG gives a derivation of a full AMR “for free”. In other words, AMR parsing can be reduced to CCG parsing (plus some additional semantic disambiguation and postprocessing). On a practical level, this should allow us to take advantage of existing CCG datasets and parsing methods for AMR parsing. In addition, explicitly storing AMR fragments in the CCG lexicon would provide a"
W19-0405,W13-2322,1,0.826703,"ions, requiring the correct word order and producing the correct semantic dependencies. For example, consider the sentence “Who did John seem to forget to invite to attend?”: the correct logical form—in propositional logic, something like seem(forget(Johni , invite(Johni , whoj , attend(whoj ))))—is nontrivial, requiring a precise account of several constructions that conspire to produce long-range dependencies. Whereas CCG traditionally uses some version of lambda calculus for its semantics, there has also been initial work using CCG to build parsers for Abstract Meaning Representation (AMR; Banarescu et al., 2013), a standard with which a large “sembank” of English sentences1 has been manually annotated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-base"
W19-0405,W15-0128,0,0.023693,"e CCG derivation when there is a preposed adjunct (as in “Tomorrow, John may eat rice”) because the modifier will semantically attach under the root of the semantics of the rest of the clause (possible-01 from “may”) rather than the main event predicate eat-01. Full derivations for these problem cases, as well as examples of purpose clauses, raising, and subject and object control, are given in appendix A. We will explore whether such limitations can be addressed via postprocessing of the parse, or whether additional expressive power in the combinators is necessary. Finally, as pointed out by Bender et al. (2015), AMR annotations sometimes go beyond the compositional ‘sentence meaning’ and incorporate elements of ‘speaker meaning’, though an empirical study of AMR data found the rate of noncompositional structures to be relatively low (Szubert et al., 2018). Beschke and Menzel (2018) give interesting examples of AMR fragments that would be difficult to derive compositionally, e.g., “settled on Indianapolis for its board meeting”, where the AMR attaches Indianapolis as the location of the meeting and the meeting as the thing that was settled on (reflecting the inference settle on LOCATION for ACTIVITY"
W19-0405,S18-2006,0,0.807885,"son to apply CCG to the AMR parsing task: apart from transparency of the syntax-semantics interface, state-of-the-art AMR parsers are known to be weak at reentrancy (e.g., Lyu and Titov, 2018), which presumably can be partially attributed to syntactic reentrancy in control constructions, for example. Prior work applying CCG to AMR parsing has begun to address this, but some of the important mechanisms that make CCG a linguistically powerful and robust theory have yet to be incorporated into these approaches. In this paper, we build on a core insight of previous work (e.g., Artzi et al., 2015; Beschke and Menzel, 2018) that AMR fragments can be directly represented as the semantics of CCG lexical entries. With appropriate definitions of the lexical items and combinatorial rules of CCG, the compositionality of CCG gives a derivation of a full AMR “for free”. In other words, AMR parsing can be reduced to CCG parsing (plus some additional semantic disambiguation and postprocessing). On a practical level, this should allow us to take advantage of existing CCG datasets and parsing methods for AMR parsing. In addition, explicitly storing AMR fragments in the CCG lexicon would provide a level of interpretability n"
W19-0405,W14-0816,0,0.0230057,"mantic arguments via prepositional phrases, possessives, and light verb constructions, as shown in table 2. AMR uses a canonical form with a predicate (typically based on a verbal paraphrase), treating John decided, John’s decision, and John made a/his decision as semantically equivalent. Despite some work on integrating event nominals and multiword expressions into CCG (Constable and Curran, 2009; Honnibal et al., 2010; de Lhoneux, 2014), we are not aware of any CCG analyses of light verb constructions, which have been studied computationally in other frameworks (e.g., Baldwin and Kim, 2010; Bonial et al., 2014; Ramisch et al., 2018), that gives them semantics equivalent to a content verb paraphrase. We offer such an analysis based on three principles: 1. The event frame is in the semantics of the eventive noun or verb. 2. For any syntactic argument of a noun or verb, the corresponding edge (and free variable) is in the semantics of the noun or verb. 3. Any function word (light verb, ’s, preposition, or infinitival to) that links the eventive noun to its semantic argument has an associated edge (and free variables) in its semantics. Note that when a verb or noun takes a PP complement, principles 2 a"
W19-0405,P13-1091,0,0.0255137,"h as type raising to have no effect on the semantics, whereas we will take another route for type raising (§3.4), and will introduce new, relation-wise versions of application and composition (§3.3). Finally, whereas Beschke and Menzel devote most of their paper to a lexicon induction algorithm and experiments, we focus on the linguistic motivation for our definition of the combinators, and leave the development of suitable lexicon induction techniques to future work. A related graph formalism called hyperedge replacement grammar is also used in the AMR parsing literature (Jones et al., 2012; Chiang et al., 2013; Peng et al., 2015; Peng and Gildea, 2016; Björklund et al., 2016; Groschwitz et al., 2018). Hyperedge replacement grammars (Rozenberg, 1997) are a formal way of combining subgraphs to derive a larger graph, based on an extension of Context Free Grammars to graphs instead of strings. Readers may assume that the graph formalism described in this paper is a simplified hyperedge replacement grammar which only allows hyperedges of rank 1. 3 Graph Semantics AMR is designed to represent semantics at the sentence level. For CCG lexical entries and combinators to parse AMR semantics, we need to forma"
W19-0405,D18-1217,0,0.0158233,"one profitable direction exploits computational linguistic grammar formalisms that make explicit the correspondence between the linguistic form of a sentence and the semantics (e.g., broad-coverage logical forms, or database queries in a domain-specific query language). In particular, English semantic parsers using Combinatory Categorial Grammar (CCG; Steedman, 2000) have been quite successful thanks to the CCGBank resource (Hockenmaier and Steedman, 2007; Honnibal et al., 2010) and the broad-coverage statistical parsing models trained on it (e.g., Clark and Curran, 2004; Lewis et al., 2016; Clark et al., 2018). The CCG formalism assumes that all language-specific grammatical information is stored in a lexicon: each word in the lexicon is associated with a structured syntactic category and a semantic form, such that the compositional potentials of the category and the semantics are isomorphic. A small universal set of combinators are responsible for assembling constituents into a full syntactic derivation; each combinator operates on adjacent constituents with appropriate categories to produce a new constituent and its compositional semantics, subject to constraints. A full grammar thus allows wellf"
W19-0405,P04-1014,0,0.0553985,"Among many techniques for semantic parsing, one profitable direction exploits computational linguistic grammar formalisms that make explicit the correspondence between the linguistic form of a sentence and the semantics (e.g., broad-coverage logical forms, or database queries in a domain-specific query language). In particular, English semantic parsers using Combinatory Categorial Grammar (CCG; Steedman, 2000) have been quite successful thanks to the CCGBank resource (Hockenmaier and Steedman, 2007; Honnibal et al., 2010) and the broad-coverage statistical parsing models trained on it (e.g., Clark and Curran, 2004; Lewis et al., 2016; Clark et al., 2018). The CCG formalism assumes that all language-specific grammatical information is stored in a lexicon: each word in the lexicon is associated with a structured syntactic category and a semantic form, such that the compositional potentials of the category and the semantics are isomorphic. A small universal set of combinators are responsible for assembling constituents into a full syntactic derivation; each combinator operates on adjacent constituents with appropriate categories to produce a new constituent and its compositional semantics, subject to cons"
W19-0405,U09-1017,0,0.0222653,"the process will illustrate our treatment of prepositional phrase complements (as opposed to adjuncts: beginning of §4), which in CCG are traditionally given the category PP. In English, many eventive nouns can be linked to semantic arguments via prepositional phrases, possessives, and light verb constructions, as shown in table 2. AMR uses a canonical form with a predicate (typically based on a verbal paraphrase), treating John decided, John’s decision, and John made a/his decision as semantically equivalent. Despite some work on integrating event nominals and multiword expressions into CCG (Constable and Curran, 2009; Honnibal et al., 2010; de Lhoneux, 2014), we are not aware of any CCG analyses of light verb constructions, which have been studied computationally in other frameworks (e.g., Baldwin and Kim, 2010; Bonial et al., 2014; Ramisch et al., 2018), that gives them semantics equivalent to a content verb paraphrase. We offer such an analysis based on three principles: 1. The event frame is in the semantics of the eventive noun or verb. 2. For any syntactic argument of a noun or verb, the corresponding edge (and free variable) is in the semantics of the noun or verb. 3. Any function word (light verb,"
W19-0405,kingsbury-palmer-2002-treebank,0,0.223387,"aph semantics in CCG (§3). §4 gives example derivations for well-known linguistic phenomena including control, complex coordination, and eventive nouns. §5 discusses some implications of our approach. 2 Related Work AMR formalizes sentence meaning via a graph structure. The AMR for an English sentence is a directed acyclic graph that abstracts away from morphological and syntactic details such as word order, voice, definiteness, and morphology, focusing instead on lexical semantic predicates, roles, and relations. Semantic predicate-argument structures are based on the PropBank frame lexicon (Kingsbury and Palmer, 2002) and its frame-specific core argument roles (named ARG0, ARG1, etc.). AMR supplements these with its own inventory of noncore relations like :time and :purpose, and some specialized frames for the semantics of comparison, for example. Named entities are typed and linked to Wikipedia pages; dates and other values are normalized. Edges in the graph correspond to roles/relations, and nodes to predicate or non-predicate “concepts”, which are lemmatized. Reentrancy is used for within-sentence coreference. A limited amount of prior research has combined CCG and AMR. Artzi et al. (2015) and Misra and"
W19-0405,W15-0127,0,0.21073,"Misra and Artzi (2016) develop an AMR parser using CCG by reformulating AMR graphs as logical forms in lambda calculus. We opt here for an approach similar to that of Beschke and Menzel (2018), where AMR subgraphs with free variables are treated as the semantics in the CCG lexicon. This requires definitions of the combinators that operate directly on AMR subgraphs rather than lambda calculus expressions. Beschke and Menzel (2018) situate their formalization within the literature on graph grammars. They formulate their approach in terms of the HR algebra (Courcelle and Engelfriet, 2012), which Koller (2015) had applied to AMR graphs (but not with CCG). In this formalism, graph fragments called s-graphs are assembled to derive full graphs. S-graphs are equivalent to the AMR subgraphs described in this paper. 3 4 is a categorized list of publications about or using AMR. Due to space constraints, we assume the reader is familiar with the basics of both CCG and AMR. https://nert-nlp.github.io/AMR-Bibliography/ a b l4 3 X : :re :rel l1 :re :rel 2 p ... x y Figure 1: Basic shape of AMR subgraph: Free variables (square, blue) are represented with x, y, z, etc. AMR nodes (round, red) are represented wit"
W19-0405,D15-1169,0,0.0199016,"ach is reasonably intuitive, flowing naturally from CCG syntax, AMR semantics, and the notion of free variables in subgraphs, without the additional need for complicated lambda calculus notation or a highly general graph grammar formalism. To realize this vision in practice, an approach is needed to build a CCG parser enriched with graph semantics for deriving AMRs. We anticipate that existing CCG parsing frameworks can be adapted—for example, by developing an alignment algorithm to induce the semantics for lexical entries from the AMR corpus, and running an off-the-shelf parser like EasySRL (Lewis et al., 2015) at training and test time for the syntactic side of the derivation. This approach would take advantage of the fact that our analysis assumes the ordinary CCG syntax for obtaining the compositional structure of the derivation. The only additional steps would be a) disambiguating the semantics of lexical entries in the derivation, and b) applying the semantics of the combinators as specified in table 1. For each use of application or 5 The category N/NP(N/PPon ) for “on” is suggested by Mark Steedman’s analysis of English prepositions as particles (personal communication) and also maintains th"
W19-0405,D14-1107,0,0.0552931,"ns about or using AMR. Due to space constraints, we assume the reader is familiar with the basics of both CCG and AMR. https://nert-nlp.github.io/AMR-Bibliography/ a b l4 3 X : :re :rel l1 :re :rel 2 p ... x y Figure 1: Basic shape of AMR subgraph: Free variables (square, blue) are represented with x, y, z, etc. AMR nodes (round, red) are represented with a, b, c, etc. Dots indicate that part of the graph may be present or not. In particular, Beschke and Menzel define the semantics of CCG combinators in terms of HR-algebraic operations on s-graphs. They discuss a small set of combinators from Lewis and Steedman (2014) that includes forward and backward application and forward, backward, crossed, and generalized variants of composition. We introduce equivalent semantics for application and composition (§3.2), avoiding the conceptually heavy notation and formalism from the HR algebra. They also specify Conjunction and Identity combinators, which we adapt slightly to suit our needs, and a Punctuation combinator. More significantly, they treat unary operators such as type raising to have no effect on the semantics, whereas we will take another route for type raising (§3.4), and will introduce new, relation-wis"
W19-0405,W16-1702,0,0.0199295,"vial, requiring a precise account of several constructions that conspire to produce long-range dependencies. Whereas CCG traditionally uses some version of lambda calculus for its semantics, there has also been initial work using CCG to build parsers for Abstract Meaning Representation (AMR; Banarescu et al., 2013), a standard with which a large “sembank” of English sentences1 has been manually annotated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-based or transition-based parsing methods (e.g., Flanigan et al., 2014; Wang et al., 2016; Lyu and Titov, 2018) to transform the sentence string or syntactic parse into a semantic graph via a learned statistical model, without any explicit characterization of the syntax-semantics interface. There is go"
W19-0405,P18-1037,0,0.0461674,"ated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-based or transition-based parsing methods (e.g., Flanigan et al., 2014; Wang et al., 2016; Lyu and Titov, 2018) to transform the sentence string or syntactic parse into a semantic graph via a learned statistical model, without any explicit characterization of the syntax-semantics interface. There is good reason to apply CCG to the AMR parsing task: apart from transparency of the syntax-semantics interface, state-of-the-art AMR parsers are known to be weak at reentrancy (e.g., Lyu and Titov, 2018), which presumably can be partially attributed to syntactic reentrancy in control constructions, for example. Prior work applying CCG to AMR parsing has begun to address this, but some of the important mechanis"
W19-0405,L18-1486,0,0.0127734,"version of lambda calculus for its semantics, there has also been initial work using CCG to build parsers for Abstract Meaning Representation (AMR; Banarescu et al., 2013), a standard with which a large “sembank” of English sentences1 has been manually annotated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-based or transition-based parsing methods (e.g., Flanigan et al., 2014; Wang et al., 2016; Lyu and Titov, 2018) to transform the sentence string or syntactic parse into a semantic graph via a learned statistical model, without any explicit characterization of the syntax-semantics interface. There is good reason to apply CCG to the AMR parsing task: apart from transparency of the syntax-semantics interface, state-of-the-art AMR parsers are known to be weak at"
W19-0405,D16-1183,0,0.146482,"er, 2002) and its frame-specific core argument roles (named ARG0, ARG1, etc.). AMR supplements these with its own inventory of noncore relations like :time and :purpose, and some specialized frames for the semantics of comparison, for example. Named entities are typed and linked to Wikipedia pages; dates and other values are normalized. Edges in the graph correspond to roles/relations, and nodes to predicate or non-predicate “concepts”, which are lemmatized. Reentrancy is used for within-sentence coreference. A limited amount of prior research has combined CCG and AMR. Artzi et al. (2015) and Misra and Artzi (2016) develop an AMR parser using CCG by reformulating AMR graphs as logical forms in lambda calculus. We opt here for an approach similar to that of Beschke and Menzel (2018), where AMR subgraphs with free variables are treated as the semantics in the CCG lexicon. This requires definitions of the combinators that operate directly on AMR subgraphs rather than lambda calculus expressions. Beschke and Menzel (2018) situate their formalization within the literature on graph grammars. They formulate their approach in terms of the HR algebra (Courcelle and Engelfriet, 2012), which Koller (2015) had appl"
W19-0405,S16-1183,0,0.0197175,"he semantics, whereas we will take another route for type raising (§3.4), and will introduce new, relation-wise versions of application and composition (§3.3). Finally, whereas Beschke and Menzel devote most of their paper to a lexicon induction algorithm and experiments, we focus on the linguistic motivation for our definition of the combinators, and leave the development of suitable lexicon induction techniques to future work. A related graph formalism called hyperedge replacement grammar is also used in the AMR parsing literature (Jones et al., 2012; Chiang et al., 2013; Peng et al., 2015; Peng and Gildea, 2016; Björklund et al., 2016; Groschwitz et al., 2018). Hyperedge replacement grammars (Rozenberg, 1997) are a formal way of combining subgraphs to derive a larger graph, based on an extension of Context Free Grammars to graphs instead of strings. Readers may assume that the graph formalism described in this paper is a simplified hyperedge replacement grammar which only allows hyperedges of rank 1. 3 Graph Semantics AMR is designed to represent semantics at the sentence level. For CCG lexical entries and combinators to parse AMR semantics, we need to formalize how AMR subgraphs can represent the s"
W19-0405,K15-1004,0,0.0180398,"have no effect on the semantics, whereas we will take another route for type raising (§3.4), and will introduce new, relation-wise versions of application and composition (§3.3). Finally, whereas Beschke and Menzel devote most of their paper to a lexicon induction algorithm and experiments, we focus on the linguistic motivation for our definition of the combinators, and leave the development of suitable lexicon induction techniques to future work. A related graph formalism called hyperedge replacement grammar is also used in the AMR parsing literature (Jones et al., 2012; Chiang et al., 2013; Peng et al., 2015; Peng and Gildea, 2016; Björklund et al., 2016; Groschwitz et al., 2018). Hyperedge replacement grammars (Rozenberg, 1997) are a formal way of combining subgraphs to derive a larger graph, based on an extension of Context Free Grammars to graphs instead of strings. Readers may assume that the graph formalism described in this paper is a simplified hyperedge replacement grammar which only allows hyperedges of rank 1. 3 Graph Semantics AMR is designed to represent semantics at the sentence level. For CCG lexical entries and combinators to parse AMR semantics, we need to formalize how AMR subgra"
W19-0405,N18-1106,1,0.910879,"gure 8 shows the derivation for “decision” in its light verb construction form. The preposition “on” redundantly represents the :ARG1 edge, and is merged with “decision” by relation-wise application.5 The light verb “made” specifies the :ARG0 edge. 5 Discussion Unlike many semantic formalisms, AMR does not specify a ‘compositional story’: annotations do not include any sort of syntactic derivation, or even gold alignments between semantic units and words in the sentence. This presents a challenge for AMR parsing, which in practice relies on various forms of automatic or latent alignments (see Szubert et al., 2018). Above, we have presented an analysis that lays the foundation for a linguistically principled treatment of CCG-to-AMR parsing that meets a variety of challenges in the syntax-semantics interface, and does so in a transparent way so that parsing errors can be diagnosed. We believe the approach is reasonably intuitive, flowing naturally from CCG syntax, AMR semantics, and the notion of free variables in subgraphs, without the additional need for complicated lambda calculus notation or a highly general graph grammar formalism. To realize this vision in practice, an approach is needed to build a"
W19-0405,N18-2040,0,0.0145254,"precise account of several constructions that conspire to produce long-range dependencies. Whereas CCG traditionally uses some version of lambda calculus for its semantics, there has also been initial work using CCG to build parsers for Abstract Meaning Representation (AMR; Banarescu et al., 2013), a standard with which a large “sembank” of English sentences1 has been manually annotated.2 To 1 See https://amr.isi.edu/download.html As originally defined, AMR is English-specific. However, a companion annotation standard, corpus, and parsers exist for Chinese (Xue et al., 2014; Li et al., 2016; Wang et al., 2018), and initial investigations have been made toward adapting AMR to several other languages (Xue et al., 2014; Migueles-Abraira et al., 2018; Anchiêta and Pardo, 2018). 2 date, dozens of publications3 have used the corpus to train and evaluate semantic parsers—most using graph-based or transition-based parsing methods (e.g., Flanigan et al., 2014; Wang et al., 2016; Lyu and Titov, 2018) to transform the sentence string or syntactic parse into a semantic graph via a learned statistical model, without any explicit characterization of the syntax-semantics interface. There is good reason to apply C"
W19-0405,C12-1083,0,\N,Missing
W19-0405,P10-1022,0,\N,Missing
W19-0405,P14-1134,0,\N,Missing
W19-0405,J07-3004,0,\N,Missing
W19-0405,xue-etal-2014-interlingua,0,\N,Missing
W19-0405,S16-1181,0,\N,Missing
W19-0405,N16-1026,0,\N,Missing
W19-3316,P13-1023,1,0.855673,"e subject cannot be ranked lower than the direct object (e.g., a subject construed as a T HEME cannot have a direct object construed as an AGENT). Indirect objects in the English double object construction4 are treated as R ECIPIENT construals. (18) I sent [John]R ECIPIENT↝R ECIPIENT a cake. (19) I sent a cake [to John]R ECIPIENT↝G OAL . (20) I baked [John]R ECIPIENT↝R ECIPIENT a cake. (21) I paid [John]R ECIPIENT↝R ECIPIENT [$10]C OST↝C OST . Interannotator Agreement Study Data. We piloted our guidelines using a sample of 100 scenes from the English UCCA-annotated Wiki corpus5 as detailed by Abend and Rappoport (2013). UCCA is a scheme for annotating coarsegrained predicate-argument structure such that syntactically varied paraphrases and translations should receive similar analyses. It captures both static and dynamic scenes and their participants, but does not mark semantic roles. Annotators. Four annotators (A, B, C, D), all authors of this paper, took part in this study. All are computational linguistics researchers. Datasets. Prior to development of guidelines for subjects and objects, one of the annotators (Annotator A) sampled 106 Wiki documents (44k tokens) and tagged all 10k instances of UCCA Part"
W19-3316,S17-1022,1,0.738698,"Missing"
W19-3316,J05-1004,0,0.198024,"positions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabulary predicates are encountered. But is a lexicon really necessary for role annotation? A general-purpose set of role labels with detailed criteria for each can potentially bypass coverage limitations of lexicon-based approaches, while still supporting some degree of generalization across grammatical paraphrases. Second, the nonreliance on a lexicon potentially simplifies the annotation process in some respects. For example, no explicit predicate disamb"
W19-3316,Q15-1034,0,0.0554848,"Missing"
W19-3316,P18-1018,1,0.601633,"orical semantic roles (Fillmore, 1968, 1982; Levin, 1993) or bundles of proto-properties (Dowty, 1991; Reisinger et al., 2015) that generalize across verbs. A parallel line of work (§2) has looked at the meanings coded by grammatical phrase-markers such as prepositions and possessives and how to disambiguate them. These inquiries necessarily overlap because many prepositions mark verb arguments or modifiers. Consequently, insights from the study of prepositions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabular"
W19-3319,P13-1023,1,0.797973,"identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice of creating mention annotations from scratch on the raw or tokenized text, and we suggest that they could be overcome by reusing structures from existing semantic annotation, thereby ensuring compatibility between the layers. We advocate Here we argue that Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is an ideal choice, as it defines a foundational layer of predicate-argument structure whose main design principles are cross-linguistic applicability and fast annotatability by non-experts. To that end, we develop and pilot a new layer for UCCA which adds coreference information.2 This coreference layer is constrained by the spans already specified in the foundational predicate-argument layer. We compare these manual annotations to existing gold coreference annotations in multiple frameworks, finding a healthy level of overlap. ∗ Contact: jakob@cs.georgetown.edu 1 https://sites.google.com/vi"
W19-3319,W13-2322,1,0.668005,"” tectogrammatical layer, which is built on the syntactic analytic layer, can be considered quasi-semantic (Böhmová et al., 2003). Transformed syntax. In other cases, semantic label annotations enrich skeletal semantic representations that have been deterministically converted from syntactic structures. One example is Universal Decompositional Semantics (White et al., 2016), whose annotations are anchored with PredPatt, a way of converting Universal Dependencies trees (Nivre et al., 2016) to approximate predicateargument structures. Sentence-anchored. The Abstract Meaning Representation (AMR; Banarescu et al., 2013) is an example of a highly integrative (anti-modular) approach to sentence-level meaning, without anchoring below the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which ha"
W19-3319,P10-1143,0,0.0242326,"nces. RED always uses minimum spans, except for time expressions, which follow the TIMEX3 standard (Pustejovsky et al., 2010). One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06; Doddington et al., 2004). 8 A separate layer records all named entities, however, and non-coreferent ones can be considered singleton mentions. 9 The GUM guidelines specify that clausal modifiers should not be included in a nominal mention. ally, UCoref has ‘null’ spans, corresponding to implicit units in UCCA.10 cludes prepositions and case markers within mentions. This does not have a major effect on coreference, but contributes to consistency between languages that vary in the grammaticalizati"
W19-3319,E17-1053,0,0.0129158,"nnotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained,"
W19-3319,M98-1001,0,0.483388,"Missing"
W19-3319,doddington-etal-2004-automatic,0,0.0114935,". One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06; Doddington et al., 2004). 8 A separate layer records all named entities, however, and non-coreferent ones can be considered singleton mentions. 9 The GUM guidelines specify that clausal modifiers should not be included in a nominal mention. ally, UCoref has ‘null’ spans, corresponding to implicit units in UCCA.10 cludes prepositions and case markers within mentions. This does not have a major effect on coreference, but contributes to consistency between languages that vary in the grammaticalization of their case marking. Coordination. Our treatment of coordinate entity mentions is adopted and expanded from the GUM gu"
W19-3319,P14-1134,0,0.0145439,"without anchoring below the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 U"
W19-3319,N06-2015,0,0.672377,"ing semantic annotation schemes with regard to two closely related criteria: a) anchoring, i.e. the previously determined underlying structure (characters, tokens, syntax, etc.) that defines the set of possible annotation targets in a new layer; and b) modularity, the extent to which multiple kinds of information are expressed as separate (possibly linked) structures/layers, which may be annotated in different phases. Massively multilayer corpora. A few corpora comprise several layers of annotation, including semantics, with an emphasis on modularity of these layers. One example is OntoNotes (Hovy et al., 2006), annotated for syntax, named entities, word senses, PropBank (Palmer et al., 2005) predicateargument structures, and coreference. Another example is GUM (Zeldes, 2017), with layers for syntactic, coreference, discourse, and document structure. Both of these resources cover multiple genres. Different layers in these resources are anchored differently, as noted below. Token-anchored. Many semantic annotation layers are specified in terms of character or token offsets. This is the case for UCCA’s Foundational Layer (§2.2), FrameNet (Fillmore and Baker, 2009), RED (O’Gorman et al., 2016), all of"
W19-3319,W16-1702,0,0.0601897,"entence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained, typologically-motivated scheme for analyzing abstract semantic structures in text. It is designed to expose commonalities in semantic str"
W19-3319,D18-1264,0,0.0151652,"r sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained, typologically-motivated scheme for analy"
W19-3319,D14-1048,0,0.0170057,"w the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UC"
W19-3319,W04-2705,0,0.330429,"ntities that are contributing to or affected by a scene/event (including locations and other scenes/events). Special attention should be paid to relational nouns like teacher or friend that both refer to an entity and evoke a process or state in which the entity generally or habitually participates.6 According to the UCCA guidelines, these words are analyzed internally (as both P/S and A within a nested unit over the same span), in addition to the 6 A teacher is a person who teaches and a friend is a person who stands in a friendship relation with another person. Cf. Newell and Cheung (2018); Meyers et al. (2004). 167 context-dependent incoming edge from their parent. However, the inherent scene (of teaching or friendship) is merely evoked, but not referred to, and it is usually invariant with respect to the explicit context it occurs in. Moreover, treating one span of words as two mentions would pose a significant complication. Thus, we consider these units only in their role as Participant (and not scene) mentions. Non-scene-non-participant units. A certain subset of the remaining unit types are considered to be mention candidates. This subset is comprised of the categories, Time, Elaborator, Relato"
W19-3319,P14-2006,0,0.0273479,"allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes. 1 Introduction Unlike some NLP tasks, coreference resolution lacks an agreed-upon standard for annotation and evaluation (Poesio et al., 2016). It has been approached using a multitude of different markup schemas, and the several evaluation metrics commonly used (Pradhan et al., 2014) are controversial (Moosavi and Strube, 2016). In particular, these schemas use divergent and often (languagespecific) syntactic criteria for defining candidate mentions in text. This includes the questions of whether to annotate entity and/or event coreference, whether to include singletons, and how to identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice o"
W19-3319,pustejovsky-etal-2010-iso,0,0.0391117,"mention spans is often one of two extremes: minimum spans (also called triggers or nuggets), which typically only consist of the head word or expression that sufficiently describes the type of entity or event; or maximum spans (also called full mentions), containing all arguments and modifiers. GUM and OntoNotes generally apply a maximum span policy for nominal mentions, with just a few exceptions.9 For verbal mentions, OntoNotes chooses minimum spans, whereas GUM annotates full clauses or sentences. RED always uses minimum spans, except for time expressions, which follow the TIMEX3 standard (Pustejovsky et al., 2010). One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06;"
W19-3319,P16-1060,0,0.0257005,"p some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes. 1 Introduction Unlike some NLP tasks, coreference resolution lacks an agreed-upon standard for annotation and evaluation (Poesio et al., 2016). It has been approached using a multitude of different markup schemas, and the several evaluation metrics commonly used (Pradhan et al., 2014) are controversial (Moosavi and Strube, 2016). In particular, these schemas use divergent and often (languagespecific) syntactic criteria for defining candidate mentions in text. This includes the questions of whether to annotate entity and/or event coreference, whether to include singletons, and how to identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice of creating mention annotations from scratch o"
W19-3319,W16-0701,0,0.0618674,"Missing"
W19-3319,L16-1026,0,0.0942791,"r, 2009), RED (O’Gorman et al., 2016), all of the layers in GUM, and the named entity and word sense annotations in OntoNotes. Though the guidelines may mention syntactic criteria for deciding what units to semantically annotate, the annotated data does not explicitly tie these layers to syntactic units, and to the best of our knowledge the annotator is not constrained by the syntactic annotation. Syntax-anchored. Semantic annotations explicitly defined in terms of syntactic units include: PropBank (such as in OntoNotes); and the coreference annotations in the Prague Dependency Treebank (PDT; Nedoluzhko et al., 2016). In addition, PDT’s “deep syntactic” tectogrammatical layer, which is built on the syntactic analytic layer, can be considered quasi-semantic (Böhmová et al., 2003). Transformed syntax. In other cases, semantic label annotations enrich skeletal semantic representations that have been deterministically converted from syntactic structures. One example is Universal Decompositional Semantics (White et al., 2016), whose annotations are anchored with PredPatt, a way of converting Universal Dependencies trees (Nivre et al., 2016) to approximate predicateargument structures. Sentence-anchored. The Ab"
W19-3319,N18-1106,1,0.896893,"Missing"
W19-3319,L18-1537,0,0.016262,"entions as they describe entities that are contributing to or affected by a scene/event (including locations and other scenes/events). Special attention should be paid to relational nouns like teacher or friend that both refer to an entity and evoke a process or state in which the entity generally or habitually participates.6 According to the UCCA guidelines, these words are analyzed internally (as both P/S and A within a nested unit over the same span), in addition to the 6 A teacher is a person who teaches and a friend is a person who stands in a friendship relation with another person. Cf. Newell and Cheung (2018); Meyers et al. (2004). 167 context-dependent incoming edge from their parent. However, the inherent scene (of teaching or friendship) is merely evoked, but not referred to, and it is usually invariant with respect to the explicit context it occurs in. Moreover, treating one span of words as two mentions would pose a significant complication. Thus, we consider these units only in their role as Participant (and not scene) mentions. Non-scene-non-participant units. A certain subset of the remaining unit types are considered to be mention candidates. This subset is comprised of the categories, Ti"
W19-3319,C18-1313,0,0.106736,"Missing"
W19-3319,W16-5706,0,0.0526032,"Missing"
W19-3319,J05-1004,0,0.23596,"nchoring, i.e. the previously determined underlying structure (characters, tokens, syntax, etc.) that defines the set of possible annotation targets in a new layer; and b) modularity, the extent to which multiple kinds of information are expressed as separate (possibly linked) structures/layers, which may be annotated in different phases. Massively multilayer corpora. A few corpora comprise several layers of annotation, including semantics, with an emphasis on modularity of these layers. One example is OntoNotes (Hovy et al., 2006), annotated for syntax, named entities, word senses, PropBank (Palmer et al., 2005) predicateargument structures, and coreference. Another example is GUM (Zeldes, 2017), with layers for syntactic, coreference, discourse, and document structure. Both of these resources cover multiple genres. Different layers in these resources are anchored differently, as noted below. Token-anchored. Many semantic annotation layers are specified in terms of character or token offsets. This is the case for UCCA’s Foundational Layer (§2.2), FrameNet (Fillmore and Baker, 2009), RED (O’Gorman et al., 2016), all of the layers in GUM, and the named entity and word sense annotations in OntoNotes. Th"
W19-3319,poesio-artstein-2008-anaphoric,0,0.078252,"Missing"
W19-3319,D16-1177,0,0.1746,"Missing"
W19-3319,W18-0704,0,0.0266861,"Missing"
W19-3319,W16-0713,0,0.0135454,"and how we can (re)cover annotations in established schemas from our new schema. We can interpret this experiment as asking: If we had a perfect system for UCoref, could we use that to predict GUM/OntoNotes/RED-style coreference? And vice versa, if we had an oracle in one of those schemes, and possibly oracle UCoref mentions, how closely could we convert to UCoref?16 Exact mention matches. A naïve approach would be to look at the token spans covered by all mentions and reference clusters and count how often we can find an exact match between UCoref and one of the existing schemes. 16 See also Zeldes and Zhang (2016), who base a full coreference resolution system on this idea. In UCoref, we use maximum spans by default, but thanks to the nature of the UCCA foundational layer, minimum spans can easily be recovered from Centers and scene-evokers. For schemas with a minimum span approach, we can switch to a minimum span approach in UCoref by choosing the head unit of each maximum span unit as its representative mention. This works well between UCoref and RED as they have similar policies for determining semantic heads, which is crucial for, e.g., light verb constructions. This would be problematic, however,"
