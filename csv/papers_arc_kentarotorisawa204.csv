2021.acl-long.164,{BERTAC}: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks,2021,-1,-1,4,0,12929,jonghoon oh,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac."
2020.lrec-1.82,Understanding User Utterances in a Dialog System for Caregiving,2020,-1,-1,6,0,16767,yoshihiko asao,Proceedings of the 12th Language Resources and Evaluation Conference,0,"A dialog system that can monitor the health status of seniors has a huge potential for solving the labor force shortage in the caregiving industry in aging societies. As a part of efforts to create such a system, we are developing two modules that are aimed to correctly interpret user utterances: (i) a yes/no response classifier, which categorizes responses to health-related yes/no questions that the system asks; and (ii) an entailment recognizer, which detects users{'} voluntary mentions about their health status. To apply machine learning approaches to the development of the modules, we created large annotated datasets of 280,467 question-response pairs and 38,868 voluntary utterances. For question-response pairs, we asked annotators to avoid direct {``}yes{''} or {``}no{''} answers, so that our data could cover a wide range of possible natural language responses. The two modules were implemented by fine-tuning a BERT model, which is a recent successful neural network model. For the yes/no response classifier, the macro-average of the average precisions (APs) over all of our four categories (Yes/No/Unknown/Other) was 82.6{\%} (96.3{\%} for {``}yes{''} responses and 91.8{\%} for {``}no{''} responses), while for the entailment recognizer it was 89.9{\%}."
P19-1414,Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts,2019,0,0,5,0,12929,jonghoon oh,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we propose a method for why-question answering (why-QA) that uses an adversarial learning framework. Existing why-QA methods retrieve {``}answer passages{''} that usually consist of several sentences. These multi-sentence passages contain not only the reason sought by a why-question and its connection to the why-question, but also redundant and/or unrelated parts. We use our proposed {``}Adversarial networks for Generating compact-answer Representation{''} (AGR) to generate from a passage a vector representation of the non-redundant reason sought by a why-question and exploit the representation for judging whether the passage actually answers the why-question. Through a series of experiments using Japanese why-QA datasets, we show that these representations improve the performance of our why-QA neural model as well as that of a BERT-based why-QA model. We show that they also improve a state-of-the-art distantly supervised open-domain QA (DS-QA) method on publicly available English datasets, even though the target task is not a why-QA."
D19-1590,Event Causality Recognition Exploiting Multiple Annotators{'} Judgments and Background Knowledge,2019,0,0,3,0,16770,kazuma kadowaki,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We propose new BERT-based methods for recognizing event causality such as {``}smoke cigarettes{''} {--}{\textgreater} {``}die of lung cancer{''} written in web texts. In our methods, we grasp each annotator{'}s policy by training multiple classifiers, each of which predicts the labels given by a single annotator, and combine the resulting classifiers{'} outputs to predict the final labels determined by majority vote. Furthermore, we investigate the effect of supplying background knowledge to our classifiers. Since BERT models are pre-trained with a large corpus, some sort of background knowledge for event causality may be learned during pre-training. Our experiments with a Japanese dataset suggest that this is actually the case: Performance improved when we pre-trained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction."
L18-1556,Annotating Zero Anaphora for Question Answering,2018,0,0,3,0,16767,yoshihiko asao,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W16-3903,{DISAANA} and {D}-{SUMM}: Large-scale Real Time {NLP} Systems for Analyzing Disaster Related Reports in Tweets,2016,0,0,1,1,12932,kentaro torisawa,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"This talk presents two NLP systems that were developed for helping disaster victims and rescue workers in the aftermath of large-scale disasters. DISAANA provides answers to questions such as {``}What is in short supply in Tokyo?{''} and displays locations related to each answer on a map. D-SUMM automatically summarizes a large number of disaster related reports concerning a specified area and helps rescue workers to understand disaster situations from a macro perspective. Both systems are publicly available as Web services. In the aftermath of the 2016 Kumamoto Earthquake (M7.0), the Japanese government actually used DISAANA to analyze the situation."
D16-1132,Intra-Sentential Subject Zero Anaphora Resolution using Multi-Column Convolutional Neural Network,2016,26,13,2,0,12930,ryu iida,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-2055,"{WISDOM} {X}, {DISAANA} and {D}-{SUMM}: Large-scale {NLP} Systems for Analyzing Textual Big Data",2016,10,4,7,1,16768,junta mizuno,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We demonstrate our large-scale NLP systems: WISDOM X, DISAANA, and D-SUMM. WISDOM X provides numerous possible answers including unpredictable ones to widely diverse natural language questions to provide deep insights about a broad range of issues. DISAANA and D-SUMM enable us to assess the damage caused by large-scale disasters in real time using Twitter as an information source."
Y15-1063,Recognizing Complex Negation on {T}witter,2015,14,1,5,1,16768,junta mizuno,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"After the Great East Japan Earthquake in 2011, an abundance of false rumors were disseminated on Twitter that actually hindered rescue activities. This work presents a method for recognizing the negation of predicates on Twitter to find Japanese tweets that refute false rumors. We assume that the predicate xe2x80x9coccurxe2x80x9d is negated in the sentence xe2x80x9cThe guy who tweeted that a nuclear explosion occurred has watched too many SF movies.xe2x80x9d The challenge is in the treatment of such complex negation. We have to recognize a wide range of complex negation expressions such as xe2x80x9cit is theoretically impossible that...xe2x80x9d and xe2x80x9cThe guy who... watched too many SF movies.xe2x80x9d We tackle this problem using a combination of a supervised classifier and clusters of n-grams derived from large un-annotated corpora. The n-gram clusters give us a gain of about 22% in F-score for complex negations."
D15-1190,Large-Scale Acquisition of Entailment Pattern Pairs by Exploiting Transitivity,2015,21,6,2,1,12931,julien kloetzer,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel method for acquiring entailment pairs of binary patterns on a large-scale. This method exploits the transitivity of entailment and a self-training scheme to improve the performance of an already strong supervised classifier for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as xe2x80x9cuse Y to distribute Xxe2x80x9d!xe2x80x9cX is available on Yxe2x80x9d whose extraction is considered difficult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision."
D15-1260,Intra-sentential Zero Anaphora Resolution using Subject Sharing Recognition,2015,31,12,2,0,12930,ryu iida,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In this work, we improve the performance of intra-sentential zero anaphora resolution in Japanese using a novel method of recognizing subject sharing relations. In Japanese, a large portion of intrasentential zero anaphora can be regarded as subject sharing relations between predicates, that is, the subject of some predicate is also the unrealized subject of other predicates. We develop an accurate recognizer of subject sharing relations for pairs of predicates in a single sentence, and then construct a subject shared predicate network, which is a set of predicates that are linked by the subject sharing relations recognized by our recognizer. We finally combine our zero anaphora resolution method exploiting the subject shared predicate network and a state-ofthe-art ILP-based zero anaphora resolution method. Our combined method achieved a significant improvement over the the ILPbased method alone on intra-sentential zero anaphora resolution in Japanese. To the best of our knowledge, this is the first work to explicitly use an independent subject sharing recognizer in zero anaphora resolution."
P14-1093,"Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features",2014,29,30,2,1,26950,chikara hashimoto,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture! exacerbate desertification from the web using semantic relation (between nouns), context, and association features. Experiments show that our method outperforms baselines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture! exacerbate desertification! increase Asian dust (from China)! asthma gets worse. Experiments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation continues! global warming worsens! sea temperatures rise! vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we xe2x80x9cpredictedxe2x80x9d the future event sequence in a sense."
C14-1135,Million-scale Derivation of Semantic Relations from a Manually Constructed Predicate Taxonomy,2014,35,3,2,0,39198,motoki sano,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We manually created a semantic taxonomy called Phased Predicate Template Taxonomy (PPTT) that covers 12,023 predicate templates (i.e., predicates with one argument slot like xe2x80x9crescue Xxe2x80x9d) and derived from it various semantic relations between these templates on a million-instance scale (70%-80% precision level). The derived relations include entailment (e.g., rescue Xxe2x8ax83X is alive), happens-before (e.g., buy Xxe2x87x92drink X), and a novel relation type anomalous obstruction (e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTTxe2x80x99s design and the use of statistical methods."
P13-1159,Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster,2013,25,61,3,1,39032,istvan varga,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The 2011 Great East Japan Earthquake caused a wide range of problems, and as countermeasures, many aid activities were carried out. Many of these problems and aid activities were reported via Twitter. However, most problem reports and corresponding aid messages were not successfully exchanged between victims and local governments or humanitarian organizations, overwhelmed by the vast amount of information. As a result, victims could not receive necessary aid and humanitarian organizations wasted resources on redundant efforts. In this paper, we propose a method for discovering matches between problem reports and aid messages. Our system contributes to problem-solving in a large scale disaster situation by facilitating communication between victims and humanitarian organizations."
P13-1170,Why-Question Answering using Intra- and Inter-Sentential Causal Relations,2013,29,37,2,1,12929,jonghoon oh,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we explore the utility of intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the first work that uses both intra- and inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientation of excitation proposed by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved precision by 4.4% against all the questions in our test set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only."
N13-1007,Minimally Supervised Method for Multilingual Paraphrase Extraction from Definition Sentences on the Web,2013,27,8,3,0,41586,yulan yan,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the Web. Hashimoto et al. (2011) extracted paraphrases from Japanese definition sentences on the Web, assuming that definition sentences defining the same concept tend to contain paraphrases. However, their method requires manually annotated data and is language dependent. We extend their framework and develop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.xe2x80x99s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese."
I13-2008,{NICT} Disaster Information Analysis System,2013,4,7,4,1,35675,kiyonori ohtake,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"Immediately after the 2011 Great East Japan Earthquake, the Internet was flooded by a huge amount of information concerning the damage and problems caused by the earthquake, the tsunami, and the nuclear disaster. Many reports about aid efforts and advice to victims were also transmitted into cyberspace. However, since most people were overwhelmed by the massive amounts of information, they could not make proper decisions, and much confusion was caused. Furthermore, false rumors spread on the Internet and fanned such confusion. We demonstrate NICTxe2x80x99s prototype disaster information analysis system, which was designed to properly organize such a large amount of disaster-related information on social media during future large-scale disasters to help people understand the situation and make correct decisions. We are going to deploy it using a large-scale computer cluster in fiscal year 2014."
I13-2012,{WISDOM}2013: A Large-scale Web Information Analysis System,2013,9,6,7,1,35674,masahiro tanaka,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"We demonstrate our large-scale web information analysis system called WISDOM2013, which consists of several deep semantic analysis systems such as a factoid QA, a non-factoid QA and a sentiment analyzer, and a software platform on which its semantic analysis systems can be applied to a billion-page-scale web archive. The software platform has an extendable architecture, and we are planning to enhance WISDOM2013 in the future by adding more semantic analysis systems and inference mechanisms."
D13-1065,Two-Stage Method for Large-Scale Acquisition of Contradiction Pattern Pairs using Entailment,2013,21,13,3,1,12931,julien kloetzer,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as Xdrug prevents Ydisease and Ydisease caused by Xdrug. In the first stage, we train an SVM classifier to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity (Hashimoto et al., 2012) of the patterns. In the second stage, we enlarge the first stage classifierxe2x80x99s training data with new contradiction pairs obtained by combining the output of the first stagexe2x80x99s classifier and that of an entailment classifier. We acquired this way 750,000 typed Japanese contradiction pattern pairs with an estimated precision of 80%. We plan to release this resource to the NLP community."
D12-1034,Why Question Answering using Sentiment Analysis and Word Classes,2012,22,44,2,1,12929,jonghoon oh,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus. Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable. To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering. We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6."
D12-1057,Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web,2012,29,47,2,1,26950,chikara hashimoto,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer b develop cancer) and causality pairs (e.g., increase in crime xe2x87x92 heighten anxiety). Our experiments show that with automatically acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision."
C12-1169,{C}hinese Evaluative Information Analysis,2012,16,1,4,1,29973,yiou wang,Proceedings of {COLING} 2012,0,"Together with the ever-growing amount of Chinese web data, the number of opinions voiced by Chinese users is rapidly increasing, and analyzing them is an important task. This paper introduces a Chinese Evaluative Information Analyzer (CEIA) and proposes a method to improve its performance. We use evaluative information as a unifying term for the information about attitudes, opinions, sentiments and so on. This paper makes three contributions: (i) CEIA can identify and analyze a more diverse and richer set of evaluative information than previous studies for Chinese; (ii) to implement the system, we constructed an original annotated corpus for Chinese evaluative information and built a large sentiment dictionary; (iii) we introduce syntactic dependency, semantic class and distance features to improve the evaluative information extraction. The performance of the system and the effectiveness of the newly introduced features are evaluated in a series of experiments on our Chinese evaluative information corpus. Title and Abstract in Chinese xc2xa5 xc2xa5' 'xc2xb5$fxc3x82 xc3x82I Ixc2xa4 xe2x80x90 d{Xxe2x80xb0xcbx9aoxcbx99xc2xa5'~iUdxe2x80xa0xe2x80x94fi ,{xc2xb5$u'xcbx9d{jfiU xe2x80xa2xe2x81x84"
P11-1109,Extracting Paraphrases from Definition Sentences on the Web,2011,29,22,2,1,26950,chikara hashimoto,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%."
I11-1035,Improving {C}hinese Word Segmentation and {POS} Tagging with Semi-supervised Methods Using Large Auto-Analyzed Data,2011,26,59,6,1,29973,yiou wang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper presents a simple yet effective semi-supervised method to improve Chinese word segmentation and POS tagging. We introduce novel features derived from large auto-analyzed data to enhance a simple pipelined system. The auto-analyzed data are generated from unlabeled data by using a baseline system. We evaluate the usefulness of our approach in a series of experiments on Penn Chinese Treebanks and show that the new features provide substantial performance gains in all experiments. Furthermore, the results of our proposed method are superior to the best reported results in the literature."
I11-1060,Similarity Based Language Model Construction for Voice Activated Open-Domain Question Answering,2011,15,6,3,1,39032,istvan varga,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper describes a novel method of constructing a language model for speech recognition of inputs with a particular style, using a large-scale Web archive. Our target is an open domain voice-activated QA system and our speech recognition module must recognize relatively short, domain independent questions. The central issue is how to prepare a large scale training corpus with low cost, and we tackled this problem by combining an existing domain adaptation method and distributional word similarity. From 500 seed sentences and 600 million Web pages we constructed a language model covering 413,000 words. We achieved an average improvement of 3.25 points in word error rate over a baseline model constructed from randomly sampled Web sentences."
I11-1098,Extending {W}ord{N}et with Hypernyms and Siblings Acquired from {W}ikipedia,2011,16,5,4,1,300,ichiro yamada,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper proposes a method for extending WordNet with terms in Wikipedia. Our method identifies a WordNet synset by integrating evidence derived from the structure of an article in Wikipedia and distributional similarity of terms. Unlike previous methods, utilizing the hypernym and siblings of the target term acquired from Wikipedia, the proposed method can deal with terms other than Wikipedia article titles and can work well even when reliable distributional similarity of a target term is unavailable. Experiments show that the proposed method can identify synsets for 2,039,417 inputs at precision rate of 84%. Furthermore, it is estimated from the experimental results that there should be 328,572 terms among all the inputs whose synset our method can correctly identify, while previous methods relying only on distributional similarity and lexico-syntactic patterns cannot."
I11-1101,Toward Finding Semantic Relations not Written in a Single Sentence: An Inference Method using Auto-Discovered Rules,2011,18,5,2,0.833333,37701,masaaki tsuchida,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Recent advances in automatic knowledge acquisition methods have enabled us to construct massive knowledge bases of semantic relations. Most previous work has focused on semantic relations explicitly expressed in single sentences. Our goal in this work is to obtain valid non-single sentence relation instances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages."
D11-1007,{SMT} Helps Bitext Dependency Parsing,2011,22,9,7,1,21231,wenliang chen,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems. Previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain. Instead, our approach uses an auto-generated bilingual treebank to produce bilingual constraints. However, because the auto-generated bilingual treebank contains errors, the bilingual constraints are noisy. To overcome this problem, we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-the-art baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT."
D11-1076,Relation Acquisition using Word Classes and Partial Patterns,2011,26,11,2,1,41513,stijn saeger,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns (e.g. X causes Y for causal relations) but instead learns a combination of indirect evidence for the target relation --- semantic word classes and partial patterns. This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus --- in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach of current pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance."
Y10-1075,Using Various Features in Machine Learning to Obtain High Levels of Performance for Recognition of {J}apanese Notational Variants,2010,17,2,9,0,44118,masahiro kojima,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We proposed a method of using machine learning with various features for the recognition of Japanese notational variants. We increased 0.06 at the F-measure by specific features using existing dictionaries and character pairs useful for recognizing notational variants and obtained 0.91 at the F-measure for the recognition of notational variants. By using the method, we could extract 160 thousand word pairs with a precision rate of 0.9. We also constructed a method using patterns in addition to machine learning and observed that we could extract 4.2 million notational variant pairs with a precision rate of 0.78. We confirmed that our method was much better than an existing method through experiments."
Y10-1080,Generation of Summaries that Appropriately and Adequately Express the Contents of Original Documents Using Word-Association Knowledge,2010,15,0,6,0,45050,kazuki takigawa,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary xe2x80x9cterrorxe2x80x9d for sentences xe2x80x9cA bomb went off. Some people were killed. This was triggered by rebel campaign.xe2x80x9d In this study, we proposed a new method that generates summaries that can appropriately and adequately express the contents of their respective original documents using word-association knowledge. In this method, we assumed that a good summary comprises words that can express the contents of the original document and does not contain words that are unable to express the contents of the original document. Using statistical tests, we confirmed that the use of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the xe2x80x9clenientxe2x80x9d case of experiments."
W10-3907,A Look inside the Distributionally Similar Terms,2010,11,4,3,0,45029,kow kuroda,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"We analyzed the details of a Web-derived distributional data of Japanese nominal terms with two aims. One aim is to examine if distributionally similar terms can be in fact equated with xe2x80x9csemantically similarxe2x80x9d terms, and if so to what extent. The other is to investigate into what kind of semantic relations constitute (strongly) distributionally similar terms. Our results show that over 85% of the pairs of the terms derived from the highly similar terms turned out to be semantically similar in some way. The ratio of xe2x80x9cclassmate,xe2x80x9d synonymous, hypernym-hyponym, and meronymic relations are about 62%, 17%, 8% and 1% of the classified data, respectively."
P10-1003,Bitext Dependency Parsing with Bilingual Subtree Constraints,2010,19,19,3,1,21231,wenliang chen,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a target-side tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English."
P10-1026,A {B}ayesian Method for Robust Estimation of Distributional Similarities,2010,19,27,5,1,41587,junichi kazama,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures."
wang-etal-2010-adapting,Adapting {C}hinese Word Segmentation for Machine Translation Based on Short Units,2010,9,6,5,1,29973,yiou wang,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In Chinese texts, words composed of single or multiple characters are not separated by spaces, unlike most western languages. Therefore Chinese word segmentation is considered an important first step in machine translation (MT) and its performance impacts MT results. Many factors affect Chinese word segmentations, including the segmentation standards and segmentation strategies. The performance of a corpus-based word segmentation model depends heavily on the quality and the segmentation standard of the training corpora. However, we observed that existing manually annotated Chinese corpora tend to have low segmentation granularity and provide poor morphological information due to the present segmentation standards. In this paper, we introduce a short-unit standard of Chinese word segmentation, which is particularly suitable for machine translation, and propose a semi-automatic method of transforming the existing corpora into the ones that can satisfy our standards. We evaluate the usefulness of our approach on the basis of translation tasks from the technology newswire domain and the scientific paper domain, and demonstrate that it significantly improves the performance of Chinese-Japanese machine translation (over 1.0 BLEU increase)."
C10-2015,Improving Graph-based Dependency Parsing with Decision History,2010,27,6,4,1,21231,wenliang chen,Coling 2010: Posters,0,This paper proposes an approach to improve graph-based dependency parsing by using decision history. We introduce a mechanism that considers short dependencies computed in the earlier stages of parsing to improve the accuracy of long dependencies in the later stages. This relies on the fact that short dependencies are generally more accurate than long dependencies in graph-based models and may be used as features to help parse long dependencies. The mechanism can easily be implemented by modifying a graph-based parsing model and introducing a set of new features. The experimental results show that our system achieves state-of-the-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese.
C10-1095,Co-{STAR}: A Co-training Style Algorithm for Hyponymy Relation Acquisition from Structured and Unstructured Text,2010,20,2,3,1,12929,jonghoon oh,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper proposes a co-training style algorithm called Co-STAR that acquires hyponymy relations simultaneously from structured and unstructured text. In Co-STAR, two independent processes for hyponymy relation acquisition -- one handling structured text and the other handling unstructured text -- collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on large-scale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data."
W09-3506,Machine Transliteration using Target-Language Grapheme and Phoneme: Multi-engine Transliteration Approach,2009,10,12,3,1,12929,jonghoon oh,Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration ({NEWS} 2009),0,"This paper describes our approach to NEWS 2009 Machine Transliteration Shared Task. We built multiple transliteration engines based on different combinations of two transliteration models and three machine learning algorithms. Then, the outputs from these transliteration engines were combined using re-ranking functions. Our method was applied to all language pairs in NEWS 2009 Machine Transliteration Shared Task. The official results of our standard runs were ranked the best for four language pairs and the second best for three language pairs."
W09-1209,Multilingual Dependency Learning: Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies,2009,14,33,5,0,305,hai zhao,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes our system about multilingual syntactic and semantic dependency parsing for our participation in the joint task of CoNLL-2009 shared tasks. Our system uses rich features and incorporates various integration technologies. The system is evaluated on in-domain and out-of-domain evaluation data of closed challenge of joint task. For in-domain evaluation, our system ranks the second for the average macro labeled F1 of all seven languages, 82.52% (only about 0.1% worse than the best system), and the first for English with macro labeled F1 87.69%. And for out-of-domain evaluation, our system also achieves the second for average score of all three languages."
P09-1049,Bilingual Co-Training for Monolingual Hyponymy-Relation Acquisition,2009,18,16,3,1,12929,jonghoon oh,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper proposes a novel framework called bilingual co-training for a large-scale, accurate acquisition method for monolingual semantic knowledge. In this framework, we combine the independent processes of monolingual semantic-knowledge acquisition for two languages using bilingual resources to boost performance. We apply this framework to large-scale hyponymy-relation acquisition from Wikipedia. Experimental results show that our approach improved the F-measure by 3.6--10.3%. We also show that bilingual co-training enables us to build classifiers for two languages in tandem with the same combined amount of data as required for training a single classifier in isolation while achieving superior performance."
P09-1058,An Error-Driven Word-Character Hybrid Model for Joint {C}hinese Word Segmentation and {POS} Tagging,2009,24,92,5,0.443861,8038,canasai kruengkrai,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error-driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-of-the-art approaches reported in the literature."
D09-1060,Improving Dependency Parsing with Subtrees from Auto-Parsed Data,2009,30,63,4,1,21231,wenliang chen,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto-parsed data. First, we use a baseline parser to parse large-scale unannotated data. Then we extract subtrees from dependency parse trees in the auto-parsed data. Finally, we construct new subtree-based features for parsing algorithms. To demonstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach significantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data."
D09-1069,Can {C}hinese Phonemes Improve Machine Transliteration?: A Comparative Study of {E}nglish-to-{C}hinese Transliteration Models,2009,20,3,3,1,12929,jonghoon oh,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese transliteration models. However, such approaches have severely suffered from the errors in Chinese phoneme-to-grapheme conversion. To address this issue, we propose a new English-to-Chinese transliteration model and make systematic comparisons with the conventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration."
D09-1097,Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures,2009,21,40,2,1,300,ichiro yamada,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents. We attach new words to the hyponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web. For a given target word, our algorithm first finds k similar words from the Wikipedia database. Then, the hypernyms of these k similar words are assigned scores by considering the distributional similarities and hierarchical distances in the Wikipedia database. Finally, new hyponymy relations are output according to the scores. In this paper, we tested two distributional similarities. One is based on raw verb-noun dependencies (which we call RVD), and the other is based on a large-scale clustering of verb-noun dependencies (called CVD). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexico-syntactic patterns to Web documents."
D09-1122,Large-Scale Verb Entailment Acquisition from the {W}eb,2009,17,33,2,1,26950,chikara hashimoto,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Textual entailment recognition plays a fundamental role in tasks that require indepth natural language understanding. In order to use entailment recognition technologies for real-world applications, a large-scale entailment knowledge base is indispensable. This paper proposes a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale. We targeted 52,562 verb types that were derived from 108 Japanese Web documents, without regard for whether they were used in daily life or only in specific fields. In an evaluation of the top 20,000 verb entailment pairs acquired by previous methods and ours, we found that our similarity measure outperformed the previous ones. Our method also worked well for the top 100,000 results."
2009.iwslt-keynotes.3,Monolingual knowledge acquisition and a multilingual information environment,2009,0,0,1,1,12932,kentaro torisawa,Proceedings of the 6th International Workshop on Spoken Language Translation: Plenaries,0,None
P08-1047,Inducing Gazetteers for Named Entity Recognition by Large-Scale Clustering of Dependency Relations,2008,24,69,2,1,41587,junichi kazama,Proceedings of ACL-08: HLT,1,"We propose using large-scale clustering of dependency relations between verbs and multiword nouns (MNs) to construct a gazetteer for named entity recognition (NER). Since dependency relations capture the semantics of MNs well, the MN clusters constructed by using dependency relations should serve as a good gazetteer. However, the high level of computational cost has prevented the use of clustering for constructing gazetteers. We parallelized a clustering algorithm based on expectationmaximization (EM) and thus enabled the construction of large-scale MN clusters. We demonstrated with the IREX dataset for the Japanese NER that using the constructed clusters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases."
sumida-etal-2008-boosting,Boosting Precision and Recall of Hyponymy Relation Acquisition from Hierarchical Layouts in {W}ikipedia,2008,18,40,3,0,47429,asuka sumida,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper proposes an extension of Sumida and TorisawaÂs method of acquiring hyponymy relations from hierachical layouts in Wikipedia (Sumida and Torisawa, 2008). We extract hyponymy relation candidates (HRCs) from the hierachical layouts in Wikipedia by regarding all subordinate items of an item x in the hierachical layouts as xÂs hyponym candidates, while Sumida and Torisawa (2008) extracted only direct subordinate items of an item x as xÂs hyponym candidates. We then select plausible hyponymy relations from the acquired HRCs by running a filter based on machine learning with novel features, which even improve the precision of the resulting hyponymy relations. Experimental results show that we acquired more than 1.34 million hyponymy relations with a precision of 90.1{\%}."
I08-2126,Hacking {W}ikipedia for Hyponymy Relation Acquisition,2008,15,39,2,0,47429,asuka sumida,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"This paper describes a method for extracting a large set of hyponymy relations from Wikipedia. The Wikipedia is much more consistently structured than generic HTML documents, and we can extract a large number of hyponymy relations with simple methods. In this work, we managed to extract more than 1.4 xc3x97 106 hyponymy relations with 75.3% precision from the Japanese version of the Wikipedia. To the best of our knowledge, this is the largest machine-readable thesaurus for Japanese. The main contribution of this paper is a method for hyponymy acquisition from hierarchical layouts in Wikipedia. By using a machine learning technique and pattern matching, we were able to extract more than 6.3 xc3x97 105 relations from hierarchical layouts in the Japanese Wikipedia, and their precision was 76.4%. The remaining hyponymy relations were acquired by existing methods for extracting relations from definition sentences and category pages. This means that extraction from the hierarchical layouts almost doubled the number of relations extracted."
C08-1024,Looking for Trouble,2008,16,16,2,1,41513,stijn saeger,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a method for mining potential troubles or obstacles related to the use of a given object. Some example instances of this relation are (medicine, side effect) and (amusement park, height restriction). Our acquisition method consists of three steps. First, we use an un-supervised method to collect training samples from Web documents. Second, a set of expressions generally referring to troubles is acquired by a supervised learning method. Finally, the acquired troubles are associated with objects so that each of the resulting pairs consists of an object and a trouble or obstacle in using that object. To show the effectiveness of our method we conducted experiments using a large collection of Japanese Web documents for acquisition. Experimental results show an 85.5% precision for the top 10,000 acquired troubles, and a 74% precision for the top 10% of over 60,000 acquired object-trouble pairs."
D07-1033,A New Perceptron Algorithm for Sequence Labeling with Non-Local Features,2007,28,34,2,1,41587,junichi kazama,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We cannot use non-local features with current major methods of sequence labeling such as CRFs due to concerns about complexity. We propose a new perceptron algorithm that can use non-local features. Our algorithm allows the use of all types of non-local features whose values are determined from the sequence and the labels. The weights of local and non-local features are learned together in the training process with guaranteed convergence. We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm.
D07-1073,Exploiting {W}ikipedia as External Knowledge for Named Entity Recognition,2007,16,229,2,1,41587,junichi kazama,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER."
W06-2908,Semantic Role Recognition Using Kernels on Weighted Marked Ordered Labeled Trees,2006,16,0,2,1,41587,junichi kazama,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classification during runtime remained to be solved. In this paper, we give a solution that uses an efficient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classification."
N06-1008,Acquiring Inference Rules with Temporal Constraints by Using {J}apanese Coordinated Sentences and Noun-Verb Co-occurrences,2006,9,33,1,1,12932,kentaro torisawa,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co-occurrences. For example, our unsupervised acquisition method could obtain the inference rule If someone enforces a law, usually someone enacts the law at the same time as or before the enforcing of the law since the verbs enact and enforce frequently co-occurred in coordinated sentences and the verbs also frequently co-occurred with the noun law. We also show that the accuracy of the acquisition is improved by using the occurrence frequency of a single verb, which we assume indicates how generic the meaning of the verb is."
I05-1010,Automatic Discovery of Attribute Words from Web Documents,2005,13,40,3,0,51047,kosuke tokunaga,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We propose a method of acquiring attribute words for a wide range of objects from Japanese Web documents. The method is a simple unsupervised method that utilizes the statistics of words, lexico-syntactic patterns, and HTML tags. To evaluate the attribute words, we also establish criteria and a procedure based on question-answerability about the candidate word."
H05-1018,Speeding up Training with Tree Kernels for Node Relation Labeling,2005,18,18,2,1,41587,junichi kazama,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a method for speeding up the calculation of tree kernels during training. The calculation of tree kernels is still heavy even with efficient dynamic programming (DP) procedures. Our method maps trees into a small feature space where the inner product, which can be calculated much faster, yields the same value as the tree kernel for most tree pairs. The training is sped up by using the DP procedure only for the exceptional pairs. We describe an algorithm that detects such exceptional pairs and converts trees into vectors in a feature space. We propose tree kernels on marked labeled ordered trees and show that the training of SVMs for semantic role labeling using these kernels can be sped up by a factor of several tens."
W04-1210,Improving the Identification of Non-Anaphoric it using Support Vector Machines,2004,10,9,3,0,51590,jose litran,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"Identification of non-anaphoric use of the pronoun it is crucial to achieve full anaphora resolution. Nevertheless, this problem has been either ignored or considered too simple to deserve a deeper study. In this paper we present a machine-learning approach using Support Vector Machines. We collected several instances of both anaphoric and non-anaphoric it from the GENIA corpus, together with syntactic information about the context. We show how by using a limited amount of knowledge our approach can achieve better accuracy than previous methods. We also analyze the relevance of features used to predict non-anaphoric uses."
N04-1010,Acquiring Hyponymy Relations from Web Documents,2004,7,77,2,0,15889,keiji shinzato,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"This paper describes an automatic method for acquiring hyponymy relations from HTML documents on the WWW. Hyponymy relations can play a crucial role in various natural language processing systems. Most existing acquisition methods for hyponymy relations rely on particular linguistic patterns, such as xe2x80x9cNP such as NPxe2x80x9d. Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences."
C04-1135,Extracting Hyponyms of Prespecified Hypernyms from Itemizations and Headings in Web Documents,2004,7,12,2,0,15889,keiji shinzato,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper describes a method to acquire hyponyms for given hypernyms from HTML documents on the WWW. We assume that a heading (or explanation) of an itemization (or listing) in an HTML document is likely to contain a hypernym of the items in the itemization, and we try to acquire hyponymy relations based on this assumption. Our method is obtained by extending Shinzato's method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods."
P03-2036,Comparison between {CFG} Filtering Techniques for {LTAG} and {HPSG},2003,22,1,2,1,4617,naoki yoshinaga,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented. We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG. We also investigate the reason for that difference.
C02-1120,An Unsupervised Learning Method for Associative Relationships between Verb Phrases,2002,5,10,1,1,12932,kentaro torisawa,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes an unsupervised learning method for associative relationships between verb phrases, which is important in developing reliable Q&A systems. Consider the situation that a user gives a query How much petrol was imported by Japan from Saudi Arabia? to a Q&A system, but the text given to the system includes only the description X tonnes of petrol was conveyed to Japan from Saudi Arabia. We think that the description is a good clue to find the answer for our query, X tonnes. But there is no large-scale database that provides the associative relationship between imported and conveyed. Our aim is to develop an unsupervised learning method that can obtain such an associative relationship, which we call scenario consistency. The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases."
W01-1510,Resource Sharing Amongst {HPSG} and {LTAG} Communities by a Method of Grammar Conversion between {FB}-{LTAG} and {HPSG},2001,23,0,3,1,4617,naoki yoshinaga,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques."
C00-1060,A Hybrid {J}apanese Parser with Hand-crafted Grammar and Statistics,2000,13,25,2,0,1260,hiroshi kanayama,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique. The key feature of our system is that in order to estimate likelihood for a parse tree, the system uses information taken from alternative partial parse trees generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model. We show that this model can capture a certain tendency in Japanese syntactic structures and this point contributes to improvement of parsing accuracy on a shallow level. We report that, with an underspecified HPSG-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR annotated corpus, and that it outperformed other purely statistical parsing methods on the same corpus. This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level."
W98-0127,Packing of feature structures for optimizing the {HPSG}-style grammar translated from {TAG},1998,0,1,2,0.882353,5928,yusuke miyao,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
W98-0141,Translating the {XTAG} {E}nglish grammar to {HPSG},1998,2,22,2,0,35318,yuka tateisi,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
P98-2132,{L}i{LF}e{S} - Towards a Practical {HPSG} Parser,1998,12,23,3,0,53249,takaki makino,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG. The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu. Basic design policies, the current status, and performance evaluation of the LiLFeS system are described. The paper discusses two implementations of the LiLFeS. The first one is based on an emulator of the abstract machine, while the second one uses a native-code compiler and therefore is much more efficient than the first one."
P98-2144,{HPSG}-Style Underspecified {J}apanese Grammar with Wide Coverage,1998,5,21,2,0,39370,yutaka mitsuishi,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one."
P98-2159,An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines,1998,10,0,2,0,3213,takashi ninomiya,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers' point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers."
C98-2128,{L}i{LF}e{S}- Towards a Practical {HPSG} Parser,1998,12,23,3,0,53249,takaki makino,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG. The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu. Basic design policies, the current status, and performance evaluation of the LiLFeS system are described. The paper discusses two implementations of the LiLFeS. The first one is based on an emulator of the abstract machine, while the second one uses a native-code compiler and therefore is much more efficient than the first one."
C98-2139,{HPSG}-Style Underspecified {J}apanese Grammar with Wide Coverage,1998,5,21,2,0,39370,yutaka mitsuishi,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes a wide-coverage Japanese grammar based on HPSG. The aim of this work is to see the coverage and accuracy attainable using an underspecified grammar. Underspecification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely. The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (POSs) ). Furthermore, word-specific constraints such as subcategorization of verbs are not fixed in the grammar. However, this grammar can generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus. The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu is attached to the nearest possible one."
C98-2154,An Efficient Parallel Substrate for Typed Feature Structures on Shared Memory Parallel Machines,1998,10,0,2,0,3213,takashi ninomiya,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines. We call the system Parallel Substrate for TFS (PSTFS). PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other. Such agents use PSTFS as their low-level module for solving constraints on TFSs and sending/receiving TFSs to/from other agents in an efficient manner. From a programmers' point of view, PSTFS provides a simple and unified mechanism for building high-level parallel NLP systems. The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers. The speed-up was more than 10 times on both parsers."
C96-2160,Computing Phrasal-signs in {HPSG} prior to Parsing,1996,5,14,1,1,12932,kentaro torisawa,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper describes techniques to compile lexical entries in HPSG (Pollard and Sag, 1987; Pollard and Sag, 1993)-style grammar into a set of finite state automata. The states in automata are possible signs derived from lexical entries and contain information raised from the lexical entries. The automata are augmented with feature structures used by a partial unification routine and delayed/frozen definite clause programs."
1995.iwpt-1.29,An {HPSG}-based Parser for Automatic Knowledge Acquisition,1995,-1,-1,1,1,12932,kentaro torisawa,Proceedings of the Fourth International Workshop on Parsing Technologies,0,
