2018.gwc-1.48,2016.gwc-1.9,0,0.0213772,"linguistics.ntu.edu.tw/ swn 5 to by the lexical sense has an equivalent in English. Meanwhile, the links to WordNet serve as key to the Linguistic Linked Open Data cloud (Chiarcos et al., 2013) and interface with other linguistic resources. Moreover, Sinitic Wordnet can be integrated into the Global WordNet Grid when organized by the ontology consisting of 71 Base Types proposed by the Global WordNet Association.6 An initial mapping has identified 169 synsets comparable to the Base Types.7 4.2 Integrating the Resource with Collaborative Interlingual Index The Collaborative Interlingual Index (Bond et al., 2016) has been proposed as a method to enable cross-lingual development of wordnets. Chief among the primary objectives of the project is to establish a standard operating procedure by which new synsets can be defined and added to a common repository, resolving compatibility issues that may occur when wordnets for languages other than English introduce concept not lexicalized in English. In order to facilitate the integration of Sinitic Wordnet with the Collaborative Interlingual Index, we are making the full version of the resource available in the Global WordNet Association’s recommended formats"
2019.gwc-1.19,P18-1031,0,0.0129142,"eologisms and ever-changing novel word usage. Shu-Kai Hsieh Graduate Institute of Linguistics National Taiwan University shukaihsieh@ntu.edu.tw Recent algorithmic advancements shed lights on how we can augment lexical resources, at least semi-automatically. Thanks to the bloom of internet and social media, voluminous textual data are easily available, where emergent concepts and their relations could be discovered from the real-world and most updated data. This process is further facilitated by recent development of deep learning and machine learning models, such as pretrained language model (Howard and Ruder, 2018), word embeddings (Joulin et al., 2017), or contextualized embeddings. These computation resources allows us to leverage the ample data, without going through considerable efforts to actually collect, and store the vast amount of data, and setup a model training infrastructure. In this paper, we took advantage the recent development on contextualized embeddings. Specifically, we used a pre-trained bidirectional encoder representations from transformer (BERT) (Devlin et al., 2018), basing on which we semi-automatically predicted new related senses in CWN. The predictions were only possible with"
2019.gwc-1.19,W14-0139,1,0.861391,"oing through considerable efforts to actually collect, and store the vast amount of data, and setup a model training infrastructure. In this paper, we took advantage the recent development on contextualized embeddings. Specifically, we used a pre-trained bidirectional encoder representations from transformer (BERT) (Devlin et al., 2018), basing on which we semi-automatically predicted new related senses in CWN. The predictions were only possible with the constraints encoded in Chinese morphology, where the semantic relationship between the whole word and its composing sub-word were suggested (Hsieh and Chang, 2014). We introduced how we applied BERT to construct sense vectors from existing example sentences in each CWN senses, and how to use sense vectors and heuristics rule s regarding Chinese word morphology to semi-automatically generate new relationships (hyponymy/troponymy pairs) among CWN senses. We evaluated these sense vectors with a simulation study and conducted an experiment on the model-predicted sense relation pairs. The procedures described in this paper was shown in Figure 1. Sense vectors CWN & Chinese word morphology example sentences in CWN senses two-character words in CWN compute con"
2019.gwc-1.19,P15-1010,0,0.0313823,"ly used by the model. The resulting word vectors were therefore undifferentiated representations of word senses. Other models have the potential to accommodate, or even represent, word senses information, but not without caveats. For example, latent Dirichlet allocation (LDA) (Griffiths et al., 2007), representing meanings of each word as a probability distribution over different topics, could describe each word sense as a mixture of different topic components. But the problems remains on how to relate latent topics with the word senses. Other endeavors relies on a sense-disambiguated corpus (Iacobacci et al., 2015), and inferred the sense vectors through the disambiguated context. But this approach required a mature word sense disambiguation (WSD) algorithm or sense-tagged corpus with given sets of word sense distinctions. Chinese WSD is an active and productive research topic, but the word sense disambiguation on CWN word senses remains a challenging task. Instead of relying on sense-disambiguated corpus, recent models tried to incorporate word context into deep learning models and construct contextualized vectors (Peters et al., 2018; McCann et al., 2017). Inspired by the deep learning models in compu"
2019.gwc-1.19,E17-2068,0,0.0151873,"ge. Shu-Kai Hsieh Graduate Institute of Linguistics National Taiwan University shukaihsieh@ntu.edu.tw Recent algorithmic advancements shed lights on how we can augment lexical resources, at least semi-automatically. Thanks to the bloom of internet and social media, voluminous textual data are easily available, where emergent concepts and their relations could be discovered from the real-world and most updated data. This process is further facilitated by recent development of deep learning and machine learning models, such as pretrained language model (Howard and Ruder, 2018), word embeddings (Joulin et al., 2017), or contextualized embeddings. These computation resources allows us to leverage the ample data, without going through considerable efforts to actually collect, and store the vast amount of data, and setup a model training infrastructure. In this paper, we took advantage the recent development on contextualized embeddings. Specifically, we used a pre-trained bidirectional encoder representations from transformer (BERT) (Devlin et al., 2018), basing on which we semi-automatically predicted new related senses in CWN. The predictions were only possible with the constraints encoded in Chinese mor"
2019.gwc-1.19,N18-1202,0,0.298125,"the potential hypernyms were identified, the rule cannot provide further guidance on the senses upon which the hypernymy relation should be created. 2.2 Contextualized Embeddings Vector semantics are models in which researcher use a formal mathematical structure (i.e. vectors) to represent how lexical meanings of words reside in a vector space. The vectors representing each words also encode, to some extent, their mutual semantic relations in that space. This general approach, while being a heated topic in recent years (Landauer and Dumais, 1997; Griffiths et al., 2007; Mikolov et al., 2013; Peters et al., 2018), could be traced back to mid-20th century (Firth, 1957). The idea was to explore the co-occurrence of the words in context (sentences, or a groups of preceding and following words), and use the context to determine the location of a word vector in semantic space, where thus location could best reflect the relationships with other words. While models of vector semantics enjoyed great successes in various NLP tasks, even were indispensable constructs in virtually all deep learning models, challenges emerged when they came to WordNet. WordNet, as a lexical resource of word senses and linguistic"
2019.rocling-1.19,W17-1101,0,0.0773712,"ade up the big picture of the gender-related cyberhate speech. The emphasis on Distributional Hypotheses (Sahlgren, 2008) also depicts the importance of exploring distributional differences of word representations under the contexts related to a sexually biased speech based on a quantitative computational method. Instead of unsystematic regulations of stopping the propagation of cyberhate speech, there were some methods applying Natural Language Processing (NLP) models for efficiently detecting abusive language online with regard to, but not limited to, cyberbullying and gender-biased speech (Schmidt and Wiegand, 2017;Davidson et al., 2017;Burnap and Williams, 2016). On the ground of previous studies, the relationships and connections between cyberhate speech and gender-biased language on PTT could be reported in both quantitative and qualitative ways. The aim of the present paper is threefold: (1) to analyze the large text sources and compare the distributed representations of words represented by a word vector encoding semantic similarity;(2) to capture linguistic properties from the word vectors;(3) to specify the polarity fluctuation with regard to the keywords from the word vector. 2 2.1 Previous Stud"
2020.coling-main.258,P19-1379,0,0.0992551,"he question of why very few words remain in active use must also be considered (Zhou, 2009). The morphological function affixoids perform drives disyllabic development in the Chinese language and facilitates a more clear and accurate delivery of meaning. How characters are combined to form a word is phonologically, semantically, and even pragmatically dependent (Zhou, 2009; Zhang, 2008). For instance, Wang & Guo (2005, page 141) do not classify 家 jiā “-ist” as an affix, for the word still indicates a person’s occupation or profession, rather than having undergone semantic bleaching. Following Hu et al. (2019), we track the sense evolution of the affixoid 家 jiā “-ist” by capturing its different senses with BERT-based Chinese pre-trained contextualized embeddings (Devlin et al., 2018). The definition and example sentences are extracted from the Chinese WordNet3 as the knowledge source for sense representations, and the texts from the Chinese Text Project(Sturgeon, 2019)4 and Sinica Corpus 5 are used to compute the cosine similarity and derive the belonging sense of a word. As Figure 3 shows, a number of senses carry the meaning of an affixoid (e.g., sense 8, 9), and sense 8 and 9 are the dominant re"
2020.coling-main.258,O03-4003,0,0.406428,"nsparent to and yet distinct from another lexeme to which it corresponds formally and/or historically (Booij, 2005). However, some scholars argue against the delineation of affixoids as a separate class (Arcodia, 2012). In this study, we aim to empirically revisit this long-standing debate by way of computational modeling of affix(oid) behavior based on a comprehensive database of Chinese. 2 Related works To date, only a few quantitative works measure affix behavior in Chinese. Most of them focus on quantifying the morphological productivity of a small number of Chinese affixes. For instance, Nishimoto (2003) contrasts type-based and token-based measures, selecting five Chinese suffixes for comparison: the nominal suffixes 兒 -ér, 子 -zi, and 頭 -tóu, the verbal suffix 化 -huà “-ize”, as well as the plural suffix 們 -men “-s/-es”. The latter is the only suffix of the four that may be considered an inflectional suffix in Mandarin, while the other four are used productively in word formation, i.e., are derivational affixes. As a token-based measure, Nishimoto uses Baayen’ s hapax-based measure (Baayen and Lieber, 1991), P , defined as P = nN1 , where n1 counts the number of hapax legomena of a particular"
2020.coling-main.258,D19-1410,0,0.0178391,"ame word distribution as output. To achieve this goal, the model tries to compress the information into a topic distribution, θ, by learning two sets of variational 2882 parameters: µ and Σ (Eq. 4). The output distribution is then reconstructed with the topic distribution, θ, and the vocabulary parameters, β (Eq. 5). θ = σ(µ + Σ1/2 ϵ), ϵ ∼ N (0, I) wn |β, θ ∼ Multinomial(1, σ(βθ)) (4) (5) Reconstructing the word distribution with the variational autoencoder opens up new possibilities of model specification. In a recent study, Bianchi, Terragni, & Hovy (2020a) used a sentence embedding, SBERT (Reimers and Gurevych, 2019), as input, and word distribution as output. They argue the embeddings from BERT encodes more contextual information than the word distribution from the bag-of-words (Bianchi et al., 2020b). Following Bianchi, Terragni, & Hovy (2020a)’s model, we construct a contextualized variational topic model on affixoids. We first encode each occurrence of affixoid-containing words with an additional token specific to the affixoid. Therefore, the vocabulary of the model includes not only all of the words in the text, but also each of the individual affixoids contained within the text. Since affixoids in C"
2020.lrec-1.33,J12-2003,0,0.0381084,"Missing"
2020.lrec-1.33,D15-1189,0,0.0469535,"Missing"
2020.lrec-1.33,W10-2102,0,0.0310621,"Missing"
2020.lrec-1.33,J94-2004,0,0.812101,"Missing"
2020.paclic-1.13,N06-2015,0,0.0433553,"tertwined assumptions underlying the conventional WSD task are (1) word senses can be operationalized as discrete and distinguishable ones, (2) word senses (as included in the sense inventory) can be shared by the entire language community, and (3) WSD with the finegrained sense specification can be successfully applied to actual language data, and facilitate a wide range of downstream NLP tasks. However, the reported poor inter-annotator agreement (IAA) and low reliability of sense distinction/annotation in the task seem to falsify these assumptions and thus motivate projects like OntoNotes (Hovy et al., 2006; Cinková et al., 2012). This paper aims to serve as a first attempt to propose an alternative to the underlying assumptions from the functional and granular linguistic perspective. First, the notion wordhood of as assumed in the WSD task is not self-evident, particularly for languages whose writing systems do not provide the delimiter of a word boundary. In this aspect, word segmentation or determination is rather theoryladen and would be best regarded as the wordhood annotation rather than the preprocessing task with ground truth as conventionally taken. Second, word-meaning pairs are fluid"
2020.paclic-1.13,J17-1002,0,0.061848,"Missing"
2020.paclic-1.47,J90-1003,0,0.366181,"Missing"
2020.rocling-1.20,D19-1469,0,0.0314749,"Missing"
2020.rocling-1.20,P19-1455,0,0.0553388,"Missing"
2020.rocling-1.20,P17-5002,0,0.0591284,"Missing"
2020.rocling-1.20,N18-2107,0,0.0451067,"Missing"
2020.rocling-1.20,D19-1410,0,0.0320705,"Missing"
2020.rocling-1.4,hsieh-2014-chinese,1,0.773331,"onsense—instead of being recurrent “word” combinations, the “collocations” may in fact be “character” combinations that have a tendency to co-occur. With the prevalence of the internet, large corpora constructed from texts collected from the web has become common. At the same time, manual checking of the automatic segmentation and tagging of the corpora to ensure the quality has become nearly impossible, as the amount of data collected is enormous. In addition, out-of-vocabulary words such as named entities, new terms, and special usage of particular subcultures frequently appear in web texts [2], further casting doubt on the performance of automatic segmentation of the constructed corpora. Since manual checking and corrections are not practical solutions to counter automatic preprocessing errors in large corpora, it is crucial to be aware of the negative impacts that such errors could have on downstream tasks. For instance, collocations extracted from Chinese social media texts may contain several instances of false collocations that resulted from word 9 The 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020) Taipei, Taiwan, September 24–26, 2020. The As"
2020.rocling-1.4,L18-1550,0,0.0228662,"ollocates were extracted. For each of the two lists, collocations were extracted using seven measures (the eight measures except Dice mentioned in section 2.1)—20 collocations ranked as highest were retrieved for each measure, resulting in a list of 140 collocations (with duplications). Then, collocations that appeared less than 3 times were removed from the list (i.e., a collocation needs a rank of at least 20 in at least 3 measures to retain in the list). We then calculated the semantic similarities (cosine similarity of word vectors) between all words with FastText pre-trained word vectors [7]. The results are represented as network plots shown in Figure 4 and 5. The node in the network represents a word (either a node word or its collocates) in a list of collocations. The thickness of the edge between a pair of words indicates the degree of similarity between them, with higher similarity represented by a thicker edge. One feature that instantly pops out from the figures is that correctly segmented collocates (blue nodes) form clusters. That is, these collocates are similar to each other in terms of semantic similarities as measured by the cosine similarity of their word vectors. O"
2020.rocling-1.4,Q15-1016,0,0.0480343,". One feature that instantly pops out from the figures is that correctly segmented collocates (blue nodes) form clusters. That is, these collocates are similar to each other in terms of semantic similarities as measured by the cosine similarity of their word vectors. On the other hand, collocates resulting from word segmentation errors are much more spread out throughout the network. This contrast between correctly and erroneously segmented collocates makes sense since word vectors are known to capture the extent to which words are replaceable (i.e., second-order, or paradigmatic, similarity) [8]. Thus, the collocates appearing within the same paradigm, such as 林{同學/老師/醫生} or 三{次/位/名/天/年/秒/小時}, are expected to have high pairwise similarities. Word segmentation errors, on the other hand, distort the wellformedness of the words, which may result in noisy patterns in similarities between these anomalous words, and the patterns are likely to vary case to case for collocations retrieved with different node words. This simple ad hoc study, which shows that erroneously word segmented collocates may have a different profile to correctly segmented collocates, thus hints at a potential directio"
C10-2108,Y09-2029,1,0.677352,"b’s meaning specificity. The distributions in context represent not only the linguistic behaviors but the semantic contents of lexical items. 3 The Influence of Specificity on Acquisition This section assesses the influence of semantic space on the acquisition of the verbal lexicon. With the examination of Specific verb (S verb) progress, this study proposes that Generic verbs (G verbs) are acquired earlier than S verbs due to the closer semantic space. It also testifies whether the S verb development is a developing trend parallel with the acquisition of conventional verbs(Chen et al., 2008; Hsieh et al., 2009)9 from the experimental data collected by M3 project. Based on the developing trend of conventional lexical items, the following parts analyze the relation of meaning specificity and the acquisition of lexical items. 3.1 Decreasing in Lexical Variation The section is concerned with lexical variation among participants within the same age group. 9 They rearranged the five groups of participants into three units and then investigated the learning trend by Replacing Rate (Frequency of V 2f req / Frequency of V 1f req ). By defining adults’ usages as the conventional one called V1, children’s seco"
C10-2108,widdows-ferraro-2008-semantic,0,0.0245034,"rbs based on Distance in Paradigmatic Graphs. Project website: http://140.112.147.149:81/m3/ 937 Coling 2010: Poster Volume, pages 937–945, Beijing, August 2010 Sinica Balanced Corpus (ASBC)2 . It includes 190 files containing about 96000 word types3 . The original matrix (M ) is further decomposed into the product of three matrices (T SDT ). These matrices are then reduced into k dimensions. In the following reconstruction process based on k dimensions, it multiplies out the truncated matrices Tk Sk Dk0 and then gets a Mk matrix (the approximation of X)(Landauer et al., 1998; Sahlgren, 2005; Widdows and Ferraro, 2008). The following shows an example of finding the nearest neighbors of the word da (打 / to hit) via two methods (see Table 1). For the convenience of visualization and cluster analysis, Euclidean distance is applied in the following study. Cosine Distance qu ‘go’ 0.928 0.377 na ‘take’ 0.926 0.382 zhao ‘find’ 0.920 0.397 Table 1: Associating words of da ‘hit’. 2.2 Semantic Clustering The primary objective of cluster analysis is to examine the formation of a taxonomy: whether G verbs and S verbs form two groups separately. The clusters also help us grasp the semantic space among verbs as well as t"
C10-3002,O98-3003,0,0.114469,".1 Chinese Wordnet The construction of Chinese Wordnet developed by Academia Sinica follows two lines of thought: (i) multilingual wordnets bootstrapping approach (cf. Sinica BOW[2]), and (ii) linguistically oriented analysis from scratch (cf. CWN[3]). Both of them can be merged organically. In this paper, we focus only on the CWN part. Generally speaking, NLTK WordnetCorpusReader cannot be seamlessly applied to CWN with the following reasons: z z z z 3.2 Distinction of Sense and Meaning Facet: CWN proposed that lexical polysemy can be distinguished into two levels: senses and meaning facets (Ahrens et al. 1998). These two levels of polysemies result in a special design for synset. Labeling of Paronymy: CWN defines paronymy as the relation between any two lexical items belonging to the same semantic classification. (Huang et al, 2007), and label the relation among senses instead of synsets. Distinction of Synonyms and Chinese Written Variants: CWN regards synonyms and variants differently. Variants are the corresponding words/characters that have different written forms but the same meaning and the identical pronunciation as the target word. In PyCWN, the variants are integrated into the synset of th"
chung-etal-2008-extracting,P94-1019,0,\N,Missing
chung-etal-2008-extracting,huang-etal-2004-sinica,1,\N,Missing
hsieh-2014-chinese,J03-3001,0,\N,Missing
I05-3008,Y04-1015,0,0.0671922,"Missing"
I05-3008,C00-1026,0,0.428686,"ergeneration on the other.1 Empirical data have Introduction This paper describes the theoretical consideration concerning with the interaction of ontology and morpho-semantics, and an NLP experiment is performed to do semantic class prediction of unknown two-character words based on the ontological and lexical knowledge of Chinese morphemic components of words (i.e., characters). The task that the semantic predictor (or classifier) performs is to automatically assign the (predefined) semantic thesaurus classes to the unknown two-character words of Chinese. Among these types of unknown words, Chen and Chen (2000) pointed out that compound words constitute the most productive type of unknown words in Chinese texts. However, the caveat at this point should be carefully formulated, due to the fact that there are no unequivocal opinions concerning with some basic theoretical settings in Chinese morphology. The no1 For example, in applying Lieber’s (1992) analysis of argument structure and theta-grid in Chinese V-V compounds, Chang (1998) found some examples which may satisfy the semantic and syntactic constraints, but they may not be ac56 also shown that in many cases, – e.g., the abundance of phrasal lex"
I05-3008,W04-1106,0,0.187295,"cilitated using Chinese characters as a knowledge resource. 1 2 Word Meaning Inducing via Character Meaning 2.1 Morpho-Semantic Description As known, “bound roots” are the largest classes of morpheme types in Chinese morphology, and they are very productive and represent lexical rather than grammatical information (Packard 2000). This morphological phenomena leads many Chinese linguists to view the word components (i.e., characters) as building blocks in the semantic composition process of dis- or multisyllabic words. In many empirical studies (Tseng and Chen (2002); Tseng (2003); Lua (1993); Chen (2004)), this view has been confirmed repeatedly. In the semantic studies of Chinese word formation, many descriptive and cognitive semantic approaches have been proposed, such as argument structure analysis (Chang 1998) and the frame-based semantic analysis (Chu 2004). However, among these qualitative explanation theoretical models, problems often appear in the lack of predictability on the one end of spectrum, or overgeneration on the other.1 Empirical data have Introduction This paper describes the theoretical consideration concerning with the interaction of ontology and morpho-semantics, and an"
I08-1052,C00-1014,0,0.7112,"Missing"
I08-1052,C94-1091,1,0.807876,"Missing"
I17-3011,Q17-1010,0,0.0083609,"en words (Morgado da Costa et al., 2016). 2.2.3 Context-based models Previous approaches predominantly rely on ontological resources, which require a lot of human effort to build and maintain, resulting in limited coverage for new words and domains. We use distributed representations to capture word similarity based on syntactic behaviour, as they can be trained unsupervised on a large scale and are easily adapted on new language material. We train word embeddings with word2vec (Mikolov et al., 2013) on sentences from the original three corpora and also obtain pre-trained word embeddings from Bojanowski et al. (2017). The pre-trained embeddings consistently achieve better results and are hence used in all subsequent experiments. Since the head word is linguistically the most important factor for classifier selection, we first train two widely used machine learning models (SVM, Logistic Regression) on the embedding vector of the head word (head). In order to investigate to which extend context may help with classifier prediction, we then gradually add more contextual features to the models: With the motivation of reducing head word ambiguity, we include embedding vectors of words within window size n=2 of"
I17-3011,I05-3004,0,0.59573,"r Prediction Models Interactive Web Interface Figure 1: Overview of proposed system 41 The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 41–44, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP Figure 1 gives an overview of our system. It comprises data collection, pre-processing and the compilation of the Chinese Classifier Database (section 2.1), the training of classifier prediction models (section 2.2), and the interactive online interface (section 3). 2.1 2.2 Classifier Prediction 2.2.1 Task Following the only previous machine learning approach (Guo and Zhong, 2005), we frame classifier prediction as a multi-class classification problem. However, in contrast to previous work that focused on word-based classifier prediction, we adapt the prediction task for a sentence-based scenario, which is a more natural and less ambiguous task than predicting classifiers without context. Not all sentences in the Chinese classifier database contain head words, due to co-referential and anaphoric usage. Hence, we query the database for sentences in which both the head word and corresponding classifier were identified, resulting in 681,102 sentences. This subset is rando"
I17-3011,P03-1056,0,0.0453445,"Missing"
I17-3011,mcenery-xiao-2004-lancaster,0,0.0206318,"Missing"
I17-3011,2016.gwc-1.36,0,0.535359,"quantity or frequency of its head word and requires a certain degree of shared properties between classifier and head. Although native speakers select classifiers intuitively, language learners often struggle with the correct usage of classifiers due to the lack of a similar word class in their native language. Moreover, no dictionary or finite set of rules covers all possible classifier-head combinations exhaustively. Previous research has focused on associations between classifiers and nominal head words in isolation and included approaches based on ontologies (Mok et al., 2012; Morgado da Costa et al., 2016), databases with semantic features of Chinese classifiers (Gao, (2) 一 场 精彩 的 球 one chang exciting DE ball ‘an exciting match’ This study introduces a large-scale dataset of everyday Chinese classifier usage for machine learning experiments. We present a model that outperforms previous frequency and ontology baselines for classifier prediction without the need for extensive linguistic preprocessing and head word identification. We further demonstrate the usefulness of the database and our models in use cases. 2 System Design Preprocessing sentence extraction Corpora 1 Following Huang (1998) and"
L18-1207,P16-1039,0,0.0437546,"Missing"
L18-1207,P14-5010,0,0.00256401,"rofoundly constrained how Chinese text can be annotated and interpreted in later processing steps and analysis. Fluid Segmenter, instead of pursuing the unique ”golden answer”, aimed to present the whole spectrum of possibilities on how multiple syllables in Chinese, which represented by multiple individual characters conglomerate into a larger linguistic pattern. Most of Chinese word segmenters based on algorithms which can identify words in a pre-defined segmented corpus. Different segmenters differs on the particular algorithms they implemented. For instance, segmenter in Stanford CoreNLP (Manning et al., 2014) implemented a sophisticated conditional random field model that performed well on segmentation task. However, since the segmenter solely focused on aligning themselves with a predefined word segmentation, different possibilities of segmentation became difficult, if not impossible, to shown themselves in the model outputs. The segmenter provides words with different granularity by multiple passes of maximal matching and segmentation alignments. To start segmentation, segmenter firstly tries to start with a coarse-grained level (e.g. granularity parameter = 0). Lexicon are queried with the char"
L18-1207,I17-1018,0,0.0261155,"d annotation, disentangling itself from the errorprone role in the NLP pipeline architecture. 2. Review Word segmentation has been a thorny issue in NLP for many decades. In addition to structural ambiguity resolution and unknown word detection, the current focus is concerned with propagation error and domain adaption. As the pre-processing task in the pipeline architecture, word segmentation errors can propagate to later processing stages. To handle with this, joint approaches exploiting various machine learning models including the latest neural network have been proposed (Lyu et al., 2016; Shao et al., 2017). Second, it has been recognized that different applications and domains have different calls for different granularities of word segmentation. Recent neural domain adaptation approaches also work through cross-domain embeddings to improve the cross-domain performance (Cai and Zhao, 2016; Zhang et al., 2014). However, a critical examination of the underlying assumption, and their assessment in the light of naturally occurring linguistic data, reveal its inherent contradictions (Taylor, 2012). In the following sections, we introduce the proposed Fluid Annotation model in more details. 3. Fluid"
L18-1207,E14-1062,0,0.0313677,"in adaption. As the pre-processing task in the pipeline architecture, word segmentation errors can propagate to later processing stages. To handle with this, joint approaches exploiting various machine learning models including the latest neural network have been proposed (Lyu et al., 2016; Shao et al., 2017). Second, it has been recognized that different applications and domains have different calls for different granularities of word segmentation. Recent neural domain adaptation approaches also work through cross-domain embeddings to improve the cross-domain performance (Cai and Zhao, 2016; Zhang et al., 2014). However, a critical examination of the underlying assumption, and their assessment in the light of naturally occurring linguistic data, reveal its inherent contradictions (Taylor, 2012). In the following sections, we introduce the proposed Fluid Annotation model in more details. 3. Fluid Annotation The scheme of Fluid Annotation comprised of three main components: DeepLexicon, Fluid Segmentation & Tagger, and Annotation UI (Figure 1). Six crucial steps were identified in the scheme: (1) unprocessed text was fed into fluid segmentation and tagging preprocessor, where (2) text was segmented wi"
O06-1003,W02-1106,1,0.899142,"Missing"
O06-1003,I05-4007,1,0.801,"Missing"
O06-1003,W06-3909,0,0.0924504,"Missing"
O08-1001,N07-1058,0,0.0244061,"ssumed to be able to determine vocabulary complexity, thus text complexity. Vocabulary complexity is otherwise measured in terms of word length, e.g., the Flesch formula [4] and FOG formula [5]. This is based on another assumption that the longer a word is, the more difficult it is to comprehend [6]. Many readability formulae presume the correlation between comprehension difficulty and syntactic complexity. For Dale and Chall [7], Flesch formula [4], and FOG index [5], syntactic complexity boils down to the average length of sentences in a text. Heilman, Collins-Thompson, Callan, and Eskenazi [8] also take morphological features as a readability index for morphosyntactically rich languages. Das & Roychoudhury’s readability index [9] for Bangla has two variables: average sentence length and number of syllables per word. Flesch [4] and Cohen [10] take semantic factors into account by counting the abstract words of a text. Kintsch [11] focuses on propositional density and inferences. Wiener, M., Rubano, M., and Shilkret, R. [12] propose a scale based on ten categories of semantic relations including, e.g., temporal ordering and causality. They show that the utterances of fourth-, sixth-,"
O08-1001,J87-3007,0,\N,Missing
O08-1001,C04-1087,0,\N,Missing
O08-1001,W03-1806,0,\N,Missing
O08-1001,C94-2167,0,\N,Missing
O08-1001,E95-1003,0,\N,Missing
O08-1001,P04-3019,0,\N,Missing
O08-1001,P90-1034,0,\N,Missing
O08-1001,P97-1009,0,\N,Missing
O08-1001,nenadic-etal-2002-automatic,0,\N,Missing
O08-1001,N03-1028,0,\N,Missing
O08-1001,P93-1023,0,\N,Missing
O08-1001,W04-3205,0,\N,Missing
O08-1009,C92-1019,0,0.116062,"Missing"
O08-1009,P04-1059,0,0.0746096,"Missing"
O08-1009,Y06-1001,0,0.0305597,"Missing"
O08-1009,O99-2001,0,0.0297197,"Missing"
O08-1009,W06-0115,0,0.165904,"Missing"
O08-1009,O03-4002,0,0.27067,"Missing"
O08-1009,P07-2018,1,0.896796,"hinese word segmentation tool must adapt to textual variations with minimal training input and yet robust enough to yield reliable segmentation result for all variants. Various lexicon-driven approaches to Chinese segmentation, e.g. [1,16], achieve high f-scores yet require massive training for any variation. Text-driven approach, e.g. [12], can be easily adapted for domain and genre changes yet has difficulty matching the high f-scores of the lexicon-driven approaches. In this paper, we refine and implement an innovative text-driven word boundary decision (WBD) segmentation model proposed in [15]. The WBD model treats word segmentation simply and efficiently as a binary decision on whether to realize the natural textual break between two adjacent characters as a word boundary. The WBD model allows simple and quick training data preparation converting characters as contextual vectors for learning the word boundary decision. Machine learning experiments with four different classifiers show that training with 1,000 vectors and 1 million vectors achieve comparable and reliable results. In addition, when applied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall rates"
O08-1009,W06-0127,0,0.0459218,"Missing"
O08-1009,I08-4017,0,0.0263458,"Missing"
O08-1009,J96-3004,0,\N,Missing
O08-2004,J06-1005,0,0.0217952,"ing of lexical items. This paper is organized as follows: In the next section we briefly outline the main research on the automatic discovery of lexical semantic relations, which motivates the present study. Then we discuss the concept of troponymy between verbs. Section 3 introduces our proposal and experiments. Section 4 shows the results and discussion of this method; Section 5 concludes this paper with future directions. 2 Literature Review There has been a variety of studies on the automatic acquisition of lexical semantic relations, such as hypernymy/hyponymy [6], antonymy [7], meronymy [5] and so on. In Section 2.1 we will review Hearst’s approach, which most of the works on automatic labeling of word sense relations are based upon. To the best of our knowledge, there is no study targeting at troponymy extraction yet, so in Section 2.2, we first define what troponymy is, the complexity of troponymy, and discuss how we can infer troponymy motivated by Hearst’s approach. 2.1 Syntactic patterns and semantic relation The structure of a lexical entry in a dictionary reflects the relatedness of words and concepts; also, certain structures or syntactic patterns usually define the sema"
O08-2004,C92-2082,0,0.103402,"nguage can indicate the meaning of lexical items. This paper is organized as follows: In the next section we briefly outline the main research on the automatic discovery of lexical semantic relations, which motivates the present study. Then we discuss the concept of troponymy between verbs. Section 3 introduces our proposal and experiments. Section 4 shows the results and discussion of this method; Section 5 concludes this paper with future directions. 2 Literature Review There has been a variety of studies on the automatic acquisition of lexical semantic relations, such as hypernymy/hyponymy [6], antonymy [7], meronymy [5] and so on. In Section 2.1 we will review Hearst’s approach, which most of the works on automatic labeling of word sense relations are based upon. To the best of our knowledge, there is no study targeting at troponymy extraction yet, so in Section 2.2, we first define what troponymy is, the complexity of troponymy, and discuss how we can infer troponymy motivated by Hearst’s approach. 2.1 Syntactic patterns and semantic relation The structure of a lexical entry in a dictionary reflects the relatedness of words and concepts; also, certain structures or syntactic patt"
O09-3003,N07-1058,0,0.0368412,"Missing"
O10-1012,P02-1053,0,0.0166498,"Missing"
O11-1012,C02-1114,0,0.0598472,"Missing"
O11-1012,cimiano-etal-2004-clustering,0,0.0644409,"Missing"
O11-1012,P98-2127,0,0.306553,"Missing"
O11-1012,O09-5004,0,0.0691695,"Missing"
O11-1012,C98-2122,0,\N,Missing
O12-3003,O09-5004,0,0.0603184,"Missing"
O12-3003,cimiano-etal-2004-clustering,0,0.0194625,"Missing"
O12-3003,P98-2127,0,0.239836,"Missing"
O12-3003,C02-1114,0,0.0770974,"Missing"
O13-1024,H05-1044,0,0.046164,"Missing"
O13-1024,W10-0206,0,\N,Missing
O13-1024,D08-1027,0,\N,Missing
O13-1024,D09-1030,0,\N,Missing
O13-1024,W10-0204,0,\N,Missing
O13-1024,O10-1012,1,\N,Missing
O13-1024,C10-1021,0,\N,Missing
O13-1024,lee-etal-2010-emotion,0,\N,Missing
O13-1025,C02-1049,0,\N,Missing
O13-3004,zhang-etal-2004-distributional,0,0.088731,"Missing"
O14-1014,ambati-etal-2012-word,0,0.0504276,"Missing"
O14-1014,P98-1013,0,0.170723,"事。 The SD has been widely used in NLP-related fields such as sentiment analysis (Meena & Prabhakar, 2007), textual entailment (Androutsopoulos & Malakasiotis, 2010). The Chinese version of SD (Chang et al., 2009) is also available on the Stanford Dependencies page6. The SD can even distinguish 45 typed dependencies among Chinese words, as shown in Table 1. 6 http://nlp.stanford.edu/software/stanford-dependencies.shtml#Chinese 143 Table 1. Chinese dependency relations (Chang et al., 2009) On the other hand, most semantic resources like PropBank (Palmer, Gildea, & Kingsbury, 2005) and FrameNet (Baker, Fillmore, & Lowe, 1998) either provide coarse-grained information or with limited coverage. In this paper, we propose a lexical resource tool to describe more detailed information for all words in a text corpus. We choose Sinica Corpus (Chen, Huang, Chang, & Hsu, 1996) as our texts and evaluate the results with Chinese Sketch Engine in terms of corresponding thesaurus function. 144 3. Method In this case study, untagged texts of 567,702 sentences from Sinica Corpus 3.0 7 were parsed with dependency relations by the Stanford Parser (Chang et al., 2009). We obtained 574,552 dependency relations (of 23 types) between"
O14-1014,W09-2307,0,0.119439,"bc.*""|tag=""Ncd.*""] However, the writing of grammar is time-consuming, running risk of ‘low recall’, so we turn to exploit the dependency parser for enriching the relational information. Unlike phrase-structure grammar, dependency grammar concentrates on the typed dependency between words, rather than constituent information. It is highly advantageous to our study, for it is linguistically-rich capturing not only syntactic information such as nsubj (nominal subject) but also abstract semantic ones such as loc (localizer) - and can be further applied to other syntactic-semantic interface tasks (Chang, Tseng, Jurafsky, & Manning, 2009). The Stanford lexicalized probabilistic parser (Levy & Manning, 2003) works out the grammatical structure of sentences with a factored product model efficiently combing preferences of PCFG phrase structure and lexical dependency experts. In addition to phrase structure tree, the parser also provides Stanford Dependencies (SD) 5 that are known as grammatical relations between words in a sentence. Take the following Chinese sentence for example: 我 很 喜歡 兩 則 惜福 與 惜緣 的 故事。 The head 喜歡 has a dependent of 我 as its nominal subject, and another dependent of 故事 as direct object (Fig. 2). 4 5 http://wo"
O14-1014,O13-3003,0,0.0418486,"Missing"
O14-1014,Y96-1018,0,0.0656283,"rd Dependencies page6. The SD can even distinguish 45 typed dependencies among Chinese words, as shown in Table 1. 6 http://nlp.stanford.edu/software/stanford-dependencies.shtml#Chinese 143 Table 1. Chinese dependency relations (Chang et al., 2009) On the other hand, most semantic resources like PropBank (Palmer, Gildea, & Kingsbury, 2005) and FrameNet (Baker, Fillmore, & Lowe, 1998) either provide coarse-grained information or with limited coverage. In this paper, we propose a lexical resource tool to describe more detailed information for all words in a text corpus. We choose Sinica Corpus (Chen, Huang, Chang, & Hsu, 1996) as our texts and evaluate the results with Chinese Sketch Engine in terms of corresponding thesaurus function. 144 3. Method In this case study, untagged texts of 567,702 sentences from Sinica Corpus 3.0 7 were parsed with dependency relations by the Stanford Parser (Chang et al., 2009). We obtained 574,552 dependency relations (of 23 types) between 44,257 words. To sketch a word, we make use of the dependency tuples from the parsed corpus (see the right panel of Fig. 2) to extract the relations of each word with its dependents, and obtain the sketch of words such as 打 “hit” shown below: Tab"
O14-1014,Y06-1024,0,0.0262869,"tudies of syntax and semantics. With the rapid development of corpora, various corpus query, profiling and visualization tools have emerged quickly over the past years. Among these tools, Word Sketch Engine (Kilgarriff et al., 2004; Huang et al., 2005) has provided an effective approach to quantitatively summarize grammatical and collocation behavior 1 . The provided functions include Concordancer, Word Sketch, Sketch Diff, Thesaurus, and other web corpus crawling and processing tools. Previous literatures have revealed that corpus linguistics has benefited greatly from Chinese Sketch Engine (Hong & Huang, 2006). Although proprietary, Word Sketch Engine system is popular among corpus linguists and language teachers because of its functions in language analysis. However, the construction of Sketch Engine is time-consuming due to the  Graduate Institute of Linguistics, National Taiwan University, Taipei, Taiwan E-mail: {simon.xian, shukai}@gmail.com 1http://www.sketchengine.co.uk 139 manually edited sketch grammar. Here we propose an alternative approach to sketch the grammar profile of words automatically from a text corpus. The paper is organized as follows: Section 2 reviews the current design of r"
O14-1014,I05-3007,0,0.200449,"based on dependency parses from untagged texts. The advantage of word sketch based on parsed corpora is, compared to Sketch Engine (Kilgarriff, Rychly, Smrz, & Tugwell, 2004), to provide more details about the different usage of each word such as various types of modification, which is also important in language pedagogy. Although some language resources of other languages have attempted to sketch words based on parsed data, in Chinese we have not seen a resource for dependency sketch of words in customized texts. Therefore, we propose such a resource and evaluate with Chinese Sketch Engine (Huang et al., 2005) in terms of corresponding thesaurus function. Keywords: Dependency grammar, Grammatical relation, NLP tools/resources. 1. Introduction Syntagmatic relational information has been the focus of the interface studies of syntax and semantics. With the rapid development of corpora, various corpus query, profiling and visualization tools have emerged quickly over the past years. Among these tools, Word Sketch Engine (Kilgarriff et al., 2004; Huang et al., 2005) has provided an effective approach to quantitatively summarize grammatical and collocation behavior 1 . The provided functions include Conc"
O14-1014,P03-1056,0,0.310458,"w recall’, so we turn to exploit the dependency parser for enriching the relational information. Unlike phrase-structure grammar, dependency grammar concentrates on the typed dependency between words, rather than constituent information. It is highly advantageous to our study, for it is linguistically-rich capturing not only syntactic information such as nsubj (nominal subject) but also abstract semantic ones such as loc (localizer) - and can be further applied to other syntactic-semantic interface tasks (Chang, Tseng, Jurafsky, & Manning, 2009). The Stanford lexicalized probabilistic parser (Levy & Manning, 2003) works out the grammatical structure of sentences with a factored product model efficiently combing preferences of PCFG phrase structure and lexical dependency experts. In addition to phrase structure tree, the parser also provides Stanford Dependencies (SD) 5 that are known as grammatical relations between words in a sentence. Take the following Chinese sentence for example: 我 很 喜歡 兩 則 惜福 與 惜緣 的 故事。 The head 喜歡 has a dependent of 我 as its nominal subject, and another dependent of 故事 as direct object (Fig. 2). 4 5 http://wordsketch.ling.sinica.edu.tw http://nlp.stanford.edu/software/stanford-d"
O14-1014,I11-1079,0,\N,Missing
O14-1014,P13-4006,0,\N,Missing
O14-1014,C98-1013,0,\N,Missing
O14-1014,J05-1004,0,\N,Missing
O14-5002,W11-3702,0,0.106925,"Missing"
O16-1009,E14-2025,0,0.152134,"luating’: contributors assign words in context with different senses, and ’GWAP’: contributors annotate word senses through playing games in system A and contribute the game-result to system B. As an open platform for linguistic annotation, the CS system usually recruits contributors without having the ability to preview their profiles. This leads to five primary issues: the recruitment and retention of contributors, what can contributors do, how to organize the contributions, how to evaluate (Doan, Anhai, et al 2011) [4] as well as the infrastructure of system (Bontcheva, Kalina, et al 2014) [7]. Crowdsource workers can be recruited by several ways: providing payments; volunteering; by requiring; ask users to pay for the usage of system A service, then contribute to system B(crowdsourcing), such as Captcha. As to the retention of contributors, the encouragement and retention scheme (E&R scheme) provides well-structured solutions. Systems can automatically provide instant user-gratification, display how their contributions make differences immediately. 87 Providing ownership is another way making users feel they own a part of the system. Previous study (Hong and Baker 2011) [8] of WSD"
O16-1009,W11-0404,0,0.192548,"2014) [7]. Crowdsource workers can be recruited by several ways: providing payments; volunteering; by requiring; ask users to pay for the usage of system A service, then contribute to system B(crowdsourcing), such as Captcha. As to the retention of contributors, the encouragement and retention scheme (E&R scheme) provides well-structured solutions. Systems can automatically provide instant user-gratification, display how their contributions make differences immediately. 87 Providing ownership is another way making users feel they own a part of the system. Previous study (Hong and Baker 2011) [8] of WSD using crowdsource approach, aggregating the inputs from contributors with majority vote. Another fact that greatly affect the results is the contributor quality, thus leads to the necessity of evaluation.The target of contributor evaluation is to prevent malicious cheating, for such problem, four solutions had been introduced by Doan in 2011. In order to manage contributors, system owner can block malicious contributors by limiting the level of contributions for individuals. We may also detect bad-intention contributions by using both manual(direct monitor) and automatic techniques(ran"
O16-1009,W13-0215,0,0.103043,"s are unable to perform” (Von Ahn, L., & Dabbish, L., 2008) [10]. In other words, the game developer channeled the player to work under the disguise of entertainment. The ESP Game (Google Image Labeler) is the first major success of combining game with computation task, which successfully labeled 50,000,000 images with related word. GWAP further developed in NLP field for anaphora analysis (Chamberlain et al., 2008) [11], term relations (Artignan et al., 2009) [12], semantic annotation for word sense disambiguation, known as the Wordrobe (Venhuizen, N., Basile, V., Evang, K., & Bos, J., 2013) [13], the Knowledge Towers (Vannella et al., 2014) [14], and Puzzle Racer (Jurgens, D., & Navigli, R., 2014) [15]. 90 The key of a successful game is that people are willing to spend long-enough time to play, because they are ‘enjoyed’ and ‘entertained.’ And to disguise a puzzle to a game needs a well-structured design that inspires appropriate output with an enticing winning conditions and plain dos-don’ts (Von Ahn, L., & Dabbish, L., 2008) [10]. Aiming to make GWAP a universalized approach, Luis Von Ahn and Laura Dabbish addressed three templates to solve diverse computation tasks: Output-agreem"
O16-1009,P14-1122,0,0.0855219,"., 2008) [10]. In other words, the game developer channeled the player to work under the disguise of entertainment. The ESP Game (Google Image Labeler) is the first major success of combining game with computation task, which successfully labeled 50,000,000 images with related word. GWAP further developed in NLP field for anaphora analysis (Chamberlain et al., 2008) [11], term relations (Artignan et al., 2009) [12], semantic annotation for word sense disambiguation, known as the Wordrobe (Venhuizen, N., Basile, V., Evang, K., & Bos, J., 2013) [13], the Knowledge Towers (Vannella et al., 2014) [14], and Puzzle Racer (Jurgens, D., & Navigli, R., 2014) [15]. 90 The key of a successful game is that people are willing to spend long-enough time to play, because they are ‘enjoyed’ and ‘entertained.’ And to disguise a puzzle to a game needs a well-structured design that inspires appropriate output with an enticing winning conditions and plain dos-don’ts (Von Ahn, L., & Dabbish, L., 2008) [10]. Aiming to make GWAP a universalized approach, Luis Von Ahn and Laura Dabbish addressed three templates to solve diverse computation tasks: Output-agreement games, Inversionproblem games, and Input-agreem"
O16-1009,Q14-1035,0,0.069705,"d the player to work under the disguise of entertainment. The ESP Game (Google Image Labeler) is the first major success of combining game with computation task, which successfully labeled 50,000,000 images with related word. GWAP further developed in NLP field for anaphora analysis (Chamberlain et al., 2008) [11], term relations (Artignan et al., 2009) [12], semantic annotation for word sense disambiguation, known as the Wordrobe (Venhuizen, N., Basile, V., Evang, K., & Bos, J., 2013) [13], the Knowledge Towers (Vannella et al., 2014) [14], and Puzzle Racer (Jurgens, D., & Navigli, R., 2014) [15]. 90 The key of a successful game is that people are willing to spend long-enough time to play, because they are ‘enjoyed’ and ‘entertained.’ And to disguise a puzzle to a game needs a well-structured design that inspires appropriate output with an enticing winning conditions and plain dos-don’ts (Von Ahn, L., & Dabbish, L., 2008) [10]. Aiming to make GWAP a universalized approach, Luis Von Ahn and Laura Dabbish addressed three templates to solve diverse computation tasks: Output-agreement games, Inversionproblem games, and Input-agreement games. And this paper is based on the outputagreement"
O16-1027,maynard-greenwood-2014-cares,0,0.139087,"Missing"
O16-1027,P11-2102,0,0.230804,"Missing"
O16-1027,W13-1605,0,0.0821314,"Missing"
O16-1027,D14-1181,0,0.00843857,"Missing"
O16-1027,filatova-2012-irony,0,0.198059,"Missing"
O16-1027,C14-1120,0,0.564058,"Missing"
O16-1027,D13-1066,0,0.0322445,"Missing"
O17-1007,J86-2003,0,0.258692,"s, there is no doubt language can be classified via types of sexuality. People who have opposite- or same-sex desires will have different language behaviors. Since the present study discusses texting strategies of homosexual population, this section reviews previous works on how homosexual males and females produce language differently. 2.1 Homosexual Male Language Compared to lesbian language, linguistic behavior of gay males has been studied extensively. 70 It has been claimed that gay people tend to use specialized lexicon, or argot, containing words not normally used in mainstream society [5][6][7]. However, not only argot but also gay language is in general characterized by the use of innuendo, categorizations, and strategic evasions such as omitting or changing gendered pronouns [4]. In the past, the word &apos;gay&apos; was (and still) associated with negative thoughts, which is believed to be the main reason gay men shifted toward a more heterosexual masculine image [8] with their needs to distinct themselves from appearing obviously gay [9]. The appearance of masculine items [9] or the replacement of masculine pronouns with feminine pronouns [10] in gay men&apos;s language is considered str"
O17-1007,D11-1120,0,0.158069,"an language (flat intonation, cursing) [4]. In other words, the mix of linguistic styles is the main reason why lesbian-specific language is less prominent than gay language. 3. Exploration of Gendered Features In order to prove that previous studies on GenderNLP ignored homosexual language and language behavior should be categorized not only based on sex but also on sexuality, the present paper takes both heterosexual and homosexual linguistic features into consideration in the forthcoming tests. Since most of the studies on GenderNLP use both SVM and NB models to predict author&apos;s gender [13][14][15][16], this study will also adopt the same models under the 5-fold cross-validation test in predicting homosexual texts from Chinese social media. 71 This paper uses and translates all the gender-norm linguistic features from English to Chinese based on Huang, Li and Lin&apos;s study which detected author&apos;s gender with a number of linguistic cues [16]. However, features such as articles, capitalization, long/short words, abbreviation etc. which are absent in Chinese and statistical measures which do not fit our data are omitted. Also, Chinese-specific enumeration comma (、) is further added in th"
P06-2050,I05-7002,1,0.891992,"Missing"
P07-2018,P04-1059,0,0.0824876,"Missing"
P07-2018,O99-2001,0,0.0950061,"Missing"
P07-2018,W03-1719,0,0.081678,"Missing"
P07-2018,O03-4002,0,\N,Missing
P07-2018,C92-1019,0,\N,Missing
P07-2018,W03-1726,0,\N,Missing
P07-2039,ma-huang-2006-uniform,1,0.831199,"are those between PRC and Taiwan Mandarin, we will start with known contrasting pairs of these two language variants and mine potential variant pairs from their collocates. These potential variant pairs are then checked for their phonological similarity to determine whether they are true variants or not. In order to effectively select collocates from specific grammatical constructions, the Chinese Word Sketch3 is adopted. In particular, we use the Word Sketch dif2 To facilitate processing, the complete CGC was segmented and POS tagged using the Academia Sinica segmentation and tagging system (Ma and Huang, 2006). 3 http://wordsketch.ling.sinica.edu.tw 154 ference (WSDiff) function to pick the grammatical contexts as well as contrasting pairs. It is important to bear in mind that Chinese texts are composed of Chinese characters, hence it is impossible to compare a transliterated NE with the alphabetical form in its original language. The following characteristics of a transliterated NE’s in CGC are exploited to allow discovery of transliteration variations without referring to original NE. • frequent co-occurrence of named entities within certain syntagmatic relations – named entities frequently co-oc"
P07-2039,W03-1508,0,0.0496861,"Missing"
P07-2039,P98-2220,0,0.0377422,"teration concentrate on the identification of either the transliterated term or the original term, given knowledge of the other (e.g. (Virga and Khudanpur, 1 For instance, we found at least 14 transliteration variants for Lewinsky,such as 呂茵斯基，呂文絲基，呂茵斯，陸文斯基，陸茵斯 基， 柳思基，陸雯絲姬，陸文斯基，呂茵斯基，露文斯基，李文斯基，露溫 斯基，蘿恩斯 基，李雯斯基 and so on. 153 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 153–156, c Prague, June 2007. 2007 Association for Computational Linguistics 2003)). These studies are typically either rule-based or statistics-based, and specific to a language pair with a fixed direction (e.g. (Wan and Verspoor, 1998; Jiang et al., 2007)). To the best of our knowledge, ours is the first attempt to discover transliterated NE’s without assuming prior knowledge of the entities. In particular, we propose that transliteration variants can be discovered by extracting and comparing terms from similar linguistic context based on CGC and WSE tools. This proposal has great potential of increasing robustness of future NER work by enabling discovery of new and unknown transliterated NE’s. Our study shows that resolution of transliterated NE variations can be fully automated. This will have strong and positive implica"
P07-2039,C98-2215,0,\N,Missing
S10-1013,J07-4005,0,0.597125,"0.505 ±0.026 0.350 R verbs 0.450 ±0.034 0.454 ±0.034 0.291 ±0.025 0.403 ±0.033 0.293 R 0.529 ±0.021 0.521 ±0.018 0.496 ±0.019 0.462 ±0.020 0.294 R nouns 0.530 ±0.024 0.522 ±0.023 0.507 ±0.020 0.472 ±0.024 0.308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Re"
S10-1013,W04-0807,0,0.0284195,"omain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The"
S10-1013,E09-1005,1,0.474766,".308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Rel. Cliques 0.3 0.35 0.4 0.45 0.5 0.55 Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond to one system (denoted in axis Y) according each recall and confidence"
S10-1013,S07-1016,0,0.0570391,"English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to the participants included"
S10-1013,S07-1097,0,0.0246659,"wledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most Frequent Sense from SemCor; 2) Conceptual Density ; 3) Supervised Domain Relative Entropy classifier based on WordNet Domains; 4) Supervised Bayesian classifier based on WordNet Domains probabilities; and 5) Unsupervised Knownet-20 classifiers. The best run ranked 24th. UMCC-DLSI (Relevant): The team submitted three different runs using a knowledge-based system. The first two runs use domain vectors and the third is based on cliques, which measure how much a concept is correlated to the sentence by obtaining Relevant S"
S10-1013,W08-2114,0,0.0667055,"e was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most"
S10-1013,W04-0811,0,0.196041,"anguages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to th"
S10-1013,E09-1045,0,0.0508574,"Missing"
S10-1013,vossen-etal-2008-kyoto,1,0.763195,"ses in specific domains, the context of the senses might change, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. The main goal of this task is to provide a multilingual testbed to evaluate WSD systems when faced with full-texts from a specific domain. All datasets and related information are publicly available from the task websites1 . This task was designed in the context of Kyoto (Vossen et al., 2008)2 , an Asian-European project that develops a community platform for modeling knowledge and finding facts across languages and cultures. The platform operates as a Wiki system with an ontological support that social communities can use to agree on the meaning of terms in specific domains of their interest. Kyoto focuses on the environmental domain because it poses interesting challenges for information sharing, but the techniques and platforms are Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervis"
S10-1013,S01-1004,0,0.00916615,"the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dut"
S10-1013,H05-1053,0,0.486241,". Note that this method of estimating statistical significance might be more strict than other pairwise methods. We also include the results of two baselines. The random baseline was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods"
S10-1013,N09-1004,0,\N,Missing
S10-1013,W00-0901,0,\N,Missing
S10-1093,E09-1005,1,0.892281,"sources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future. 1 2 We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t"
S10-1093,S10-1013,1,0.884699,"Missing"
S10-1093,bosma-vossen-2010-bootstrapping,1,0.72995,"et 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3 . Cornetto comprises taxonomic relations and equivalence rela2 #rels. Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2 . Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus w"
S10-1093,P98-2127,0,0.0122903,"y et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run2: UKB using related words Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate eac"
S10-1093,J07-4005,0,0.0212144,"ea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, . . .”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Fin"
S10-1093,C98-2122,0,\N,Missing
tokunaga-etal-2008-adapting,bel-etal-2000-simple,1,\N,Missing
tokunaga-etal-2008-adapting,P06-2106,1,\N,Missing
tokunaga-etal-2008-adapting,I08-1052,1,\N,Missing
tokunaga-etal-2008-adapting,francopoulo-etal-2006-lexical,1,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
W09-3303,baroni-etal-2008-cleaneval,0,0.0117928,"stencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources constitute a crucial prerequisite for various NLP tasks. However their building cost keeps them rare. In"
W09-3303,zesch-etal-2008-extracting,0,0.0625772,"more languages only makes this observation more acute. In spite of various initiatives, costs make resource development extremely slow or/and result in non freely accessible resources. Collaborative resources might bring an attractive solution 19 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 19–27, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP cal resources. We describe below the Wiktionary resource and we explain the restrictions and problems we are facing when trying to exploit it. This description may complete few earlier ones, for example Zesch et al. (2008a). 2.1 2.2.2 Layouts In the following paragraph, we outline wiktionary’s general structure. We only consider words in the wiktionary’s own language. An entry consists of a graphical form and a corresponding article that is divided into the following, possibly embedded, sections: • etymology sections separate homonyms when relevant; • among an etymology section, different parts of speech may occur; • definitions and examples belong to a part of speech section and may be subdivided into subsenses; • translations, synonyms/antonyms and hypernyms/hyponyms are linked to a given part of speech, wit"
W09-3303,W08-1912,1,0.934088,"er, V is 4 http://it.wiktionary.org/w/index.php? title=cardinale&oldid=758205 5 http://en.wiktionary.org/wiki/WT:ELE 6 http://meta.wikimedia.org/wiki/List_ of_Wiktionaries 21 a set of words and E is defined by a relation R R E 7−→ E : (w1 , w2 ) ∈ E if and only if w1 → w2 . Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks. Comparing structural characteristics of wiktionary-based lexical networks to some standard resource should be done according to wellknown properties of SW networks (Watts and Strogatz, 1998; Barabasi et al., 2000; Newman, 2003; Gaume et al., 2008). These properties are: • Edge sparsity: SW are sparse in edges m = O(n) or m = O(n log(n)) • Short paths: in SW, the average path length (L)7 is short. Generally there is at least one short path between any two nodes. • High clustering: in SW, the clustering coefficient (C) that expresses the probability that two distinct nodes adjacent to a given third one are adjacent, is an order of magnitude higher than for Erdos-Renyi (random) graphs: CSW  Crandom ; this indicates that the graph is locally dense, although it is globally sparse. • Heavy-tailed degree distribution: the distribution of the"
W09-3303,W02-1030,0,0.0283747,"ample, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources co"
W09-3303,J03-3001,0,0.0277846,"of lexical resources, be it due to the low-quality or non-existence of such resources, or to copyrightsrelated problems. As an example, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Gradua"
W09-3418,W06-1001,0,0.116172,"ies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework. Section 3 describes how we represent Chinese WordNet in the Lexical Markup Framework (CWN-LMF). Section 4 presents an example on Chinese word sense distinction using CWN-LMF format. Quantitative analysis of compiled CWN-LMF is presented in Section 5. We also describe the application scenario using CWN-LMF for information interoperab"
W09-3418,magnini-cavaglia-2000-integrating,0,0.458758,"integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 in"
W09-3418,huang-etal-2004-sinica,1,0.944371,"MF used to represent lexical semantics in Chinese WordNet. The compiled CWN-LMF will be released to the community for linguistic researches. 1 Introduction Princeton WordNet 1 is an English lexical database that groups nouns, verbs, adjectives and adverbs into sets of cognitive synonyms, which are named as synsets (Fellbaum, 1998; Miller, 1995). The Global WordNet Association (GWA) 2 built on the results of Princeton WordNet and Euro WordNet (Vossen, 2004) is a free and public association that provides a platform to share and connect all languages in the world. For Mandarin Chinese in Taiwan, Huang et al. (2004) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) which integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the"
W09-3418,I05-3014,1,0.746685,"analyzed according to the guidelines of Chinese word sense distinctions (CKIP, 2003; Huang et al. 2003) which contain information including Partof-Speech, sense definition, example sentences, corresponding English synset(s) from Princeton WordNet, lexical semantic relations and so on. Unlike Princeton WordNet, CWN has not been constructed mainly on the synsets and semantic relations. Rather it focuses to provide precise expression for the Chinese sense division and the semantic relations needs to be based on the linguistic theories, especially lexical semantics (Huang et al., 2008). Moreover, Huang et al. (2005) designed and implemented the Sinica Sense Management System (SSMS) to store and manage word sense data generated in the analysis stage. SSMS is meaning-driven. Each sense of a lemma is identified specifically using a unique identifier and given a separate entry. There are 8,646 lemmas / 25,961 senses until December 2008 have been analyzed and stored in SSMS. Figure 1 shows the result of sense distinction for 足跡 zu-ji ‘footprint’ as an example in Chinese WordNet. Huang et al. (2004) proposed Domain LexicoTaxonomy (DLT) as a domain taxonomy populated with lexical entries. By using DLT with Chin"
W09-3418,strapparava-valitutti-2004-wordnet,0,0.058237,"n Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework"
W09-3418,P06-2106,1,0.884198,"Missing"
W09-3418,vossen-etal-2008-kyoto,1,0.832458,"guages. A unified framework is needed for information exchange. LMF is hence adopted as the framework at lexical semantic level in this project. The WordNet in these languages are compiled with designed WordNet-LMF format. CWN-LMF will also be involved and benefit for cross-language interpretabilities in semantic search field. 7 6 Application Scenarios The EU-7 project, KYOTO (Knowledge Yielding Ontologies for Transition-based Organization), wants to make knowledge sharable between communities of people, culture, language and computers, by assigning meaning to text and giving text to meaning (Vossen et al., 2008a; 2008b). The goal of KYOTO is a system that allows people in communities to define the meaning of their words and terms in a shared Wiki platform so that it becomes anchored across languages and cultures but also so that a computer can use this knowledge to detect knowledge and facts in text. KYOTO is a generic system offering knowledge transition and information across different target groups, transgressing linguistic, cultural and geographic boundaries. Initially developed for the environmental domain, KYOTO will be usable in any knowledge domain for mining, organizing, and distributing in"
W09-3418,O05-5001,1,\N,Missing
W09-3421,francopoulo-etal-2006-lexical,1,0.857972,"Missing"
W09-3421,bel-etal-2000-simple,1,0.763962,"the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian langua"
W09-3421,W03-1905,1,0.826021,"pproaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian languages, we elucidated the shortco"
W09-3421,P06-2106,1,0.822025,"Missing"
W09-3421,tokunaga-etal-2008-adapting,1,0.730656,"Missing"
W09-3421,W06-1001,1,\N,Missing
W13-5415,J91-4003,0,0.287035,"s, carrots and so on) from artifacts (e.g. cookies, cakes, bread). In addition, the agentive role of a lexical item would be represented as an event predicate while the lexical item is a noun. For example, “potato” and “cake” could all be event predicates in “bake a potato” and “bake a cake”; however, the verb “bake” is polysemous with two meanings: a sense of change of state and a sense of creation, as stated in Atkins et al. (1988). Since this kind of logical polysemy occurs in many cases, a relation of co-composition is introduced by Pustejovsky (1995) (originally named as cospecification (Pustejovsky, 1991)) to capture the words’ meanings. Under the notion of co-composition, the verb “bake” itself is not polysemous but the complement that follows derives other meanings can be re-examined, not only through the agentive quale, but also constitutive role. From the example (51) provided by Pustejovsky (1995), it is further discovered that though a complement makes reference to an agentive quale, the constitutive quale plays an important role to the baking act. That is, if the material in a constitutive quale of a complement is an individual as a default argument, the derived sense from agentive role"
W14-0139,C92-2082,0,0.543753,"with other approaches, which will hold promise for the robust lexical relations acquisition. 1 Introduction Semantic relations are at the core of WordNetalike architecture, and constitute the essential and integral part of linguistic and conceptual knowledge formalization. However, the manual labeling task of semantic relations is very laborious. To minimize the labor, in recent years, automatic ways of extracting semantic relations from textual data have been proposed. Among these methods, extensive works have been done based on the so-called pattern-based approaches, which was pioneered by (Hearst, 1992). The patterns predeﬁned or plucked out of a corpus are often referred to as lexicosyntactic patterns, which serve as an information marker for a certain relation between two concepts. Later representative works using such approaches include (Cimiano et al., 2005), and (Pantel and Pennacchiotti, 2006), etc. Pattern-based extraction has shown quite reasonable success characterized by a (relatively) high precision rate, but suﬀers from a very low recall resulting from the fact that the patterns are rare in corpora. Remedies against the problem involve exploiting scaled Yu-Yun Chang Graduate Inst"
W14-0139,O08-2004,1,0.80989,"els are similar. The bilingual bootstrapping experiments showed that lexical relations turn out to be not subject to automatic importing and would still require tremendous human eﬀorts of validations. 2.2 Pattern-based Approach There has been a variety of studies on the automatic acquisition of lexical semantic relations, Hearst (Hearst, 1992) ﬁrst proposed a lexico-syntactic pattern based method for automatic acquisition of hyponymy from unrestricted texts, and since then automatically ﬁnding semantic relations by using various pattern-based algorithm has become the most common approach. We (Lo et al., 2008) have tried to deﬁne some patterns (e.g., a manner of ) to extract troponymy among verbs in Chinese. To avoid the interference of unnecessary contextual information which may include modal verbs, hedging, negation that often occur in diﬀerent corpus genres, we applied the proposed patterns on the gloss of CWN. The results were evaluated with the substitution tests. Substitution test is commonly used in linguistic literature (Tsai et al., 2002); EuroWordnet provided linguistic tests for each semantic relation to examine the validity. In (Tsai et al., 2002), sentence formulae were created follow"
W14-0139,W07-1710,0,0.0657391,"Missing"
W14-0139,P06-1015,0,0.0557154,"he manual labeling task of semantic relations is very laborious. To minimize the labor, in recent years, automatic ways of extracting semantic relations from textual data have been proposed. Among these methods, extensive works have been done based on the so-called pattern-based approaches, which was pioneered by (Hearst, 1992). The patterns predeﬁned or plucked out of a corpus are often referred to as lexicosyntactic patterns, which serve as an information marker for a certain relation between two concepts. Later representative works using such approaches include (Cimiano et al., 2005), and (Pantel and Pennacchiotti, 2006), etc. Pattern-based extraction has shown quite reasonable success characterized by a (relatively) high precision rate, but suﬀers from a very low recall resulting from the fact that the patterns are rare in corpora. Remedies against the problem involve exploiting scaled Yu-Yun Chang Graduate Institute of Linguistics National Taiwan University Taipei, Taiwan yuyun.unita@gmail.com data from the web (Cimiano et al., 2005), but runs the risk of inﬂuenced by the web genre (Alain, 2010). To enrich the relations coverage in Chinese Wordnet (CWN), in this paper, we propose an in situ approach by expl"
W15-4209,O98-3003,0,0.136539,"nets CWN provides knowledge about lexicalized concepts, including their representing lexical item’s part-ofspeech, definition, and a set of other lexicalized concepts with which they form a synset. To date, CWN contains more than 28,000 word-sense pairs that are organized in some 20,000 synsets. In addition to the synonymy implicitly present in synsets, CWN includes other lexical-semantic relations to connect the lexicalized concepts, meronomy and hypernymy-hyponymy in particular. What distinguishes CWN from its counterparts for other languages are primarily the distinction of meaning facets (Ahrens et al., 1998; Hsieh, 2011) Although the rationale underlying synsets remains disputable (Maziarz et al., 2013), the practical value of wordnet as lexical resource is undeniable, particularly that of the first and foremost of its kind, Princeton WordNet (PWN) (Fellbaum, 1998). According to a search run by Morato et al. (Morato et al., 2004) on some major bibliographic databases like LISA, INSPEC and IEEE, the decade between 1994 and 2003 saw a wide range of wordnet applications, including conceptual disambiguation, information retrieval, query expansion and machine translation, among others. At present, mo"
W15-4209,P13-1133,0,0.022798,"representation. Converting CWN into Linked Data with lemon To improve its interoperability with other lexical resources, CWN is converted in RDF format using the lemon model. The following subsections provide a general introduction to lemon and Linked Data, followed by a discussion of the idiosyncrasies of Mandarin (as reflected in CWN) to be considered for a thorough conversion to a linked, lemonized version of CWN. 3.1 While lemon has proven adequate for modeling well-documented languages as those found in major lexical resources like PWN (McCrae et al., 2014) and Open Multilingual Wordnet (Bond and Foster, 2013), it remains to be seen whether the model is comprehensive enough for describing less privileged languages too. For instance, it is claimed that “the morphology module of lemon may serve less for Bantu languages lexica” (Chavula and Keet, 2014). In our case, while lemon suffices for modeling most of the structures in Chinese Wordnet at the lexical level, it does not allow for the representation of meaning facets. Consider the different uses of the lemma shu1 “book” in the following sentences adapted from Bond et al. (2014): The lemon Model and CWN lemon (McCrae et al., 2011) is an ontologylexi"
W15-4209,huang-etal-2004-sinica,0,0.0450331,"or meaning facets, a linguistic feature also attended to in Chinese Wordnet. As for the representation of synsets, we use the WordNet RDF ontology for integration’s sake. Also, we use another ontology proposed by the Global WordNet Association to show how Chinese Wordnet as Linked Data can be integrated into the Global WordNet Grid. 1 2 Introduction Chinese Wordnet Chinese Wordnet (CWN) is a lexical-conceptual network for Mandarin Chinese, its contents structured along the same lines of PWN. First constructed based on translational equivalents of PWN mapped to Suggested Upper Merged Ontology (Huang et al., 2004), CWN has been reconstructed from scratch in 2014 and released with an open-source license. As with most wordnets CWN provides knowledge about lexicalized concepts, including their representing lexical item’s part-ofspeech, definition, and a set of other lexicalized concepts with which they form a synset. To date, CWN contains more than 28,000 word-sense pairs that are organized in some 20,000 synsets. In addition to the synonymy implicitly present in synsets, CWN includes other lexical-semantic relations to connect the lexicalized concepts, meronomy and hypernymy-hyponymy in particular. What"
W16-5315,N13-1090,0,0.029104,"r semantic relations. 1 Introduction Automatic discovery of semantically-related words is one of the most important NLP tasks, and has great impact on the theoretical psycholinguistic modeling of the mental lexicon. In this shared task, we employ the word embeddings model (Mikolov 2013a) to reflect paradigmatic relationships between words. Previous work has shown that word representations extracted from simple recurrent neural networks could hierarchically categorize words based on their collocational distribution (Elman 1990). Word representations also hold other regularities. More recently, Mikolov et al. (2013b) showed that word vectors could be added or subtracted to isolate certain semantic and syntactic features. The wellknown example is to take the representations for king, subtract man, and add woman. This produces a vector very near by queen. This method was tweaked by Levy and Goldberg (2014) by representing the same idea as three pairwise similarities, and is the basis for the post hoc revisions to our system. The particular semantic relations we are concerned with in this paper are synonymy, antonymy, hypernymy, and meronymy. The shared task consists of two subtasks. The first is to, given"
Y10-1093,Y10-1094,1,0.609681,"Missing"
Y10-1093,W08-1912,1,0.883313,"Missing"
Y10-1093,huang-etal-2004-sinica,0,0.264866,"stribution ∆0 = δv where δv is the certitude to be on v; 2 3 4 5 6 Dicosyn is a compilation of synonym relations extracted from seven other dictionaries (Bailly, Benac, Du Chazaud, Guizot, Lafaye, Larousse et Robert). Dicosyn has been first realized at ATILF (Analyse et Traitement Informatique de la Langue Francaise), before being corrected at CRISCO laboratory (http://elsap1.unicaen.fr/dicosyn.html) (Ploux and Victorri, 1998) Chinese Wordnet is a lexical resource modelled on Princeton WordNet, with many novel linguistic considerations for Chinese. It is proposed and launched by Huang et al. (Huang et al., 2004), at the time of writing it contains 28,815 synonyms The Tongyici Cilin (Mei et al., 1984) is a Chinese synonym dictionary known as a thesaurus in the tradition of Roget’s Thesaurus in English. It contains about 70000 lexical items under 12 broad semantic classes marked from A to L. These broad classes are further divided into 94 subclasses, and 1,428 heads. But in our experiment, classes of A-E and L are removed, for they refer to non-verbal entities like human, physical object, time and space, features, etc. n: number of vertices, m: number of edges, L: average path length, C: clustering coe"
Y10-1094,J02-2001,0,0.0368556,"Missing"
Y10-1094,huang-etal-2004-sinica,0,0.283191,"RISCO laboratory (http://elsap1.unicaen.fr/dicosyn.html). PACLIC 24 Proceedings one dictionary. Dicosyn provides three graphs: one for nouns (DicoSyn.Noun), one for verbs (DicoSyn.Verb) and one for adjectives (DicoSyn.Adjective). The resource we used to build the Mandarin synonym graph is the CilinCWN.verb, a graph of verbs extracted from CilinCWN, a fusion of the Chinese Wordnet (CWN) and the Chinese thesaurus TongYiCi CiLin (Cilin). The Chinese Wordnet is a lexical resource modelled on Princeton WordNet, with many novel linguistic considerations for Chinese. It was proposed and launched by (Huang et al., 2004), and, up to February 2010, it contained 10,533 lemmas with 30,898 senses, and 41,169 lexical semantic relations in total. Among them there are 28,815 synonyms. The Tongyici Cilin (Mei et al., 1984) is a Chinese synonym dictionary known as a thesaurus in the tradition of Roget’s Thesaurus in English. It contains about 70000 lexical items under 12 broad semantic classes marked from A to L. These broad classes are further divided into 94 subclasses, and 1,428 heads. But in our experiment, classes of A-E and L were removed, for they refer to nonverbal entities like human, physical object, time an"
Y10-1094,Y09-1036,1,0.442884,"Missing"
Y10-1094,Y10-1096,0,0.0424539,"Missing"
Y10-1094,Y10-1093,1,\N,Missing
Y12-1053,O10-1012,1,0.936802,"methods to detect the word polarity, which were Mincuts, Randomized Mincuts, and Label Propagation. Most previous papers chose to use the existed large databases for their experimental usages, and followed with different training approaches to extract or detect word polarities. Since our goal in this paper does not focus on the machine learning performance in this experimental task, we would rather demonstrate the data collection on the ﬂy, so we use a naive PMI method enriched with emoticon information to dynamically and semi-automatically detect Chinese word polarity based on Plurk API 493 (Chen et al., 2010), 1 by which all the training and testing tasks are constructed and pipelined to Google Form. 4 Pipelining Cloud and Crowd Computing in Lexicon Resource Development This session explains the proposed framework, generally speaking, the data retrieved from Plurk API and preprocessed (segmented and POS-tagged) by other Chinese NLP APIs, is sent to the collaborative platform of Google called Google Form for evaluation, once evaluated they are sent to Google Fusion Table for data exploration and visualization, and stored in Google Cloud Storage with Google BigQuery. For leaning purpose, the corpus"
Y12-1053,P97-1023,0,0.728101,"and exploratory data analysis] The collected WaC data in the cloud storage, can be processed seamlessly online (without downloading the data) for the preliminary data preprocessing (e.g., Chinese segmentation and POS tagging), and may further apply to early data introspection with online preprocessing statistical analysis and data visualization. These techniques could be accomplished by using various application programming language interfaces (e.g., APIs for R and Python). By hosting a web interface could even facilitates the scattered tasks that used to be. messages derived from the corpus (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Kanayama and Nasukawa, 2006) to assign and determine the word polarity. Notwithstanding their signiﬁcant success in achieving accuracy rate, in this paper, we will argue that current approaches to the problem might face with the methodological drawbacks due to the lack of scalability on the one hand, and indifference to the individual sentimental varieties on the other hand. First, referring to the lack of scalability, it is rather difﬁcult to handle out-of-vocabulary (OOV) issue on the lexical and corpus resources, in particular, those OOV words and phrases (or called as neolo"
Y12-1053,kamps-etal-2004-using,0,0.0446727,"Missing"
Y12-1053,W06-1642,0,0.018089,"ata in the cloud storage, can be processed seamlessly online (without downloading the data) for the preliminary data preprocessing (e.g., Chinese segmentation and POS tagging), and may further apply to early data introspection with online preprocessing statistical analysis and data visualization. These techniques could be accomplished by using various application programming language interfaces (e.g., APIs for R and Python). By hosting a web interface could even facilitates the scattered tasks that used to be. messages derived from the corpus (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Kanayama and Nasukawa, 2006) to assign and determine the word polarity. Notwithstanding their signiﬁcant success in achieving accuracy rate, in this paper, we will argue that current approaches to the problem might face with the methodological drawbacks due to the lack of scalability on the one hand, and indifference to the individual sentimental varieties on the other hand. First, referring to the lack of scalability, it is rather difﬁcult to handle out-of-vocabulary (OOV) issue on the lexical and corpus resources, in particular, those OOV words and phrases (or called as neologisms) often carried with popular usage mean"
Y12-1053,C04-1200,0,0.0374287,"arity lexicon has attracted much attention in recent years for classiﬁers to train on the lexical dataset, and is becoming important for applications such as Sentiment Analysis and Opinion Mining. For the purpose of constructing automatic identifying and classifying polarity lexicon systems, a lot of (semi-) unsupervised machine learning methods for recognizing polarities of words and phrases have been proposed. In terms of language resources, these approaches either consider the information provided from the synonyms or glosses of a thesaurus or WordNet (Hu and Liu, 2004; Kamps et al., 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2005), or based on the co-occurrence relationship Copyright 2012 by Shu-Kai Hsieh, Yu-Yun Chang, and Meng-Xian Shih 26th Pacific Asia Conference on Language,Information and Computation pages 491–497 2. [Seamless data preprocessing and exploratory data analysis] The collected WaC data in the cloud storage, can be processed seamlessly online (without downloading the data) for the preliminary data preprocessing (e.g., Chinese segmentation and POS tagging), and may further apply to early data introspection with online preprocessing statistical analysis and data visualizatio"
Y12-1053,W11-1514,0,0.0178543,"cting and detecting word polarity from the text or corpora. For collecting data, Turney and Littman (2003), and Rao and Ravichandran (2009) had taken the General Inquirer lexicon (Stone et al., 1966) as their reference data, which the word polarity list was already constructed by manually tagged and evaluated via a group of people. In addition, other papers used different methods for collecting data, such as taking the 1987 Wall Street Journal with tagged data as corpus (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000), WordNet (Rao and Ravichandran, 2009; Wiebe, 2000) and via crowd sourcing (Mohammad, 2011). As for detecting polarity, Hatzivassiloglou and McKeown (1997) introduced using conjunctions of adjectives to train the model and then labeled an orientation to each adjective through clustering. Also, Rao and Ravichandran (2009) tried using three graph-based semi-supervised learning methods to detect the word polarity, which were Mincuts, Randomized Mincuts, and Label Propagation. Most previous papers chose to use the existed large databases for their experimental usages, and followed with different training approaches to extract or detect word polarities. Since our goal in this paper does"
Y12-1053,E09-1077,0,0.0133792,"further reﬁned as well. Hatzivassiloglou and McKeown (1997) had taken the polarity of words into a system, and tried to investigate antonyms from the collected corpus and also to disambiguate the synonyms automatically. Also, Turney and Littman (2003) mentioned that an automated system containing polarity information, could be applied to text classiﬁcation, analysis of survey response, ﬁltering, tracking online opinions, and even generating chatbots. There are a lot of ways for collecting and detecting word polarity from the text or corpora. For collecting data, Turney and Littman (2003), and Rao and Ravichandran (2009) had taken the General Inquirer lexicon (Stone et al., 1966) as their reference data, which the word polarity list was already constructed by manually tagged and evaluated via a group of people. In addition, other papers used different methods for collecting data, such as taking the 1987 Wall Street Journal with tagged data as corpus (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000), WordNet (Rao and Ravichandran, 2009; Wiebe, 2000) and via crowd sourcing (Mohammad, 2011). As for detecting polarity, Hatzivassiloglou and McKeown (1997) introduced using conjunctions of adjectives to train the mo"
Y12-1053,P02-1053,0,0.0070418,"ollected WaC data in the cloud storage, can be processed seamlessly online (without downloading the data) for the preliminary data preprocessing (e.g., Chinese segmentation and POS tagging), and may further apply to early data introspection with online preprocessing statistical analysis and data visualization. These techniques could be accomplished by using various application programming language interfaces (e.g., APIs for R and Python). By hosting a web interface could even facilitates the scattered tasks that used to be. messages derived from the corpus (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Kanayama and Nasukawa, 2006) to assign and determine the word polarity. Notwithstanding their signiﬁcant success in achieving accuracy rate, in this paper, we will argue that current approaches to the problem might face with the methodological drawbacks due to the lack of scalability on the one hand, and indifference to the individual sentimental varieties on the other hand. First, referring to the lack of scalability, it is rather difﬁcult to handle out-of-vocabulary (OOV) issue on the lexical and corpus resources, in particular, those OOV words and phrases (or called as neologisms) often c"
Y15-2004,W11-1701,0,0.0613624,"Missing"
Y15-2004,I13-1191,0,0.103106,"in meetings to detect arguing and sentiment. In the annotated data, sentiment includes emotions, evaluations, judgements, feelings, and stances. Arguing refers to cues that indicate the speaker’s attempt to convince one another. The extracted subjective cues are utilized in classification of texts online for users’ stance, defined as “an overall position held by a person toward an object, idea, or proposition” (Somasundaran et al., 2009). Stance classification deals with two sided debates and seeks an automated approach to categorization whether a person is for or against the topic discussed (Hasan and Ng, 2013). In Somasundaran and Wiebe’s study in 2010, they tested a combined feature set of arguing based features and sentiment based features. Arguing based features included arguing trigger expressions and modal verbs. Sentiment lexicon compiled by Wilson et al. in 2005 was used as sentiment based features. They reached an average accuracy of 64 percent classifying online debates based on the lexicon. 28 Anand et al. (2011) combined the feature set with metalinguistic features like word length and number of characters and approach arguing language with dependency parsers that capture words and its m"
Y15-2004,D14-1083,0,0.018242,"f the topics The three boards, though all discussion oriented, involves the exchange of information in different topics. For Boy_Girl board, most of the topic is centered on romantic relationships. As for WomenTalk board, most of the discussions focus on things that girls care about, such as products for women, boyfriends, etc. These two boards might have a clearer group of users than Gossiping, where all kinds of questions could be relevant. The topics cover from debates on international political events to opinions on superhero characters. In previous studies in English (Hasan and Ng, 2013; Hasan and Ng, 2014; Faulkner, 2014), domains are usually selected and separated so that the classification is performed on one central idea, such as gay rights or death penalty. The variety of topics might be a reason why classification on Gossiping data is less accurate than the others. 2. Different language use due to the different culture of the board Since each board on PTT has its own purpose of discussion, every board attracts different group of users and forms its unique “culture”. In general, speakers on Gossiping board is more direct and more quick to criticize than users on the other two boards, as in"
Y15-2004,W03-1014,0,0.106847,"e, or it could also be individual differences. In example (6), the comment was tagged with “boo” while the beginning of the sentence is the word push. Both the classifier and human readers would consider this sentence to be a push comment rather than a boo comment. This could be a result of the user’s own tagging error. Thus would not be considered a very important issue in the current study. (6) 推 投幣式 女友 tuī tóubìshì nǚyǒu push coin-op girlfriend “I agree with coin-operated girlfriend” To further improve the classifier, the following approaches could be taken into consideration. According to Riloff and Wiebe (2003), it is important to incorporate large amount of data because infrequent words can sometimes be strong subjective clue. Thus, it might be helpful to expand the coverage of annotated data. Context of the comments should also be taken into account. If the classifier is able to capture the relationship between the target and the comment being given, the errors caused by context dependent comments could be solved. 5 Conclusion The purpose of this study is to compile lexical resources in Mandarin on arguing and stancetaking and to test the applicability of these resources in machine training on sta"
Y15-2004,D09-1018,0,0.0275824,") extracted subjective cues by combining manually annotated subjective elements and expanding it with collocations and clustering method. Somasundaran et al. (2007) inspected dialogues in meetings to detect arguing and sentiment. In the annotated data, sentiment includes emotions, evaluations, judgements, feelings, and stances. Arguing refers to cues that indicate the speaker’s attempt to convince one another. The extracted subjective cues are utilized in classification of texts online for users’ stance, defined as “an overall position held by a person toward an object, idea, or proposition” (Somasundaran et al., 2009). Stance classification deals with two sided debates and seeks an automated approach to categorization whether a person is for or against the topic discussed (Hasan and Ng, 2013). In Somasundaran and Wiebe’s study in 2010, they tested a combined feature set of arguing based features and sentiment based features. Arguing based features included arguing trigger expressions and modal verbs. Sentiment lexicon compiled by Wilson et al. in 2005 was used as sentiment based features. They reached an average accuracy of 64 percent classifying online debates based on the lexicon. 28 Anand et al. (2011)"
Y15-2004,2007.sigdial-1.5,0,0.463989,"ts on differentiating factual information from opinionated information. Opinionated information reveals a person’s private states through the use of subjective language. Private state is a term that covers a person’s overall attitude, including opinions, evaluations, emotions, and speculations (Quirk et al., 1985). Identifying these cues could assist automatic tasks on detecting attitudes online by providing resources (Wiebe et al., 2005). Wiebe et al. (2004) extracted subjective cues by combining manually annotated subjective elements and expanding it with collocations and clustering method. Somasundaran et al. (2007) inspected dialogues in meetings to detect arguing and sentiment. In the annotated data, sentiment includes emotions, evaluations, judgements, feelings, and stances. Arguing refers to cues that indicate the speaker’s attempt to convince one another. The extracted subjective cues are utilized in classification of texts online for users’ stance, defined as “an overall position held by a person toward an object, idea, or proposition” (Somasundaran et al., 2009). Stance classification deals with two sided debates and seeks an automated approach to categorization whether a person is for or against"
Y15-2004,W10-0214,0,0.0657999,"Missing"
Y15-2004,N12-1072,0,0.0217601,"topic. The phrase perfectly demonstrates how online discussions reflect the public’s reaction to certain event or certain individual, often a political figure. In Taiwan, the mass media often resort to online forums as a source of understanding how the public responds to political events like new policy and candidates running for elections. Online discussion forums and social media give citizens an easier access to information and more power in shaping what information or idea gets passed on. Users of these online forums participate in a process of framing discussions and forming opinions. As Walker et al. (2012) pointed out, these debates involve not only the expression of opinions but also the formation of opinions. Through posting articles online, users talk about their beliefs on what is true or not, what is important, and what should be done. Their shared opinions thus stimulate more discussions. These users play an important role on how the discussions are framed and shape the form of the arguments. One characteristics of these forums is that users usually have to express their position in a very short text. This implies that stance classification on short text would be different from identifyin"
Y15-2004,J94-2004,0,0.186514,"Missing"
Y15-2004,J04-3002,0,0.0738999,"ne data have triggered interests in related research to achieve automated methods in understanding affections and opinions. Previous research has made efforts on differentiating factual information from opinionated information. Opinionated information reveals a person’s private states through the use of subjective language. Private state is a term that covers a person’s overall attitude, including opinions, evaluations, emotions, and speculations (Quirk et al., 1985). Identifying these cues could assist automatic tasks on detecting attitudes online by providing resources (Wiebe et al., 2005). Wiebe et al. (2004) extracted subjective cues by combining manually annotated subjective elements and expanding it with collocations and clustering method. Somasundaran et al. (2007) inspected dialogues in meetings to detect arguing and sentiment. In the annotated data, sentiment includes emotions, evaluations, judgements, feelings, and stances. Arguing refers to cues that indicate the speaker’s attempt to convince one another. The extracted subjective cues are utilized in classification of texts online for users’ stance, defined as “an overall position held by a person toward an object, idea, or proposition” (S"
Y15-2004,wilson-2008-annotating,0,0.11941,"tences. Table 1 shows the details of the corpus used in the study. Number of Number of tokens comments 3786034 28341656 1222735 9000728 998327 10006638 53376 508778 167473 1655771 36381 354672 Table 1. Corpus information Number of token types 11538420 493926 462780 66186 121794 47904 “it’d better be” is treated as an element used to reject other people’s opinion. Annotation criteria Since comments on these forums are used as a way for users to express their opinion, to oppose to others’ ideas, and to justify their reasons for believing in or not believing in something (Wilson and Wiebe, 2005; Wilson, 2008; Somasundaran et al., 2007), the lexicon used in the classifier is compiled with a set of categories that are related to stance-taking and arguing. Following previous studies, we look for linguistic cues that indicate the author’s opinion or position on the discussed topic. The following are categories included in the annotation. In the tagging process, the identified “element” is not restricted to the word level. Considering the fact that subjectivity is often revealed in a common phrase or expression, function words are also included in the tagged set. For example, expression like 最 好 是 zuì"
