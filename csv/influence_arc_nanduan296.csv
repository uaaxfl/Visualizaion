2011.mtsummit-papers.20,P08-1023,0,0.0201031,"MBR definition, where  ܯis a constant large enough. We define ܩሺߩǡ ߩᇱ ሻ as the similarity measure between two hypotheses ߩ and ߩᇱ . In this sense, ܵ ሺߩሻ can be viewed as the expected similarity between ߩ and all hypotheses in ሺሻ. W formulate ܩሺߩǡ ߩᇱ ሻ as a weighted combination of a set of similarity features: Step 1: Baseline Phrase Extraction ܩሺߩǡ ߩᇱ ሻ ൌ  ߣ ߠ ሺߩǡ ߩᇱ ሻ In this step, all potential phrase pairs that are consistent with word alignments are extracted from a given training corpus ࣝ using the standard phrase extraction method. Furthermore, inspired by several studies (Mi et al., 2008; Dyer et al., ሺͳሻ ఘᇲ אሺሻ ሺʹሻ  where ߠ is the  th feature with its weight ߣ . x 1 Given a source phrase  and a target phrase , the phrase pair ሺǡ ሻ is said to be consistent with word alignment if and only if: (1) at least one word in one phrase is aligned to one word in the other phrase; (2) no words in one phrase can be aligned to a word outside the other phrase. ܲሺߩȁሻ is the hypothesis distribution over all hypotheses contained in ሺሻ: ܲሺߩȁሻ ൌ 190 σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩሻܲሺࣛȁܧǡ ܨሻ σఘᇲ אுሺሻ σሺாǡிሻ ࣝאσࣛ ߜሺࣛǡாǡிሻ ሺߩᇱ ሻܲሺࣛȁܧǡ ܨሻ where ߜሺࣛǡாǡிሻ ሺߩሻ equals to 1 when ߩ"
2011.mtsummit-papers.20,P08-1010,0,0.0501013,"Missing"
2011.mtsummit-papers.20,W04-3243,0,0.205854,"Missing"
2011.mtsummit-papers.20,P08-1115,0,0.0374457,"Missing"
2011.mtsummit-papers.20,E99-1010,0,0.0253761,"BR model, and use them as extra phrasal features. In our experimental part, we will show that besides alignment pruning, using similarity scores as additional features can provide further improvements as well. When using n-best alignment results instead of 1-best ones, translation probabilities and lexical weights are estimated based on fractional counts instead of absolute frequencies of phrases. 3 alignment-based features 3) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s class) link pairs in ߩ co-occur in ߩᇱ . Word clusters are obtained by using mkcls toolkit (Och, 1999) that trains word classes based on the maximum-likelihood criterion. The total numbers of word classes are set to be 80 for both Chinese and English. One question may be asked is the reason that we remove all alignment links from phrase pairs of relative low MBR scores. In fact, although all alignment links are not necessarily bad in those low-quality phrase pairs, we just remove all of them from training corpus for convenience. By varying different values of ݐ, we can empirically find an optimal setting, where alignment pruning can bring benefits for final translation quality. 2.4 x Similar"
2011.mtsummit-papers.20,P06-1097,0,0.0229039,"rs. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Although discriminative methods have already shown comparable word alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large scale corpus. As a result, we evaluate our approach based on two different word aligners. x Disc-Aligner. A discriminative word aligner (Fraser and Marcu, 2006) is re-implemented to predict alignments for the training corpus. A data set of 491 sentence pairs with human annotated word alignments is used to tune model parameters. Disc-Aligner can produce n-best alignment alternatives. x GIZA-Aligner: An unsupervised word aligner GIZA++ (Och and Ney, 2003) is used with the default parameters. In this paper, we only use its Viterbi (1-best) alignment outputs. 4.4 Baseline Phrase Extraction Method The standard phrase extraction method (Base-PE) proposed by Och and Ney (2004) is utilized to generate the baseline phrase table. The length limitations are set"
2011.mtsummit-papers.20,P03-1021,0,0.0600191,"re peak, while Ͳ  ߙ  ͳ makes the distribution more uniform. Due to the fact that varying ߙ to modify the entropy of the alignment distribution doesn’t have consistent impacts on translation quality (Venugopal et al., 2008), in this paper we just fix this value to be 1.0. ሼܵ ሺߩǡ ሺሻሽ maintained for each phase pair as additional phrasal features; then, we use this phrase table in our log-linear SMT system and optimize the weights of these similarity scores together with the weights of original SMT model features to maximize BLEU on development data set using the MERT algorithm proposed by Och (2003) 3 ; last, we collect the well-tuned feature weights ሼߣ ሽ and ሼߣ ሽ and compute ܵ ሺߩሻ and ܵ ሺߩሻ for each ߩ based on Equation (3) and (4). Algorithm 1: MBR Phrase Scoring 1: 2: 3: 4: 5: 6: 7: 8 We rewrite Equation (1) by replacing ܩሺߩǡ ߩᇱ ሻ using Equation (2) as: ܵ ሺߩሻ ൌ  ߣ ሼ  ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻሽ ఘᇲ אሺሻ  9: 10: 11: 12: 13: 14: 15: 16: 17: 18: ሺ͵ሻ ൌ  ߣ ܵ ሺߩǡ ሺሻሻ  ܵ ൫ߩǡ ሺሻ൯ ൌ σఘᇲ אሺሻ ߠ ሺߩǡ ߩᇱ ሻܲሺߩᇱ ȁሻ is defined as the expected value of the  th similarity feature ߠ for ߩ based on the entire ሺሻ. We then consider scoring phrase pairs based on their target phra"
2011.mtsummit-papers.20,J07-3002,0,0.0249927,"Missing"
2011.mtsummit-papers.20,J04-4002,0,0.157296,"in the parallel data; last, a new phrase table is learned from the link-pruned parallel data and used in SMT decoding. We evaluate our approach on the NIST Chinese-to-English MT tasks, and show significant improvements on parallel data sets of different scales. 1 ≒ᕩ ≒ᕩ hydrogen bomb bomb hydrogen bombs (a) (b) ≒ᕩ ≒ᕩ was (c) detonated against (d) Figure 1: Phrase pairs extracted from different bilingual sentence pairs with the same source phrases, in which dashed lines denote wrong alignment links. Introduction Bilingual phrases are the fundamental building blocks for phrase-based SMT systems (Och and Ney, 2004; Koehn et al., 2004a; Chiang, 2005), and their abilities to handle local reorderings and translation ambiguity as well as many-to-many word translations are key factors to the success of phrasal SMT models. The common practice of extracting bilingual phrases from the parallel data usually consists of three steps: first, words in bilingual sentence pairs are aligned using state-of-the-art automatic word alignment tools, such as GIZA++ (Och and Ney, 2003), in both directions; second, word alignment links are refined using heuristics, such as Grow-Diagonal-Final (GDF) method; third, bilingual ph"
2011.mtsummit-papers.20,C10-1056,0,0.218567,"Missing"
2011.mtsummit-papers.20,P02-1040,0,0.0824251,"investigate the impacts of different parameter settings on this corpus; the second data set includes the following data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100"
2011.mtsummit-papers.20,P09-1104,0,0.0455259,"Missing"
2011.mtsummit-papers.20,W96-0213,0,0.212647,"ܽǡ ǡ ሽ do find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ find ܵመ ሺߩሻ ൌ ݔܽܯఘᇲאሺሻ ሼܵ ሺߩᇱ ሻሽ from ሺሻ ܵ௫ ሺߩሻ ൌ ܵመ ሺߩሻ ܵ כመ ሺߩሻ if ܵ ሺߩሻ ܵ כ ሺߩሻൟ ൏ ሼܵ௫ ሺߩሻ  ݐ כሽ then prune all alignment links contained in ߩ from the positions they were extracted in ࣝ end if end for return ࣝ with link-pruned word alignments 1) ߠௐଶௐ ሺߩǡ ߩᇱ ሻ . A feature that counts how many (source word)-to-(target word) link pairs in ߩ co-occur in ߩᇱ . 2) ߠௐଶ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s POS) link pairs in ߩ co-occur in ߩᇱ . Two MaxEnt-based POS taggers (Ratnaparkhi, 1996) are used to tag Chinese and English words contained in the bilingual corpus respectively. 4) ߠௐଶௌ ሺߩǡ ߩᇱ ሻ. A feature that counts how many (source word)-to-(target word’s stem) link pairs in ߩ co-occur in ߩᇱ . A stem dictionary that contains 22,660 entries is used to convert English words into their stem forms. We consider the stem for each Chinese word as the Chinese word itself. 5) ߠி௧ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word fertilities for ߩ and ߩᇱ : Step 4: Phrase Re-Extraction Last, we re-extract bilingual phrases based on the link-pruned training corpus to learn a new"
2011.mtsummit-papers.20,P09-2031,0,0.0430818,"Missing"
2011.mtsummit-papers.20,D08-1065,0,0.0163685,"ሺߩǡ ߩᇱ ሻ. A feature that counts how many ngrams in ߩ௧ co-occur in ߩ௧ ᇱ : 192 4.2 ߠ ሺߩǡ ߩᇱ ሻ ൌ  ఠ ሺߩ௧ ሻߜఘᇲ  ሺሻ ఠאఘ ఠ ሺߩ௧ ሻ is the number of times that  occurs in ߩ௧ , ߜఘᇲ  ሺሻ equals to 1 when  occurs in ߩ௧ᇱ , and 0 otherwise. In this paper, the order of n-gram considered varies from 1 to 4. 8) ߠ ሺߩǡ ߩᇱ ሻ . A feature that reflects the agreement on word lengths for ߩ௧ and ߩ௧ᇱ : ߠ ሺߩǡ ߩᇱ ሻ ൌ ȁߩ௧ ȁߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ ߜȁఘ ȁ ሺȁߩ௧ᇱ ȁሻ equals to 1 when ȁߩ௧ ȁ ൌ ȁߩ௧ᇱ ȁ, and 0 otherwise. These features are motivated by the success of consensus-based techniques (Kumar and Byrne, 2004; Tromble et al., 2008; Kumar et al., 2009). To summarize, 6 features are contained in the first category and 5 features are contained in the second category. Because that source and target phrases are exchangeable for each phrase pair, there will be (2*11=22) similarity features in total for each bilingual phrase5. 4 4.1 Experiments Data and Metric We evaluate on the NIST Chinese-to-English MT tasks. The NIST 2003 (MT03) data set is used as the development set to tune model parameters, and evaluation results are reported on the NIST 2005 (MT05) and 2008 (MT08) data sets. Two parallel data sets with different scale"
2011.mtsummit-papers.20,J07-1003,0,0.011579,"al times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used i"
2011.mtsummit-papers.20,W04-3250,0,0.0256834,"g data sets, LDC2003E07, LDC2003E14, LDC2005T06 and LDC2005T10 with 354K sentence pairs contained after pre-processing. We confirm the effectiveness of our method on it using the optimal parameter setting determined on the first data set. We train a 5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspire"
2011.mtsummit-papers.20,W02-1019,0,0.0800667,"Because of appearing in training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learne"
2011.mtsummit-papers.20,N04-1022,0,0.138498,"n training corpus several times, all these bilingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned p"
2011.mtsummit-papers.20,2008.amta-papers.18,0,0.0751871,"sing a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We evaluate on a state-of-the-art phrase-based SMT decoder on the NIST Chinese-to-English MT tasks, and experiments show that our MBRbased approach outperforms the standard phrase extraction method by up to 1.45 BLEU points. 2008; Venugopal et al., 2008; Liu et al., 2009), in which n-best alternatives of annotations to SMT systems are leveraged to improve translation quality, we allow our proposed phrase extraction method to operate on n-best word alignments as well: given a sentence pair with n-best alignment candidates, we use alignments in the n-best list one at a time with the same sentence pair to form a new word-aligned sentence pair, and annotate it with the posterior probability of the alignment it used. These posterior probabilities will be used in the next step to compute MBR model scores. 2 The objective of this step is to score p"
2011.mtsummit-papers.20,P06-1066,0,0.0260477,"5-gram language model on the Xinhua portion of LDC English Gigaword Version 3.0. Translation quality is measured in terms of the case-insensitive IBM-BLEU scores that compute the brevity penalty using the closest reference translation (Papineni et al., 2002). Statistical significance is computed by using the bootstrap resampling method proposed by Koehn (2004b). 5 The stability of MERT should be concerned when using so many features. Here, we alleviate this issue by enlarging the beam size and increasing the rounds of MERT iterations. 193 SMT Decoder A re-implemented phrase-based SMT decoder (Xiong et al., 2006) with a lexicalized reordering component based on maximum entropy is used to generate translation outputs. Both the phrase table and the reordering model are trained on the same bilingual corpus used. The default beam size is set to be 100, and MERT algorithm (Och, 2003) is utilized to optimize model parameters. Because of using MBR-inspired techniques, we also investigate the impacts of our method on MBR decoding over translation hypergraphs of the baseline system. We re-implement an MBR decoder (Kumar et al., 2009) that uses the linear BLEU score as its loss function. 4.3 Word Aligner Althou"
2011.mtsummit-papers.20,P09-2060,0,0.218602,"Missing"
2011.mtsummit-papers.20,W04-3227,0,0.0699735,"Missing"
2011.mtsummit-papers.20,P09-1019,0,0.105678,"ingual phrases were maintained in the generated phrase table as valid entries. Incorrect phrase entries fed into SMT decoder are one of the major reasons for translation errors in phrase-based SMT systems. For example, even (d) doesn’t occupy a large portion of probabilities in all translation alternatives of the source phrase ≒ᕩ, it is still picked up by SMT decoder sometimes, because it is strongly preferred by the language model in certain circumstances. Motivated by the success of consensus-based methods in SMT research (Kumar and Byrne, 2002; Kumar and Byrne, 2004; Ueffing and Ney, 2007; Kumar et al., 2009), this paper proposes a novel approach that makes use of MBR principle to improve the accuracy of phrase extraction. Our approach operates as a four-stage pipeline: first, bilingual phrases are extracted from parallel corpus using a standard phrase induction method; then, phrases are separated into groups under specific constraints and scored using an MBR model; next, word alignment links contained in phrases with their MBR scores lower than a certain threshold are pruned in the parallel data; last, a new phrase table is learned from the linkpruned parallel data and used in SMT decoding. We ev"
2011.mtsummit-papers.20,2006.amta-papers.11,0,0.0384085,"Missing"
2020.acl-main.392,P19-1033,1,0.583974,"sponding author:Chuan Shi(shichuan@bupt.edu.cn) https://news.google.com/ A core problem in news recommendation is how to learn better representations of users and news. Recently, many deep learning based methods have been proposed to automatically learn informative user and news representations (Okura et al., 2017; Wang et al., 2018). For instance, DKN (Wang et al., 2018) learns knowledge-aware news representation via multi-channel CNN and gets a representation of a user by aggregating her clicked news history with different weights. However, these methods (Wu et al., 2019b; Zhu et al., 2019; An et al., 2019) usually focus on news contents, and seldom consider the collaborative signal in the form of high-order connectivity underlying the user-news interactions. Capturing high-order connectivity among users and news could deeply exploit structure characteristics and alleviate the sparsity, thus improving the rec4255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4255–4264 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ommendation performance (Wang et al., 2019). For example, as shown in Figure 1, the high-order relationship u1 –d1"
2020.acl-main.392,D14-1181,0,0.00246914,"documents that have not previously existed in the user-news interaction graph G during training or testing. Our model takes these news documents as isolated nodes in the graph G. Their representations are based on only content feature hd without neighbor aggregation, and can also be disentangled via Eq. 3. 5.2 Performance Evaluation We evaluate the performance of our model GNUD by comparing it with the following state-of-the-art baseline methods: LibFM (Rendle, 2012), a feature-based matrix factorization method, with the concatenation of TF-IDF vectors of news title and profile as input. CNN (Kim, 2014), applying two parallel CNNs to word sequences in news titles and profiles respectively and concatenate them as news features. The user representation is learned from the user’s news history. DSSM (Huang et al., 2013), a deep structured semantic model. In our experiments, we model the user’s clicked news as the query and the candidate news as the documents. Wide & Deep (Cheng et al., 2016), a deep model for recommendation which combines a (Wide) linear model and (Deep) feed-forward neural network. We also use the concatenation of news title and profile embeddings as features. DeepFM (Guo et al"
2020.acl-main.392,D19-1671,1,0.648001,"Missing"
2020.acl-main.392,D18-1430,1,0.833329,"latent variable which can be inferred in an iterative process. The motivation of the iterative process is as follows. Given zu,k , the value of the latent variables {rd,k : 1 ≤ k ≤ K, (u, d) ∈ E} can be obtained by measuring the similarity between user u and her clicked news d under the k-th subspace, which is computed as Eq. 4. Initially, we set zu,k = su,k . On the other hand, after obtaining the latent variables {rd,k }, we can find an estimate of zu,k by aggregating information from the clicked news, which is computed as Eq. 5: 4258 Algorithm 1 Neighborhood Routing Algorithm According to (Yang et al., 2018), the mutual information maximization can be converted to the following form. Given the representation of a user u in k-th (1 ≤ k ≤ K) latent subspace, the preference regularizer P (k|zu,k ) estimates the probability of the k-th subspace (w.r.t. the k-th preference) that zu,k belongs to: Require: S si,k , i ∈ {u} {d : (u, d) ∈ E}, 1 ≤ k ≤ K; Ensure: zu,k , 1 ≤ k ≤ K; 1: ∀k = 1, ...K, zu,k ← su,k 2: for T iterations do 3: for d that satisfies (u, d) ∈ E do 4: ∀k = 1, · · · , K : rd,k ← z> u,k sd,k 5: ∀k = 1, · · · , K : rd,k ← softmax(rd,k ) 6: end for 7: for factor k = 1, P 2, ...K do 8: zu,k"
2020.acl-main.539,D13-1160,0,0.0230982,"-module is generated, which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50."
2020.acl-main.539,P18-1071,0,0.0989351,"Missing"
2020.acl-main.539,D18-1192,0,0.0336961,"ate). In LPA settings, (Weighted) Voting means assigning each program with (score-weighted) equal weight to vote for the final result. Ranking means using the result generated by the top program ranked by the discriminator. As shown in Figure 3, a program in TABFACT is structural and follows a grammar with over 50 functions. To effectively capture the structure of the program and also generate legitimate programs following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig"
2020.acl-main.539,P17-1003,0,0.0467597,"which top is denoted as ym . After that, we make the final prediction by feedtop ing the combination of ym and the final hidden vector h(x)T from § 3.3 through an MLP (Multilayer Perceptron) layer. The motivation of this operation is to retain the complete semantic meaning of the whole contexts because some linguistic cues are discarded during the synthesizing process of the program. 3.5 Program Generation In this part, we describe our semantic parser for synthesizing a program for a textual statement. We tackle the semantic parsing problem in a weaklysupervised setting (Berant et al., 2013; Liang et al., 2017; Misra et al., 2018), since the ground-truth program is not provided. 6057 Model Val Test Human Performance Majority Guess BERT classifier w/o Table Table-BERT (Horizontal-S+T-Concatenate) Table-BERT (Vertical-S+T-Template) Table-BERT (Vertical-T+S-Template) Table-BERT (Horizontal-S+T-Template) Table-BERT (Horizontal-T+S-Template) LPA-Voting w/o Discriminator LPA-Weighted-Voting w/ Discriminator LPA-Ranking w/ Discriminator LogicalFactChecker (program from LPA) LogicalFactChecker (program from Seq2Action) 50.7 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.7 71.8 50.4 50.5 50.4 56.2 57.0 65."
2020.acl-main.539,D19-1603,0,0.11077,"Missing"
2020.acl-main.539,D18-2002,0,0.014426,"grams following a grammar in the generation process, we develop a sequence-to-action approach, which is proven to be effective in solving many semantic parsing problems (Chen et al., 2018; Iyer et al., 2018; Guo et al., 2018). The basic idea is that the generation of a program tree is equivalent to the generation of a sequence of action, which is a traversal of the program tree following a particular order, like depth-first, left-to-right order. Specifically, our semantic parser works in a top-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Foll"
2020.acl-main.539,D18-1010,0,0.0610233,"used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fa"
2020.acl-main.539,D14-1162,0,0.0848898,"-down manner in a sequence-to-sequence paradigm. The generation of a program follows an ASDL grammar (Yin and Neubig, 2018), which is given in Appendix C. At each step in the generation phase, candidate tokens to be generated are only those legitimate according to the grammar. Parent feeding (Yin and Neubig, 2017) is used for directly passing information from parent actions. We further regard column names of the table as a part of the input (Zhong et al., 2017) to generate column names as program arguments. We implement the approach with the LSTMbased recurrent network and Glove word vectors (Pennington et al., 2014) in this work, and the framework could be easily implemented with Transformer-based framework. Following Chen et al. (2019), we employ the label of veracity to guide the learning process of the semantic parser. We also employ programs produced by LPA (Latent Program Algorithm) for comparison, which is provided by Chen et al. (2019). In the training process, we train the semantic parser and the claim verification model separately. The training of semantic parser includes two steps: candidate search and sequence-to-action learning. For candidate search, we closely follow LPA by first collecting"
2020.acl-main.539,D17-1317,0,0.036168,"ost influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural abstractive summarization systems (Goodrich et al., 2019; Kry´sci´nski et al., 2019), as well as the use of this factual accuracy as a reward"
2020.acl-main.539,P13-1045,0,0.0675626,"gical form, in a semantic parsing manner (Liang, 2016). Then, our system builds a heterogeneous graph to capture the connections among the statement, the table and the program. Such connections reflect the related context of each token in the graph, which are used to define attention masks in a Transformer-based (Vaswani et al., 2017) framework. The attention masks are used to learn graph-enhanced contextual representations of tokens1 . We further develop a program-guided neural module network to capture the structural and compositional semantics of the program for semantic compositionality. (Socher et al., 2013; Andreas et al., 2015). Graph nodes, whose representations are computed using the contextual representations of their constituents, are considered as arguments, and logical operations are considered as modules to recursively produce representations of higher level nodes along the program. Experiments show that our system outperforms previous systems and achieves the state-of-the-art verification accuracy. The contributions of this paper can be summarized as follows: • We propose LogicalFactChecker, a graphbased neural module network, which utilizes logical operations for fact-checking. 1 Here"
2020.acl-main.539,D19-1216,0,0.0218259,"nction of difference time, which is not covered by the current set. 5 Related Work There is a growing interest in fact checking in NLP with the rising importance of assessing the truthfulness of texts, especially when pre-trained language models (Radford et al., 2019; Zellers et al., 2019; Keskar et al., 2019) are more and more powerful in generating fluent and coherent texts. Previous studies in the field of fact checking differ in the genres of supporting evidence used for verification, including natural language (Thorne et al., 2018), semi-structured tables (Chen et al., 2019), and images (Zlatkova et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al.,"
2020.acl-main.539,N18-1074,0,0.15998,"Missing"
2020.acl-main.539,D19-6601,0,0.0280888,"a et al., 2019; Nakamura et al., 2019). The majority of previous works deal with textual evidence. FEVER (Thorne et al., 2018) is one of the most influential datasets in this direction, where evidence sentences come from 5.4 million Wikipedia documents. Systems developed on FEVER are dominated by pipelined approaches with three separately trained models, i.e. document retrieval, evidence sentence selection, and claim verification. There also exist approaches (Yin and Roth, 2018) that attempt to jointly learn evidence selection and claim verification. More recently, the second FEVER challenge (Thorne et al., 2019) is built for studying adversarial attacks in fact checking4 . Our work also relates to fake news detection. For example, Rashkin et al. (2017) study fact checking by considering stylistic lexicons, and Wang (2017) builds LIAR dataset with six finegrained labels and further uses meta-data features. There is a fake news detection challenge5 hosted in WSDM 2019, with the goal of the measuring the truthfulness of a new article against a collection of existing fake news articles before being published. There are very recent works on assessing the factual accuracy of the generated summary in neural"
2020.acl-main.539,D19-5316,0,0.0844217,": An example of table-based fact checking. Given a statement and a table as the input, the task is to predict the label. Program reflects the underlying meaning of the statement, which should be considered for fact checking. Introduction Fact checking for textual statements has emerged as an essential research topic recently because of the unprecedented amount of false news and rumors spreading through the internet (Thorne et al., 2018; ∗ Work done while this author was an intern at Microsoft Research. Chen et al., 2019; Goodrich et al., 2019; Nakamura et al., 2019; Kry´sci´nski et al., 2019; Vaibhav et al., 2019). Online misinformation may manipulate people’s opinions and lead to significant influence on essential social events like political elections (Faris et al., 2017). In this work, we study fact checking, with the goal of automatically assessing the truthfulness of a textual statement. The majority of previous studies in fact checking mainly focused on making better use of the meaning of words, while rarely considered symbolic reasoning about logical operations (such as “count”, “superlative”, “aggregation”). However, modeling logical operations is an essential step towards the modeling of compl"
2020.acl-main.539,P17-1041,0,\N,Missing
2020.acl-main.539,D18-1266,0,\N,Missing
2020.acl-main.544,2020.acl-main.23,0,0.0747168,"Missing"
2020.acl-main.544,2021.ccl-1.108,0,0.0750586,"Missing"
2020.acl-main.544,P18-2119,0,0.0142462,"at focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recentl"
2020.acl-main.544,D16-1031,0,0.027836,"and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-cen"
2020.acl-main.544,N16-1098,0,0.0228979,"ated text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP com"
2020.acl-main.544,P18-1043,0,0.387556,"two kinds of reasonable inferences for the event under different background knowledge that is absent in the dataset. Introduction Inferential text generation aims to understand dailylife events and generate texts about their underlying causes, effects, and mental states of event participants, which is crucial for automated commonsense reasoning. Taking Figure 1 as an example, given an event “PersonX reads PersonY’s diary”, the cause of the participant “PersonX” is to “obtain Person Y’s secrets” and the mental state of “PersonX” is “guilty”. Standard approaches for inferential text generation (Rashkin et al., 2018; Sap et al., 2019; Bosselut et al., 2019; Du et al., 2019) typically only ∗ Work done while this author was an intern at Microsoft Research. take the event as the input, while ignoring the background knowledge that provides crucial evidence to generate reasonable inferences. For example, if the background knowledge of this example is “PersonY invites PersonX to read his diary”, the outputs should be different. In this paper, we present an evidence-aware generative model, which ﬁrst retrieves relevant evidence from a large text corpus and then leverages retrieved evidence to guide the generati"
2020.acl-main.544,L16-1233,0,0.0270969,"y using better semantic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and"
2020.acl-main.544,D17-1006,0,0.0182539,"ic-based retrieval model. The second problem is that the model cannot effectively leverage selected evidence. Although the selected evidence is closely related to the event and the inference can be obtained from the evidence, the model still generate incorrect texts since lacking of supervised information. A potential direction to mitigate the problem is to annotate background knowledge of events in the training dataset. 5 5.1 Related Work Event-Related Text Understanding Recently, event-related text understanding has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018; Rashkin et al., 2018; Sap et al., 2019; Guo et al., 2020), which is crucial to artiﬁcial intelligence systems for automated commonsense reasoning. There are a variety of tasks that focus on event-related text understanding in different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Gr"
2020.acl-main.544,D18-1009,0,0.022466,"different forms. Script (Schank and Abelson, 1977) uses a line to represent temporal and causal relations between events, and the task of script event prediction (Chambers and Jurafsky, 2008) requires models to predict the subsequent event given an event context. Previous works on the task are mainly based on event pairs (Chambers and Jurafsky, 2008; Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017), and event evolutionary graph (Li et al., 2018) to predict script event. In addition, our task relates to story ending prediction (Sharma et al., 2018; Mostafazadeh et al., 2016; Zellers et al., 2018). Mostafazadeh et al. (2016) introduce a dataset for story ending prediction, which requires models to choose the most sensible ending given a paragraph as context. In this work, we study inferential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 20"
2020.acl-main.544,D16-1050,0,0.0283868,"erential text generation proposed by Rashkin et al. (2018) and Sap et al. (2019), both of which focus on generating texts about causes and effects of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse"
2020.acl-main.544,P17-1061,0,0.0188185,"of events and mental states of event participants. 5.2 Variational Autoencoder Based Text Generation Natural Language Generation, also known as text generation (McKeown, 1992; Sutskever et al., 2011), has recently become popular in NLP community (Feng et al., 2018; Duan et al., 2020). Recently, Variational Autoencoder (VAE) (Kingma and Welling, 2013) has achieved promising performance on various text generation tasks, including machine translation (Zhang et al., 2016; Su et al., 2018), text summarization (Miao and Blunsom, 2016; Li et al., 2017), and dialogue generation (Serban et al., 2017; Zhao et al., 2017). For machine translation, Zhang et al. (2016) and Su et al. (2018) introduce a continuous latent variable to explicitly model the semantics of a source sentence, which is used to guide the translation. In dialogue genration, Serban et al. (2017) apply a latent variable hierarchical encoder-decoder model to facilitate longer response, while Zhao et al. (2017) uses latent vari6125 ables to capture potential conversational intents and generates diverse responses. A recent work CWVAE (Du et al., 2019) on event-centered If-Then reasoning is the most related to our work, which introduces an additio"
2020.acl-main.549,D14-1059,0,0.0802266,"ly assessing the truthfulness of a textual claim by looking for textual evidence. Introduction Internet provides an efficient way for individuals and organizations to quickly spread information to massive audiences. However, malicious people spread false news, which may have significant influence on public opinions, stock prices, even presidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reaso"
2020.acl-main.549,W04-2412,0,0.194303,"Missing"
2020.acl-main.549,N16-1138,0,0.0557513,") Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER chall"
2020.acl-main.549,W18-5516,0,0.192352,"by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of trai"
2020.acl-main.549,2021.ccl-1.108,0,0.0443279,"Missing"
2020.acl-main.549,P14-1095,0,0.05237,"ce Medal of Honor , an astronaut must perform feats of extraordinary accomplishment while participating in space flight under the authority of NASA . Tuples: ('awarded', 'the Congressional Space Medal Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Ra"
2020.acl-main.549,W18-5527,0,0.0209756,"EVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, participants typically extract named entities from a claim as the query and use Wikipedia search API. In the evidence selection phase, participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the informati"
2020.acl-main.549,D17-1317,0,0.0495327,"4). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of evidence from Wikipedia for making the conclusion. FEVER contains 185,445 annotated instances, which to the best of our knowledge is the largest benchmark dataset in this area. The majority of participating teams in the FEVER challenge (Thorne et al., 2018b) use the same pipeline consisting of three components, namely document selection, evidence sentence selection, and claim verification. In document selection phase, par"
2020.acl-main.549,N18-2074,0,0.154527,"rd w5j from s5 are connected on the graph, simply concatenating evidence sentences as a single string fails to capture their semantic-level structure, and would give a large distance to w1i and w5j , which is the number of words between them across other three sentences (i.e., s2 , s3 , and s4 ). An intuitive way to achieve our goal is to define an N × N matrix of distances of words along the graph, where N is the total number of words in the evidence. However, this is unacceptable in practice because the representation learning procedure will take huge memory space, which is also observed by Shaw et al. (2018). In this work, we adopt pre-trained model XLNet (Yang et al., 2019) as the backbone of our approach because it naturally involves the concept of relative position5 . Pre-trained models capture rich contextual representations of words, which is helpful for our task which requires sentence-level reasoning. Considering the aforementioned issues, we implement an approximate solution to trade off between the efficiency of implementation and the informativeness of the graph. Specifically, we reorder evidence sentences with a topology sort algorithm with the intuition that closely linked nodes shoul"
2020.acl-main.549,D18-1209,0,0.108334,"Missing"
2020.acl-main.549,N18-1074,0,0.293102,"Missing"
2020.acl-main.549,W18-5501,0,0.137843,"Missing"
2020.acl-main.549,W14-2508,0,0.231712,"al Evidence #2 of Honor’) ('To be awarded the Congressional Space Medal of Honor',’an astronaut','perform','feats of extraordinary accomplishment’) ('an astronaut', 'participating','in space flight','under the authority of NASA' ) Figure 5: A case study of our approach. Facts shared across the claim and the evidence are highlighted with different colors. a text or a subject-predicate-object triple (Nakashole and Mitchell, 2014). In this work, we only consider textual claims. Existing datasets differ from data source and the type of supporting evidence for verifying the claim. An early work by Vlachos and Riedel (2014) constructs 221 labeled claims in the political domain from POLITIFACT.COM and CHANNEL4.COM, giving metadata of the speaker as the evidence. POLIFACT is further investigated by following works, including Ferreira and Vlachos (2016) who build Emergent with 300 labeled rumors and about 2.6K news articles, Wang (2017) who builds LIAR with 12.8K annotated short statements and six fine-grained labels, and Rashkin et al. (2017) who collect claims without meta-data while providing 74K news articles. We study FEVER (Thorne et al., 2018a), which requires aggregating information from multiple pieces of"
2020.acl-main.549,D18-1010,0,0.18542,"st to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning module have model innovations of taking the graph information into consideration. Instead of training each component separately, Yin and Roth (2018) show that joint learning could improve both claim verification and evidence selection. 7 Conclusion In this work, we present a graph-based approach for fact checking. When assessing the veracity of a claim giving multiple evidence sentences, our approach is built upon an automatically constructed graph, which is derived based on semantic role labeling. To better exploit the graph information, we propose two graph-based modules, one for calculating contextual word embeddings using graph-based distance in XLNet, and the other for learning representations of graph components and reasoning over t"
2020.acl-main.549,W18-5515,0,0.222915,", participants measure the similarity between the claim and an evidence sentence candidate by training a classification model like Enhanced LSTM (Chen et al., 2016) in a supervised setting or using string similarity function like TFIDF without trainable parameters. Padia et al. (2018) utilizes semantic frames for evidence selection. In this work, our focus is the claim classification phase. Top-ranked three systems aggregate pieces of evidence through concatenating evidence sentences into a single string (Nie et al., 2019), classifying each evidence-claim pair separately, merging the results (Yoneda et al., 2018), and encoding each evidence-claim pair followed by pooling operation (Hanselowski et al., 2018). Zhou et al. (2019) are the first to use BERT to calculate claim-specific evidence sentence representations, and then develop a graph network to aggregate the information on top of BERT, regard-1 ing each evidence as a node in the graph. Our work differs from Zhou et al. (2019) in that (1) the construction of our graph requires understanding the syntax of each sentence, which could be viewed as a more fine-grained graph, and (2) both the contextual representation learning module and the reasoning m"
2020.acl-main.549,P19-1085,0,0.40704,"sidential elections (Faris et al., 2017). Vosoughi et al. (2018) show that false news reaches more people ∗ Work done while this author was an intern at Microsoft Research. Previous works are dominated by natural language inference models (Dagan et al., 2013; Angeli and Manning, 2014) because the task requires reasoning of the claim and retrieved evidence sentences. They typically either concatenate evidence sentences into a single string, which is used in top systems in the FEVER challenge (Thorne et al., 2018b), or use feature fusion to aggregate the features of isolated evidence sentences (Zhou et al., 2019). However, both methods fail to capture rich semantic-level structures among multiple evidence, which also prevents the use of deeper reasoning model for fact checking. In Figure 1, we give a motivating example. Making the correct prediction requires a model to reason based on the understanding that “Rodney King riots” is occurred in “Los Angeles County” from the first evidence, and that “Los Angeles County” is “the most populous county 6170 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6170–6180 c July 5 - 10, 2020. 2020 Association for Computa"
2020.acl-main.599,P19-1620,0,0.156639,"nswer” should be given if there is no suitable short answer. 2.2 • We achieve state-of-the-art performance on both long and short answer leaderboard of NQ at the time of submission (Jun. 25th, 2019), and our model surpasses single human performance on the development dataset at both long and short answer criteria. Preliminary Data Preprocessing Since the average length of the documents in NQ is too long to be considered as one training instance, we first split each document into a list of document fragments with overlapping windows of tokens, like in the original BERT model for the MRC tasks (Alberti et al., 2019b; Devlin et al., 2019). Then we generate an instance from a document fragment by concatenating a “[CLS]” token, tokenized question, a “[SEP]” token, tokens from the content of the doc6709 Output Layer Document Fragment Add & Norm Paragraph Feed-Forward Sentence Add & Norm Concatenate Token Graph Integration Token-Level Self-Attention Sentence-Level Self-Attention N× Paragraph-Level Self-Attention Figure 4: The graph on the left is an illustration of the graph integration layer. The graph on the right shows the incoming information when updating a paragraph node. The solid lines represent the"
2020.acl-main.599,P17-1171,0,0.270549,"al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. According to Kwiatkowski et al. (2019), a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer. For instance,"
2020.acl-main.599,P16-1046,0,0.238693,"agraphs 6708 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained joi"
2020.acl-main.599,P17-1147,0,0.0527221,"dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al"
2020.acl-main.599,N18-2075,0,0.0309702,"normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani e"
2020.acl-main.599,Q19-1026,0,0.165659,"ar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches,"
2020.acl-main.599,N18-2078,0,0.0307711,"k et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different"
2020.acl-main.599,P18-1078,0,0.295567,"ides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answe"
2020.acl-main.599,P17-1055,1,0.862058,"ns that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extrac"
2020.acl-main.599,N18-1158,0,0.0177026,"ion for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other. At inference time, we use a pipeline s"
2020.acl-main.599,D16-1244,0,0.153285,"Missing"
2020.acl-main.599,N16-1174,0,0.432339,"of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote eac"
2020.acl-main.599,D16-1264,0,0.0512332,"ong and short answer criteria. 1 Figure 1: An example from NQ dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to selec"
2020.acl-main.599,K19-1074,0,0.0605159,"Missing"
2020.acl-main.599,D16-1103,0,0.0241031,"Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have s"
2020.acl-main.599,N18-2074,0,0.0339776,"graph integration layer and pass it to the feedforward layer. 6711 3.3.4 Feed-Forward Layer Following the inner structure of the transformer (Vaswani et al., 2017), we also utilize an additional fully connected feed-forward network at the end of our graph encoder. It consists of two linear transformations with a GELU activation in between. GELU is Gaussian Error Linear Unit activation (Hendrycks and Gimpel, 2016), and we use GELU as the non-linear activation, which is consistent with BERT. 3.3.5 Inspired by positional encoding in Vaswani et al. (2017) and relative position representations in Shaw et al. (2018), we introduce a novel relational embedding on our constructed graph, which aims at modeling the relative position information between nodes on the multi-granularity document structure. We make the edges in our document modeling graph to embed relative positional information. We modify equation 1 and 2 for eij and z i to introduce our relational embedding as follows: eij = zi = X αij  h0i = j∈Ni ,oj +1=oi  h0j + aij + boi , Output Layer The objective function is defined as the negative sum of the log probabilities of the predicted distributions, averaged over all the training instances. The"
2020.acl-main.599,D18-1246,0,0.0199332,"evel encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different 6715 types of edges in the graph. 6 Jaco"
2020.acl-main.599,P18-1030,0,0.0200399,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D18-1244,0,0.031553,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D15-1167,1,0.734635,"Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibil"
2020.acl-main.599,P18-1158,0,0.0352041,"l., 2017; Lai et al., 2017; Trischler et al., 2017; Yang et al., 2018). Lots of work has begun to build end-to-end deep learning models and has achieved good results (Seo et al., 2017; Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network m"
2020.acl-main.599,W17-2623,0,\N,Missing
2020.acl-main.599,D17-1082,0,\N,Missing
2020.acl-main.599,D18-1259,0,\N,Missing
2020.acl-main.599,N19-1423,0,\N,Missing
2020.acl-main.604,P19-1620,0,0.0781126,"e amount of text, one key observation is that most answers are only related to a few words in one paragraph; (b) The final paragraph representation can be used naturally for predicting long answers. 2 NQ provides some visual examples of the data at https://ai.google.com/research/ NaturalQuestions/visualization. 6762 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6762–6771 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We describe the details of DPDA reader in § 3.1. For the second challenge, unlike prior works on NQ dataset (Alberti et al., 2019b; Pan et al., 2019) that only predict the short answer and directly select its paragraph as long answer, RikiNet employs a multi-level cascaded answer predictor which jointly predict the short answer span, the long answer paragraph, and the answer type in a cascaded manner. Another key intuition motivating our design is that even if the relevant documents are not given, humans can easily judge that some questions have no short answers (Borschinger et al., 2019). Take this question as a motivating example:“What is the origin of the Nobel prize?” The answer should be based on a long story, whic"
2020.acl-main.604,P17-1171,0,0.108717,"Since most submissions on the NQ leaderboard are ensemble models, we also report the results of our ensemble model, which consists of three RikiNet-RoBERTa large models with different hyper-parameters. At the time of submission (29 Nov. 2019), the NQ leaderboard shows that our ensemble model achieves the best performance on both LA (F1 76.1) and SA (F1 61.3). 4 The single RikiNet-BERTlarge model was submitted to the NQ public leaderboard on 7 Nov. 2019. 6767 LA Dev R F1 P P LA Test R F1 P SA Dev R F1 P SA Test R F1 DocumentQA (Clark and Gardner, 2018) DecAtt (Parikh et al., 2016) + DocReader (Chen et al., 2017) BERTjoint (Alberti et al., 2019b) BERTlarge + 4M synth NQ (Alberti et al., 2019a) BERTjoint (Alberti et al., 2019b) + RoBERTa large (Liu et al., 2019) ‡ BERTlarge + SQuAD2 PT + AoA (Pan et al., 2019)† BERTlarge + SSPT (Glass et al., 2019)† RikiNet-BERTlarge RikiNet-RoBERTa large ‡ 47.5 52.7 61.3 62.3 65.6 73.2 74.3 44.7 57.0 68.4 70.0 69.1 74.5 76.4 46.1 54.8 64.7 65.9 67.3 68.2 65.8 73.9 75.3 48.9 54.3 64.1 65.2 74.2 - 43.3 55.7 68.3 68.4 74.4 - 45.7 55.0 66.2 66.8 74.3 - 38.6 34.3 59.5 60.7 60.9 61.1 61.4 33.2 28.9 47.3 50.4 51.0 54.7 57.3 35.7 31.4 52.7 55.1 55.5 57.2 54.2 57.7 59.3 40.6 3"
2020.acl-main.604,P18-1078,0,0.156481,"arge . These results demonstrate the effectiveness of our RikiNet. Since most submissions on the NQ leaderboard are ensemble models, we also report the results of our ensemble model, which consists of three RikiNet-RoBERTa large models with different hyper-parameters. At the time of submission (29 Nov. 2019), the NQ leaderboard shows that our ensemble model achieves the best performance on both LA (F1 76.1) and SA (F1 61.3). 4 The single RikiNet-BERTlarge model was submitted to the NQ public leaderboard on 7 Nov. 2019. 6767 LA Dev R F1 P P LA Test R F1 P SA Dev R F1 P SA Test R F1 DocumentQA (Clark and Gardner, 2018) DecAtt (Parikh et al., 2016) + DocReader (Chen et al., 2017) BERTjoint (Alberti et al., 2019b) BERTlarge + 4M synth NQ (Alberti et al., 2019a) BERTjoint (Alberti et al., 2019b) + RoBERTa large (Liu et al., 2019) ‡ BERTlarge + SQuAD2 PT + AoA (Pan et al., 2019)† BERTlarge + SSPT (Glass et al., 2019)† RikiNet-BERTlarge RikiNet-RoBERTa large ‡ 47.5 52.7 61.3 62.3 65.6 73.2 74.3 44.7 57.0 68.4 70.0 69.1 74.5 76.4 46.1 54.8 64.7 65.9 67.3 68.2 65.8 73.9 75.3 48.9 54.3 64.1 65.2 74.2 - 43.3 55.7 68.3 68.4 74.4 - 45.7 55.0 66.2 66.8 74.3 - 38.6 34.3 59.5 60.7 60.9 61.1 61.4 33.2 28.9 47.3 50.4 51.0"
2020.acl-main.604,P17-1055,0,0.046996,"atkowski et al. (2019) adapt Document-QA (Clark and Gardner, 2018) for NQ, and also utilizes DecAtt (Parikh et al., 2016) for paragraph selection and DocReader (Chen et al., 2017) for answer prediction. BERTjoint (Alberti et al., 2019b) modifies BERT for NQ. Besides, some works focus on using data augmentation to improve the MRC models on NQ. Alberti et al. (2019a) propose a synthetic QA corpora generation method based on roundtrip consistency. Glass et al. (2019) propose a span selection method for BERT pre-training (SSPT). More recently, Pan et al. (2019) introduce attention-over-attention (Cui et al., 2017) into the BERT model. Pan et al. (2019) also propose several techniques of data augmentation and model ensemble to further improve the model performance on NQ. Although the use of data augmentation and other advanced pre-trained language models (Lan et al., 2019) may further improve model performance, as this is not the main focus of this paper, we leave them as our future work. Our RikiNet is a new MRC model designed tailored to the NQ challenges and can effectively represent the document and question at multi-levels to jointly predict the answers, which significantly outperforms the above me"
2020.acl-main.604,N19-1240,0,0.0572071,"Missing"
2020.acl-main.604,Q19-1026,0,0.198824,"ale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is significantly longer compared to other datasets. Secondly, NQ task not only requires the model to find an answer span (called short answer) to the question like previous MRC tasks but also asks the model to find a paragraph that contains the information required to answer the q"
2020.acl-main.604,2021.ccl-1.108,0,0.252894,"Missing"
2020.acl-main.604,D16-1244,0,0.113496,"Missing"
2020.acl-main.604,P18-2124,0,0.253676,"ard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is sig"
2020.acl-main.604,D16-1264,0,0.100039,"formance on the official NQ leaderboard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives a"
2020.acl-main.604,Q19-1016,0,0.0332106,"ng comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0. Firstly, instead of providing one relatively short paragraph for each question-answer (QA) pair, NQ gives an entire Wikipedia page which is significantly longer compared to o"
2020.acl-main.604,D19-1169,0,0.0252877,"s the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and shortanswer tasks, achieving the best performance on the official NQ leaderboard1 . 1 Introduction Machine reading comprehension (MRC) refers to the task of finding answers to given questions by reading and understanding some documents. It represents a challenging benchmark task in natural language understanding (NLU). With the progress of large-scale pre-trained language models (Devlin et al., 2018), state-of-the-art MRC models (Ju et al., 2019; Yang et al., 2019; Lan et al., 2019; Zhang et al., 2019; Liu et al., 2019) have already surpassed human-level performance on certain commonly used MRC benchmark datasets, such as SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), and CoQA (Reddy et al., 2019). 1 Till our submission time, 29 Nov. 2019. We refer readers to https://ai.google.com/research/ NaturalQuestions/leaderboard for the latest results. Recently, a new benchmark MRC dataset called Natural Questions2 (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater challenge for the existing MRC models. Specifically, there are two main challenges in NQ com"
2020.acl-main.604,P19-1218,0,0.0618401,"f the i-th paragraph, D(T ) [j, :] is the representation of the j-th token at last DPDA Φ(t) [k] denotes the score of the k-th token at tth block, K is a hyperparameter, and SΦ (t) is the set that includes the index of the selected top-K tokens. This attention mask lets the paragraph representation concentrate on the selected key tokens. The final scaled dot-product attention weight A(t) ∈ Rm×m of the multi-head self-attention sublayer (Vaswani et al., 2017) in Eq. (1) with two proposed attention masks can be written as:    C DC > D(t) (t) L . √ A(t) = Softmax MΦ (t) + M + h 3 Following Zhuang and Wang (2019), our implementation pads the unselected token representations with zero embeddings and adds the scorer representation with the linear transformation to D(t) to avoid gradient vanishing for scorer training. 6765 3.2 Multi-level Cascaded Answer Predictor Due to the nature of the NQ tasks, a short answer is always contained within a long answer, and thus it makes sense to use the prediction of long answers to facilitate the process of obtaining short answers. As shown on the right in Fig. 1, we design a cascaded structure to exploit this dependency. This predictor takes the token representation"
2020.acl-main.604,N19-1423,0,\N,Missing
2020.acl-main.87,D16-1264,0,\N,Missing
2020.acl-main.87,P17-1147,0,\N,Missing
2020.acl-main.87,D17-1215,0,\N,Missing
2020.acl-main.87,P17-1132,0,\N,Missing
2020.acl-main.87,P18-1076,0,\N,Missing
2020.acl-main.87,L18-1437,0,\N,Missing
2020.acl-main.87,P19-1139,0,\N,Missing
2020.acl-main.87,P19-1493,0,\N,Missing
2020.acl-main.87,D19-1249,0,\N,Missing
2020.acl-main.87,D19-1169,0,\N,Missing
2020.coling-main.182,W05-0909,0,0.214213,"Missing"
2020.coling-main.182,D15-1075,0,0.0395072,"he effectiveness of our proposed approach. To dig into our approach, we perform ablation studies to explore the different effects of scaling module and prototype position indicator. 3.1 Prototype Collection In-Domain Corpus Din CommonGen is to describe a common scenario in our daily life, datasets of image captioning or video captioning would contain more knowledge about spatial relations, object properties, physical rules, temporal event knowledge and social conventions that contribute to build the target scene contains the these provided concepts. We utilize VaTeX (Wang et al., 2019), SNLI (Bowman et al., 2015), Activity (Krishna et al., 2017) and the training set of CommonGen as the external plain text knowledge datasets and retrieve prototype according to the concepts appear in the sentence. Out-of-Domain Corpus Dout In-domain corpus Din may only suitable for these description sentence for daily scenario and has difficulty in generalizing toother domains, thus we also employ wikipedia as our external knowledge dataset to retrieve prototypes to test the generalization of our model. The number of retrieved prototypes concepts that co-occur in ground truth sentence across different external knowledge"
2020.coling-main.182,P18-1015,0,0.105109,"ackground knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016)"
2020.coling-main.182,P16-1154,0,0.0292584,"1024 and 5k. The dropout rate is 0.1. We set the standard deviation of initialization in group embedding, scaling module and prototype position indicator to 5e-3. The optimizer of model is Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.999. During decoding, the size of beam search is 5 and the length penalty is 0.0. 3.3 Results For the compared methods, we classify them into four groups. Group 1 Models without pretraining. bRNN-CopyNet and Trans-CopyNet are based on the best popular architecture Bidirectional RNNs and Transformers (Vaswani et al., 2017) with attention and copy mechanism (Gu et al., 2016). MeanPooling-CopyNet is employed to deal with the influence of the concept ordering in the sequential based methods, where the input concepts is randomly permuted multiple times and decoding is with a mean pooling based MLP network. Levenshtein Transformer (Gu et al., 2019) is an edit-based non-autoregressive generation model, where the generated sentences go through multiple refinement. Group 2 Pretrained language generation models including GPT-2 (Radford et al., 2019), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), BERT-Gen (Bao et al., 2020), BART (Lewis et al., 2019), and T5 (Ra"
2020.coling-main.182,Q18-1031,0,0.0270196,"dy this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devl"
2020.coling-main.182,2020.acl-main.228,0,0.0155863,"g dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current que"
2020.coling-main.182,N03-1020,0,0.349131,"Missing"
2020.coling-main.182,D19-1282,0,0.104528,"nation and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense knowledge to conduct relational reasoning, but also compositional based g"
2020.coling-main.182,S19-1012,0,0.0196311,"missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning"
2020.coling-main.182,P18-1123,0,0.0227478,"etrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra encoder for the retrieved response, and the output of the encoder, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenates the original query and the retrieves response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduces to encodes the lexical differences between the current query and the retrieved query. Pandey et al. (2018) proposes to weight different training instances by context similarity. Different from these work, We explore the retrieve-and-edit framework on the basis of pretrained encoder-decoder model, and identify the importance of each token in prototype in a more fine-grained manner. 5 Conclusion and Future Work In this paper, we have proposed a pretraining enhanced retrieve-and-edit model for commonsense generation. The key of CommonGen is to identify the priority of the scene based on the concept combination, we have scaling module to softly reduce the impact of prototype noises on generation and p"
2020.coling-main.182,P02-1040,0,0.10919,"Missing"
2020.coling-main.182,N19-1263,0,0.0107029,"such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. Song et al. (2016) introduces an extra e"
2020.coling-main.182,P19-1487,0,0.0145731,"hese noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any exis"
2020.coling-main.182,P18-1043,0,0.0246896,"e number of instance with no concept missing of EKI-BART Din is more than that of BART Din , which shows that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heteroge"
2020.coling-main.182,D15-1044,0,0.0470787,"uire a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework with retrieve, edit and rerank on the basis of BERT (Devlin et al., 2018), but they do not deal with prototype noise in an explicit manner. S"
2020.coling-main.182,W18-5713,0,0.0167139,"l knowledge and reasoning over the extracted evidence to study this problem. Consider quite a few relationship reasoning over these concepts require a variety of background knowledge such as spatial relations, object properties, physical rules, temporal event knowledge, social conventions, etc., which may not be recorded in any existing knowledge bases, this paper focuses on retrieve knowledge from plain text in order to introduce scenario bias for concepts-set based generation. 4.2 Retrieve-and-Edit The retrieve-and-edit approaches are developed for many tasks, including dialogue generation (Weston et al., 2018; Song et al., 2016), language modeling (Guu et al., 2018), code generation (Hashimoto et al., 2018) and text summarization (Rush et al., 2015; Cao et al., 2018a; Peng et al., 2019). Ji et al. (2014) and Yan et al. (2016) focus on prototype ranking in the retrieval-based model but they do not edit these retrieved prototype. Re3Sum (Cao et al., 2018b) is an LSTM-based model developed under the retrieve-and-edit framework that retrieves multiple headlines and pick the single best retrieved headline, then edited. Hashimoto et al. (Hashimoto et al., 2018) Hossain et al. (2020) presents a framework"
2020.coling-main.182,2020.findings-emnlp.217,1,0.871963,"Missing"
2020.coling-main.182,D18-1009,0,0.103227,"rpus would benefit to discriminate the priority of different concept combination and complete the scenario with introducing additional concepts. We propose to use two kind of corpus as out of domain and in domain external knowledge to retrieve the prototypes respectively. To better model the prototypes, we design two attention mechanisms to enhance the knowledge injection procedure. We conduct experiment on CommonGen benchmark, experimental results show that our method significantly improves the performance on all the metrics. 1 Introduction Recently, commonsense reasoning tasks such as SWAG (Zellers et al., 2018), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are presented to investigate the model’s ability to make acceptable and logical assumptions about ordinary scenes in our daily life. SWAG requires to infer the probable subsequent event based on the given textual description of an event. CommonsenseQA focuses on commonsense question answering that collects commonsense questions at scale by describing the relation between concepts from CONCEPTNET. Different from these discriminative tasks, CommonGen is a generation task that not only needs to use background commonsense know"
2020.coling-main.182,P19-1472,0,0.0167693,"ws that BART Din is more likely to ignore the provided concepts than BART Din and being dominated by noises in prototype. This also verifies that the ability of BART Dout in dealing with prototype noises is stronger than BART Din , and removing these noises benefits to finding out a more plausible scenario with these concepts. 4 4.1 Related Work Commonsense Reasoning Recently, there are emerging works to investigate machine commonsense reasoning ability. ATOMIC (Sap et al., 2019), Event2Mind (Rashkin et al., 2018), MCScript 2.0 (Ostermann et al., 2019), SWAG (Zellers et al., 2018), HellaSWAG (Zellers et al., 2019), Story Cloze Test (Mostafazadeh et al., 2017), CommonsenseQA (Talmor et al., 2018) and CommonGen (Lin et al., 2019b) are released to reasoning over external knowledge besides the inputs for question answering or generation. Rajani et al. (2019) explores adding human-written explanations to solve the problem. Lin et al. (2019a) constructs 2021 schema graphs from ConceptNet to reason over relevant commonsense knowledge. lv et al. (2020) focuses on automatically extracting evidence from heterogeneous external knowledge and reasoning over the extracted evidence to study this problem. Consider qui"
2020.coling-main.289,Q13-1005,0,0.150251,"lti-level alignment and semantic parsing tasks. We conduct experiments on a publicly available multi-lingual semantic parsing dataset ATIS and a newly constructed dataset. Experimental results show that our model outperforms state-of-the-art methods on both datasets. 1 Introduction 1 The goal of semantic parsing is to convert a natural language sentence to an executable logical form, which has been studied in the past few years and used on various applications, such as question answering (Kwiatkowski et al., 2011), task-oriented dialog systems (Yih et al., 2015) and interpreting instructions (Artzi and Zettlemoyer, 2013). Due to the importance of semantic parsing, various approaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to convert the question from different languages into the corresponding lambda calculus. For multi-lingual semantic"
2020.coling-main.289,P04-3031,0,0.214096,"s, 491 validation instances, and 448 test instances. Each pair contains a question and the lambda-calculus expression with the identified values for the variables of date, time, city, aircraft code, airport, airline, and number. The corpus was translated into Chinese with segmentation from (Susanto and Lu, 2017). For our MLSP dataset introduced in Section 3, we randomly split the data into 0.8/0.1/0.1 as train/dev/test sets in our model. In pretraining, we use the English-Chinese translation corpus, News Commentary v12 of WMT 2017 (Bojar et al., 2017). The English corpus is tokenized by NLTK (Bird and Loper, 2004) and the Chinese corpus is tokenized by Jieba segmenter4 . In space-level and word-level alignment, we use the unsupervised corpus of Wikipedia5 . We also randomly sample the same number of sentence pairs as the MT dataset used as the negative samples in sentence level alignment experiment. In experiment of word level alignment, we also construct a simple bilingual lexicon dictionary by translating the words contained in the English version into Chinese. We randomly collect 1k word pairs as bilingual lexicons. If the word pair appears in the sentence pair, we will mark their positions with lab"
2020.coling-main.289,Q17-1010,0,0.258511,"can align the sentence representation space of different languages to help our model learn shared semantic information. Word-Level Alignment Space-level alignment strategy can align the distribution space of the two languages. However, the shared semantic information is not aligned. In this section, we will introduce our word level alignment strategy to map monolingual word embedding into shared cross-lingual semantic space with the dictionary of bilingual lexicons. The model is first initialized with a pretrained word embedding matrix, trained by word2vec based methods (Mikolov et al., 2013; Bojanowski et al., 2017) in the two different languages. Here we define the two word embedding matrices, XE = R|XE |∗d in English and XC = R|XC |∗d in Chinese, d is the dimension of word embedding. The word embedding matrix in each language is pretrained respectively, embeddings of words that have the same meanings are unaligned, which will increase difficulty to encode sentences in our model. Thus, in order to map XE and XC into a shared semantic space, we define two linear transformation matrices W E and W C using as the multi-lingual projection. The matrices apply a linear transformation on XE and XC to align thei"
2020.coling-main.289,P18-1071,0,0.212979,"Missing"
2020.coling-main.289,P16-1004,0,0.378775,", we will release this dataset to help the research of multi-lingual semantic parsing tasks. • We conduct an experiment on ATIS and our dataset. Experimental results show that our model achieves new state-of-the-art results on both datasets. 2 Model In this section, we will first briefly introduce the basic sequence-to-sequence (S2S) model as our baseline model. Then, we introduce the architecture of our Multi-level Alignment pretraining for multi-lingual Semantic Parsing (MASP) model. 2.1 S2S Model for Semantic Parsing The S2S model has been successfully used in recent semantic parsing task (Dong and Lapata, 2016). The input of the model is a natural language question q = [x1 , x2 ...x|q |] and output is a logical form sequence l = [y1 , y2 , ...y|l |]. The tokens of the question q are fed one-by-one into the encoder, producing a sequence of encoder hidden states h = [h1 , h2 , ...h|q |]. In the decoding process, at each time step t, the decoder computes the attention distribution to obtain a context vector ct as follows: eti = uT f (We [hi ; st ] + be ); et ati = P|q|i t j=1 ej ; ct = |q| X ati hi (1) i=1 where f is a non-linear function, and we use tanh here. u, We and be are parameters. st is the de"
2020.coling-main.289,P18-1068,0,0.0251881,"Missing"
2020.coling-main.289,W17-2607,0,0.0140653,"e the semantic parsing performance. And also we see that our model achieves state-of-the-art results on all the results, which shows the effectiveness and robustness of our method. This experiment illustrates that our methods can be used in real world scenarios with the help of existing machine translator. 5 Related Work Semantic parsing, as an important task in natural language understanding, has attracted significant attention in the research and industry. Recently, various semantic parsing models have been proposed such as (Kwiatkowski et al., 2011; Xiao et al., 2017; Yin and Neubig, 2017; Fan et al., 2017; Dong and Lapata, 2016; Chen et al., 2018; Dong and Lapata, 2018). Kwiatkowski et al. (2011) propose a combinatory categorical grammar induction technique for semantic parsing. Xiao et al. (2017; Yin and Neubig (2017) use grammar and syntax information to improve semantic parsing models. (Fan et al., 2017) apply a transfer learning method to semantic parsing. Dong and Lapata (2016) propose a tree-based decoder to model structure of logical forms. Chen et al. (2018) translate the decode process as a sequence of actions with a sequence-to-sequence model. Recently, Dong and Lapata (2018) propose"
2020.coling-main.289,H90-1021,0,0.152464,"ing method to align the space level, word level and sentence level semantic representations for different languages. We design an adversarial training method to align the space level representation using unsupervised data. And to align the semantic level representation of parallel corpus in different languages, we use machine translation corpus and bilingual tokens to learn a shared cross-lingual encoder for our semantic parsing model. To better evaluate our method, we construct an open domain multi-lingual semantic parsing dataset, since most existing multi-lingual semantic parsing datasets (Hemphill et al., 1990; Zettlemoyer and Collins, 2012) are for specific domain and relatively small in scale. The main contributions of this paper are: • We design a multi-level alignment pretraining method to pretrain the multi-lingual semantic parsing model. • We construct a new multi-lingual semantic parsing dataset on open domain, we will release this dataset to help the research of multi-lingual semantic parsing tasks. • We conduct an experiment on ATIS and our dataset. Experimental results show that our model achieves new state-of-the-art results on both datasets. 2 Model In this section, we will first briefl"
2020.coling-main.289,P16-1002,0,0.0611579,"d dataset. Experimental results show that our model outperforms state-of-the-art methods on both datasets. 1 Introduction 1 The goal of semantic parsing is to convert a natural language sentence to an executable logical form, which has been studied in the past few years and used on various applications, such as question answering (Kwiatkowski et al., 2011), task-oriented dialog systems (Yih et al., 2015) and interpreting instructions (Artzi and Zettlemoyer, 2013). Due to the importance of semantic parsing, various approaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to convert the question from different languages into the corresponding lambda calculus. For multi-lingual semantic parsing, previous works such as Jie and Lu (2014) and Susanto and Lu (2017) study it from different perspectives. Jie and Lu (2014) train the model for"
2020.coling-main.289,C14-1122,0,0.078446,"ic parsing, various approaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to convert the question from different languages into the corresponding lambda calculus. For multi-lingual semantic parsing, previous works such as Jie and Lu (2014) and Susanto and Lu (2017) study it from different perspectives. Jie and Lu (2014) train the model for each language respectively and use ensemble method to combine the models on a multi-lingual semantic parsing dataset. Susanto and Lu (2017) propose a hybrid combination method to model multi-source input. Both of them need enough multi-lingual semantic parsing data for training. However, it is very hard to collect enough multi-lingual semantic parsing data. EN ZH LF Who is the director of Inception? 谁是电影Inception的导演 λx.f ilm f ilm director(Inception, x) Table 1: An example of our multi-lingua"
2020.coling-main.289,D11-1140,0,0.172424,"as supervision information to align the semantic of different languages. Finally, we jointly train the multi-level alignment and semantic parsing tasks. We conduct experiments on a publicly available multi-lingual semantic parsing dataset ATIS and a newly constructed dataset. Experimental results show that our model outperforms state-of-the-art methods on both datasets. 1 Introduction 1 The goal of semantic parsing is to convert a natural language sentence to an executable logical form, which has been studied in the past few years and used on various applications, such as question answering (Kwiatkowski et al., 2011), task-oriented dialog systems (Yih et al., 2015) and interpreting instructions (Artzi and Zettlemoyer, 2013). Due to the importance of semantic parsing, various approaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to con"
2020.coling-main.289,D14-1162,0,0.0836632,"ord-level alignment experiment. For ATIS, the pre-processing is the same as (Dong and Lapata, 2016), which 3 https://www.bing.com/translator https://github.com/fxsjy/jieba 5 https://dumps.wikimedia.org/ 4 3251 replace entities with their type name. To evaluate our method in situations when there is not an annotated multi-lingual semantic parsing dataset, we translate the English semantic parsing corpus by the open translation service of Microsoft. This is expected to be a common scenario in practice. 4.2 Settings We set the vocabulary size to 50k for both languages in our model. We use Glove (Pennington et al., 2014) 6B and Fasttext pretrained Chinese (Bojanowski et al., 2017) as English and Chinese pretrained word embedding. For words in vocabulary which do not have pretrained embeddings, we assign them uniform randomized values. The size of the word embedding is set to 300. During training, we update all word embeddings. We use accuracy on the development set to implement early stopping. Parameters are randomly initialized from a uniform distribution (-0.01, 0.01). For regularization, we use dropout and set the dropout rate to 0.5. Dimensions of hidden vectors in encoder and decoder are 300. α in joint"
2020.coling-main.289,P17-1161,0,0.0250298,"o model multi-source input. Both of them need enough multi-lingual semantic parsing data for training. However, it is very hard to collect enough multi-lingual semantic parsing data. EN ZH LF Who is the director of Inception? 谁是电影Inception的导演 λx.f ilm f ilm director(Inception, x) Table 1: An example of our multi-lingual semantic parsing dataset, including a lambda calculus (LF) with the English (EN) and Chinese (ZH) question. Recently, various pretraining methods have been successfully used to solve the labeled data insufficient problem in different tasks. In these methods, unsupervised data (Peters et al., 2017; Alec Radford, 2018; Devlin et al., 2018) or richly supervised data (McCann et al., 2017; Lample and Conneau, 2019) from other tasks are used to pretrain their models and achieve significant performance improvement in different tasks. 1 This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. License details: http: 3246 Proceedings of the 28th International Conference on Computational Linguistics, pages 3246–3256 Barcelona, Spain (Online), December 8-13, 2020 In this paper, we propose a multi-level alignment pretraining meth"
2020.coling-main.289,P17-1099,0,0.0180678,"coder computes the attention distribution to obtain a context vector ct as follows: eti = uT f (We [hi ; st ] + be ); et ati = P|q|i t j=1 ej ; ct = |q| X ati hi (1) i=1 where f is a non-linear function, and we use tanh here. u, We and be are parameters. st is the decoder hidden state at step t. The context vector ct is used to compute the generation distribution Pv of the target vocabulary with the hidden state st : Pv = sof tmax(Wp (W [st ; ct ] + b) + bp ) (2) where Wp , W, b, bp are parameters. In particular, to tackle out-of-vocabulary words, we incorporate the same copy mechanism as in (See et al., 2017) in our decoder. Attention score ai is used as probability distribution of the copy mechanism over the source words. The copy distribution Pc is defined as follows: X Pc (yt ) = ati (3) i:xi =yt To combine the copy distribution with the generation distribution, we use a gate gc to choose whether to copy from q or generate from the target vocabulary: gc = σ(W ∗ [ct ; st ; z t−1 ] + b∗ ) 3247 (4) where vectors W ∗ , b∗ are parameters. z t−1 is the word embedding of the previous word. We get final distribution score on each step t: Pf (yt ) = (1 − gc )Pv (yt ) + gc Pc (yt ) (5) where Pf (yt ) is"
2020.coling-main.289,P17-2007,0,0.319329,"proaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to convert the question from different languages into the corresponding lambda calculus. For multi-lingual semantic parsing, previous works such as Jie and Lu (2014) and Susanto and Lu (2017) study it from different perspectives. Jie and Lu (2014) train the model for each language respectively and use ensemble method to combine the models on a multi-lingual semantic parsing dataset. Susanto and Lu (2017) propose a hybrid combination method to model multi-source input. Both of them need enough multi-lingual semantic parsing data for training. However, it is very hard to collect enough multi-lingual semantic parsing data. EN ZH LF Who is the director of Inception? 谁是电影Inception的导演 λx.f ilm f ilm director(Inception, x) Table 1: An example of our multi-lingual semantic parsing dataset"
2020.coling-main.289,N18-1059,0,0.0420539,"Missing"
2020.coling-main.289,P15-1128,0,0.112059,"erent languages. Finally, we jointly train the multi-level alignment and semantic parsing tasks. We conduct experiments on a publicly available multi-lingual semantic parsing dataset ATIS and a newly constructed dataset. Experimental results show that our model outperforms state-of-the-art methods on both datasets. 1 Introduction 1 The goal of semantic parsing is to convert a natural language sentence to an executable logical form, which has been studied in the past few years and used on various applications, such as question answering (Kwiatkowski et al., 2011), task-oriented dialog systems (Yih et al., 2015) and interpreting instructions (Artzi and Zettlemoyer, 2013). Due to the importance of semantic parsing, various approaches have been proposed for this task, such as (Kwiatkowski et al., 2011; Jia and Liang, 2016; Dong and Lapata, 2018; Chen et al., 2018). However, most existing methods only handle monolingual semantic parsing, while in real world applications such as Chatbot and search engine, we generally need to handle multi-lingual semantic parsing. Table 1 shows an example of the multi-lingual semantic parsing task, and the task aims to convert the question from different languages into t"
2020.coling-main.289,P17-1041,0,0.0601316,"LSP effectively improve the semantic parsing performance. And also we see that our model achieves state-of-the-art results on all the results, which shows the effectiveness and robustness of our method. This experiment illustrates that our methods can be used in real world scenarios with the help of existing machine translator. 5 Related Work Semantic parsing, as an important task in natural language understanding, has attracted significant attention in the research and industry. Recently, various semantic parsing models have been proposed such as (Kwiatkowski et al., 2011; Xiao et al., 2017; Yin and Neubig, 2017; Fan et al., 2017; Dong and Lapata, 2016; Chen et al., 2018; Dong and Lapata, 2018). Kwiatkowski et al. (2011) propose a combinatory categorical grammar induction technique for semantic parsing. Xiao et al. (2017; Yin and Neubig (2017) use grammar and syntax information to improve semantic parsing models. (Fan et al., 2017) apply a transfer learning method to semantic parsing. Dong and Lapata (2016) propose a tree-based decoder to model structure of logical forms. Chen et al. (2018) translate the decode process as a sequence of actions with a sequence-to-sequence model. Recently, Dong and Lap"
2020.coling-main.289,D18-1194,0,0.0267339,"Missing"
2020.coling-main.289,P18-2107,0,0.0175689,"ensemble method to combine outputs from parsers for certain languages to apply on multi-lingual semantic parsing. Zhang et al. (2018) use a sequence-to-sequence model to map the questions in the source language into decompositional semantic representations in the target language. In Susanto and Lu (2017)’s work, they propose a combination method to combine questions in different language simultaneously for multi-source input and achieve promising improvement on ATIS (Hemphill et al., 1990). They also explore different architectures for single-source input without their combination mechanism. Zou and Lu (2018) propose a method to learn a cross lingual representation and use it in their semantic parsing model (Zettlemoyer and Collins, 2012). 6 Conclusion In this paper, we propose a multi-lingual semantic parsing model, which is first pretrained using a multilevel alignment mechanism, and then we jointly train the multi-lingual semantic parsing and multi-level alignment tasks. Most existing multi-lingual semantic parsing datasets are based on specific domain, to better evaluate our method on open domain, we annotate a relative large scale multi-lingual semantic parsing dataset on open domain. Experim"
2020.emnlp-main.193,I08-2115,0,0.0513539,"ies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization of their scoring functions. Schuster et al. (2019) indicate that simple provenance-based detection methods are insufficient for solving the problem and"
2020.emnlp-main.193,2021.ccl-1.108,0,0.0824142,"Missing"
2020.emnlp-main.193,P17-1161,0,0.0592401,"Missing"
2020.emnlp-main.193,N18-1074,0,0.060846,"Missing"
2020.emnlp-main.193,P19-3019,0,0.0188039,"evaluate on datasets produced by both GPT-2 and GROVER. Advances in generative models have promoted the development of detection methods. Previous studies in the field of DeepFake detection of generated text are dominated by deep-learning based document classification models and studies about discriminating features of generated text. GROVER (Zellers et al., 2019) detects generated text by a fine-tuned model of the generative model itself. Ippolito et al. (2019) fine-tune the BERT model for discrimination and explore how sampling strategies and text excerpt length affect the detection. GLTR (Gehrmann et al., 2019) develops a statistical method of computing per-token likelihoods and visualizes histograms over them to help deepfake detection. Badaskar et al. (2008) and P´erez-Rosas et al. (2017) study language distributional features including n-gram frequencies, text coherence and syntax features. Vijayaraghavan et al. (2020) study the effectiveness of different numeric representations (e.g., TFIDF and Word2Vec) and different neural networks (e.g., ANNs, LSTMs) for detection. Bakhtin et al. (2019) tackle the problem as a ranking task and study about the cross-architecture and cross-corpus generalization"
2020.emnlp-main.320,2020.acl-main.499,0,0.0209775,"rounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification"
2020.emnlp-main.320,D19-1565,0,0.0399722,"Missing"
2020.emnlp-main.320,D16-1146,0,0.0382263,"Missing"
2020.emnlp-main.320,N19-1423,0,0.173838,"ency between sentencelevel and token-level predictions (§3.2) and textual knowledge from literal definitions of propaganda techniques (§3.3). At last, we describe the training and inference procedures (§3.4). 3.1 Base Model To better exploit the sentence-level information and further benefit token-level prediction, we develop a fine-grained multi-task method as our base model, which makes predictions for 18 propaganda techniques at both sentence level and token level. Inspired by the success of pre-trained language models on various natural language processing downstream tasks, we adopt BERT (Devlin et al., 2019) as the backbone model here. For each input sentence, the sequence is modified as “[CLS]sentence tokens[SEP ]”. Specifically, on top of BERT, we add 19 binary classifiers for finegrained sentence-level predictions, and one 19-way classifier for token-level predictions, where all classifiers are implemented as linear layers. At sentence level, we perform multiple binary classifications and this can further support leveraging declarative knowledge. The last representation of the special token [CLS] which is regarded as a summary of the semantic content of the input, is adopted to perform multipl"
2020.emnlp-main.320,Q15-1027,0,0.0123863,"ta. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural networ"
2020.emnlp-main.320,D19-1405,0,0.0184536,"h first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need a lot of training data and are not interpretable. On the other hand, logicbased expert systems are interpretable and require less or no training data. It is appealing to leverage the advantages from both worlds. In NLP community, the injection of logic to neural network can be generally divided into two groups. Methods in the first group regularize neural network with logicdriven loss functions (Xu et al., 2018; Fischer et al., 2019; Li et al., 2019). For example, Rockt¨aschel et al. (2015) target on the problem of knowledge base completion. After extracting and annotating propositional logical rules about relations in knowledge graph, they ground these rules to facts from knowledge graph and add a differentiable training loss function. Kruszewski et al. (2015) map text to Boolean representations, and derive loss functions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number o"
2020.emnlp-main.320,P19-1028,0,0.154388,"ugh creating new examples when positive causal relationship between two cause-effect questions holds. The second group is to incorporate logic-specific modules into the inference process (Yang et al., 2017; Dong et al., 2019). For example, Rockt¨aschel and Riedel (2017) target at the problem of knowledge base completion, and use neural unification modules to recursively construct model similar to the backward chaining algorithm of Prolog. Evans and Grefenstette (2018) develop a differentiable model of forward chaining inference, where weights represent a probability distribution over clauses. Li and Srikumar (2019) inject logic-driven neurons to existing neural networks by measuring the degree of the head being true measured by probabilistic soft logic (Kimmig et al., 2012). Our approach belongs to the first direction, and to the best of knowledge our work is the first one that augments neural network with logical knowledge for propaganda detection. 6 Conclusion In this paper, we propose a fine-grained multitask learning approach, which leverages declarative knowledge to detect propaganda techniques in news articles. Specifically, the declarative knowledge is expressed in both first-order logic and natu"
2020.emnlp-main.320,D17-1317,0,0.0842897,"fluence an audience. 4. Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, h"
2020.emnlp-main.320,N15-1118,0,0.421682,"Missing"
2020.emnlp-main.320,N18-1074,0,0.0442108,"njection of first-order logic into neural networks. We will describe related studies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of conven"
2020.emnlp-main.320,D18-1215,0,0.0256681,"tions based on implication at Boolean level for entailment detection. Demeester et al. (2016) propose lifted regularization for knowledge base completion to improve the logical loss functions to be independent of the number of grounded instances and to further extend to unseen constants, The basic idea is that hypernyms have ordering relations and such relations correspond to component-wise comparison in semantic vector space. Hu et al. (2016) introduce a teacher-student model, where the teacher model is a rule-regularized neural network, whose predictions are used to teach the student model. Wang and Poon (2018) generalize virtual evidence (Pearl, 2014) to arbitrary potential functions over inputs and outputs, and use deep probabilistic logic to integrate indirection supervision into neural networks. More recently, Asai and Hajishirzi (2020) regularize question answering systems with symmetric consistency and symmetric consistency. The former creates a symmetric question by replacing words with their antonyms in comparison question, while the latter is for causal reasoning questions through creating new examples when positive causal relationship between two cause-effect questions holds. The second gr"
2020.emnlp-main.320,P17-2067,0,0.193758,". Slogan: A brief and striking phrase that may include labeling and stereotyping. Figure 1: Examples of propagandistic texts, and definitions of corresponding propaganda techniques (Bold denotes propagandistic texts). Introduction ∗ “To all Americans tonight, in all our cities and towns, I make this promise: We Will Make America Strong Again. We Will Make America Proud Again. We Will Make America Safe Again. And We Will Make America Great Again 3, 4.” thanks to the recent release of Propaganda Techniques Corpus (Da San Martino et al., 2019). Different from earlier works (Rashkin et al., 2017; Wang, 2017) that mainly study propaganda detection at a coarse-grained level, namely predicting whether a document is propagandistic or not, the fine-grained propaganda detection requires to identify the tokens of particular propaganda techniques in news articles. Da San Martino et al. (2019) propose strong baselines in a multi-task learning manner, which are trained by binary detection of propaganda at sentence level and fine-grained propaganda detection over 18 techniques at token level. Such data-driven methods have the merits of convenient end-to-end learning and strong generalization, however, they"
2020.emnlp-main.320,D19-1216,0,0.0257362,"tudies in these two directions. Fake news detection draws growing attention as the spread of misinformation on social media becomes easier and leads to stronger influence. Various types of fake news detection problems are intro3901 duced. For example, there are 4-way classification of news documents (Rashkin et al., 2017), and 6way classification of short statements (Wang, 2017). There are also sentence-level fact checking problems with various genres of evidence, including natural language sentences from Wikipedia (Thorne et al., 2018), semi-structured tables (Chen et al., 2020), and images (Zlatkova et al., 2019; Nakamura et al., 2019). Our work studies propaganda detection, a fine-grained problem that requires tokenlevel prediction over 18 fine-grained propaganda techniques. The release of a large manually annotated dataset (Da San Martino et al., 2019) makes the development of large neural models possible, and also triggers our work, which improves a standard multi-task learning approach by augmenting declarative knowledge expressed in both first-order logic and natural language. Neural networks have the merits of convenient end-to-end training and good generalization, however, they typically need"
2020.emnlp-main.320,P16-1228,0,\N,Missing
2020.emnlp-main.467,P19-1620,0,0.165147,"tream model performance. Question data augmentation (QDA) aims to automatically generate context-relevant questions to further improve the model performance for the above tasks (Yang et al., 2019; Dong et al., 2019). Existing QDA methods mainly employ the round-trip 2 It can also be a document span or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausibl"
2020.emnlp-main.467,W05-0909,0,0.14492,"Missing"
2020.emnlp-main.467,K16-1002,0,0.0209034,"We compare our CRQDA against the following baselines: (1) EDA (Wei and Zou, 2019): it augments question data by performing synonym replacement, random insertion, random swap, or random deletion operation. We implement EDA with their source code5 to synthesize a new question data for each question of SQuAD 2.0; (2) Back-Translation (Yu et al., 2018; Prabhumoye et al., 2018): it uses machine translation model to translate questions into French and back into English. We implement Back-Translation based on the source code6 to generate a new question data for each original question; (3) Text-VAE (Bowman et al., 2016; Liu et al., 2019a): it uses RNNbased VAE to generate a new question data for each question of SQuAD 2.0. The implementation is based on the source code7 ; (4) AE with Noise: it uses the same autoencoder of CRQDA for question data rewriting. The only difference is that the autoencoder cannot utilize the MRC gradient but only uses a noise (sampled from Gaussian distribution) to revise the question embedding. This experiment is designed to show necessity of the pre-trained MRC. (5) 3M synth (Alberti et al., 2019): it employs round-trip consistency technique to synthesize 3M questions on SQuAD 2"
2020.emnlp-main.467,P17-1123,0,0.283174,"nd machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-paragraph pair, existing textual DA methods th"
2020.emnlp-main.467,P17-2090,0,0.0212487,"e processing (NLP) tasks remains a challenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in N"
2020.emnlp-main.467,N10-1086,0,0.0461113,"te and Sefara, 2019) and paraphrasing (Kumar et al., 2019) are proposed to generate new textual samples. All the methods mentioned above usually generate individual sentences separately. For QDA of MRC, QG, and QNLI tasks, these DA approaches cannot guarantee the generating question are relevant to the given paragraph. In order to generate contextrelevant answerable and unanswerable questions, our CRQDA method utilizes a pre-trained MRC as guidance to revise the question in continuous embedding space, which can be seen as a special constrained paraphrasing method for QDA. Question generation (Heilman and Smith, 2010; Du et al., 2017; Zhao et al., 2018; Zhang and 5799 Bansal, 2019) is attracting attention in the field of natural language generation (NLG). However, most previous works are not designed for QDA. That is, they do not aim to generate context-relevant questions for improving downstream model performance. Compared to QG, QDA is relatively unexplored. Recently, some works (Alberti et al., 2019; Dong et al., 2019) utilize round-trip consistency technique to synthesize answerable questions. They first use a generative model to generate the question with the paragraph and answer as model input, and"
2020.emnlp-main.467,P17-1147,0,0.0344156,"Missing"
2020.emnlp-main.467,N18-2072,0,0.11471,"sks remains a challenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC"
2020.emnlp-main.467,N19-1363,0,0.0219264,"les, including using variational autoencodes (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014), generative adversarial networks (GANs) (Tanaka and Aranha, 2019), and pretrained language generation models (Radford et al., 2019; Kumar et al., 2020; Anaby-Tavor et al., 2020). Back-translation (Sennrich et al., 2016; Yu et al., 2018) is also a major way for textual DA, which uses machine translation model to translate English sentences into another language (e.g., French), and back into English. Besides, data noising techniques (Xie et al., 2017; Marivate and Sefara, 2019) and paraphrasing (Kumar et al., 2019) are proposed to generate new textual samples. All the methods mentioned above usually generate individual sentences separately. For QDA of MRC, QG, and QNLI tasks, these DA approaches cannot guarantee the generating question are relevant to the given paragraph. In order to generate contextrelevant answerable and unanswerable questions, our CRQDA method utilizes a pre-trained MRC as guidance to revise the question in continuous embedding space, which can be seen as a special constrained paraphrasing method for QDA. Question generation (Heilman and Smith, 2010; Du et al., 2017; Zhao et al., 201"
2020.emnlp-main.467,2020.lifelongnlp-1.3,0,0.0763259,"hub.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to"
2020.emnlp-main.467,Q19-1026,0,0.0194933,"t can also be a document span or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausible answers. Inspired by the recent progress in controllable text revision and text attribute transfer (Wang et al., 2019; Liu et al., 2020), we propose a new QDA method called Controllable Rewriting based Question Data Augmentation (CRQDA), which can generate both new context"
2020.emnlp-main.467,2021.ccl-1.108,0,0.0566718,"Missing"
2020.emnlp-main.467,P16-1009,0,0.253175,"iverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. G"
2020.emnlp-main.467,P02-1040,0,0.106571,"Missing"
2020.emnlp-main.467,W18-5446,0,0.032856,"Missing"
2020.emnlp-main.467,P18-1080,0,0.0249486,"data, resulting in about 220K data samples (including the original data samples). The hyperparameter of βs , βt , βa , βb , and max-step are set to 0.9, 0.5, 0.5, 0.99, and 5, respectively. 4 https://github.com/huggingface/ transformers. Baselines We compare our CRQDA against the following baselines: (1) EDA (Wei and Zou, 2019): it augments question data by performing synonym replacement, random insertion, random swap, or random deletion operation. We implement EDA with their source code5 to synthesize a new question data for each question of SQuAD 2.0; (2) Back-Translation (Yu et al., 2018; Prabhumoye et al., 2018): it uses machine translation model to translate questions into French and back into English. We implement Back-Translation based on the source code6 to generate a new question data for each original question; (3) Text-VAE (Bowman et al., 2016; Liu et al., 2019a): it uses RNNbased VAE to generate a new question data for each question of SQuAD 2.0. The implementation is based on the source code7 ; (4) AE with Noise: it uses the same autoencoder of CRQDA for question data rewriting. The only difference is that the autoencoder cannot utilize the MRC gradient but only uses a noise (sampled from Ga"
2020.emnlp-main.467,D19-1670,0,0.254005,"llenging problem. Unlike the general image DA techniques such as rotation and cropping, it is more difficult to synthesize new high-quality and diverse text. ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset will be available at https: //github.com/dayihengliu/CRQDA. Recently, some textual DA techniques have been proposed for NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model"
2020.emnlp-main.467,P18-2124,0,0.269168,"or NLP, which mainly focus on text classification and machine translation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-par"
2020.emnlp-main.467,D16-1264,0,0.482031,"s computed by: J (q, qˆ0 ) = count(wq ∩ wqˆ) , count(wq ∪ wqˆ) (11) here wq is the word in q and wqˆ is the word in qˆ0 . The whole question rewriting procedure is summarized in Algorithm 1. 4 Experiments In this section, we describe the experimental details and results. We first conduct the experiment on the SQuAD 2.0 dataset (Rajpurkar et al., 2018) to compare CRQDA with other strong baselines, which is 5802 reported in § 4.1. The ablation study and further analysis are provided in § 4.2. Then we evaluate our method on additional two tasks including question generation on SQuAD 1.1 dataset (Rajpurkar et al., 2016) in § 4.3, and question-answering language inference on QNLI dataset (Wang et al., 2018) in § 4.4. Methods BERTlarge (Devlin et al., 2018) (original) + EDA (Wei and Zou, 2019) + Back-Translation (Yu et al., 2018) + Text-VAE (Liu et al., 2019a) + AE with Noise + 3M synth (Alberti et al., 2019) + UNANSQ (Zhu et al., 2019) + CRQDA (ours) EM 78.7 78.3 77.9 75.3 76.7 80.1 80.0 80.6 F1 81.9 81.6 81.2 78.6 79.8 82.8 83.0 83.3 Table 1: Comparison results on SQuAD 2.0. 4.1 SQuAD The extractive MRC benchmark SQuAD 2.0 dataset contains about 100,000 answerable questions and over 50,000 unanswerable quest"
2020.emnlp-main.467,Q19-1016,0,0.0495117,"Missing"
2020.emnlp-main.467,P18-1156,0,0.0298439,"Missing"
2020.emnlp-main.467,D18-1259,0,0.0378266,"Missing"
2020.emnlp-main.467,D19-1253,0,0.0920265,"Missing"
2020.emnlp-main.467,D18-1424,0,0.253689,"ation tasks. One way is directly modifying the text data locally with word deleting, word order changing, and word replacement (Fadaee et al., 2017; Kobayashi, 2018; Wei and Zou, 2019; Wu et al., 2019). Another popular way is to utilize the generative model to generate new text data, such as back-translation (Sennrich et al., 2016; Yu et al., 2018), data noising technique (Xie et al., 2017), and utilizing pre-trained language generation model (Kumar et al., 2020; Anaby-Tavor et al., 2020). Machine reading comprehension (MRC) (Rajpurkar et al., 2018), question generation (QG) (Du et al., 2017; Zhao et al., 2018) and, question-answering natural language inference (QNLI) (Demszky et al., 2018; Wang et al., 2018) are receiving attention in NLP community. MRC requires the model to find the answer given a paragraph2 and a question, while QG aims to generate the question for a given paragraph with or without a given answer. Given a question and a sentence in the relevant paragraph, QNLI requires the model to infer whether the sentence contains the answer to the question. Because the above tasks require the model to reason about the question-paragraph pair, existing textual DA methods that directly augment"
2020.emnlp-main.467,P19-1415,0,0.500464,"an or a passage. For notational simplicity, we use the “paragraph” to refer to it in the rest of this paper. 5798 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5798–5810, c November 16–20, 2020. 2020 Association for Computational Linguistics consistency (Alberti et al., 2019; Dong et al., 2019) to synthesize answerable questions. However, the round-trip consistency method is not able to generate context-relevant unanswerable questions, where MRC with unanswerable questions is a challenging task (Rajpurkar et al., 2018; Kwiatkowski et al., 2019). Zhu et al. (2019) firstly study unanswerable question DA, which relies on annotated plausible answer to constructs a small pseudo parallel corpus of answerable-to-unanswerable questions for unanswerable question generation. Unfortunately, most question answering (QA) and MRC datasets do not provide such annotated plausible answers. Inspired by the recent progress in controllable text revision and text attribute transfer (Wang et al., 2019; Liu et al., 2020), we propose a new QDA method called Controllable Rewriting based Question Data Augmentation (CRQDA), which can generate both new context-relevant answerabl"
2020.emnlp-main.484,D19-1252,1,0.908102,"LUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu"
2020.emnlp-main.484,D17-1302,0,0.176422,"e select 11 cross-lingual tasks in XGLUE, which are categorized into 3 groups: single-input understanding tasks, pair-input understanding tasks, and generation tasks. For each task, training set is only available in English. In order to obtain a good performance on XGLUE, a model should be able to learn how to do a task well using its English training set, and then transfer this ability to test sets in other languages. Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks. 2 https://commoncrawl.org/. Single-input Understanding Tasks POS Tagging (POS) Following (Kim et al., 2017), we select a subset of Universal Dependencies (UD) Treebanks (v2.5) (Zeman et al., 2019), which covers 18 languages. Accuracy (ACC) of the predicted POS tags is used as the metric. News Classification (NC) This task aims to predict the category given a news article. It covers 5 languages, including English, Spanish, French, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tas"
2020.emnlp-main.484,2020.acl-main.703,0,0.0930192,"Missing"
2020.emnlp-main.484,P19-4007,0,0.0660349,"Missing"
2020.emnlp-main.484,2021.ccl-1.108,0,0.046651,"Missing"
2020.emnlp-main.484,D18-1269,0,0.125055,"rench, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad titl"
2020.emnlp-main.484,N19-1423,0,0.68133,"oder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microso"
2020.emnlp-main.484,W03-0419,0,0.651554,"Missing"
2020.emnlp-main.484,D19-1382,0,0.266447,"understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1 1 Introduction Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b; Dong et al., 2019; Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019; Conneau and Lample, 2019; Huang et al., 2019; Conneau et al., 2019) and multimodal pre-trained models (Lu et al., 2019; Li et al., 2020; Chen et al., 2019; Zhou et al., 2020). In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, 1 The dataset is available at https://microsoft. github.io/XGLUE/, The code and model is available at https://github.com/microsoft/Unicoder where an NLP task often h"
2020.emnlp-main.484,N19-1131,0,0.0455297,"t News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric. 2.2.2 Pair-input Understanding Tasks MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric. XNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE. PAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric. Query-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad title, ad description, label>. The label indicates whether the ad is relevant to the query (Good), or not (Bad). We con6009 Task # of Languages |Train|en |Dev|avg"
2020.emnlp-main.484,2020.findings-emnlp.217,1,\N,Missing
2020.emnlp-main.505,D18-1443,0,0.0163097,"cle to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims"
2020.emnlp-main.505,P16-1154,0,0.168923,". All sub-layers are interconnected with residual connections (He et al., 2016) and layer normalization (Ba et al., 2016). Similarly, the decoder is also composed of a stack of N identical block. In addition to the two sub-layers in each encoder block, the decoder contains a third sub-layer which performs multi-head 6243 attention over the output of the encoder. Figure 2 (a) shows the architecture of the block in the decoder. BASE uses the pre-trained BERT-base model (Devlin et al., 2018) to initialize the parameters of the encoder. Also, it uses the transformer decoder with a copy mechanism (Gu et al., 2016), whose hidden size, the number of multi-head h, and the number of blocks N are the same as its encoder. 3.2.2 Keyphrase-Aware Headline Generation Model In order to explore more effective ways of incorporating keyphrase information into BASE, we design 5 variants of multi-source Transformer decoders. Article + Keyphrase. The basic idea is to add the keyphrase into the decoder directly. The keyphrase Xkey is represented as a sequence of word embeddings. As shown in Figure 2 (b), we add an extra sub-layer that performs multi-head attention over the Xkey in each block of the decoder. (n+1) Xdec ="
2020.emnlp-main.505,C18-1148,0,0.0418051,"Missing"
2020.emnlp-main.505,N16-1014,0,0.280999,"agnostic baselines as follows. (10) PT-NET, the original pointergenerator network (See et al., 2017) , which are widely used in text summarization and headline generation tasks. (11) SEASS (Zhou et al., 2017b), the GRU-based (Cho et al., 2014) sequence-tosequence model with selective encoding mechanism, which is widely used in text summarization. (12) Transformer + Copy (Vaswani et al., 2017; Gu et al., 2016), which has the same architecture hyperparameters as BASE, the only difference is that it does not use BERT to initialize the encoder. (13) BASE + Diverse, which applies diverse decoding (Li et al., 2016b) in beam search to BASE during inference to improve the generation diver6245 Method PT-GEN SEASS Transformer + Copy BASE BASE + Diverse BASE + Filter BASE + KEY BASE + AddFuse BASE + ParallelFuse BASE + StackFuse BASE + AddFuse + KEY BASE + ParallelFuse + KEY BASE + StackFuse + KEY K=1 35.66 31.20 38.91 42.09 39.44 43.53 44.30 43.74 43.97 43.12 43.09 43.87 ROUGE-1 K=3 39.82 34.52 43.80 45.40 45.83 41.52 47.07 47.36 47.28 47.63 46.82 47.70 47.71 K=5 41.59 35.98 45.72 47.21 47.89 43.89 49.08 49.46 49.69 49.74 49.16 49.84 49.96 K=1 19.80 14.82 21.85 24.10 20.82 25.27 25.98 25.20 25.32 24.66 24."
2020.emnlp-main.505,D15-1044,0,0.126297,"Missing"
2020.emnlp-main.505,P17-1099,0,0.177725,"(c) original article to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multih"
2020.emnlp-main.505,D16-1112,0,0.0197807,"est to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims to generate multiple independent headlines, which allows us to recommend news with different news headlines based on the interests of users. Besides, multi-headline generation can provide multiple hints for human news editors to assist them in writing news headlines. However, most existing methods (Takase et al., 2016; Ayana et al., 2016; Murao et al., 2019; Colmenares et al., 2019; Zhang et al., 2018) focus on single-headline generation. The headline generation process is treated as an one-to-one mapping (the input is an article and the output is a headline), which trains and tests the models without any additional guiding information or constraints. We argue that this may lead to two problems. Firstly, since it is reasonable to generate multiple headlines for the news, training to generate the single ground-truth might result in a lack of more detailed guidance. Even worse, a single ground-truth without"
2020.emnlp-main.505,W17-6001,0,0.029021,"e essential substring from a question and use this substring as the headline of the forums. Zhang et al. (2018) propose a method for question headline generation, which designs a dual-attention seq2seq model. However, most previous headline generation methods focus on one-to-one mapping, and the headline generation process is not controllable. In this work, we focus on the news multi-headline generation problem and design a keyphrase-aware headline generation method. Different information aware methods have been successfully used in natural language generation tasks (Zhou et al., 2017a, 2018; Wang et al., 2017), such as responses generation in the dialogue system. Similar to our task, responses generation in a dialogue system is also a one-to-many problem, Zhou et al. (2017a) propose a mechanism-aware seq2seq model for controllable response generation. They model different mechanisms as latent embeddings and learn the latent 6248 embeddings in their seq2seq model. Incorporating these mechanisms, their model can generate controllable responses. Zhou et al. (2018) propose a commonsense knowledge aware conversation generation method. More concretely, in the first stage, their model retrieves subgraphs"
2020.emnlp-main.505,N19-2011,0,0.0143952,"52.60 84.26 83.18 30.13 59.60 65.13 53.82 81.32 84.05 63.91 87.04 89.08 Table 1: Keyphrase Generation Results (2) Seq2Seq. Since our KeyAware News corpus contains the article-keyphrase pairs, we treat the keyphrase generation as a sequence-to-sequence learning task. We train the model BASE with article-keyphrase pairs. During inference, we use beam search with length penalty to generate ngrams (n = 2, 3, and 4) as the keyphrases. (3) Slot Tagging. Because the keyphrases also appear in the news articles, we can formulate the keyphrase generation task as a slot tagging task (Zhang et al., 2016; Williams, 2019). We finetune the BERT-base model to achieve that. Concretely, we use the output sequence of the model to predict the beginning and end position of the keyphrase in the article. During inference, we follow the answer span prediction method used in Seo et al. (2017) to predict n-grams (n = 2, 3, and 4) with the highest probabilities as the keyphrases. 4 Experiments 4.1 Keyphrase Generation In the first experiment, we evaluate the performance of three keyphrase generation methods: (a) unsupervised TF-IDF Ranking, (b) supervised sequence-to-sequence model (SEQ2SEQ), and (c) supervised slot taggin"
2020.emnlp-main.505,D16-1080,1,0.831187,"5 42.05 78.45 76.94 52.60 84.26 83.18 30.13 59.60 65.13 53.82 81.32 84.05 63.91 87.04 89.08 Table 1: Keyphrase Generation Results (2) Seq2Seq. Since our KeyAware News corpus contains the article-keyphrase pairs, we treat the keyphrase generation as a sequence-to-sequence learning task. We train the model BASE with article-keyphrase pairs. During inference, we use beam search with length penalty to generate ngrams (n = 2, 3, and 4) as the keyphrases. (3) Slot Tagging. Because the keyphrases also appear in the news articles, we can formulate the keyphrase generation task as a slot tagging task (Zhang et al., 2016; Williams, 2019). We finetune the BERT-base model to achieve that. Concretely, we use the output sequence of the model to predict the beginning and end position of the keyphrase in the article. During inference, we follow the answer span prediction method used in Seo et al. (2017) to predict n-grams (n = 2, 3, and 4) with the highest probabilities as the keyphrases. 4 Experiments 4.1 Keyphrase Generation In the first experiment, we evaluate the performance of three keyphrase generation methods: (a) unsupervised TF-IDF Ranking, (b) supervised sequence-to-sequence model (SEQ2SEQ), and (c) super"
2020.emnlp-main.505,P19-1100,0,0.0128883,"se-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity1 . 1 Introduction News Headline Generation is an under-explored subtask of text summarization (See et al., 2017; Gehrmann et al., 2018; Zhong et al., 2019). Unlike text summaries that contain multiple contextrelated sentences to cover the main ideas of a document, news headlines often contain a single short sentence to encourage users to read the news. Since one news article typically contains multiple keyphrases or topics of interest to different users, ∗ Work is done during internship at Microsoft Research Asia. 1 The source code will be available at https://github.com/ dayihengliu/KeyMultiHeadline. it is useful to generate multiple headlines covering different keyphrases for the news article. Multiheadline generation aims to generate multiple"
2020.emnlp-main.505,P17-1101,0,0.0957522,"ith stack-fusing, addition-fusing, and parallel-fusing mechanism, respectively. Based on BASE + StackFuse, BASE + AddFuse, and BASE + ParallelFuse, we further use the keyphrase as their additional inputs, like BASE + KEY. Then we obtain three additional variants (7) BASE + StackFuse + KEY, (8) BASE + AddFuse + KEY, and (9) BASE + ParallelFuse + KEY. In addition to BASE, We also compare four other keyphrase-agnostic baselines as follows. (10) PT-NET, the original pointergenerator network (See et al., 2017) , which are widely used in text summarization and headline generation tasks. (11) SEASS (Zhou et al., 2017b), the GRU-based (Cho et al., 2014) sequence-tosequence model with selective encoding mechanism, which is widely used in text summarization. (12) Transformer + Copy (Vaswani et al., 2017; Gu et al., 2016), which has the same architecture hyperparameters as BASE, the only difference is that it does not use BERT to initialize the encoder. (13) BASE + Diverse, which applies diverse decoding (Li et al., 2016b) in beam search to BASE during inference to improve the generation diver6245 Method PT-GEN SEASS Transformer + Copy BASE BASE + Diverse BASE + Filter BASE + KEY BASE + AddFuse BASE + Paralle"
2020.emnlp-main.505,W04-1013,0,\N,Missing
2020.emnlp-main.505,D14-1179,0,\N,Missing
2020.emnlp-main.505,N19-2010,0,\N,Missing
2020.emnlp-main.505,N19-1423,0,\N,Missing
2020.emnlp-tutorials.1,2020.acl-tutorials.8,0,0.152569,"ns, which are interpretable to developers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty i"
2020.emnlp-tutorials.1,P18-1043,0,0.0282424,"y conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 198"
2020.emnlp-tutorials.1,N16-3020,0,0.0292639,"natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al., 2019), reasoning over rules in natural language (Clark et al., 2020), and logical reasoning (Yu et al., 2020). Afterwards, we will review model interpretation methods, including post-hoc ones and intrinsic ones. Post-hoc methods aim to interpret what an existing model learned without making changes to the original model. We will cover saliency maps (Simonyan et al., 2013), local interpretable model-agnostic explanations (LIME) (Ribeiro et al., 2016), testing with concept activation vectors (TCAV) (Kim et al., 2018), and visual explanation generation (Hendricks et al., 2016). Intrinsic methods are that inherently interpretable (to some extent). We will cover attention (Bahdanau et al., 2014), interpretable CNN (Zhang et al., 2018), and neural 2 Dilemma: Interpretability vs. Performance (30 min.) will review post-hoc models and intrinsic models for interpretation, and discuss the dilemma of “interpretability versus performance”. module network (Andreas et al., 2016). We last summarize the content of this tutorial and discuss possible futur"
2020.emnlp-tutorials.1,N15-1118,0,0.0314425,"ng (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science e"
2020.emnlp-tutorials.1,2020.acl-tutorials.7,0,0.0323856,"ce; Outline Opening (15 min.) will describe the motivation and outline of this tutorial and give our definition on machine reasoning. • Du et al. (2020) - a survey on interpretable machine learning techniques; Symbolic Reasoning (20 min.) will review typical methods based on propositional logic and first order logic, respectively. • Chen and Yih (2020) - a tutorial on opendomain question answering, in which many work can be categorized as neural-evidence reasoning; Probabilistic Reasoning (20 min.) will review typical methods based on Bayesian Network and Markov Logic Network, respectively. • Sap et al. (2020) - a tutorial on commonsense reasoning for natural language processing. Neural-Symbolic Reasoning (40 min.) will review typical methods including knowledge graph reasoning, neural semantic parsing, neural module network and symbolic knowledge as constraints. 6 Tutorial Abstract Machine reasoning research aims to build interpretable AI systems that can solve problems or draw conclusions from what they are told (i.e. facts and observations) and already know (i.e. models, common sense and knowledge) under certain constraints. In this tutorial, we will (1) describe the Neural-Evidence Reasoning (4"
2020.emnlp-tutorials.1,P19-1028,0,0.0227561,"; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural"
2020.emnlp-tutorials.1,P18-1034,1,0.817959,"erentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box"
2020.emnlp-tutorials.1,N19-1421,0,0.0253981,"opers and users at concept level. The design of such symbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represe"
2020.emnlp-tutorials.1,N18-1074,0,0.0259547,"ymbolic functions in real applications are typically conducted by domain experts, thus these models cannot be easily extend to broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies bet"
2020.emnlp-tutorials.1,2020.emnlp-main.320,1,0.824035,"orot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretability versus performance” by showing the empirical success of pretrained models on natural language understanding challenges, including Grade 8 New York Regents science exam (Clark et al., 2019), discrete reasoning over natural language (Dua et al"
2020.emnlp-tutorials.1,D17-1060,0,0.0200163,"bolic reasoning system (1) integrates existing reasoning technologies with symbolic knowledge based on neural networks and (2) implements inference as a chain of differentiable modules, where each module represents a program with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned m"
2020.emnlp-tutorials.1,D18-1259,0,0.0285158,"o broader applications. Here, we review neural-evidence models that find external evidence and combine evidence with the input to make predictions. We group existing methods into three categories, including unstructured textual evidence retrieval models, structured fact evidence retrieval models, and iterative evidence retrieval models. Applications include open question answering (Chen and Yih, 2020), CommonsenseQA (Talmor et al., 2019), fact checking and verification (Thorne et al., 2018), inferential text generation (Rashkin et al., 2018; Sap et al., 2019), and multihop question answering (Yang et al., 2018). We then talk about the dilemma between blackbox neural networks with state-of-the-art performance and machine reasoning approaches with better interpretability. certainty. To alleviate this problem, probabilistic reasoning is proposed, which integrates probabilistic models with symbolic knowledge in a unified framework. In such systems, probabilistic models handle the uncertainty issue while the symbolic logic represents types, relations, and the complex dependencies between them. We will use Bayesian Network (Pearl, 1988) and Markov Logic Network (Richardson and Domingos, 2006) as two repre"
2020.emnlp-tutorials.1,2020.emnlp-main.558,0,0.0147533,"gram with a specific function. By doing these, such systems are usually more interpretable than black-box neural networks. We will review knowledge graph reasoning (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Wang et al., 2017; Glorot et al., 2013; Socher et al., 2013; Dong et al., 2014; Liu et al., 2016; Dettmers et al., 2018; Guo et al., 2019; Ren et al., 2020; Xiong et al., 2017; Dong et al., 2019; Rockt¨aschel and Riedel, 2017; Qu and Tang, 2019; K. Teru et al., 2020), neural semantic parsing (Dong and Lapata, 2016, 2018; Sun et al., 2018; Guo et al., 2018; Mao et al., 2019; Zhong et al., 2020), neural module network (Andreas et al., 2016; Hu et al., 2017; Gupta et al., 2020; Chen et al., 2020) and symbolic knowledge as constraints (Rocktaschel et al., 2015; Hu et al., 2016; Xu et al., 2018; Li and Srikumar, 2019; Wang et al., 2020) as four representative models. Dilemma: Interpretability vs. Performance Despite the appealing properties of the previously mentioned machine reasoning approaches in terms of interpretability, the reality is that the leading systems on open benchmarks, evaluated by accuracy, are typically black-box models. We will discuss this dilemma of “interpretabilit"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2020.findings-emnlp.217,P18-1015,0,0.0541138,"ch 2405 Method LEAD-3 (Nallapati et al., 2017) PTGEN (See et al., 2017) PTGEN+Coverage (See et al., 2017) S2S-ELMo (Edunov et al., 2019) Bottom-Up (Gehrmann et al., 2018) BERTSUMABS (Liu and Lapata, 2019) BERTSUMEXTABS (Liu and Lapata, 2019) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet ROUGE-1 ROUGE-2 ROUGE-L 40.42 36.44 39.53 41.56 41.22 41.72 42.13 42.12 43.33 43.68 17.62 15.66 17.28 18.94 18.68 19.39 19.60 19.50 20.21 20.64 36.67 33.42 36.38 38.47 38.34 38.76 39.18 39.01 40.51 40.72 Table 1: Results on the CNN/DailyMail test set. Method OpenNMT (Klein et al., 2017) Re3Sum (Cao et al., 2018) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet R-1 R-2 R-L 36.73 37.04 38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet out"
2020.findings-emnlp.217,P18-1177,0,0.142803,"tNet R-1 R-2 R-L 36.73 37.04 38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. F"
2020.findings-emnlp.217,P17-1123,0,0.322918,"Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against following baselines: OpenNMT (Klein et al., 2017) which 2405 Method LEAD-3 (Nallapati et al.,"
2020.findings-emnlp.217,N19-1409,0,0.0121283,"utput to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the bes"
2020.findings-emnlp.217,D18-1443,0,0.145819,"rmup steps, and the total fine-tune epoch are set to 512, 1000, and 10. We limit the length of the output to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The r"
2020.findings-emnlp.217,P17-4012,0,0.0209716,"res of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against following baselines: OpenNMT (Klein et al., 2017) which 2405 Method LEAD-3 (Nallapati et al., 2017) PTGEN (See et al., 2017) PTGEN+Coverage (See et al., 2017) S2S-ELMo (Edunov et al., 2019) Bottom-Up (Gehrmann et al., 2018) BERTSUMABS (Liu and Lapata, 2019) BERTSUMEXTABS (Liu and Lapata, 2019) MASS (Song et al., 2019) UniLM (Dong et al., 2019) ProphetNet ROUGE-1 ROUGE-2 ROUGE-L 40.42 36.44 39.53 41.56 41.22 41.72 42.13 42.12 43.33 43.68 17.62 15.66 17.28 18.94 18.68 19.39 19.60 19.50 20.21 20.64 36.67 33.42 36.38 38.47 38.34 38.76 39.18 39.01 40.51 40.72 Table 1: Results on the CNN/DailyMail test set. Method OpenNMT (Klein et al., 2017) Re3S"
2020.findings-emnlp.217,D19-1001,0,0.0248556,"RT (Lewis et al., 2019) uses the encoder-decoder structure to generate the original sentence with its spoiled input to denoise. In the BART decoder, the undamaged language model is learned thus brings improvement to NLG tasks. Natural language generation methods are typically based on the left-to-right or right-to-left language models and generate one token in each time step. These methods can not capture the information of future tokens. Recently, incorporating future information into language generation tasks has attracted the attention of researchers (Li et al., 2017; Serdyuk et al., 2018; Lawrence et al., 2019; Oord et al., 2018). Li et al. (2017) propose an actor-critic model which designs a value function as a critic to estimate the future success. In their method, they not only consider the MLE-based learning but also incorporate an RL-based value function into the decoder process. (Oord et al., 2018) do not predict future tokens directly but tried to model a density ratio to preserve the mutual information between context and future token. Serdyuk et al. (2018) point out traditional Recurrent Neural Networks (RNNs) may prefer to generate each token based on the recent tokens, it is hard to lear"
2020.findings-emnlp.217,2020.acl-main.703,0,0.276307,"Missing"
2020.findings-emnlp.217,D19-1387,0,0.114656,"hetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with fu"
2020.findings-emnlp.217,K16-1028,0,0.101321,"ext summarization datasets: (a) the non-anonymized version of the CNN/DailyMail dataset (See et al., 2017), and (b) Gigaword corpus (Rush et al., 2015). CNN/DailyMail We use Adam optimizer (Kingma and Ba, 2015) with a peak learning rate 1 × 10−4 . The batch size, warmup steps, and the total fine-tune epoch are set to 512, 1000, and 10. We limit the length of the output to between 45 and 110 tokens with a 1.2 length penalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et"
2020.findings-emnlp.217,N18-1202,0,0.0221085,"enalty during inference. We set beam size to 5 and remove the duplicated trigrams in beam search (Fan et al., 2017). We compare our ProphetNet against following baselines: LEAD-3 (Nallapati et al., 2016) which takes the first three sentences as the summary; PTGEN (See et al., 2017) which is Seq2Seq model incorporated with the pointer-generator network; PTGEN+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam op"
2020.findings-emnlp.217,D16-1264,0,0.076299,"2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we split the SQuAD 1.1 (Rajpurkar et al., 2016) dataset into training, development and test sets. We also report the results on the data split as did in Zhao et al. (2018), which reverses the development set and test set. The question generation task is typically formulated as a Seq2Seq problem. The input passage and the answer are packed as “answer [SEP] input passage” as input, and the question is used as the target output sequence. We fine-tune the ProphetNet model 10 epochs in the training set and report the results of the two kinds of data splits as mentioned above. The first 512 tokens of the passage are fed to the model. The peak le"
2020.findings-emnlp.217,D15-1044,0,0.367465,"Missing"
2020.findings-emnlp.217,P17-1099,0,0.120286,"N+Coverage (See et al., 2017) which introduce a coverage mechanism to PTGEN; BottomUp (Gehrmann et al., 2018) which employs a bottom-up content selector based on Seq2Seq model; S2S-ELMo (Edunov et al., 2019) which uses the pre-trained ELMo (Peters et al., 2018) representations. Besides, we also compare our method with several pre-training based strong baselines: BERTSUMABS (Liu and Lapata, 2019), MASS (Song et al., 2019), and UniLM (Dong et al., 2019). These pre-training-based strong baselines are all pre-trained on the same 16GB BookCorpus + English Wikipedia dataset as ProphetNet. Following See et al. (2017), we report the F1 scores of ROUGE-1, ROUGE-2 and ROUGEL (Lin, 2004). Du et al. (2017) The results are presented in Table 1. From the results, we can see that the ProphetNet achieves the best performances on all metrics. Gigaword We use Adam optimizer with a peak learning rate 1 × 10−4 . The batch size is set to 128 and warm up steps to 1000. We fine-tune model 10 epochs with future bigram prediction training. During inference, we set the length penalty to 1.0 and beam size to 4. We set the hyper-parameters according to the performance on the dev set. We compare our ProphetNet against followin"
2020.findings-emnlp.217,D19-1253,0,0.226997,"38.73 38.45 39.55 17.86 19.03 19.71 19.45 20.27 33.68 34.46 35.96 35.75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we s"
2020.findings-emnlp.217,D18-1424,0,0.262924,".75 36.57 Table 2: Results on Gigaword test set. R is short for ROUGE. implements the standard Seq2Seq model with attention mechanism; Re3Sum (Cao et al., 2018) which employs an extended Seq2Seq model to generate summaries based on the retrieved candidate summaries. And two pre-training based strong baselines: MASS (Song et al., 2019), and UniLM (Dong et al., 2019). The results are presented in Table 2. It can be observed that ProphetNet outperforms previous models on all metrics. Method CorefNQG (Du and Cardie, 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet MP-GSN (Zhao et al., 2018) SemQG (Zhang and Bansal, 2019) UniLM (Dong et al., 2019) ProphetNet B4 MTR R-L 15.16 18.37 21.63 23.91 16.38 20.76 23.08 25.80 19.12 22.65 25.04 26.60 20.25 24.20 25.57 27.54 46.68 51.09 52.26 44.48 48.91 52.03 53.65 Table 3: Results on SQuAD 1.1 test set (with reference of Du et al. (2017) tokenized). B4 is short for BLEU4, MTR is short for METEOR, and R-L is short for ROUGE-L. The same model is used to evaluate on the two different data splits. this task to further evaluate the ProphetNet model. Following Du et al. (2017), we split the SQuAD 1.1 (Rajpurkar et al., 2016) dataset into trainin"
2020.findings-emnlp.217,P02-1040,0,\N,Missing
2020.findings-emnlp.217,W05-0909,0,\N,Missing
2020.findings-emnlp.217,W04-1013,0,\N,Missing
2020.findings-emnlp.217,2020.tacl-1.5,0,\N,Missing
2020.findings-emnlp.370,P17-1171,0,0.490746,"es/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at bot"
2020.findings-emnlp.370,P84-1044,0,0.185057,"Missing"
2020.findings-emnlp.370,P18-1078,0,0.283266,"and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and pass"
2020.findings-emnlp.370,D18-1456,0,0.0158676,"mance isn’t as good as Reflection model. 5 Related Work Machine Reading Comprehension: Machine reading comprehension (Hermann et al., 2015; Chen et al., 2017; Rajpurkar et al., 2016; Clark and Gardner, 2018) is mostly based on the attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) that take as input hquestion, paragraphi, compute an interactive representation of them and predict the start and end positions of the answer. When dealing with no-answer cases, popular method is to jointly model the answer position probability and no-answer probability by a shared softmax normalizer (Kundu and Ng, 2018; Clark and Gardner, 2018; Devlin et al., 2019), or independently model the answerability as a binary classification problem (Hu et al., 2019; Yang et al., 2019; Liu et al., 2019). For long document processing, there are pipeline approaches of IR + Span Extraction (Chen et al., 2017), DecAtt + DocReader (Kwiatkowski et al., 2019), sliding window approach (Alberti et al., 2019) and recently proposed long sequence handling Transformers (Kitaev et al., 2020; Guo et al., 2019; Beltagy et al., 2020) Answer Verifier: Answer verifier (Tan et al., 2018; Hu et al., 2019) is proposed to validate the leg"
2020.findings-emnlp.370,P17-1055,0,0.0312392,"uding no-answer, yes/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search"
2020.findings-emnlp.370,Q19-1026,0,0.195755,"ure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover, there are richer answer types in the NQ task. In addition to indicating textual answer spans (long and short), the ‡ Corresponding author. https://ai.google.com/research/ NaturalQuestions/lead"
2020.findings-emnlp.370,N19-1423,0,0.557435,"this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover, there are richer an"
2020.findings-emnlp.370,2020.acl-main.604,1,0.724645,"Missing"
2020.findings-emnlp.370,2021.ccl-1.108,0,0.106043,"Missing"
2020.findings-emnlp.370,P18-2124,0,0.0424119,"We transform head features by scale to [0, 1], sqrt, log, minus mean then divided by standard deviation. for RoBERTa; then use sliding window approach to slice document into instances as described in Section 2.1. For NQ, since the document is quite long, we add special atomic markup tokens to indicate which part of the document the model is reading. 3.1 Implementation Our implementation is based on Huggingface Transformers (Wolf et al., 2019). All the pretrained models are large version (24 layers, 1024 hidden size, 16 heads, etc.). For MRC model training, we firstly finetune it on squad2.0 (Rajpurkar et al., 2018) data and then continue to finetune on NQ data. For Reflection model, we firstly leverage the MRC model to generate training data, and then finetune Reflection model which is initialized by MRC model parameters. We use one MRC model to deal with all answer types in NQ, but two Reflection models, one for long answer, the other for short. We manually tune the hyperparameters based on dev data F1 and submit best models to NQ organizer for leaderboard, list the best setting in Appendices. Experiments are performed on 4 NVIDIA Tesla P40 24GB cards, both MRC and Reflection model can be trained withi"
2020.findings-emnlp.370,D16-1264,0,0.0630444,"ning target are quite different from MRC model, further training will hurt the accuracy of answer prediction. This configuration save a lot memory and computation cost of prediction: all the data only need to pass through one Transformer. The results show it can improve most of the metrics. However, the [cls] representation in MRC model targets at answer types classification which include no-answer but not predicted wrong-ans, the performance isn’t as good as Reflection model. 5 Related Work Machine Reading Comprehension: Machine reading comprehension (Hermann et al., 2015; Chen et al., 2017; Rajpurkar et al., 2016; Clark and Gardner, 2018) is mostly based on the attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) that take as input hquestion, paragraphi, compute an interactive representation of them and predict the start and end positions of the answer. When dealing with no-answer cases, popular method is to jointly model the answer position probability and no-answer probability by a shared softmax normalizer (Kundu and Ng, 2018; Clark and Gardner, 2018; Devlin et al., 2019), or independently model the answerability as a binary classification problem (Hu et al., 2019; Yang et al., 2019; L"
2020.findings-emnlp.370,P18-1158,0,0.0201174,"paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard∗ , with F1 scores of 77.2 and 64.1, respectively. 1 (b) (c) Deep neural network models, such as (Cui et al., 2017; Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), have greatly advanced the state-of-the-arts of machine reading comprehension (MRC). Natural Questions (NQ) (Kwiatkowski et al., 2019) is a new Question Answering benchmark released by Google, which brings new challenges to the MRC area. One challenge is that the answers are provided at two-level granularity, i.e., long answer (e.g., a paragraph in the document) and short answer (e.g., an entity or entities in a paragraph). Therefore, the task requires the models to search for answers at both document level and passage level. Moreover"
2020.findings-emnlp.6,P19-1051,0,0.171914,"Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each sentence with two different Aspect te"
2020.findings-emnlp.6,P17-1036,0,0.0454591,"Missing"
2020.findings-emnlp.6,P19-1048,0,0.0149936,"is carried out 10 epochs on 8 NVIDIA Tesla V100 GPU. We use fp16 to speed up training and to reduce memory usage. The pretraining process takes more than 5 days. 3.3 extraction. We use the BERT+GRU for DL and BERT+SAN for DR as our baselines due to their best-reported performance. Besides, we produce the results on DT with BERT+SAN keeping the settings the same as on DR 8 . We compare our model with the above baselines on DL , DR , and DT , and compare it with the following baselines on DL , DR-14 , DR-15 , and DR-16 because of the common datasets reported by the official implementation. IMN (He et al., 2019) uses an interactive architecture with multi-task learning for end-to-end ABSA tasks. It contains aspect term and opinion term extraction besides aspect-level sentiment classification. DREGCN (Liang et al., 2020a) designs a dependency syntactic knowledge augmented interactive architecture with multi-task learning for end-toend ABSA. DREGCN is short for the official DREGCN+CNN+BERT due to its better performance. WHW (Peng et al., 2020) develops a two-stage framework to address aspect term extraction, aspect sentiment classification, and opinion extraction. TAS-BERT (Wan et al., 2020) proposes a"
2020.findings-emnlp.6,P11-1013,0,0.0979624,"Missing"
2020.findings-emnlp.6,P19-1356,0,0.0142943,"the representation of each layer of BERT. It varies from the 1st layer to the L-th layer. L is the max layer of BERT, e.g., 12 in BERT-Base. He ∈ R(ˆn+2)×h is the representation HL belonging to the last layer, in which two extra embeddings belong to special tokens [CLS] and [SEP], and the labels of them are set to ‘O’ in the experiments. h is the hidden size, n ˆ is the length of S after tokenizing by the wordpiece vocabulary. Different layers of BERT capture different levels of information, e.g., phrase-level information in the lower layers and linguistic information in intermediate layers (Jawahar et al., 2019). The higher layers are usually task-related. Thus, a shared BERT between ATE and ASC tasks is the right choice. We extract the representation Hc for ASC task from the l-th layer of BERT: Hc = Hl . Gc = Transformer-Decoder(Hc , Q), (4) where Q represents the aspect term labels generated by the ATE module (ground-truth labels in the training phase). The vocab size is |T e |in the word embedding of the Transformer-Decoder. Note that the Transformer-Decoder here is not the same as the original transformer decoder. The difference is that we use Multi-Head Attention instead of Masked Multi-Head Att"
2020.findings-emnlp.6,D16-1060,0,0.0605274,"Missing"
2020.findings-emnlp.6,P18-1234,0,0.0378307,"Missing"
2020.findings-emnlp.6,P14-1033,0,0.0691541,"Missing"
2020.findings-emnlp.6,P19-1052,0,0.0323022,"Missing"
2020.findings-emnlp.6,2020.acl-main.340,0,0.0361081,"Missing"
2020.findings-emnlp.6,D19-5505,0,0.0691065,"ost-pretraining BERT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each"
2020.findings-emnlp.6,S15-2082,0,0.0723186,"Missing"
2020.findings-emnlp.6,S16-1002,0,0.0243253,"nsion. wh is a trainable weight matrix. f (·) is the ReLU function. We use hi to calculate loss as Eq. (5) and Eq. (9). It is a consistent strategy to generate sentiment labels, although it cannot improve the performance in our preliminary experiments. 3 #NEG 963 1,751 977 451 581 271 come from the SemEval challenges, and the last comes from an English Twitter dataset, as shown in Table 1. DL contains laptop reviews from SemEval 2014 (Pontiki et al., 2014), and DR are restaurant reviews merged from SemEval 2014 (DR-14 ), SemEval 2015 (DR-15 ) (Pontiki et al., 2015), and SemEval 2016 (DR-16 ) (Pontiki et al., 2016). We keep the official data division of these datasets for the training set, validation set, and testing set. The reported results of DL and DR are average scores of 5 runs. DT consists of English tweets. Due to a lack of standard train-test split, we report the ten-fold cross-validation results of DT as done in (Li et al., 2019b; Luo et al., 2019b). The evaluation metrics are precision (P), recall (R), and F1 score based on the exact match of aspect term and its polarity. where Le and Lc are calculated by Eq. (9), denote the loss for aspect term and polarity, respectively. LVAT denotes the VA"
2020.findings-emnlp.6,S14-2004,0,0.109383,"Missing"
2020.findings-emnlp.6,J11-1002,0,0.167574,"Missing"
2020.findings-emnlp.6,C16-1311,0,0.0804478,"Missing"
2020.findings-emnlp.6,P19-1056,1,0.29574,"RT as its backbone. Experimental results demonstrate that the proposed model achieves consistency improvement on multiple benchmark datasets and generates state-of-the-art results. Joint Collapsed B I O B O POS POS O POS O B-POS I-POS O B-POS (a) Label statistics (b) Gradient statistics Figure 2: Label statistics and gradient distribution on the laptop dataset of SemEval-14. The y-axis in (b) uses a log scale. To better satisfy the practical applications, the aspect term-polarity co-extraction, which solves ATE and ASC simultaneously, receives much attention in recent years (Li et al., 2019b; Luo et al., 2019b; Hu et al., 2019; Wan et al., 2020). A big challenge of the aspect term-polarity co-extraction in a unified model is that ATE and ASC belong to different tasks: ATE is usually a sequence labeling task, and ASC is usually a classification task. Previous works usually transform the ASC task into sequence labeling. Thus the ATE and ASC have the same formulation. There are two approaches of sequence labeling on the aspect term-polarity co-extraction. As shown in Figure 1, one is the joint approach, and the other is the collapsed approach. The preceding one jointly labels each sentence with two d"
2020.findings-emnlp.6,D18-1504,0,0.0328348,"Missing"
2020.findings-emnlp.6,2020.acl-main.295,0,0.0233536,"Missing"
2020.findings-emnlp.6,P19-1344,0,0.0340413,"Missing"
2020.findings-emnlp.6,P18-1088,0,0.0505874,"Missing"
2020.findings-emnlp.6,D16-1059,0,0.0440344,"Missing"
2020.findings-emnlp.6,D13-1171,0,0.0663062,"Missing"
2020.findings-emnlp.6,D16-1058,0,0.0666723,"Missing"
2020.findings-emnlp.6,P18-2094,0,0.0293293,"Missing"
2020.findings-emnlp.6,N19-1242,0,0.0460036,"Missing"
2020.findings-emnlp.6,D15-1073,0,0.119076,"r Computational Linguistics tag sets: aspect term tags and polarity tags. The subsequent one uses collapsed labels as the tags set, e.g., “B-POS” and “I-POS”, in which each tag indicates the aspect term boundary and its polarity. Except for the joint and collapsed approaches, a pipelined approach first labels the given sentence using aspect term tags, e.g., “B” and “I” (the beginning and inside of an aspect term), and then feeds the aspect terms into a classifier to obtain their corresponding polarities. Several related works have been published in these approaches. Mitchell et al. (2013) and Zhang et al. (2015) found that the joint and collapsed approaches are superior to the pipelined approach on named entities and their sentiments co-extraction. Li et al. (2019b) proposed a unified model with the collapsed approach to do aspect term-polarity co-extraction. Hu et al. (2019) solved this task with a pipelined approach. Luo et al. (2019b) adopted the joint approach to do such a co-extraction. We follow the joint approach in this paper, and believe that it has a more apparent of responsibilities than the collapsed approach through learning parallel sequence labels. However, previous works on the joint"
2020.nlpbt-1.4,2020.acl-main.231,0,0.0319372,"core of the best matching substring with the length of the shorter string in comparison. Note that this third metric will bias towards shorter, correct phrases and thus we should have a holistic view of all 3 metrics during the evaluation. Details of two fuzzy metrics are described in Appendix D. 36 should be more than mere object detection. et al., 2018c; Tang et al., 2019; Alayrac et al., 2016; Song et al., 2015; Sener et al., 2015; Huang et al., 2016; Sun et al., 2019b,a; Plummer et al., 2017; Palaskar et al., 2019), combining video & text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al.,"
2020.nlpbt-1.4,Q13-1005,0,0.0340971,"traction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts. Instructional video understanding. Beyond image se"
2020.nlpbt-1.4,W18-2501,0,0.0230217,"Missing"
2020.nlpbt-1.4,W04-2412,0,0.229122,"Missing"
2020.nlpbt-1.4,J02-3001,0,0.293495,"Missing"
2020.nlpbt-1.4,D15-1090,0,0.401954,"being recognized as “add flower” and “sriracha sauce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals an"
2020.nlpbt-1.4,D15-1114,0,0.576155,"Missing"
2020.nlpbt-1.4,D11-1142,0,0.0172339,"task, given a narrative video, say a cooking video on YouTube about making clam chowder as shown in Figure 1, our goal is to extract a series of tuples representing the procedure, e.g. (heat, cast iron skillet), (fry, bacon, with heated skillet), etc. We created a manually annotated, large test dataset for evaluation of the task, including over 350 instructional cooking videos along with over 15,000 English sentences in the transcripts spanning over 89 recipe types. This verb-argument structure using arbitrary textual phrases is motivated by open information extraction (Schmitz et al., 2012; Fader et al., 2011), but focuses on procedures rather than entity-entity relations. This task is challenging with respect to both video and language understanding. For video, it requires understanding of video contents, with a speIntroduction Instructional videos are a convenient way to learn a new skill. Although learning from video seems natural to humans, it requires identifying and understanding procedures and grounding them to the real world. In this paper, we propose a new task and dataset for extracting procedural knowledge into a fine-grained structured representation from multimodal information containe"
2020.nlpbt-1.4,P19-1659,0,0.0522706,"Missing"
2020.nlpbt-1.4,D16-1155,0,0.0259752,"r” and “sriracha sauce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts dire"
2020.nlpbt-1.4,D16-1264,0,0.0333159,"fuzzy matching. The first is straight forward, we count true positives if and only if the predicted phrase is an exact string match in the gold phrases. However, because our task lies in the realm of open phrase extraction without predefined labels, it is unfairly strict to count only the exact string matches as T P . Also by design, the gold extraction results cannot always be found in the original transcript sentence (refer to §3.2), so we are also unable to use token-based metrics as in sequence tagging (Sang and De Meulder, 2003), or span-based metrics as in some question answering tasks (Rajpurkar et al., 2016). Thus for the second metric we call “fuzzy”, we leverage edit distance to enable fuzzy matching and assign a “soft” score for T P . In some cases, the two strings of quite different lengths will hurt the fuzzy score due to the nature of edit distance, even though one string is a substring of another. To get around this, we propose a third metric, “partial fuzzy” to get the score of the best matching substring with the length of the shorter string in comparison. Note that this third metric will bias towards shorter, correct phrases and thus we should have a holistic view of all 3 metrics durin"
2020.nlpbt-1.4,P16-1138,0,0.026742,"auce” being recognized as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from impe"
2020.nlpbt-1.4,Q13-1003,0,0.0385252,"unstructured instructional video (clip and utterances) to structured procedures, defining what actions should be performed on which objects, with what arguments and in what order. We define the input to such an extraction system: Dataset & Analysis While others have created related datasets, they fall short on key dimensions which we remedy in our work. Specifically, In Table 1 we compare to AllRecipes (Kiddon et al., 2015) (AR), YouCook2 (Zhou et al., 2018b) (YC2), CrossTask (Zhukov et al., 2019) (CT), COIN (Tang et al., 2019), How2 (Sanabria et al., 2018), HAKE (Li et al., 2019) and TACOS (Regneri et al., 2013). Additional details about datasets are included in the Appendix A.2 In summary, none have both structured and open extraction annotations for the procedural knowledge extraction task, since most focus on either video summarization/captioning or action localization/classification. • Task R, e.g. “Create Chicken Parmesan” and instructional video VR describing the procedure to achieve task R, e.g. a video titled “Chicken Parmesan - Let’s Cook with ModernMom”.1 • A sequence of n sentences TR = {t0 , t1 , ..., tn } representing video VR ’s corresponding transcript. According to the time stamps of"
2020.nlpbt-1.4,W15-2206,0,0.3589,"zed as “sarrah cha sauce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts."
2020.nlpbt-1.4,N15-1015,0,0.0311496,"; Palaskar et al., 2019), combining video & text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional vide"
2020.nlpbt-1.4,W14-2407,0,0.433384,"auce” causing wrong extraction outputs. Coreference and ellipsis are also challenging and hurting current benchmark performance, as our baselines do not tackle any of these explicitly. Visual co-reference and language grounding (Huang et al., 2018, 2017) provides a feasible method for us to tackle these cases in the future. 7 Related Work Text-based procedural knowledge extraction. Procedural text understanding and knowledge extraction (Chu et al., 2017; Park and Motahari Nezhad, 2018; Kiddon et al., 2015; Jermsurawong and Habash, 2015; Liu et al., 2016; Long et al., 2016; Maeta et al., 2015; Malmaud et al., 2014; Artzi and Zettlemoyer, 2013; Kuehne et al., 2017) has been studied for years on step-wise textual data such as WikiHow. Chu et al. (2017) extracted open-domain knowledge from how-to communities. Recently Zhukov et al. (2019) also studied to adopt the well-written how-to data as weak supervision for instructional video understanding. Unlike existing work on action graph/dependency extraction (Kiddon et al., 2015; Jermsurawong and Habash, 2015), our approach differs as we extract knowledge from the visual signals and transcripts directly, not from imperative recipe texts. Instructional video u"
2020.nlpbt-1.4,W03-0419,0,0.27491,"Missing"
2020.nlpbt-1.4,D12-1048,0,0.0253518,"et al., 2018a). In our task, given a narrative video, say a cooking video on YouTube about making clam chowder as shown in Figure 1, our goal is to extract a series of tuples representing the procedure, e.g. (heat, cast iron skillet), (fry, bacon, with heated skillet), etc. We created a manually annotated, large test dataset for evaluation of the task, including over 350 instructional cooking videos along with over 15,000 English sentences in the transcripts spanning over 89 recipe types. This verb-argument structure using arbitrary textual phrases is motivated by open information extraction (Schmitz et al., 2012; Fader et al., 2011), but focuses on procedures rather than entity-entity relations. This task is challenging with respect to both video and language understanding. For video, it requires understanding of video contents, with a speIntroduction Instructional videos are a convenient way to learn a new skill. Although learning from video seems natural to humans, it requires identifying and understanding procedures and grounding them to the real world. In this paper, we propose a new task and dataset for extracting procedural knowledge into a fine-grained structured representation from multimodal"
2020.nlpbt-1.4,P19-1641,1,0.798391,"l-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional videos. Inspired by cross-task sharing (Zhukov et al., 2019), which is a weakly supervised method to learn shared actions betw"
2020.nlpbt-1.4,I17-1033,0,0.0296986,"& text information in procedures (Yagcioglu et al., 2018; Fried et al., 2020), visual-linguistic reference resolution (Huang et al., 2018, 2017), visual planning (Chang et al., 2019), joint learning of object and actions (Zhukov et al., 2019; Richard et al., 2018; Gao et al., 2017; Damen et al., 2018b), pretraining joint embedding of high level sentence with video clips (Sun et al., 2019b; Miech et al., 2019), our task proposal requires explicit structured knowledge tuple extraction. In addition to closely related work (§3) there is a wide literature (Malmaud et al., 2015; Zhou et al., 2018b; Ushiku et al., 2017; Nishimura et al., 2019; Tang et al., 2019; Huang et al., 2016; Shi et al., 2019; Ushiku et al., 2017) that aims to predict/align dense procedural captions given the video, which are the most similar works to ours. Zhou et al. (2018c) extracted temporal procedures and then generated captioning for each procedure. Sanabria et al. (2018) proposes a multimodal abstractive summarization for how-to videos with either human labeled or speech-to-text transcript. Alayrac et al. (2016) also introduces an unsupervised step learning method from instructional videos. Inspired by cross-task sharing (Zhuko"
2020.nlpbt-1.4,D18-1166,0,\N,Missing
2021.acl-demo.26,P84-1044,0,0.143927,"Missing"
2021.acl-demo.26,P17-4012,0,0.070771,"Missing"
2021.acl-demo.26,2020.acl-main.703,0,0.0373925,"line code change for models in FairSeq (Ott et al., 2019) and HuggingfaceTransformers (Wolf et al., 2020). The design principle of FastSeq is to improve the inference speed without losing model accuracy and usability. Our optimization approaches include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough for a wide range of Transformer-based model (Vaswani et al., 2017) architectures, including the encoder-decoder architecture (e.g., T5 Raffel et al. 2020, BART Lewis et al. 2020, ProphetNet Qi et al. 2020), the decoder-only architecture (e.g., GPT2 Radford et al. 2019), and the encoder-only architecture (e.g., UniLM Dong et al. 2019). FastSeq is also designed to be flexible for extension on supporting other models and frameworks. Our technologies are partially adopted by FairSeq2 . A demo video can be found at https: //www.youtube.com/watch?v=jrdsEUxhSEE. 2 Preliminary Analysis For models with similar size, the sequence generation is much slower than classification, regression or language score computation. Why is the generation so time-consuming? Before analyzing th"
2021.acl-demo.26,N19-4009,0,0.0185064,"del architectures, like model distillation (Shleifer and Rush, 2020) and sparse attention (Beltagy et al., 2020). Although these techniques can alleviate the performance issue, there may be still some tradeoff between model accuracy and speed. On the other hand, efficient infrastructures have been developed to accelerate the inference speed, e.g., TensorRT (Vanholder, 2016) and FasterTransformers1 . In this paper, we present FastSeq framework to make sequence generation faster. FastSeq can accelerate the sequence generation by 4x to 9x with a simple one-line code change for models in FairSeq (Ott et al., 2019) and HuggingfaceTransformers (Wolf et al., 2020). The design principle of FastSeq is to improve the inference speed without losing model accuracy and usability. Our optimization approaches include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough for a wide range of Transformer-based model (Vaswani et al., 2017) architectures, including the encoder-decoder architecture (e.g., T5 Raffel et al. 2020, BART Lewis et al. 2020, ProphetNet Qi et al. 2020), the decode"
2021.acl-demo.26,W18-6301,0,0.0589018,"Missing"
2021.acl-demo.26,2020.findings-emnlp.217,1,0.842319,"FairSeq (Ott et al., 2019) and HuggingfaceTransformers (Wolf et al., 2020). The design principle of FastSeq is to improve the inference speed without losing model accuracy and usability. Our optimization approaches include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough for a wide range of Transformer-based model (Vaswani et al., 2017) architectures, including the encoder-decoder architecture (e.g., T5 Raffel et al. 2020, BART Lewis et al. 2020, ProphetNet Qi et al. 2020), the decoder-only architecture (e.g., GPT2 Radford et al. 2019), and the encoder-only architecture (e.g., UniLM Dong et al. 2019). FastSeq is also designed to be flexible for extension on supporting other models and frameworks. Our technologies are partially adopted by FairSeq2 . A demo video can be found at https: //www.youtube.com/watch?v=jrdsEUxhSEE. 2 Preliminary Analysis For models with similar size, the sequence generation is much slower than classification, regression or language score computation. Why is the generation so time-consuming? Before analyzing the reasons, let’s recap the g"
2021.acl-demo.28,2021.naacl-main.211,0,0.177047,"human evaluation as in (Zhao et al., 2020). We randomly collect 500 single-turn and 500 multi-turn context-response pairs from the online logs of the real-word dialog system Xiaoice. Then, we recruit 3 native speakers as human annotators. The annotators have to judge which response is better, based on informativeness, consistency, and fluency of the responses. If an annotator cannot tell which response is better, he/she is required to label a “Tie”. With the Models Seq2Seq (Vinyals and Le, 2015) Transformer (Vaswani et al., 2017) RoBERTa (Liu et al., 2019) CodeBERT (Feng et al., 2020) PLBART (Ahmad et al., 2021) Prophetnet-Code Ruby 9.64 11.18 11.17 12.16 14.11 14.37 Javascript 10.21 11.59 11.90 14.90 15.56 16.60 Go 13.98 16.38 17.72 18.07 18.91 18.43 Python 15.93 15.81 18.14 19.06 19.30 17.87 Java 15.09 16.26 16.47 17.65 18.45 19.39 PHP 21.08 22.12 24.02 25.16 23.58 24.57 overall 14.32 15.56 16.57 17.83 18.32 18.54 Table 9: Results of ProphetNet-Code on CodeXGLUE for code-to-text summarization task. Numbers in this table are smoothed BLEU-4 scores. Method LSTM (Bahdanau et al., 2014) Transformer (Vaswani et al., 2017) MASS (Song et al., 2019) BART (Lewis et al., 2019) ProphetNet-En R-1 37.3 39.5 42."
2021.acl-demo.28,2020.acl-main.9,0,0.146754,"C (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovanov et al., 2019) PLATO w/o latent (Bao et al., 2020) PLATO (Bao et al., 2020) ProphetNet-Dialog-En B-1 0.336 0.309 0.405 0.397 0.461 DailyDialog B-2 D-1 D-2 0.238 0.03 0.128 0.249 0.029 0.25 0.322 0.046 0.246 0.311 0.053 0.291 0."
2021.acl-demo.28,P19-1608,0,0.0400526,"Missing"
2021.acl-demo.28,D15-1229,0,0.0821035,"ream task finetuning scripts, including ProphetNet-En pre-trained with 160GB English raw text, ProphetNet-Zh pre-trained with 160GB Chinese raw text, ProphetNet-Multi with 101GB Wiki-100 corpus and 1.5TB Common Crawl2 data, ProphetNet-Dialog-En with 60 million sessions Reddit open-domain dialog corpus, ProphetNetDialog-Zh with collected Chinese dialog corpus over 30 million sessions, and ProphetNet-Code pre-trained with 10 million codes and documents. ProphetNet-X achieves new state-of-the-art results on 10 benchmarks, including Chinese summarization (MATINF-SUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015)), Chinese question answering (MATINF-QA (Xu et al., 2020a)), cross-lingual generation (XGLUE NTG (Liang et al., 2020) and † 1 Corresponding Author. https://github.com/microsoft/ProphetNet 2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share th"
2021.acl-demo.28,P19-4007,0,0.0224534,"Missing"
2021.acl-demo.28,D19-1407,0,0.0536767,"Missing"
2021.acl-demo.28,2020.acl-main.703,0,0.0861707,"Missing"
2021.acl-demo.28,I17-1099,0,0.321053,"2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share the same model structure and cover various languages and domains. XGLUE QG (Liang et al., 2020)), English summarization (MSNews (Liu et al., 2020a)), English dialog generation (DailyDialog (Li et al., 2017), PersonaChat (Zhang et al., 2018), and DSTC7AVSD (Alamri et al., 2019)), and code summarization (CodeXGLUE (Lu et al., 2021)). Users can simply download the ProphetNet-X repository and find corresponding pre-trained model with downstream task finetuning scripts. The main contributions of ProphetNet-X can be described as follows: • We provide a family of pre-trained models named ProphetNet-X, with six models including English and Chinese natural language generation in open-domain and dialog, multilingual generation, and code generation. • All the pre-trained ProphetNet-X models share the same"
2021.acl-demo.28,P18-2027,0,0.0228438,"Missing"
2021.acl-demo.28,2021.findings-acl.36,1,0.805985,"Missing"
2021.acl-demo.28,D19-1387,0,0.0557333,"Missing"
2021.acl-demo.28,2020.tacl-1.47,0,0.406675,"updating more pre-training models and finetuning scripts. 1 Introduction In recent years, quite a few natural language generation pre-training models are proposed (Qi et al., 2020; Lewis et al., 2019; Song et al., 2019; Brown et al., 2020). Downstream generation tasks benefit from these large scale pre-training models greatly in fluency and accuracy. Researchers also extend these general pre-training works into specific domains such as DialoGPT (Zhang et al., 2019) is ∗ Work is done during internship at Microsoft Research Asia. extended from GPT (Brown et al., 2020) for dialog system, mBART (Liu et al., 2020b) is extended from BART (Lewis et al., 2019) for multi-lingual generation, CodeBERT (Feng et al., 2020) is extended from BERT (Devlin et al., 2018) for programming language modeling, etc. Although there are pre-trained models for some specific domains, it is not convenient for users to find them and set them up. Besides, even some models in the same pre-training family with the same model structure and pre-training tasks, their codes and details vary a lot because of different implementation and backends selection. ProphetNet (Qi et al., 2020) is firstly proposed as an English text pre-traini"
2021.acl-demo.28,2021.ccl-1.108,0,0.116891,"Missing"
2021.acl-demo.28,D15-1166,0,0.0212423,"Missing"
2021.acl-demo.28,N18-1018,0,0.0341321,"Missing"
2021.acl-demo.28,W04-3252,0,0.0754182,"s 2. Both the max sequence lengths of the input and output are set to 512. For ProphetNet-En, ProphetNet-Zh, ProphetNetMulti, ProphetNet-Dialog-En, and ProphetNetCode, we carry out un-supervised pre-training with masked span prediction task. Spans of continuous tokens are masked out from the encoder input sentences and predicted from the decoder side. We masked continuous 9 tokens in every 64 tokens from the encoder side, and predict the 9 tokens on the decoder side. In other words, for maximum 512 encoder sequence length, totally 8(spans) × 9(tokens per span) = 72 tokens 234 Method TextRank (Mihalcea and Tarau, 2004) LexRank (Erkan and Radev, 2004) Seq2Seq (Sutskever et al., 2014) Seq2Seq+Att (Luong et al., 2015) WEAN (Ma et al., 2018) Global Encoding (Lin et al., 2018) BertAbs (Liu and Lapata, 2019) MTF-S2Ssingle (Xu et al., 2020a) MTF-S2Smulti (Xu et al., 2020a) ProphetNet-Zh MATINF-QA R-1 R-2 R-L 16.62 4.53 10.37 19.62 5.87 13.34 20.28 5.94 13.52 21.66 6.58 14.26 24.18 6.38 15.47 MATINF-SUMM R-1 R-2 R-L 35.53 25.78 36.84 33.08 23.31 34.96 23.05 11.44 19.55 43.05 28.03 38.58 34.63 22.56 28.92 49.28 34.14 47.64 57.31 44.05 55.93 43.02 28.05 38.55 48.59 35.69 43.28 58.82 44.96 54.26 R-1 24.38 22.15 33.80"
2021.acl-demo.28,2020.findings-emnlp.217,1,0.882562,"PLG (Programming Language Generation) model ProphetNet-Code to show the generation performance besides NLG (Natural Language Generation) tasks. In our experiments, ProphetNet-X models achieve new state-of-the-art performance on 10 benchmarks. All the models of ProphetNet-X share the same model structure, which allows users to easily switch between different models. We make the code and models publicly available1 , and we will keep updating more pre-training models and finetuning scripts. 1 Introduction In recent years, quite a few natural language generation pre-training models are proposed (Qi et al., 2020; Lewis et al., 2019; Song et al., 2019; Brown et al., 2020). Downstream generation tasks benefit from these large scale pre-training models greatly in fluency and accuracy. Researchers also extend these general pre-training works into specific domains such as DialoGPT (Zhang et al., 2019) is ∗ Work is done during internship at Microsoft Research Asia. extended from GPT (Brown et al., 2020) for dialog system, mBART (Liu et al., 2020b) is extended from BART (Lewis et al., 2019) for multi-lingual generation, CodeBERT (Feng et al., 2020) is extended from BERT (Devlin et al., 2018) for programming"
2021.acl-demo.28,D16-1264,0,0.0250842,"conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovanov et al., 2019) PLATO w/o latent (Bao et al., 2020) PLATO (Bao et al., 2020) ProphetNet-Dialog"
2021.acl-demo.28,D15-1044,0,0.0440847,"2017) for chit-chat generation, Persona-Chat (Zhang et al., 2018) for knowledge grounded conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) ProphetNet-Dialog-En BLEU-1 0.629 0.718 0.784 0.823 BLEU-2 0485 0.584 0.637 0.688 BLEU-3 0.383 0.478 0.525 0.578 BLEU-4 0.309 0.394 0.435 0.482 METEOR 0.215 0.267 0.286 0.309 ROUGE-L 0.487 0.563 0.596 0.631 CIDEr 0.746 1.094 1.209 1.354 Table 5: Results of ProphetNet-Dialog-En on DSTC7-AVSD. Model Seq2Seq (Vinyals and Le, 2015) iVAE MI (Fang et al., 2019) LIC (Golovano"
2021.acl-demo.28,P15-1152,0,0.0173033,"t-Zh, we evaluate our pre-trained model with MATINF-QA (Xu et al., 2020a) for generative question answering task, MATINFSUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015) for summarization task. For ProphetNet-Multi, we follow UnicoderF N P to evaluate on XGLUE (Liang et al., 2020) for 235 For ProphetNet-Dialog-En, we carry out finetuning on DailyDialog (Li et al., 2017) for chit-chat generation, Persona-Chat (Zhang et al., 2018) for knowledge grounded conversation generation and DSTC7-AVSD (Alamri et al., 2019) for conversational question answering. For ProphetNet-Dialog-Zh, we use the STC (Shang et al., 2015) single-turn open-domain dialog dataset cleaned by Wang et al. (2020), and real-world Xiaoice Chinese dialog dataset for evaluation. For ProphetNet-Code, we evaluate the performance on code summarization task from CodeXGLUE (Lu et al., 2021). For ProphetNet-En, we reports the results on summarization tasks CNN/DM (Hermann et al., 2015), Gigaword (Rush et al., 2015), and MSNews (Liu et al., 2020a); question generation tasks SQuAD 1.1 (Rajpurkar et al., 2016) and MSQG (Liu et al., 2020a). Model AVSD Baseline (Alamri et al., 2019) CMU Sinbad’s (Sanabria et al., 2019) PLATO (Bao et al., 2020) Prop"
2021.acl-demo.28,2020.acl-main.330,0,0.126352,"re-trained models with downstream task finetuning scripts, including ProphetNet-En pre-trained with 160GB English raw text, ProphetNet-Zh pre-trained with 160GB Chinese raw text, ProphetNet-Multi with 101GB Wiki-100 corpus and 1.5TB Common Crawl2 data, ProphetNet-Dialog-En with 60 million sessions Reddit open-domain dialog corpus, ProphetNetDialog-Zh with collected Chinese dialog corpus over 30 million sessions, and ProphetNet-Code pre-trained with 10 million codes and documents. ProphetNet-X achieves new state-of-the-art results on 10 benchmarks, including Chinese summarization (MATINF-SUMM (Xu et al., 2020a) and LCSTS (Hu et al., 2015)), Chinese question answering (MATINF-QA (Xu et al., 2020a)), cross-lingual generation (XGLUE NTG (Liang et al., 2020) and † 1 Corresponding Author. https://github.com/microsoft/ProphetNet 2 https://commoncrawl.org/ 232 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework"
2021.acl-demo.28,P18-1205,0,0.128826,"Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 232–239, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics Figure 1: A diagram of ProphetNet-X framework. ProphetNet-X models share the same model structure and cover various languages and domains. XGLUE QG (Liang et al., 2020)), English summarization (MSNews (Liu et al., 2020a)), English dialog generation (DailyDialog (Li et al., 2017), PersonaChat (Zhang et al., 2018), and DSTC7AVSD (Alamri et al., 2019)), and code summarization (CodeXGLUE (Lu et al., 2021)). Users can simply download the ProphetNet-X repository and find corresponding pre-trained model with downstream task finetuning scripts. The main contributions of ProphetNet-X can be described as follows: • We provide a family of pre-trained models named ProphetNet-X, with six models including English and Chinese natural language generation in open-domain and dialog, multilingual generation, and code generation. • All the pre-trained ProphetNet-X models share the same model structure. Users only need t"
2021.acl-demo.28,2020.emnlp-main.279,1,0.890954,"Missing"
2021.acl-long.157,W05-0909,0,0.120487,"Missing"
2021.acl-long.157,D19-1155,0,0.0110693,"(Pont-Tuset et al., 2020), and CIDEr-D (Vedantam et al., 2015). 4.4 Loss Dataset Results Baseline and +Trace methods The Baseline and +Trace methods are our re-implementations following (Pont-Tuset et al., 2020)’s method description. The Baseline method only takes image feature as input while the +Trace model take image feature 2 https://github.com/tylin/coco-caption We add an additional id to every trace-image-caption triplet and adjust some code of the standard evaluation tool to meet the ”1 trace-vs-1 caption” evaluation need. 2018 3 and trace both as input. They employ the architecture in Changpinyo et al. (2019) with a few minor differences. First, they set the number of Transformers’ layers for both the encoder and the decoder to 2 instead of 6. Second, their projection layers also consist of layer normalization(Ba et al., 2016). Third, they set the maximum number of iterations to 150k. Finally, they allow the maximum number of target captions to be as long as 225 to account for the narration’s longer nature. LoopCAG methods Our model comprises of four components: 1) the transformer encoderdecoder framework; 2) the trace input; 3) Attention Guidance(+AG for short) grounding loss; 4) Contrastive cons"
2021.acl-long.157,P04-1077,0,0.0495775,"Missing"
2021.acl-long.157,2021.naacl-main.280,1,0.748297,"Missing"
2021.acl-long.157,P02-1040,0,0.113144,"Missing"
2021.acl-long.442,P17-1152,0,0.0331223,"enforcing similar objects to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score"
2021.acl-long.442,D17-1070,0,0.0259629,"logy In this section, we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The r"
2021.acl-long.442,2020.findings-emnlp.139,1,0.917023,"x in fragments[1:]) Label: With the growing population of software developers, natural language code search, which improves the productivity of the development process via retrieving semantically relevant code given natural language queries, is increasingly important in both communities of software engineering and natural language processing (Allamanis et al., 2018; Liu et al., 2020a). The key challenge is how to effectively measure the semantic similarity between a natural language query and a code. There are recent attempts to utilize deep neural networks (Gu et al., 2018; Wan et al., 2019; Feng et al., 2020), which embed query and code as dense vectors to perform semantic matching in a unified vector space. However, these models are ∗ 0 Figure 1: Two examples in CoSQA. A pair of a web query and a Python function with documentation is annotated with “1” or “0”, representing whether the code answers the query or not. Introduction Work done during internship at Microsoft Research Asia. The CoSQA data and leaderboard are available at https://github.com/microsoft/CodeXGLUE/tree/main/TextCode/NL-code-search-WebQuery. The code is available at https://github.com/Jun-jie-Huang/CoCLR 1 1 mostly trained on"
2021.acl-long.442,2020.acl-main.758,0,0.0222518,"ring. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague queries and (7) others. Basically, queries in (2)-(7) categories are not likely to be answered only by a code function, since they may nee"
2021.acl-long.442,2021.ccl-1.108,0,0.0812067,"Missing"
2021.acl-long.442,I17-2053,0,0.0462943,"Missing"
2021.acl-long.442,P16-2022,0,0.016286,"ts to be closer while keeping dissimilar objects further apart. It is often accompanied with leveraging task-specific inductive bias to augment similar and dissimilar examples. In this work, given an example of query and code (qi , ci ), we define our contrastive learning task on example itself, inbatch augmented examples (qi , cj ), and augmented example with rewritten query (qi0 , ci ). Hence, the overall training objective can be formulated as: ci = CodeBERT(ci ). (1) L = Lb + Lib + Lqr . Next we perform query-code matching through a multi-layer perceptron. Following Chen et al. (2017) and Mou et al. (2016), we concatenate the query embedding qi and code embedding ci with the element-wiseJdifference qi − ci and elementwise product qi ci , followed by a 1-layer feedforward neural network, to obtain a relation embedding: r(i,i) = tanh(W1 · [qi , ci , qi − ci , qi K ci ]). (2) We expect such an operation can help sharpen the cross information between query and code to capture better matching relationships such as contradiction. Then we put the relation embedding r(i,i) into a final 1-layer perceptron classifier with a sigmoid output layer: s(i,i) = sigmoid(W2 · r(i,i) ). Score s(i,i) can be viewed"
2021.acl-long.442,D19-1410,0,0.0129035,"he model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed lines denote the augmented e"
2021.acl-long.442,W18-3022,0,0.0178495,"we first describe the model for query-code matching and then present our code contrastive learning method (CoCLR) to augment more training instances. 5.1 Siamese Network with CodeBERT The base model we use in this work is a siamese network, which is a kind of neural network with two or more identical subnetworks that have the same architecture and share the same parameters and weights (Bromley et al., 1994). By deriving fixed-sized embeddings and computing similarities, siamese network systems have proven effective in modeling the relationship between two text sequences (Conneau et al., 2017; Yang et al., 2018; Reimers and Gurevych, 2019). We use a pretrained CodeBERT (Feng et al., 2020) as the encoder to map any text sequence to a d-dimensional real-valued vectors. CodeBERT is a bimodal model for natural language and programming language which enables high-quality text and 5694 … CodeBERT CodeBERT CodeBERT ?????? ????? ?????1′ rewrite CodeBERT CodeBERT … CodeBERT ?????1 ????1 … ??????′ rewrite CodeBERT CodeBERT ?????? ????? Figure 2: The frameworks of the siamese network with CodeBERT (left) and our CoCLR method (right). The blue line denotes the original training example. The red lines and dashed"
2021.acl-long.442,2020.findings-emnlp.361,0,0.0315257,"Gu et al., 2018; Cambronero Related Work In this part, we describe existing datasets and methods on code search and code question answering. 2.1 Datasets A number of open-sourced datasets with a large amount of text-code pairs have been proposed for the purposes of code search (Husain et al., 2019; Gu et al., 2018; Nie et al., 2016) and code question answering (Yao et al., 2018; Yin et al., 2018; 3 We study on Python in this work, and we plan to extend to more programming languages in the future. 2.2 5691 Code Search Models et al., 2019; Yao et al., 2019; Liu et al., 2019a; Feng et al., 2020; Zhao and Sun, 2020). There are also ways to exploit code structures to learn better representations for code search (Wan et al., 2019; Haldar et al., 2020; Guo et al., 2020). 3 3.1 Data Collection Query Curation We use the search logs from the Microsoft Bing search engine as the source of queries. Queries without the keyword “python” are removed. Based on our observation and previous work (Yao et al., 2018; Yan et al., 2020), there are seven basic categories of code-related web queries, including: (1) code searching, (2) debugging, (3) conceptual queries, (4) tools usage, (5) programming knowledge, (6) vague que"
2021.acl-long.442,2020.emnlp-main.36,0,0.0559045,"Missing"
2021.acl-long.62,N19-1423,0,0.011306,"r model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report the micro-averaged (Precision = Recall = F1) and macro-averaged scores (Precision, Recall, F1) in all the settings including 2-way"
2021.acl-long.62,D19-1488,1,0.846914,"ntity e, we build a one-way directed edge from a sentence to the entity e, in order to allow only information propagation from sentences to entities. In this way, we can avoid integrating true entity knowledge directly into news representation, which may mislead the detection of fake news. 3.2 Heterogeneous Graph Convolution Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations. It considers not only the weights of different nodes with different types (Hu et al., 2019) but also the edge directions in the heterogeneous graph. Formally, we have three types T = {τ1 , τ2 , τ3 } of nodes: sentences S, topics T and entities E with different feature spaces. We apply LSTM to encode a sentence s = {w1 , · · ·, wm } and get its feature vector xs ∈ RM . The entity e ∈ E is initialized with the entity representations eKB ∈ RM learned from the external KB (see Subsection 3.3.1). The topic t ∈ T is initialized with one-hot vector xt ∈ RK . Next, consider the graph G = (V, E) where V and E represent the set of nodes and edges respectively. Let X ∈ R|V|×M be a matrix conta"
2021.acl-long.62,D14-1181,0,0.00516264,"training, Y is the corresponding label indicator matrix, Θ is the model parameters, and η is regularization factor. For model optimization, we adopt the gradient descent algorithm. 4 Experiments We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics. Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report th"
2021.acl-long.62,2020.lrec-1.747,0,0.141975,"past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality"
2021.acl-long.62,P18-1022,0,0.0202518,"representation for improving fake news detection. Some works (Wang, 2017; Khattar et al., 2019; Wang et al., 2020) also consider incorporating multi-modal features such as images for improving fake news detection. Content-based Fake News Detection On the other hand, news contents contain the clues to differentiate fake and trusted news. A lot of existing works extract specific writing styles such as lexical and syntactic features (Conroy et al., 2015; Rubin et al., 2016; Khurana and Intelligentie, 2017; Rashkin et al., 2017; Shu et al., 2020; Oshikawa et al., 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodr´ıguez and Iglesias, 2019). For example, Ibrain et al. applied deep neural networks, such as BiLSTM and convolutional neural networks (CNN) for fake news detection (Rodr´ıguez and Iglesias, 2019). However, these works fail to consider different sentence interaction patterns between trusted and fake news documents. Vaibhav et al. proposed to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for"
2021.acl-long.62,D17-1317,0,0.247703,"2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing ap"
2021.acl-long.62,W16-0802,0,0.658737,"llcott and Gentzkow, 2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Alth"
2021.acl-long.62,D18-1209,0,0.0282805,"ural information from the triplets and textual information from the entity descriptions in the KB. 3.3.2 Entity Comparison We then perform entity-to-entity comparison between the news document and the KB, to capture the semantic consistency between the news content and the KB. We calculate a comparison vector ai between each contextual entity representation ec ∈ RN and its corresponding KB-based entity embedding eKB ∈ RM . ai = fcmp (ec , We · eKB ) , (6) where fcmp () denotes the comparison function, and We ∈ RN ×M is a transformation matrix. To measure the embedding closeness and relevance (Shen et al., 2018), we design our comparison function as: fcmp (x, y) = Wa [x − y, x y], (7) where Wa ∈ RN ×2N is a transformation matrix and is hadamard product, i.e., element-wise product. The final output comparison feature vector C ∈ RN is obtained by the max pooling over the alignment vectors A = [a1 , a2 , ..., an ] of all the entities E = {e1 , e2 , ..., en } in the news document. 3.4 Model Training After obtaining the comparison vector C ∈ RN and the final news document representation vector Hd ∈ RN , we concatenate and feed them into a Softmax layer for fake news classification. Formally, Z = Softmax(W"
2021.acl-long.62,N18-1074,0,0.0329643,"Missing"
2021.acl-long.62,D19-5316,0,0.253593,"rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured subjectpredicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure 4, the news 754 Proceedings of the 59th Annual Meeting of the Association for Computation"
2021.acl-long.62,P17-2067,0,0.177697,"ial elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document. Some existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted. External KB such as Wikipedia contains a large amount of high-quality structured s"
2021.acl-long.62,D19-1471,0,0.0120727,"ou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. constructed a bipartite network of user and posts with ‘like’ stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes (Tacchini et al., 2017). Propagation-based approaches for fake news detection are based on the basic assumption that the credibility of a news event is highly related to the credibilities of relevant social media posts. Both 755 Figure 1: An example of directed heterogeneous document graph incorporating topics and entities. homogeneous (Jin et al., 2016) and heterogeneous credibility networks (Gupta"
2021.acl-long.62,2020.acl-main.549,1,0.77437,"he KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. co"
2021.acl-long.62,P19-1085,0,0.0180519,"mpare the news to the KB. 3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information. 2 Related Work Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based. 2.1 Social-based Fake News Detection Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019"
2021.emnlp-main.387,2020.emnlp-main.728,1,0.826377,"Missing"
2021.emnlp-main.387,2020.acl-main.703,0,0.0492266,"Missing"
2021.emnlp-main.387,2020.findings-emnlp.139,1,0.826354,"9; Radford et al., 2018) have become an essential part of state of the art natural language processing. Beyond the domain of natural language, these models and procedures have enabled rapid progress in the software engineering space, including applications in code completion (Svyatkovskiy et al., 2020, 2019; Clement et al., 2020; Raychev et al., 2014; Bruch et al., 2009), natural language to code (NL2Code), code feature summarization (Clement et al., 2020; Moreno et al., 2013; Scalabrino et al., 2017; Wan et al., 2018; Alon et al., 2018; Moreno et al., 2014), code search (Husain et al., 2019; Feng et al., 2020), unit test generation (Tufano et al., 2020) and even bug fixing (Drain et al., 2021) and detection (Zhai et al., 2020). A major difference between transformers and their antecedents like recurrent neural networks (RNN) is their strictly enforced finite context window. Whereas an RNN can iteratively consume as many tokens as is required, transformers can only consume up to a finite amount decided at training time. Further, it is impractical to simply expand the window as the memory and compute requirements of the attention mechanism scale quadratically with context length. There have been effo"
2021.findings-acl.121,P19-1279,0,0.0206704,"dopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In particular, (1) K-A DAPTER models outperform RoBERTa, which proves the effectiveness of infusing knowledge in"
2021.findings-acl.121,P18-1009,0,0.0216531,"d analyses with the case study and probing experiments to explore the effectiveness and ability of models for learning factual knowledge. The notations of K-A DAPTER (F+L), K-A DAPTER (F), and K-A DAPTER (L) denote our model which consists of both factual adapter and linguistic adapter, only factual adapter and only linguistic adapter, respectively. Implementation details, and statistics of datasets are in the Appendix. 4.1 Entity Typing We conduct experiments on fine-grained entity typing which aims to predict the types of a given entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for entity typing, we modify the input token sequence by adding the special token “@” before and after a certain entity, then the first “@” special token representation is adopted to perform classification. As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance. As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling and Weld, 2012) for evaluation following the same evaluation criteria used in previous works. Baselin"
2021.findings-acl.121,P19-1285,0,0.0618623,"Missing"
2021.findings-acl.121,N19-1423,0,0.0429486,"texttriplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained via dependency parsing. Results on three knowledge-driven tasks, including relation classification, entity typing, and question answering, demonstrate that each adapter improves the performance and the combination of both adapters brings further improvements. Further analysis indicates that K-A DAPTER captures versatile knowledge than RoBERTa. 1 1 Introduction Language representation models, which are pretrained on large-scale text corpus through unsupervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018, 2019), XLNet (Yang et al., ∗ Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/microsoft/K-Adapter 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), have established state-of-the-art performances on various NLP downstream tasks. Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) sugges"
2021.findings-acl.121,L18-1544,0,0.0249766,"ayer, we denote the number of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-attention heads as AA , the hidden dimension of down-projection and up-projection layers as Hd and Hu . In detail, we have the following adapter size: N = 2, HA = 768, AA = 12, Hu = 1024 and Hd = 768. The RoBERTa lay2 3.3 Factual Adapter Factual knowledge can be described as the basic information that is concerned with facts. In this work, we acquire factual knowledge from the relationships among entities in natural language. We extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than 50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge, we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification task. This task requires a model to classify relation labels of given entity pairs based on context. Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input representation, and the pooling layer is applied to the input repres"
2021.findings-acl.121,2020.findings-emnlp.71,0,0.198207,"Missing"
2021.findings-acl.121,W16-1313,0,0.118964,"-the-shell dependency parser from Stanford Parser3 on a part of Book Corpus (Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter called linAdapter on the task of dependency relation prediction. This task aims to predict the head index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and linAdapter as the input representation, and then apply a linear layer 3 https://github.com/huggingface/transformers 1408 http://nlp.stanford.edu/software/lex-parser.html OpenEntity Model FIGER P R Mi-F1 Acc Ma-F1 Mi-F1 NFGEC (Shimaoka et al., 2016) BERT-base (Zhang et al., 2019) ERNIE (Zhang et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2021) WKLM (Xiong et al., 2020) 68.80 76.37 78.42 78.60 77.20 - 53.30 70.96 72.90 73.70 74.20 - 60.10 73.56 75.56 76.10 75.70 - 55.60 52.04 57.19 60.21 75.15 75.16 75.61 81.99 71.73 71.63 73.39 77.00 RoBERTa RoBERTa + multitask K-A DAPTER (w/o knowledge) K-A DAPTER (F) K-A DAPTER (L) K-A DAPTER (F+L) 77.55 77.96 74.47 79.30 80.01 78.99 74.95 76.00 74.91 75.84 74.00 76.27 76.23 76.97 76.17 77.53 76.89 77.61 56.31 59.86 56.93 59.50 61.10 61.81 82.43 84.45 82.56 84.52 83.61 84.87 77.83 7"
2021.findings-acl.121,D18-1244,0,0.0240646,"ion classification aims to determine the correct relation between two entities in a given sentence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017). To fine-tune our models for this task, we modify the input token sequence by adding special token “@” before and after the first entity, adding “#” before and after the second entity. Then the token representations of the first special token “@” and “#” are concatenated to perform relation classification. We adopt micro F1 score as the metric to represent the model performance as previous works. Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model dependency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model. BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation without supervision from a knowledge base by matching the blanks. Other baseline models are described in Section 2 and 4.1. Results and Discussion Table 4 shows the performances of different models on TACRED. The results indicate that K-A DAPTER models significantly outperform all baselines, which directly demonstrate our models can benefit relation classification. In part"
2021.findings-acl.121,D17-1004,0,0.069357,"Missing"
2021.findings-acl.121,P19-1139,0,0.231713,"s suggest that models learned in such an unsupervised manner struggle to capture rich knowledge. For example, Poerner et al. (2020) suggest that although language models do well in reasoning about the surface form of entity names, they fail in capturing rich factual knowledge. Kassner and Sch¨utze (2020) observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa. Recently, some efforts have been made to exploit injecting knowledge into pre-trained language models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2020; Peters et al., 2019; He et al., 2020; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language modeling objective with knowledge-driven objectives and update the entire model parameters. Although these methods obtain better performance on downstream tasks, they struggle at supporting the development of versatile models with multiple kinds of knowledge injected (Kirkpatrick et al., 2017). When new kinds of knowledge are injected, model parameters need to be retrained so that previously injected knowledge would fad"
2021.findings-acl.121,P17-1018,1,0.80079,"elected. We report accuracy scores obtained from the leaderboard. Open-domain QA aims to answer questions using external resources such as collections of documents and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs corresponding to the question using the information retrieval system and then extract the answer from these retrieved paragraphs through the reading comprehension technique. Following previous work(Lin et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017) for these two datasets. To fine-tune our models for this task, the input token sequence is modified as “<SEP>question </SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to predict the start and end position of the answer span. We adopt two metrics including ExactMatch (EM) and loose F1 (Ling and Weld, 2012) scores to evaluate our models. Baselines BERT-FTRACE+SW AG (Huang et al., 2019) is the BERT model sequentially fine-tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2017) adopts a bi-directional attention network. AQA (Buck et al., 2018) propo"
2021.findings-acl.129,2021.acl-long.353,0,0.0712387,"Missing"
2021.findings-acl.129,2021.ccl-1.108,0,0.0436956,"Missing"
2021.findings-acl.129,D15-1167,1,0.706881,"redict the sentiment label of the text. More specifically, we study sentiment analysis in a few-shot learning scenario, where (1) instances in the test set are written by users never seen in the training set and (2) each user in the test set is also paired with a dozen of text-label pairs used for few-shot learning. To the best of our knowledge, there is no existing datasets meeting our demands, so we create two datasets by ourselves. One dataset comes from Diao et al. (2014), where each text is a movie review on IMDB and the sentiment label (rating) is from 1 to 10. The other dataset is from Tang et al. (2015), where each text is a restaurant review from Yelp and the sentiment label is from 1 to 5. Each dataset includes two parts: (1) part A consisting of massive user data for training a general classification model; (2) part B used for few-shot learning. To ensure that each user in part B is never seen in the training set of A, we separate these datasets based on users. To support few-shot learning, we have a constraint on the users in part B that they only write no more than 50 reviews. The data statistics are shown in Table 1. # of users 783 229 3,247 1,213 Index Input 1 2 3 4 5 6 7 8 9 ?? ?1 ?2"
2021.findings-acl.229,P19-4007,0,0.046506,"Missing"
2021.findings-acl.229,2020.acl-main.747,0,0.0919772,"Missing"
2021.findings-acl.229,W17-4718,0,0.0218182,"Missing"
2021.findings-acl.229,W16-3210,0,0.0742378,"Missing"
2021.findings-acl.229,W05-0909,0,0.195278,"By furthering considering the title signal in this retrieval task, the general performance can be improved significantly. This indicates the strong correlation between the query and the title. Besides, when taking the title signal into the zero-shot setting, we can observe a performance drop. It is due to that M3 P is pretrained with input paradigm Q-I, thus making it not suitable for evaluating Q-I-T paradigm directly. 5.1.3 Image Captioning Results As in Table 4, we report the performance of image captioning tasks on GEM-I test set with M3 P model where ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) are taken as the evaluation metrics. To study the image captioning ability of M3 P, we only use images (without title) to generate queries in GEM-I dataset. In general, the M3 P model performs relatively poor on this task, due to that most search queries are short keywords instead of a complete sentence, and they differ from our pre-training data a lot. From the above results from text-to-image retrieval task and the image captioning task, we can conclude that our proposed GEM-I dataset can demonstrate a model’s image understanding and generation ability in 2"
2021.findings-acl.229,P04-1077,0,0.249287,"led data for fine-tuning. 3) By furthering considering the title signal in this retrieval task, the general performance can be improved significantly. This indicates the strong correlation between the query and the title. Besides, when taking the title signal into the zero-shot setting, we can observe a performance drop. It is due to that M3 P is pretrained with input paradigm Q-I, thus making it not suitable for evaluating Q-I-T paradigm directly. 5.1.3 Image Captioning Results As in Table 4, we report the performance of image captioning tasks on GEM-I test set with M3 P model where ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015) are taken as the evaluation metrics. To study the image captioning ability of M3 P, we only use images (without title) to generate queries in GEM-I dataset. In general, the M3 P model performs relatively poor on this task, due to that most search queries are short keywords instead of a complete sentence, and they differ from our pre-training data a lot. From the above results from text-to-image retrieval task and the image captioning task, we can conclude that our proposed GEM-I dataset can demonstrate a model’s image unders"
2021.findings-acl.229,P16-1168,0,0.0295839,"al., 2020) are two recent benchmark efforts that extend the evaluation scenarios from monolingual to multilingual. Recent pre-trained language models benefit a lot from these datasets, by evaluating their effectiveness under a relatively fair environment. 4.2 Vision-Language Benchmarks A number of vision-language datasets have been widely used in the multimodal research. MSCOCO (Chen et al., 2015) and Flicker30K (Vinyals et al., 2015) are two datasets for imagetext retrieval and image-captioning tasks. These two benchmarks have been extended to multilingual tasks (Elliott et al., 2016, 2017; Miyazaki and Shimizu, 2016; Li et al., 2019) as well. VQA (Antol et al., 2015) and GQA (Hudson and Manning, 2019) are two datasets for visual question answering. VCR (Zellers et al., 2018) is another dataset for visual commonsense reasoning. Comparing with all these existing datasets, GEM-I has unique characteristics. First, it is a large-scale multilingual image-text dataset covering 20 different languages. Second, the query-image pairs in GEM-I come from a commercial search engine. Therefore, it has big practical values. Third, for each query-image pair, the title of the Web page that contains the image is also inclu"
2021.findings-acl.229,P18-1238,0,0.0281541,"Missing"
2021.findings-acl.229,W18-5446,0,0.0885032,"Missing"
2021.findings-acl.36,2020.emnlp-main.751,0,0.0340526,"the average number of words in source inputs. R-L: ROUGE-L. B-4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword co"
2021.findings-acl.36,W05-0909,0,0.337371,"urkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4,"
2021.findings-acl.36,2020.acl-main.9,0,0.366674,"Missing"
2021.findings-acl.36,L18-1269,0,0.017035,"the pretrained model. For these tasks, the non-pretrained model tends to generate universal responses (Li et al., 2016) or outputs. Moreover, there is still a huge gap between the bigram diversity of pretrained models and real samples (golden) on the task of XSUM, MSQG, and PersonaChat. Obviously, there exists great room for future improvement of the pretrained models in terms of output diversity. 4 Related Works Benchmarks Recently, the development of general natural language understanding (NLU) evaluation benchmarks has helped drive the progress of pretraining and transfer learning in NLP. Conneau and Kiela (2018) propose a toolkit, SentEval, for evaluating the quality of universal sentence representations. DecaNLP (McCann et al., 2018) casts ten diversified NLP tasks as a general question-answering format for evaluation. Wang et al. (2019b) propose a widely-used multi-task benchmark, GLUE, for NLU in the English language. There are nine NLU tasks in GLUE, including two single-sentence tasks, three similarity and paraphrase tasks, and four natural language inference tasks. After that, SuperGLUE (Wang et al., 2019a) is proposed as a harder counterpart of GLUE. Besides sentence- and sentence-pair classif"
2021.findings-acl.36,N19-1423,0,0.169557,"eration Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and Prophet1 Net . 1 Introduction Pretrained language models, such as BERT (Devlin et al., 2019) and other advanced pretrained models (Raffel et al., 2020; Yang et al., 2019; Liu et al., 2019; Alberti et al., 2019; Brown et al., 2020; Clark et al., 2020) have made great progress in a host of Natural Language Understanding (NLU) tasks. Meanwhile, the development of general evaluation benchmarks has also helped drive the progress of these models. These benchmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at"
2021.findings-acl.36,P17-1123,0,0.0119354,". Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MS"
2021.findings-acl.36,W18-2706,0,0.0130688,"he same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The performance gap between the pretrained model and non-pretrained model is obvious. The diff"
2021.findings-acl.36,2020.lrec-1.302,0,0.085243,"Missing"
2021.findings-acl.36,2020.acl-main.703,0,0.270812,"l language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for eva"
2021.findings-acl.36,N16-1014,0,0.162483,"mation as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a), we seek to give an overall system performance over all GLGE tasks by aggregating the scores of all tasks. We follow GLUE to adopt a simple approach that weighs each task equally. For the tasks with multiple metrics, we firstly average those metrics to get a task score. Besides, because the values of the original Distinct1 (D-1) and Distinct-2 (D-2) (Li et al., 2016) scores which are used as the metrics for dialogue task are usually quite small (less than 0.01), we re-"
2021.findings-acl.36,W04-1013,0,0.0599605,"4: BLUE-4. MTR: METEOR. D-2: Distinct-2. 2.2 Tasks and Datasets GLGE contains eight English NLG tasks, covering text summarization, question generation, generative question answering, and dialogue. Descriptions and statistics of these tasks are shown in Table 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processin"
2021.findings-acl.36,2021.ccl-1.108,0,0.0321684,"Missing"
2021.findings-acl.36,D18-1206,0,0.0116702,"summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news article, and the target output is a single-sentence summary. MSNews MicroSoft News headline generation (MSNews) is a new News headline generation dataset we collected for GLGE. We random select 151K online news articles from 2012-01-01 to 2020-09-01 from a real-world news search engine. Each article contains a professionally written single-s"
2021.findings-acl.36,J04-4002,0,0.0753216,"embedding/hidden size and 4096 feed-forward filter size. The pretraining of BARTlarge uses the same pretraining data as Liu et al. (2019), consisting of 160GB of news, books, stories, and web text. For each task in GLGE, we fine-tune BARTlarge with the same hyper-parameters used in their source 10 code for a maximum of 20000 iterations. Except BART, all the baselines adopt BERTuncased tokenizer. We fine-tune all baselines on each individual task with 4 × 16GB NVIDIA V100 GPUs. We evaluate the best model checkpoint based on the loss on the development set. During inference, we use beam search (Och and Ney, 2004) with beam size 4 or 5 and remove the duplicated trigrams in beam search (Fan et al., 2018) to obtain 9 https://github.com/pytorch/fairseq/ tree/master/examples/bart 10 https://github.com/pytorch/fairseq/ blob/master/examples/bart/README. summarization.md the generated results. 3.3 Results and Analysis Overall results. The main results are presented in Table 2. From the overall scores (highlighted in color), we can observe the fairly consistent gains moving from LSTM to Transformer, and then to pretrained base models and pretrained large models, such as ProphetNetbase and ProphetNetlarge . The"
2021.findings-acl.36,P02-1040,0,0.109921,"on generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K passages from a real world search engine. Each passage contains a highlight span and a related query, we regard the queries as questions in this dataset. After the pre-processing, there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target outpu"
2021.findings-acl.36,2020.findings-emnlp.217,1,0.68917,"on benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for evaluating the generalization cap"
2021.findings-acl.36,D16-1264,0,0.287036,"cle, and the target output is a news headline. 2.2.2 Answer-aware Question Generation The question generation task is another typical NLG task, which aims to generate a question based on a given text passage or document. Compared with answer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie,"
2021.findings-acl.36,Q19-1016,0,0.0115463,"there are 220,088 ⟨highlight span, passage, question⟩ data triples, where the source input is a news passage along with highlight span, and the target output is a user question. ROUGE-L, BLEU-4, and METEOR are used as the metrics. 2.2.3 Conversational Question Answering Conversational question answering is a classic and popular generative question answering task. Compared with the extractive question answering, such as SQuAD (Rajpurkar et al., 2016), conversational question answering requires the model to answer the question based on a running conversation history and the given passage. CoQA (Reddy et al., 2019) dataset contains 127K questions with answers, obtained from 8K conversations about text passages from seven diverse domains. After the pre-processing, there are 116,630 ⟨conversation history, passage, question, answer⟩ data 4-tuples, where the source input is a sequence of conversation history along with a given question and a given passage, and the target output is a freeform answer text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversati"
2021.findings-acl.36,2020.tacl-1.18,0,0.177338,"for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated with different tasks, datasets, and metrics, which cannot provide a coherent and comprehensive evaluation. Although there are several general evaluation benchmarks as we mentioned above, none of them are particularly designed for general language generation evaluation. To fill the gap of the NLG evaluation benchmark, we introduce the General Language Generation Evaluation (GLGE) benchmark, a new m"
2021.findings-acl.36,D15-1044,0,0.0308312,"contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the British Broadcasting Corporation (BBC), which contains professionally written single-sentence summaries. After the preprocessing, there are 226,677 ⟨article, summary⟩ data pairs, where the source input is the news"
2021.findings-acl.36,P17-1099,0,0.0378114,"ble 1, with concrete examples shown in Appendix. 2.2.1 Abstractive Text Summarization As a typical NLG task, abstractive text summarization aims to generate a short and fluent summary of a long text document. GLGE contains four abstractive text summarization tasks. As discussed in Bhandari et al. (2020), we use ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the metrics for these tasks. CNN/DailyMail (Hermann et al., 2015) dataset contains 220K articles from the Daily Mail newspapers and 93K articles from the CNN. Each article contains a bullet point summary. GLGE uses the non-anonymized variant See et al. (2017). After the pre-processing, there are 311,971 ⟨article, summary⟩ pairs, where the source input is the article, and the target output is the summary which consists of multiple sentences. Gigaword (Rush et al., 2015) contains 4M examples extracted from the news articles of the Gigaword corpus (Graff et al., 2003). After the pre-processing, there are 3,995,559 ⟨passage, summary⟩ data pairs, where the source input is the first sentence of the article, and the target output is the headline that usually contains a single sentence. XSum (Narayan et al., 2018) consists of 227K online articles from the"
2021.findings-acl.36,2020.acl-main.704,0,0.0397801,"Missing"
2021.findings-acl.36,2020.aacl-main.85,0,0.247778,"chmarks usually use an overall score to evaluate the performance of models across a wide range of NLU tasks. In addition to GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) which are general ∗ Work is done during internship at Microsoft Research Asia. 1 The source code and dataset are publicly available at https://github.com/microsoft/glge. language understanding evaluation benchmarks for English, several general language understanding evaluation benchmarks for other languages are proposed, such as CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and IndoNLU (Wilie et al., 2020) for Indonesian. Furthermore, the multilingual multi-task benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) are proposed for cross-lingual evaluation. In addition to NLU tasks, an increasing number of pretrained language models designed for Natural Language Generation (NLG) tasks have recently been proposed, such as MASS (Song et al., 2019), BERT-share (Rothe et al., 2020), BART (Lewis et al., 2020), ProphetNet (Qi et al., 2020), and ERINE-GEN (Xiao et al., 2020). However, the generalization capabilities of the language generation of these models are usually evaluated"
2021.findings-acl.36,P18-1205,0,0.0264513,"text. F1-Score (Rajpurkar et al., 2016) is used as the metric. 2.2.4 Personalized Dialogue Conversational AI is an important topic in NLG. Compared with text summarization, the responses of single-turn conversations are diverse and might lack of specification, and thus it is hard to use the single ground-truth for automatic evaluation. We select the personalizing dialogue task, which is a challenging multi-turn conversation task. In addition to the conversation history, this task gives the profile information as an additional condition to facilitate specific response generation. PersonaChat (Zhang et al., 2018) dataset consists of about 160K utterances, which requires the model to generate responses according to given multiturn conversations and persona profile. After preprocessing, there are 151,157 ⟨persona profile description text, conversation history, response⟩ data triples, where the source input is a sequence of conversation history along with several sentences of persona profile description information, and the target output is a response. BLEU-1, BLEU-2, Distinct-1, and Distinct-2 (Li et al., 2016) are used as the evaluation metrics. 2.3 Overall Score Similar to GLUE (Wang et al., 2019b) an"
2021.findings-acl.36,2020.acl-demos.30,0,0.075506,"Missing"
2021.findings-acl.36,D18-1424,0,0.0119763,"nswer-agnostic question generation tasks that can generate lots of reasonable questions, answeraware question generation (Zhou et al., 2017) is asked to generate a question asks towards the given answer span based on a given text passage or document. In order to facilitate automatic evaluation, GLGE selects two answer-aware question generation tasks: SQuAD 1.1 (Rajpurkar et al., 2016) dataset contains over 100K crowd-worker created questions with the corresponding answer spans in 536 Wikipedia articles. Since the original hidden test set of the SQuAD 1.1 is hidden, we follow (Du et al., 2017; Zhao et al., 2018) to re-split the dataset 410 with the examples from the original training set and development set. After the pre-processing, there are 98,169 ⟨answer, passage, question⟩ data triples, in which the source input is a Wikipedia passage along with an answer span, and the target output is a question. ROUGE-L, BLEU-4 (Papineni et al., 2002), and METEOR (Banerjee and Lavie, 2005) are used as the metrics. MSQG MicroSoft Question Generation (MSQG) is another dataset we collected, which is a new challenge dataset, the questions in this dataset are freely edited by daily users. For MSQG, we collect 220K"
2021.findings-acl.66,D18-1015,0,0.0665065,"Missing"
2021.findings-emnlp.23,S15-2045,0,0.0289652,"Missing"
2021.findings-emnlp.23,S14-2010,0,0.060457,"Missing"
2021.findings-emnlp.23,S16-1081,0,0.0638689,"Missing"
2021.findings-emnlp.23,S12-1051,0,0.0224819,"trix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We average the Spearman’s coefficients among the seven datasets as the final correlation score. White"
2021.findings-emnlp.23,S17-2001,0,0.0115647,"e covariance matrix Cov(E) = (E − m)T (E − m) ∈ Rd×d and U is the corresponding orthogonal matrix of eigenvectors, satisfying Cov(E) = U DU T . 4 Experiment We evaluate sentence embeddings on the task of unsupervised semantic textual similarity. We show experimental results and report the best way to derive unsupervised sentence embedding from PLMs. 4.1 Experiment Settings Task and Datasets The task of unsupervised semantic textual similarity (STS) aims to predict the similarity of two sentences without direct supervision. We experiment on seven STS datasets, namely the STS-Benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (Marelli et al., 2014), and the STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). These datasets consist of sentence pairs with labeled semantic similarity scores ranging from 0 to 5. Evaluation Procedure Following the procedures in SBERT (Reimers and Gurevych, 2019), we first derive sentence embeddings for each sentence pair and compute their cosine similarity score as the predicted similarity. Then we compute the Spearman’s rank correlation coefficient between the predicted similarity and gold standard similarity scores as the evaluation metric. We ave"
2021.findings-emnlp.23,D14-1162,0,0.103087,"Missing"
2021.findings-emnlp.23,D19-1410,0,0.402486,"We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have three main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top and bottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we"
2021.findings-emnlp.23,D19-1059,0,0.0503764,"Missing"
2021.findings-emnlp.23,2020.emnlp-main.124,0,0.136199,"ith less than 10 lines of code consistently boosts the performance. 1 1 Introduction Pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019) perform well on learning sentence semantics when fine-tuned with supervised data (Reimers and Gurevych, 2019; Thakur et al., 2020). However, in practice, especially when a large amount of supervised data is unavailable, an approach that provides sentence embeddings in an unsupervised way is of great value in scenarios like sentence matching and retrieval. While there are attempts on unsupervised sentence embeddings (Arora et al., 2017; Zhang et al., 2020), to the best of our knowledge, there is no comprehensive study on various PLMs with regard to multiple factors. Meanwhile, we aim to provide an easy-touse toolkit that can be used to produce sentence embeddings upon various PLMs. In this paper, we investigate PLMs-based unsupervised sentence embeddings from three aspects. First, a standard way of obtaining sentence embedding is to pick the vector of [CLS] token. We explore whether using the hidden vectors of other tokens is beneficial. Second, some works suggest producing sentence embedding from the last layer or the combination of the last t"
2021.findings-emnlp.249,D15-1075,0,0.0328244,"Missing"
2021.findings-emnlp.249,P18-1015,0,0.0254825,"ey elements of contrastive learning are: (1) the construction of positive and negative samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generation, Liu et al. (20"
2021.findings-emnlp.249,N19-1423,0,0.036348,"Missing"
2021.findings-emnlp.249,2021.emnlp-main.552,0,0.062778,"Missing"
2021.findings-emnlp.249,2020.acl-main.703,0,0.391352,"on, and show that our model has on heavily entity-centric tasks such as FEVER potential commercial value. (Thorne et al., 2018). However, while using sparse vector space retrieval models can retrieve relevant 1 Introduction prototypes that contain a set of concepts, there can Pre-trained language models have achieved impres- be significant domain mismatches between the resive results across a wide range of NLP tasks (De- trieved results and target distribution, making it vlin et al., 2019; Yang et al., 2019; Sun et al., 2019; difficult for generation models to bridge between Liu et al., 2019; Lewis et al., 2020a; Qi et al., 2020; prototypes and targets. We argue that a two-stage He et al., 2020b). However, their ability to accu- retrieval strategy alleviates this issue by combining rately reflect factual knowledge or perform logi- sparse vector space search and dense representacal inference is still limited. To investigate the tion filters. First, a sparse vector retrieval model is used to find passage candidates with high coverage ∗ Work done during an internship at Microsoft Research of concept words, and then a dense vector-based Asia. † Corresponding author. filter is applied to score the candid"
2021.findings-emnlp.249,2020.findings-emnlp.165,0,0.0605969,"Missing"
2021.findings-emnlp.249,2020.acl-main.228,0,0.01997,"samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generation, Liu et al. (2020) which is computationally prohibitive. Moco (He propose a knowledge graph-augmented lang"
2021.findings-emnlp.249,2020.emnlp-main.550,0,0.0380335,"Missing"
2021.findings-emnlp.249,2021.ccl-1.108,0,0.0595026,"Missing"
2021.findings-emnlp.249,P02-1040,0,0.109512,"emory bank is set to 4096, and the momentum coefficient is set to 0.999. 4.3.2 Baselines We use several state-of-the-art pre-trained language generation models as baselines: GPT-2 (Radford et al., 2019), BERT-Gen (Bao et al., 2020), UniLM (Dong et al., 2019), UniLM-v2 (Bao et al., 2020), T5 (Raffel et al., 2020), and BART (Lewis et al., 2020a). All models are fine-tuned in seq2seq mode. We also compare our model with two strong baselines that use external knowledge: EKI (Fan et al., 2020) and KG-BART (Liu et al., 2020). 4.3.3 Evaluation Metrics To evaluate generation performance, we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), in addition to evaluation metrics for captioning tasks, namely CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). As all metrics score the output in the range [0, 100], we also present the average score across all metrics. 2923 Model ROUGE-2/L BLEU-3/4 METEOR CIDEr SPICE Overall GPT-2 (Radford et al., 2019) BERT-Gen (Bao et al., 2020) UniLM (Dong et al., 2019) UniLM-v2 (Bao et al., 2020) T5 (Raffel et al., 2020) BART (Lewis et al., 2020a) 17.18 18.05 21.48 18.24 22.01 22.23 39.28 40.49 43.87 40.62 42.97 41.98 30.70 30.40"
2021.findings-emnlp.249,2020.findings-emnlp.217,1,0.868036,"model has on heavily entity-centric tasks such as FEVER potential commercial value. (Thorne et al., 2018). However, while using sparse vector space retrieval models can retrieve relevant 1 Introduction prototypes that contain a set of concepts, there can Pre-trained language models have achieved impres- be significant domain mismatches between the resive results across a wide range of NLP tasks (De- trieved results and target distribution, making it vlin et al., 2019; Yang et al., 2019; Sun et al., 2019; difficult for generation models to bridge between Liu et al., 2019; Lewis et al., 2020a; Qi et al., 2020; prototypes and targets. We argue that a two-stage He et al., 2020b). However, their ability to accu- retrieval strategy alleviates this issue by combining rately reflect factual knowledge or perform logi- sparse vector space search and dense representacal inference is still limited. To investigate the tion filters. First, a sparse vector retrieval model is used to find passage candidates with high coverage ∗ Work done during an internship at Microsoft Research of concept words, and then a dense vector-based Asia. † Corresponding author. filter is applied to score the candidates, and filter 2"
2021.findings-emnlp.249,W18-5713,0,0.0203819,"ter vision. The two key elements of contrastive learning are: (1) the construction of positive and negative samples; and (2) the learning framework. 2.2.1 Sample Construction Learning Framework Previous contrastive learning methods have required either specialized architectures (Bachman There is significant work on incorporating exter- et al., 2019; Hénaff, 2020) or a memory bank to nal knowledge from knowledge bases and incor- store large volumes of negative samples (Wu et al., porating retrieved information in language gener- 2018; Tian et al., 2020). Chen et al. (2020) present ation tasks (Weston et al., 2018; Cao et al., 2018; a simple framework consisting of a feature extracGuan et al., 2019; Hossain et al., 2020). Lewis tion module, and a non-linear transformation modet al. (2020b) explore a general-purpose fine-tuning ule, which outperforms previous work on ImageNet recipe for retrieval-augmented generation that com- (Russakovsky et al., 2015) without using a specialbines a dense passage retriever (Karpukhin et al., ized architecture or a memory bank. However, it re2020) with a BART (Lewis et al., 2020a) genera- quires a large batch size to yield high performance, tor. For commonsense generati"
2021.findings-emnlp.249,P18-1238,0,0.0426037,"Missing"
2021.findings-emnlp.249,N19-1421,0,0.0397719,"Missing"
2021.findings-emnlp.249,N18-1101,0,0.0590496,"Missing"
2021.findings-emnlp.249,D18-1009,0,0.056409,"Missing"
2021.findings-emnlp.249,N18-1074,0,0.0277987,"Missing"
2021.findings-emnlp.75,2020.acl-main.421,0,0.0284757,"age, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he hu ja jv km la lo lt lv mr my"
2021.findings-emnlp.75,Q19-1038,0,0.0232089,"ar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he hu ja jv km la lo lt lv mr my #3 ne or pa pl sa sd sk th uk wuu zh #4 az bn gu hy ka kk kn ko k"
2021.findings-emnlp.75,2020.emnlp-main.367,0,0.0200812,"ag added to the input of encoder. Their approach focuses on 23 relatively high resource languages. Fan et al. (2020) clusters languages into several groups according to language family, cultural connection and geographical proximity. They do not obtain any language representation and their language groups are human 2 Related Work annotated. Kudugunta et al. (2019) reveals the connection between language SVCCA similarity from Our approach presents a new pipeline of multilinNMT models and language family. Their evaluagual pre-training. Our representation sprachbund tion relies on parallel data. Chung et al. (2020) clasis inspired by linguistic language clustering. Our sifies languages into groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4"
2021.findings-emnlp.75,2020.tacl-1.30,0,0.0214491,"ical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc pt ro scn sco sq sv tl ur war ar arz bg bs cy fa hi hr id is mg mk ms ps ru sh sl so #2 sr su sw yi am as be ckb cs et eu fi he h"
2021.findings-emnlp.75,2020.acl-main.747,0,0.243341,"ge representation from multilingual pre-trained models and massive multilingual corpora. ii) We conduct extensive analysis to show language representation and representation sprachbunds can reflect linguistic language similarity and relatedness from multiple perspectives, therefore they can be considered as new paradigm for clustering similar languages in linguistics. iii) We use representation sprachbunds in multilingual pre-training to alleviate the cross-lingual contradiction and differences, and obtain significant improvements compared with strong baselines. ple and Conneau, 2019), XLM-R (Conneau et al., 2020), Unicoder (Huang et al., 2019) and mT5 (Xue et al., 2020). Several benchmarks are proposed to evaluate the cross-lingual ability of multilingual pre-trained models, including XGLUE (Liang et al., 2020), XTREME (Hu et al., 2020) and XTREME-R (Ruder et al., 2021) Language Clustering in Linguistics The linguists propose to classify languages in several ways from different perspectives. There are two main kinds of language clustering: genealogical clustering and typological clustering. In genealogical clustering, languages are clustered into language families (Durbin, 1985; Marcantonio, 2002) by"
2021.findings-emnlp.75,D18-1269,0,0.0294155,"tures of languages, therefore the similarity between language representation can be a good metric for clustering languages. With a 768-dimension vector as numerical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl"
2021.findings-emnlp.75,Y17-1038,0,0.0627517,"Missing"
2021.findings-emnlp.75,N19-1423,0,0.0694387,"Missing"
2021.findings-emnlp.75,E17-2002,0,0.0205942,"ical cognates between languages (Hymes, 1960), which is very time-consuming. Our language representation can even further help linguists infer lexical similarity more easily (e.g. linear regression between representation similarity and lexical similarity). The similarity data is shown in Appendix B. Relationship with Language Syntax Languages have diverse syntactic features defined by linguists and can be classified through these features. We show that the distribution of our language representation implies the syntactic features of corresponding languages. We use the lang2vec Python package (Littell et al., 2017) to query the URIEL database7 . We choose three representative syntactic features: (subject, object, verbal) word order, adjective position and adposition position. As shown in Figure 3, we find that languages with the same syntactic features approximately have similar language representation. 7 http://www.cs.cmu.edu/~dmortens/uriel. html 885 Surprise: Help for Exploring Linguistic Mystery Coincidentally, we find that our language representation connect with an existing underexplored linguistic mystery. In Figure 2, Uralic and Austronesian languages (jv, id, ms) have similar language represent"
2021.findings-emnlp.75,D19-1089,0,0.039809,"Missing"
2021.findings-emnlp.75,2020.lrec-1.494,0,0.0191531,"to groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4 (Xue on generating and analyzing language representaet al., 2020), CCNet (Wenzek et al., 2020)) to pre- tion (Tiedemann, 2018; Östling and Tiedemann, train large multilingual models like XLM (Lam- 2017). 882 Figure 1: The pipeline of our approach. We first generate language representation for each language with multilingual pre-trained models (XLM-R) and multilingual corpora. We cluster languages into several representation sprachbunds composed of languages with similar representation. We pre-train one model for each representation sprachbund with corpora from that representation sprachbund. Best viewed in color. Compared to existing works, our approach enjoys the following advantages."
2021.findings-emnlp.75,D19-1382,0,0.0170242,"he similarity between language representation can be a good metric for clustering languages. With a 768-dimension vector as numerical feature for each language, we can implement clustering algorithms to cluster similar languages into a representation sprachbund. Our language representation will be released later. 5 Representation Sprachbund For Multilingual Pre-training 5.1 Datasets We collect massive multilingual corpora for pretraining and use four datasets for downstream task evaluation. The multilingual corpora has been described in Section 4.1. We use XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), NER (Pan et al., 2017), Part of Speech Tagging (POS)(Zeman et al., 2019), MLQA (Lewis et al., 2019), TydiQA (Clark et al., 2020), XQuAD(Artetxe et al., 2020) and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019) as downstream tasks. For cross-lingual sentence retrieval, we collect 21 language pairs and extract 1000 sentence-pairs for each language-pair from tatoeba8 . This task aims to find the nearest neighbor for each sentence in the other language. #i is the ith representation sprachbund af als an ast bar br ca ceb da de en eo es el fr fy ga gd #1 gl ia it ku lb nds nl nn no oc"
2021.findings-emnlp.75,2021.acl-long.560,0,0.0340023,"anguage representation and their language groups are human 2 Related Work annotated. Kudugunta et al. (2019) reveals the connection between language SVCCA similarity from Our approach presents a new pipeline of multilinNMT models and language family. Their evaluagual pre-training. Our representation sprachbund tion relies on parallel data. Chung et al. (2020) clasis inspired by linguistic language clustering. Our sifies languages into groups based on their token analysis is closely related to methodology in linoverlap. Only lexical information of languages is guistics. used in their approach. Yu et al. (2021) uses multiMultilingual Pre-Training Multilingual pre- lingual denoising autoencoder to generate language training was proposed to pre-train a single model embeddings and analyze the clusters derived from with hundreds of languages. Many works use a the embeddings. There are also a few earlier works large amount of multilingual data (e.g., mC4 (Xue on generating and analyzing language representaet al., 2020), CCNet (Wenzek et al., 2020)) to pre- tion (Tiedemann, 2018; Östling and Tiedemann, train large multilingual models like XLM (Lam- 2017). 882 Figure 1: The pipeline of our approach. We fir"
2021.naacl-main.135,P19-1285,0,0.0508064,"Missing"
2021.naacl-main.135,N19-1122,0,0.0178446,"ion as a new self-attention network. Zhao et al. (2019) explores parallel multi-scale representation learning to capture both long-range and short-range language structures with combination of convolution and self-attention. In our work, DMAN, SAN and FFN are unified in Mask Attention Networks, where DMAN is a supplement of SAN and FFN that specializes in localness modeling. Moreover, we investigate different collaboration mechanisms. Related Work Recently, there is a large body of work on im- 6 Conclusion proving Transformer (Vaswani et al., 2017) for various issues. For recurrence modeling, Hao et al. (2019) introduces a novel attentive recurrent netIn this paper, we introduce Mask Attention Network to leverage the strengths of both attention and works and reformulate SAN and FFN to point recurrent networks. For context modeling, Yang out they are two special cases with static mask et al. (2019a) focuses on improving self-attention in MANs. We analyze the the deficiency of through capturing the richness of context and pro- SAN and FFN in localness modeling. Dynamic poses to contextualize the transformations of the Mask Attention Network is derived from MANs query and key layers. Wu et al. (2019)"
2021.naacl-main.135,N03-1020,0,0.173018,"Missing"
2021.naacl-main.135,W18-6301,0,0.0396285,"Missing"
2021.naacl-main.135,P02-1040,0,0.110664,"warmup steps is 8k and the total updates is 50k. with 0.1 label smoothing rate. Inverse-sqrt learning The optimizer of model is Adam with (0.9,0.98). rate scheduler are employed, the peak learning rates The dropout and clip-norm are both 0.1. During are 1.5e-2, 1e-2 and 7e-3 with 8k warmup, 50k decoding, the beam size are both 5, the max length update, 80k update and 80k update for transformer and length penalty are 50 and 2.0 for CNN/Daily 1696 Mail, 30 and 1.0 for Gigaword. The models are trained on 4 P40 GPUs. 3.2 Experimental Results 3.2.1 Machine Translation In machine translation, BLEU (Papineni et al., 2002) is employed as the evaluation measure. Following common practice, we use tokenized casesensitive BLEU and case-insensitive BLEU for WMT14 En-De and IWSLT14 De-En, respectively. We take Transformer (Vaswani et al., 2017) as the baseline and compare with other concurrent methods. Convolutional Transformer (Yang et al., 2019b) restricts the attention scope to a window of neighboring elements in order to model locality for self-attention model. Local Transformer (Yang et al., 2018) casts localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to"
2021.naacl-main.135,D15-1044,0,0.145263,"Missing"
2021.naacl-main.135,P17-1099,0,0.221759,"IWSLT14 German-to-English (De-En) and WMT14 EnglishAutomatic summarization aims to produce a conto-German (En-De). IWSLT14 De-En dataset con- cise and fluent summary conveying the key inforsists of about 153K/7K/7K sentence pairs for train- mation in the input text. We focus on abstractive ing/validation/testing. WMT14 En-De dataset con- summarization, a generation task where the sumsists of about 4.5M sentence pairs, and the models mary is not limited in reusing the phrases or senwere validated on newstest2013 and examined on tences in the input text. We use the CNN/Daily newstest2014. Mail (See et al., 2017) and Gigaword (Rush et al., Our data processing follows Lu et al. (2019). 2015) for model evaluation. For IWSLT2014, we set our model into the small Following Song et al. (2019), we set the hidden one, the hidden size, embeddings and attention size, embeddings and attention heads to 768, 768, heads to 512, 512, and 4 respectively. For the and 12 respectively. Our model consists of a 6-layer WMT14 dataset, following the Transformer setting encoder and 6-layer decoder. For the convenience of Vaswani et al. (2017), we set our model into the of comparison, the training follows classic seq2seq base"
2021.naacl-main.135,N18-2074,0,0.445559,"rforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long-range (∼ 8192 tokens) language modeling. Recently, some works targeting on FFN have been proposed. Lu et al. (2019) gives a new understanding of Transformer from a multi-particle dynamic"
2021.naacl-main.135,P19-1032,0,0.107927,"ial layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative positional information for the sequence. Sukhbaatar et al. (2019a) proposes attention span to control the maximum context size used in SAN and scales Transformer to long"
2021.naacl-main.135,K16-1028,0,0.0242906,"n MANs. Under our design principles, there are three elements: FFN, SAN, and DMAN. For the convenience of comparison, we take FFN as the last component in the sequential layered structure. We try different collaboration methods and test them on IWSLT2014 German-to-English (De-En). The results are shown in the Table 3. We conclude that: Abstractive Summarization We use the F1 score of ROUGE (Lin and Hovy, 2003) as the evaluation metric1 . In Table 2, we compare our model against the baseline Transformer (Vaswani et al., 2017) and several generation models on CNN/Daily Mail and Gigaword. LEAD3 (Nallapati et al., 2016) extracts the first three sentences in a document as its summary. PTGEN+Converage (See et al., 2017) is a sequenceto-sequence model based on the pointer-generator network. As shown in Table 2, our model outperforms Transformer by 1.4 in ROUGE-1, 2.2 in 1 Further Analysis 1. Our proposed C#5 achieves the best performance that verify the effectiveness of our proposed sequential layered structure. 2. All of C#3, C#4 and C#5 outperform C#1 and C#2, and the least improvement in BLEU is 0.2. This shows that no matter what collaboration method, models with the participation of DMAN perform better tha"
2021.naacl-main.135,D18-1475,0,0.336421,"ng and capture the global 1692 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1692–1701 June 6–11, 2021. ©2021 Association for Computational Linguistics semantics. In contrast, mask of FFN disables it to perceive the information of other tokens and forces it into self-evolution. We believe that these two specialties endowed by two mask matrices make the success of Transformer in text representation. Although positive results of Transformer have been reported, recent works (Shaw et al., 2018; Yang et al., 2018; Guo et al., 2019) have shown that modeling localness would further improve the performance through experiments. We argue that deficiency of Transformer in local structure modeling is caused by the attention computation with static mask matrix. In the framework of MANs, we find a problem that irrelevant tokens with overlapping neighbors incorrectly attend to each other with relatively large attention scores. For example “a black dog jump to catch the frisbee”, though “catch” and “black” are neither relevant nor neighbors, for the reason that both of them are highly related to their common nei"
2021.naacl-main.135,N19-1407,0,0.0934995,"ild a distance-dependent then globalness, and take the step for self-evolution mask matrix SM. If each token only model the in the end. relationship with those tokens within b units of itself, we can set 3 Experiments  0, |t − s |&gt; b SM[t, s] = (11) In this section, we introduce our experiments. 1, |t − s |≤ b We first describe the experimental details in § 3.1. where t, s are the positions of query and key, and Then we show the experimental results in § 3.2. 1695 Model IWSLT14 De-En small params WMT14 En-De params big params base Transformer (Vaswani et al., 2017) Convolutional Transformer (Yang et al., 2019b) Weighted Transformer (Ahmed et al., 2017) Local Transformer (Yang et al., 2018) Relative Transformer (Shaw et al., 2018) Scaling NMT (Ott et al., 2018) Dynamic Conv (Wu et al., 2019) 34.4 35.2 36M - 27.3 28.2 28.4 28.5 26.8 - 62M 88M 65M 89M - 28.4 28.7 28.9 29.2 29.2 29.3 29.7 213M 213M 268M 213M 213M Ours 36.3 37M 29.1 63M 30.4 215M Table 1: Translation performance (BLEU) on IWSLT14 De-En and WMT14 En-De testsets. Finally we conduct the ablation study and analysis in § 4. 3.1 3.1.1 Experimental Setting Machine Translation big, base and small model with max-tokens 4096, 12288 and 8192 per"
2021.naacl-main.135,K19-1074,0,0.021388,"tion network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer. 1 Introduction Recently, Transformer (Vaswani et al., 2017) has been widely applied in various natural language processing tasks, such as neural machine translation (Vaswani et al., 2017) and text summarization (Zhang et al., 2019). To further improve the performance of the text representation, Transformer-based variants have attracted a lot of attention (Lu et al., 2019; Sukhbaatar et al., 2019a,b; Bugliarello and Okazaki, 2019; Ma et al., 2020). Each building block of Transformer has two sublayers: Self-Attention Network (SAN) and Feed-Forward Network (FFN). Shaw et al. (2018) Figure 1: The mask matrices of (a) SAN, (b) DMAN and (c) FFN in Mask Attention Networks. Color that fades from black to white means the values in mask matrices decrease from 1 to 0. presents an extension to SAN which incorporates the relative po"
C10-1035,W09-0437,0,0.0415685,"Missing"
C10-1035,J07-2003,0,0.0991921,"Missing"
C10-1035,N10-1141,0,0.0470614,"Missing"
C10-1035,D07-1079,0,0.0330617,"Missing"
C10-1035,W06-1607,0,0.0862014,"Missing"
C10-1035,W07-0717,0,0.1147,"Missing"
C10-1035,P06-1121,0,0.170188,"Missing"
C10-1035,P08-1067,0,0.0606721,"Missing"
C10-1035,D08-1011,0,0.0336369,"Missing"
C10-1035,P09-1107,0,0.034877,"Missing"
C10-1035,P09-1066,1,0.872421,"Missing"
C10-1035,P09-1065,0,0.0623191,"Missing"
C10-1035,D08-1066,0,0.0418409,"Missing"
C10-1035,E06-1005,0,0.0843704,"Missing"
C10-1035,P00-1056,0,0.249082,"Missing"
C10-1035,P03-1021,0,0.0605587,"Missing"
C10-1035,J04-4002,0,0.109408,"Missing"
C10-1035,P08-1066,0,0.0397231,"Missing"
C10-1035,J97-3002,0,0.0995427,"Missing"
C10-1035,P06-1066,0,0.0484801,"Missing"
C10-1035,C08-1144,0,0.033509,"Missing"
C10-1036,J07-2003,0,0.117997,"Missing"
C10-1036,P09-1107,0,0.113563,"Missing"
C10-1036,P08-1023,0,0.055569,"Missing"
C10-1036,W05-1506,0,0.103769,"Missing"
C10-1036,P08-1067,0,0.0669287,"Missing"
C10-1036,W04-3250,0,0.0749687,"Missing"
C10-1036,N04-1022,0,0.722962,"Missing"
C10-1036,P09-1019,0,0.462334,"Missing"
C10-1036,P03-1021,0,0.202425,"Missing"
C10-1036,J04-4002,0,0.151503,"Missing"
C10-1036,P07-1040,0,0.0636909,"Missing"
C10-1036,D08-1065,0,0.787903,"n model’s distribution. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Their work has shown that MBR decoding performs better than Maximum a Posteriori (MAP) decoding for different evaluation criteria. After that, many dedi1 This work has been done while the author was visiting Microsoft Research Asia. Mu Li, Dongdong Zhang, Ming Zhou Microsoft Research Asia muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com cated efforts have been made to improve the performances of SMT systems by utilizing MBRinspired methods. Tromble et al. (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al. (2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al.’s work. DeNero et al. (2009) proposed a fast consensus decoding algorithm for MBR for both linear and non-linear similarity measures. All work mentioned above share a common setting: an MBR decoder is built based on one and only one MAP decoder. On the other hand, recent researc"
C10-1036,W02-1021,0,0.0878161,"Missing"
C10-1036,P09-1065,0,0.171644,"Missing"
C10-1036,D08-1022,0,\N,Missing
C10-1036,D07-1105,0,\N,Missing
C10-1036,P09-1066,1,\N,Missing
C10-1036,P06-1066,0,\N,Missing
C10-1036,P09-1064,0,\N,Missing
C10-1036,J97-3002,0,\N,Missing
C10-1036,2008.amta-srw.3,0,\N,Missing
C16-1236,P14-1133,0,0.0265941,"Missing"
C16-1236,Q15-1039,0,0.0714069,"Missing"
C16-1236,D13-1160,0,0.40406,": Sum of CNN scores between context pattern and binding Path for each entity constraint Po : Sum of embedding similarities between superlative phrase and binding Path for each ordinal constraint Table 3: Features and their description. 5 Experiment We introduce experiment part on these aspects. We first introduce the settings of our experiments, especially the three data sets containing question/answer (QA) pairs. On these data sets, the results of our method are given, and based on the results we analyze drawbacks. 5.1 Set Up System Components We use the entire Freebase dump which is same as Berant et al. (2013) and host it with Virtuoso engine11 . Besides, an entity linker (Yang and Chang, 2015), the Stanford NER (Finkel et al., 2005), and an in-house implementation of shift-reduce dependency parser (Zhang and Nivre, 2011) with Stanford dependency (De Marneffe et al., 2006) which is used in detecting temporal clause are adopted in this work. Data Sets We evaluate our approach on three data sets. (i) ComplexQuestions (CompQ): It is a new data set which includes 2100 QA pairs released by this work with the details in Section 2. (ii) WebQuestions (WebQ): It contains 3778 QA pairs on training set and 20"
C16-1236,D14-1067,0,0.0389445,"Missing"
C16-1236,de-marneffe-etal-2006-generating,0,0.0293992,"Missing"
C16-1236,P15-1026,1,0.872928,"Missing"
C16-1236,P05-1045,0,0.460005,"contain any word in this word set, we simply remove it. This is intuitive, as WebQuestions and SimpleQuestions are our training data, and we only consider queries that can be covered by the training data as query candidates. Last, we classify the remaining queries based on the following rules: (1) If a question contains at least two non-overlap entities, then it belongs in the Multi-Entity category; (2) If a question contains a type phrase that comes from Freebase, then it belongs in the Type category; (3) If a question contains a time expression detected by an Named Entity Recognizer (NER) (Finkel et al., 2005), then it belongs in the Explicit Temporal category; (4) If a question contains keywords “when”, “before”, “after” and “during” in the middle, then it belongs in the Implicit Temporal category; (5) If a question contains ordinal number or superlative phrase from WordNet (Miller, 1995), then it belongs in the Ordinal category; (6) If a question starts with “how many”, or includes “number of” or “count of”, then it belongs in the Aggregation category. Note, a multi-constraint question may contain multiple types of constraints. We show constraint types, examples, and distributions in Table 1. Ten"
C16-1236,Q14-1030,0,0.032321,"Missing"
C16-1236,Q16-1010,0,0.0299673,"Missing"
C16-1236,P16-1220,0,0.269382,"Missing"
C16-1236,P15-1049,0,0.115534,"of x in ascending order, return the nth one Return the number of entity set x. Table 2: Functional predicates defined in this work. query set, which contains 20,999,951 distinct 5W1H questions2 that satisfy the following two rules: (i) each query should not contain pronouns (e.g., ‘you’, ‘my’, etc.), as questions with such words are usually non-factual questions, and (ii) each query’s length is between 7 and 20, as short queries seldom contain multi-constraints, and long queries are usually difficult to answer. Then, we further sample 10 percent of questions, and use an entity linking method (Yang and Chang, 2015) to detect entities. If no entity can be detected from a query, we simply remove it. Next, both WebQuestions and SimpleQuestions are used to extract a set of words, without considering stop words and entity words. If a query does not contain any word in this word set, we simply remove it. This is intuitive, as WebQuestions and SimpleQuestions are our training data, and we only consider queries that can be covered by the training data as query candidates. Last, we classify the remaining queries based on the following rules: (1) If a question contains at least two non-overlap entities, then it b"
C16-1236,D14-1071,1,0.685243,"Missing"
C16-1236,P14-1090,0,0.15918,"Missing"
C16-1236,N15-3014,0,0.14959,"Missing"
C16-1236,P14-2105,0,0.0197143,"vertices, and x denotes the answer. For example, the basic query graph B in Figure 2 can be represented as hUnited States, officials-y0 -holder, xi. To measure the quality of each basic query graph constructed, we leverage a convolutional neural network (CNN)-based model that is similar to (Gao et al., 2015; Shen et al., 2014b; Shen et al., 2014a; Yih et al., 2015) to calculate the similarity between question and the path of the basic query graph. We will describe the training resource in Section 4.4. 4.2 Constraint Detection and Binding Basic query graph is fit for single relation questions (Yih et al., 2014; Bordes et al., 2015), but not suffices to express a question with multiple constraints, such as the question in Figure 2. Hence, we propose to use constraints to restrict the answer set by adding them into the basic query graph.Adding a constraint contains two steps: Constraint Detection and Constraint Binding. We explain how to add each of the six kinds of constraints respectively in the following parts. Entity Constraint Entity constraint is designed to understand entities and relations which are often expressed by noun phrases and verb phrases. A constraint with an entity as its constant"
C16-1236,P15-1128,0,0.706817,"s, variable &lt; Equal vertices y0 and x, and two edges officials and holder. {C1 , C2 , C3 } is an ordered con?3 ?2 ?1 1 MaxAtN straint sequence detected based on the question, where C1 = hPresident,Equal,y1 i, C2 = h2000,&lt;,y2 i, C3 = h1,MaxAtN,y2 i. By adding ? C1 , C2 , C3 in order, we can construct the MulCG United States officials ?0 holder ? in Figure 2. Note, different constraint order can result in different MulCGs. We will introduce Figure 2: MulCG for question “Who was the first how to generate a MulCG in Section 4. president of United States after 2000?” Compared to the stage graph in Yih et al. (2015), our MulCG has the following two differences: (1) Entity constraints can be added beyond single KB fact, while stage graph only considers entities that connect to the CVT node of a single KB fact as constraints. (2) Non-entity constraints are defined and handled in a systematic way, while stage graph only considers limited non-entity constraints, i.e., type and gender. 4 Our Approach Problem Formalization Given a MulCQ Q and a KB K, the question is parsed into a set of MulCGs H(Q). For each MulCG G ∈ H(Q), a feature vector F(Q, G) is extracted and the one with the highest 5 If p contains two"
C16-1236,P11-2033,0,0.0142789,"nd their description. 5 Experiment We introduce experiment part on these aspects. We first introduce the settings of our experiments, especially the three data sets containing question/answer (QA) pairs. On these data sets, the results of our method are given, and based on the results we analyze drawbacks. 5.1 Set Up System Components We use the entire Freebase dump which is same as Berant et al. (2013) and host it with Virtuoso engine11 . Besides, an entity linker (Yang and Chang, 2015), the Stanford NER (Finkel et al., 2005), and an in-house implementation of shift-reduce dependency parser (Zhang and Nivre, 2011) with Stanford dependency (De Marneffe et al., 2006) which is used in detecting temporal clause are adopted in this work. Data Sets We evaluate our approach on three data sets. (i) ComplexQuestions (CompQ): It is a new data set which includes 2100 QA pairs released by this work with the details in Section 2. (ii) WebQuestions (WebQ): It contains 3778 QA pairs on training set and 2032 on test set which is released by Berant et al. (2013). The questions are collected from query log and the answers are manually labeled based on Freebase. (iii) SimpleQuestions (SimpQ): Each question in SimpleQuest"
C16-1236,P14-1091,1,\N,Missing
D09-1114,D08-1011,0,0.0122079,"mbination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translat"
D09-1114,2008.amta-srw.3,0,0.173771,". As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment betw"
D09-1114,P05-1033,0,0.863722,"e principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level co"
D09-1114,C08-1005,0,0.0133609,"el or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diver"
D09-1114,N07-2015,0,0.183044,"Missing"
D09-1114,P05-3026,0,0.0288038,"pts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models a"
D09-1114,W04-3250,0,0.482397,"typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di"
D09-1114,P09-1066,1,0.820368,"re to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version"
D09-1114,2006.amta-papers.11,0,0.0160705,"ization project settles in Zhoushan China 's largest desalination project in Zhoushan China 's largest sea water desalination project in Zhoushan Chinese 海水 淡化 海水 淡化 English desalination sea water desalination ? ?? 0.4000 0.1748 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment error"
D09-1114,P06-1077,0,0.0298112,"e sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypot"
D09-1114,W06-1606,0,0.0444251,"systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original o"
D09-1114,D07-1105,0,0.0609944,"multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination. In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard l"
D09-1114,E06-1005,0,0.0510437,"multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand a"
D09-1114,P02-1038,0,0.183819,", for a specific sentence some individual systems could generate better translations. It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system. 3 Feature Subspace Method for SMT System Ensemble Construction In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination. 3.1 SMT System Ensemble Generation Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). Let ? (?, ?) be a feature function, and ?? be its weight, an SMT model ? can be formally written as: ? ∗ = argmax ? ?? ? (?, ?) (1) ? Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in ?, a new SMT system can be constructed based on it, which we call a sub-system. Next we will use Ω to denote the full feature space defined by the entire set of features used in ?, and ? ⊆ Ω is a feature subset that belongs to ?(Ω), the power set of Ω. The derived subsystem based on subset ? ⊆ Ω is denoted by ?? . Although"
D09-1114,P03-1021,0,0.0402856,"occurrences of n-grams of ? in ? ′ : ?? ?, ? ′ = ? −?+1 ?=1 ?(???+?−1 , ? ′ ) ?− (?, ℋ ? ) = ( ? − ? + 1 − ?? (?, ? ′ )) ? ′ ∈ℋ ? ,? ′ ≠? (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2? + 2 if ? orders of n-gram are to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. B"
D09-1114,J03-1002,0,0.00495516,"∙,∙) is the indicator function ? ???+?−1 , ? ′ is 1 when the n-gram ???+?−1 appears in ? ′ , otherwise it is 0. In order to give the combination model an opportunity to penalize long but inaccurate transla1099 Data set #Sentences MT03 (dev) 919 MT04 (test) 1,788 MT05 (test) 1,082 #Words 23,782 47,762 29,258 Table 3: Data set statistics. We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experimen"
D09-1114,J04-4002,0,0.0458893,"he baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combinatio"
D09-1114,N07-1029,0,0.0865499,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,P07-1040,0,0.17902,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,W08-0329,0,0.0169325,"ranslations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an"
D09-1114,P06-1066,0,0.0451678,"veloped systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation. Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used. We list the features in consideration as follows:  PEF and PFE: phrase translation probabilities ? ? ? and ? ? ?  PEFLEX and PFELEX: lexical weights ???? ? ? and ???? ? ?  PP: phrase penalty  WP: word penalty  BLP: bi-lexicon pair counting how many entries of a conven"
D09-1114,2005.eamt-1.20,0,\N,Missing
D12-1041,W05-0823,0,0.0512393,"Missing"
D12-1041,P11-1103,0,0.0197714,"research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) t"
D12-1041,2003.mtsummit-papers.6,0,0.0464169,"gnments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be all"
D12-1041,P05-1066,0,0.0424994,"Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are"
D12-1041,D11-1018,0,0.0476346,"03) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason o"
D12-1041,2010.amta-papers.22,0,0.0348025,"source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules fo"
D12-1041,P06-1097,0,0.0289981,",e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based"
D12-1041,C10-1043,0,0.0217631,". It can provide an averaged 1.2 BLEU improvements on these three evaluation data sets. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL PRO dev 20.60 21.21 21.13 20.84 21.07 21.83 21.89 test-1 20.27 20.71(+0.44) 20.79(+0.52) 20.50(+0.23) 20.75(+0.48) 21.34(+1.07) 21.81 test-2 13.15 13.98(+0.83) 14.25(+1.10) 13.36(+0.21) 13.59(+0.44) 14.46(+1.31) 14.69 Table 1: FDT-based model training on E-J task. Pre-reordering (PRO) is often used on language pairs, e.g. English and Japanese, with very different word orders. So we compare our method with PRO as well. We re-implement the PRO method proposed by Genzel (2010) and show its results in Table 1. On dev and test-2, FDT-ALL performs comparable to PRO, with no syntactic information needed at all. 5.4 Translation Quality on C-E Task We then evaluate the effectiveness of our FDT-based model training approach on C-E translation task, and present evaluation results in Table 2, from which we can see significant improvements as well. BLEU BTG FDT-TM FDT-DM FDT-SLM FDT-RSM FDT-ALL MT03 38.73 39.14 39.27 38.97 39.06 39.59 MT05 38.01 38.31(+0.30) 38.56(+0.55) 38.22(+0.21) 38.33(+0.32) 38.72(+0.71) MT08 23.78 24.30(+0.52) 24.50(+0.72) 24.04(+0.26) 24.13(+0.35) 24."
D12-1041,W05-1506,0,0.0391167,"se-based SMT system in Section 3. 2.1 Generation We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps: 1. Train component models needed for a specific SMT paradigm M based on training corpus C; 2. Perform MERT on the development data set to obtain a set of optimized feature weights; 3. For each {f, e} ∈ C, translate f into accurate e based on M, component models trained in step 1, and feature weights optimized in step 2; 4. For each {f, e} ∈ C, output the hypergraph (Huang and Chiang, 2005) H(f, e) generated in step 3 as its FDT space. In step 3: (1) all partial hypotheses that do not match any sequence in e will be discarded; (2) derivations covering identical source and target words but with different alignments will be kept as different partial candidates, as they can produce different FDTs for 446 the same sentence pair. For each {f, e}, the probability of each G ∈ H(f, e) is computed as: p(G|H(f, e)) = ∑ exp{ψ(G)} ′ G ′ ∈H(f,e) exp{ψ(G )} (1) where ψ(G) is the FD model score assigned to G. For each sentence pair, different alignment candidates can be induced from its differ"
D12-1041,C10-1071,0,0.0183537,"ced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlangua"
D12-1041,D09-1106,0,0.0143278,"d [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum o"
D12-1041,J10-3002,0,0.0188773,")} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Bi"
D12-1041,H05-1011,0,0.0316801,"n H(f, e) respectively. e), target-to-source translation probability • h(f¯|¯ of a translation rule r = {f¯, e¯}. ∑ ¯ ¯) {f,e}∈C fracH(f,e) (f , e ∑ h(f¯|¯ e) = ∑ ¯′ ¯) (4) {f,e}∈C f¯′ fracH(f,e) (f , e • h# (r), smoothed usage count for translation rule r = {f¯, e¯} in the whole generation step. h# (r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more"
D12-1041,P06-1065,0,0.0272001,"(r) = 1+e {− ∑ 1 {f,e}∈C fracH(f,e) (f¯,¯ e)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions,"
D12-1041,P08-1023,0,0.0541771,"Missing"
D12-1041,D08-1089,0,0.0525339,"Missing"
D12-1041,P03-1021,0,0.0690948,"s of different translation rules are in a proper value range. • word pair translation probabilities trained from IBM models (Brown et al., 1993); • log-likelihood ratio (Moore, 2005); • conditional link probability (Moore, 2005); • count of unlinked words; • counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010). Following the common practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeN"
D12-1041,J04-4002,0,0.169793,"r approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality. 1 Ming Zhou Microsoft Research Asia Introduction Most of today’s SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models. However, such annotations do have their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++ (Och and Ney, 2004) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. T"
D12-1041,P02-1040,0,0.0855811,"lity, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. A 5gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU (Papineni et al., 2002) and reported in percentage numbers. 5.2 Baseline System The phrase-based SMT system proposed by Xiong et al. (2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 5.3 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table 1, in which BTG denot"
D12-1041,N06-1002,0,0.0199934,"igned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side o"
D12-1041,2008.amta-srw.6,0,0.0202855,"dels (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al"
D12-1041,W11-2102,0,0.0123739,"mmon practice in SMT research, the MERT algorithm (Och, 2003) is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction"
D12-1041,P11-1086,0,0.020727,"e words into the targetlanguage-like order. Their work is distinct from ours 450 Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDTRSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 5 Experiments 5.1 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side of bilingual data is used to train a 4-gram LM. The development set (dev) which contains 2,000 sentences is used to optimize"
D12-1041,2008.amta-papers.18,0,0.0141752,"e source spans [i, m) and [m, j) in a monotonic way; while a ⟨·⟩ operation combines them in an inverted way. 1 2 448 Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies (Venugopal et al., 2008; Liu et al., 2009). For each sentence pair {f, e}, we first induce n alignments {A1 , ..., An } from the top n FDTs Ω = {G1 , ..., Gn } ⊂ H(f, e). Each Ak is annotated with the posterior probability of its corresponding FDT Gk as follows: ∑ exp{ i λi hi (Gk )} ∑ (6) p(Ak |Gk ) = ∑ Gk′ ∈Ω exp{ i λi hi (Gk′ )} ∑ where i λi hi (Gk ) is the model score assigned to Gk by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, i"
D12-1041,P10-1049,0,0.0638577,"word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic informatio"
D12-1041,J97-3002,0,0.0295005,"orne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, hr (G) ad"
D12-1041,P06-1066,0,0.416938,"sed loss functions (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011) or span F1 (DeNero and Uszkoreit, 2011). • hr (G), number of translation rules used in G. 3 Training in Phrase-based SMT • hd (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs (Charniak et al., 2003). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al., 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h# (r) is used to penalize those long translation rules which tend to occur in only"
D12-1041,N09-1028,0,0.0155643,"ing/Alignment Schwartz (2008) used forced decoding to leverage multilingual corpus to improve translation quality; Shen et al. (2008) used forced alignment to train a better phrase segmentation model; Wuebker et al. (2010) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 4.2 Pre-reordering Pre-reordering (PRO) techniques (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010) used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (2011) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-re"
D12-1041,W06-3108,0,0.0225538,"DTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future. because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 4.3 Distortion Models Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r2 ⇒ r3 ⇒ r1 . 4 Lexicalized distortion models (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008;) are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Related Work 4.1 Forced Decoding/Alignment Schwartz (2008) used forced decoding to"
D12-1041,D08-1022,0,\N,Missing
D12-1041,J93-2003,0,\N,Missing
D14-1071,P14-1091,1,0.898601,"that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007) heavily rely on th"
D14-1071,P11-1060,0,0.26728,"sentations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle"
D14-1071,P14-1133,0,0.0783746,"(Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the paired relationships are described as foll"
D14-1071,D13-1160,0,0.426708,"results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Mooney, 2007)"
D14-1071,D12-1104,0,0.00652345,"y types to the given context, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates. 3.2 This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1). &lt;Relation Mention, Predicate> Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following information extraction methods, such as PATTY (Nakashole et al., 2012), we extracted the &lt;relation mention, logical predicate> pairs from English W IKIPEDIA3 , which is closely connected to our KB, as follows: Given a KB-triple &lt;entitysubj , logical predicate, entityobj >, we extracted NLEentries &lt;entitysubj , relation mention, entityobj > where relation mention is the shortest path between entitysubj and entityobj in the dependency tree of sentences. The assumption is that any relation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple. With obtaining high-quality MP pai"
D14-1071,P13-1042,0,0.520324,"perties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zet"
D14-1071,D13-1136,0,0.100595,"igger extraction and model training. Due to the data annotation requirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers. Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervision by using question-answer pairs (Liang et al., 2011) or distant supervision (Krishnamurthy and Mitchell, 2012) instead of full semantic annotations. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. Bordes et al. (2012) and Weston et al. (2013) designed embedding models that connect free texts with KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Y"
D14-1071,J90-1003,0,0.164555,"Missing"
D14-1071,P13-1158,0,0.395373,"space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. 1 Introduction Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corresponding structured query for KB (KB-query) as a logical form (LF), also known as semantic parsing, is the central task for KB-QA systems. Previous works (Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Bao et al., 2014) usually leveraged mappings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical predicate often has different natural language expression (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the 2 Related Work Supervised semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins"
D14-1071,P14-1090,0,0.205332,"Missing"
D14-1071,D12-1069,0,0.140714,"Missing"
D14-1071,D13-1161,0,0.0295005,"th KBs using the relational learning method (Weston et al., 2010). Their inputs are often statement sentences which include subject and object entities for a given predicate, whereas NLquestions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion. Recently, researchers have developed open domain systems based on large scale KBs such as F REEBASE1 (Cai and Yates, 2013; Fader et al., 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Bao et al., 2014; Berant and Liang, 2014; Yao and Van Durme, 2014). Their semantic parsers for Open QA are unified formal and scalable: they enable the NL-question to be mapped into the appropriate logical form. Our method obtains similar logical forms, but using only lowdimensional embeddings of n-grams, entity types, and predicates learned from texts and KB. 3 (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c). Based on the components defined above, the"
D17-1090,N16-1152,0,0.0232147,"shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question matching score QQ(Q, Qgen"
D17-1090,D13-1160,0,0.0746789,"sing recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation ("
D17-1090,P16-1049,1,0.835962,"assage, and [Gen] denotes the generated question of the passage. WikiQA QA QA+QG ly wrong question topic, or a partially right question topic. In the future, we plan to develop an independent question topic selection model for the question generation task. 9.3 As described in Section 7, we combine question generation into QA system for answer sentence selection task, and do evaluation on SQuAD, MS MARCO, and WikiQA. Evaluation results are shown in Table 5, 6, and 7, where QA denotes the result of our in-house implementation of a retrieval-based answer selection approach (DocChat) proposed by (Yan et al., 2016), QA+QG denotes result by combining question-to-generated question matching score with DocChat score. MAP 0.8843 0.8887 MRR 0.8915 0.8963 MAP 0.5131 0.5230 MRR 0.5195 0.5291 ACC@1 0.6540 0.6624 for CQA websites; while questions from the other datasets are labeled by crowd-sourcing. In order to explain these improvements, two datasets, WikiQG+ and WikiQG-, are built from WikiQA test set: given each document and its labeled question, we pair the question with its CORRECT answer sentence as a QA pair and add it to WikiQG+; we also pair the same question with a randomly selected WRONG answer sente"
D17-1090,J15-1001,0,0.0853699,"e input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method i"
D17-1090,D15-1237,0,0.0201733,"is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics where each generated Qi can be answered by S. There are four components in our QG engine: query “what is the population of nyc” is issued to YahooAnswers2 , the returned page contains a list of related questions"
D17-1090,P17-1123,0,0.430684,"templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate 30M QA pairs, but their inputs are knowledge triples, instead of passages. Song and Zhao (2016) proposed a question generation method using question template seeds and used search engine to do question expansion. Du et al. (2017) proposed a neural question generation method using a vanilla sequence-tosequence RNN model, which is most-related to our work. But this method is still based on labeled dataset, and tried RNN only. Comparing to all these related work mentioned above, our question generation approach has two uniqueness: (1) all question patterns, that are used as training data for question generation, are automatically extracted from a large scale CQA question set without any crowdsourcing effort. Such question patterns reflect the most common user intentions, and therefore are useful to search, QA, and chatbo"
D17-1090,E17-1066,0,0.0152031,"ction task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866–874 c Copenhagen, Denmark, September 7–11, 2017. 2017 Associatio"
D17-1090,P03-1054,0,0.0495266,"ct question topic candidates from S, including: 6 Given a predicted question pattern Qp and a selected question topic Qt of an input passage S, a complete question Q can be simply generated by replacing # in Qp with Qt . We use a set of features to rank generated question candidates: • question pattern prediction score, which is the prediction score by either retrieval-based approach or generation-based approach; • Entities as question topic candidates, which are detected based on Freebase4 entities; • Noun phrases as question topic candidates, which are detected based on the Stanford parser (Klein and Manning, 2003). • question topic selection score, for retrievalbased approach, this score is computed as s(Qt , Qp ), while for generation-based approach, this score is the attention score; Once a question topic candidate Qt is extracted from S, we then measure how Qt can fit Qp by: • QA matching score, which measures relevance between generated question Q and S. 1 X s(Qt , Qp ) = · #(Qtpk ) · dist(vQt , vQtk ) p N k • word overlap between Q and S, which counts number of words that co-occur in Q and S; s(Qt , Qp ) denotes the confidence that Qt can be filled into Qp to generate a reasonable question. Qtpk d"
D17-1090,Q16-1019,0,0.11248,"egrated and evaluated in an end-to-end QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then"
D17-1090,P15-1086,0,0.265209,"ched by Qp , and 0 otherwise. All features are combined by a linear model as: p(Q|S) = https://developers.google.com/freebase/ X i 870 λi · hi (Q, S, Qp , Qt ) 8 where hi (Q, S, Qp , Qt ) is one of the features described above, and λi is the corresponding weight. 7 Related Work Yao et al. (2012) proposed a semantic-based question generation approach, which first parses the input sentence into its corresponding Minimal Recursion Semantics (MRS) representation, and then generates a question guided by the English Resource Grammar that includes a large scale handcrafted lexicon and grammar rules. Labutov et al. (2015) proposed an ’ontologycrowd-relevance’ method for question generation. First, Freebase types and Wikipedia session names are used as semantic tags to understand texts. Question are then generated based on question templates that are aligned with types/session names and labeled by crowdsourcing. All generated questions are ranked by a relevance model. Chali and Hasan (2015) proposed a topic-toquestion method, which uses about 350 generalpurpose rules to transform the semantic-role labeled sentences into corresponding questions. Serban et al. (2016) used the encoder-decoder framework to generate"
D17-1090,D16-1147,0,0.0176953,"QA task directly, and shows significant improvements. Question Generation for QA This section describes how question generation can improve existing QA systems. There are several types of QA systems, i.e. knowledge-based QA, community-based QA, text-based QA, etc, and in this paper, we focus on text-based QA task (a.k.a. answer sentence selection), which aims to select one or multiple answer sentences from a text given an input question. We select this task as it can be considered as a dual task of QG. A typical answer sentence selection method, such as (Yin et al., 2016; Santos et al., 2016; Miller et al., 2016; Tymoshenko et al., 2016), computes the relevance score between input question Q and each answer candidate A, and selects the one with the highest relevance score as the final answer: Ab = arg max P (A|Q) A Motivated by Dual Learning (He et al., 2016), we integrate question generation into answer ranking procedure, by changing the above formula to: Ab = arg max{P (A|Q) + λ · QQ(Q, Qgen max )} A λ is hyper-parameter, and in order to compute QQ(Q, Qgen max ), we generate top-10 questions gen gen {Q1 , ..., Q10 } for current answer candidate A, and then compute the question-to-generated question"
D17-1090,P02-1040,0,0.114478,"QA) website. The motivation of using CQA website for training data collection is that, such websites (e.g., YahooAnswers, Quora, etc.) contain large scale QA pairs generated by real users, and these questions reflect the most common user intentions, and therefore are useful to search, QA, and chatbot scenarios. To achieve the 2nd goal, we explore two ways to generate questions for a given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN). We evaluate the generation quality by BLEU score (Papineni et al., 2002) and human annotations, and discuss their pros and cons in Section 9. To achieve the 3rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA. Experimental results show that, the generated questions can improve the QA quality on all these three datasets. This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as t"
D17-1090,D16-1264,0,0.0988632,"questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved. 1 Introduction Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (Qiu and Huang, 2015), knowledge-based QA (Berant et al., 2013), text-based QA (Yu et al., 2014), and reading comprehension (Rajpurkar et al., 2016). Most of current QA systems, e.g. (Berant and Liang, 2014), (Qiu and Huang, 2015), (Xiong et al., 2017), (Yin and Schtze, 2017), need labeled QA pairs as training data. Although labeling efforts have been made, such as WebQuestions dataset (Berant et al., 2013) and SimpleQuestions dataset (Bordes et al., 2015) for knowledge-based QA, WikiQA dataset (Yang et al., 2015) for text-based QA, SQuAD dataset (Rajpurkar et al., 2016) and MS MARCO 2 Question Generation Formally, given a passage S, question generation (QG) engine generates a set of questions {Qi }, 866 Proceedings of the 2017 Conference"
D17-1090,P14-1133,0,\N,Missing
D18-1188,D13-1160,0,0.082852,"without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table"
D18-1188,E17-2029,0,0.0206229,"ence. ψg (yi ) = viT Wg st ψc (yi ) = tanh(hi T Wc )st Encoder-Decoder Encoder: A bidirectional RNN with gated recurrent unit (GRU) (Cho et al., 2014) is used as the encoder to read a SQL query x = (x1 , ..., xT ). The forward RNN reads a SQL query (5) 4.3 (7) Incorporating Latent Variable Increasing the diversity of generated questions is very important to improve accuracy, generalization, and stability of the semantic parser, since this 1599 increases the mount of training data and produces more diverse questions for the same intent. In this work, we incorporate stochastic latent variables (Cao and Clark, 2017; Serban et al., 2017) to the sequence-to-sequence model in order to increase question diversity. Specifically, we introduce a latent variable z ∼ p(z), which is a standard Gaussian distribution N (0, In ) in our case, and calculate the likelihood of a target sentence y as follows: Z (8) p(y|x) = p(y|z, x)p(z) dz z We maximize the evidence lower bound (ELBO), which decomposes the loss into two parts, including (1) the KL divergence between the posterior distribution and the prior distribution, and (2) a cross-entropy loss between the generated question and the ground truth. logp(y|x) ≥ −DKL (Q"
D18-1188,D17-1090,1,0.851999,"uage questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2"
D18-1188,N13-1092,0,0.044976,"Missing"
D18-1188,D17-1087,0,0.0434135,"the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that"
D18-1188,P16-1154,0,0.0599355,"ding of the previously predicted word yt−1 , and the last hidden state st−1 is fed to the next step. st = GRU (st−1 , yt−1 , ct ) After obtaining hidden states st , we adopt the copying mechanism that predicts a word from the target vocabulary or from the source sentence (detailed in Subsection 4.2). 4.2 Incorporating Copying Mechanism In our task, the generated question utterances typically include informative yet low-frequency words such as named entities or numbers. Usually, these words are not included in the target vocabulary but come from SQL queries. To address this, we follow CopyNet (Gu et al., 2016) and incorporate a copying mechanism to select whether to generate from the vocabulary or copy from SQL queries. The probability distribution of generating the t-th word is calculated as Equation 6, where ψg (·) and ψc (·) are scoring functions for generating from the vocabulary ν and copying from the source sentence x, respectively. eψg (yt ) + eψc (yt ) P ψg (v) + ψc (w) v∈ν e w∈x e p(yt |y<t , x) = P (6) The two scoring functions are calculated as follows, where Wg and Wc are model parameters, vi is the one-hot indicator vector for yi and hi is the hidden state of word yi in the source sent"
D18-1188,P17-1097,0,0.0762159,"Missing"
D18-1188,P16-1004,0,0.187589,"Missing"
D18-1188,P17-1089,0,0.15759,"s, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing relational databases w"
D18-1188,D17-1091,0,0.0405482,"ns from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the QG results as additional constraints in the training objectives (Tang et al., 2017). This work belongs to the first direct"
D18-1188,P18-1069,0,0.0366913,"Missing"
D18-1188,P17-1123,0,0.0222525,"also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates t"
D18-1188,P16-1002,0,0.0909215,"Missing"
D18-1188,P17-1014,0,0.0279021,"r et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2017; Dong et al., 2017); and (3) using the"
D18-1188,D17-1160,0,0.0464092,"xtensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, this dataset requires question-answer pairs for training. Thus, we generate question-answer pairs by follow steps. We first sample SQL queries on the tables from WikiTableQuestions, and then use our QG model to generate question-SQL pairs. After2 https://nlp.stanford.edu/software/ sempre/wikitable/ wards, we obtain question-answer pairs by executing SQL queries. The generated question-answer pairs will be combined with the original WikiTableQuestions training data to train the model. Pasupat and Liang (2015) Neelakantan et al. (2016) Haug et al. (2017)"
D18-1188,Q13-1016,0,0.0298687,"cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is"
D18-1188,P11-1060,0,0.0403622,"ions. For instance, without knowing that cells under the column name “built” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work,"
D18-1188,P16-1170,0,0.035962,"e QA model and train the QG model, and incorporate more generated question-logical form pairs to further improve the QA model. Question Generation Our work also relates to the area of question generation, which has drawn plenty of attention recently partly influenced by the remarkable success of neural networks in text generation. Studies in this area are classified based on the definition of the answer, including a sentence (Heilman, 2011), a topic word (Chali and Hasan, 2015), a fact (including a subject, a relation phrase and an object) from knowledge bases (Serban et al., 2016), an image (Mostafazadeh et al., 2016), etc. Recent studies in machine reading comprehension generate questions from an answer span and its context from the document (Du et al., 2017; Golub et al., 2017). Wang et al. (2015) first generate logical forms, and then use AMTurkers to paraphrase them to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority o"
D18-1188,P02-1040,0,0.10339,"Missing"
D18-1188,P15-1142,0,0.0229864,"imit-supervision scenarios. This is consistent with our intuition that the performance of the QG model is improved by incorporating the copying mechanism, since rare words of great importance mainly come from the input sequence. To better understand the impact of incorporating a latent variable, we show examples generated by different QG variations in Table 7. We can see that incorporating a latent variable empowers the model to generate diverse questions for the same intent. 5.5 Transfer Learning on WikiTableQuestions In this part, we conduct an extensional experiment on WikiTableQuestions2 (Pasupat and Liang, 2015) in a transfer learning scenario to verify the effectiveness of our approach. WikiTableQuestions contains 22,033 complex questions on 2,108 Wikipedia tables. Each instance consists of a natural language question, a table and an answer. Following Pasupat and Liang (2015), we report development accuracy which is averaged over the first three 80-20 training data splits. Test accuracy is reported on the train-test data. In this experiment, we apply the QG model learnt from WikiSQL to improve the state-of-theart semantic parser (Krishnamurthy et al., 2017) on this dataset. Different from WikiSQL, t"
D18-1188,P16-1003,0,0.0372546,"lt” are all building years, the model hardly predicts a question “what is the average building year for superb?” for “SELECT AVG built WHERE name = superb”. The second direction is to incorporate common knowledge, which would help the model to predict the earliest week rather than the lowest week. 6 Related Work Semantic Parsing. Semantic parsing is a fundamental problem in NLP that maps natural language utterances to logical forms, which could be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Iyer et al., 2017). Existing works can be classified into three areas, including (1) the language of the logical form, e.g. firstorder logic, lambda calculus, lambda dependencybased compositional semantics (lambda DCS) and structured query language (SQL); (2) the form of the knowledge base, e.g. facts from large collaborative knowledge bases, semi-structured tables and images; and (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs and question-logical form pairs. In this work, we regard the table as the knowledge base, which is critical for accessing re"
D18-1188,D14-1162,0,0.0812641,"pectively. Similar to hx , hy is obtained by encoding the target sentence. µ = Wµ [hx ; hy ] + bµ log(σ 2 ) = Wσ [hx ; hy ] + bσ 4.4 the initial state of the decoder, and then decoding deterministically for each sample. Here, we list our training details. We set the dimension of the encoder hidden state as 300, and the dimension of the latent variable z as 64. We use dropout with a rate of 0.5, which is applied to the inputs of RNN. Model parameters are initialized with uniform distribution, and updated with stochastic gradient decent. Word embedding values are initialized with Glove vectors (Pennington et al., 2014). We set the learning rate as 0.1 and the batch size as 32. We tune hyper parameters on the development, and use beam search in the inference process. 5 Experiment We conduct experiments on the WikiSQL dataset1 (Zhong et al., 2017). WikiSQL is the largest handannotated semantic parsing dataset which is an order of magnitude larger than other datasets in terms of both the number of logical forms and the number of schemata (tables). WikiSQL is built by crowd-sourcing on Amazon Mechanical Turk, including 61,297 examples for training, and 9,145/17,284 examples for development/testing. Each instanc"
D18-1188,P18-1034,1,0.877859,"ll-scale supervised training data that consists of SQL-question pairs. Lastly, the generated question-SQL pairs are viewed as the pseudo-labeled data, which are combined with the supervised training data to train the semantic parser. Since we conduct the experiment on WikiSQL dataset, we follow Zhong et al. (2017) and use the same template-based SQL sampler, as summarized in Table 1. The details about the semantic parser and the question generation model will be introduced in Sections 3 and Section 4, respectively. 3 Semantic Parsing Model We use a state-of-the-art end-to-end semantic parser (Sun et al., 2018) that takes a natural language question as the input and outputs a SQL query, which is executed on a table to obtain the answer. To make the paper self-contained, we briefly describe the approach in this section. The semantic parser is abbreviated as STAMP, which is short for Syntax- and Table- Aware seMantic Parser. Based on the encoder-decoder framework, STAMP takes a question as the input and generates a SQL query. It extends pointer networks (Zhong et al., 2017; Vinyals et al., 2015) by incorporating three “channels” in the decoder, in which the column channel predicts column names, the va"
D18-1188,P15-1129,0,0.431991,"Missing"
D18-1188,P07-1121,0,0.419448,"Missing"
D18-1188,P16-1127,0,0.207322,"Missing"
D18-1188,P17-1096,0,0.0226953,"to get natural language questions. Iyer et al. (2017) use a template-based approach based on the Paraphrase Database (Ganitkevitch et al., 2013) to generate questions from SQL. In this work, we generate questions from logical forms, in which the amount of information from two directions are almost identical. This differs from the majority of existing studies because a question typically conveys less semantic information than the answer. Improving QA with QG This work also relates to recent studies that uses a QG model to improve the performance of a discriminative QA model (Wang et al., 2017; Yang et al., 2017; Duan et al., 2017; Konstas et al., 2017). The majority of these works generate a question from an answer, while there also exists a recent work (Dong et al., 2017) that generates a question from a question through paraphrasing. In addition, Tang et al. (2017) consider QA and QG as dual tasks, and further improve the QG model in a dual learning framework. These works fall into three categories: (1) regarding the artificially generated results as additional training instances (Yang et al., 2017; Golub et al., 2017); (2) using generated questions to calculate additional features (Duan et al., 2"
D18-1188,D07-1071,0,0.232712,"Missing"
D18-1188,D17-1125,0,0.0685018,"Missing"
D19-1172,D14-1181,0,0.00514974,"For simplification, we use a special symbol, [S], to concatenate all the entity information together. These two parts can be regarded as different source information. Based on whether the inter-relation between different source information is explicitly explored, the classification models can be classified into two categories, unstructured models and structured models. Unstructured models concatenate different source inputs into a long sequence with a special separation symbol [SEL]. We implement several widely-used sequence classification models, including Convolutional Neural Network (CNN) (Kim, 2014), Long-Short Term Memory Network (LSTM) (Schuster and Paliwal, 1997), and Recurrent Convolutional Neural Network (RCNN) (Lai et al., 2015), Transformer. The details of the models are shown in Supplementary Materials. 1622 [A] web browser Midori operating system Midori Decoder Decoder Encoder Entity Info [B] Entity Rendering Module Ambiguous Context Template What are the languages used to create the source code of Midori? Encoder Template Generating Module Decoder When you say the source code language used in the program Midori, are you referring to [A] or [B]? Hidden vector of [A] Hidden vecto"
D19-1172,J81-4005,0,0.674542,"Missing"
D19-1172,N03-1007,0,0.0346386,"Missing"
D19-1172,D16-1076,0,0.0350947,"Missing"
D19-1172,P17-1045,0,0.0494573,"Missing"
D19-1172,P18-1068,0,0.018531,"es (e.g., “web browser Midori ”) and pattern phrases ( e.g. “When you say the source code language used in the program Midori, are you referring to [A] or [B]?” ). The entity phrase is summarized from the given entity information for distinguishing between two entities. The pattern phrase is used to locate the position of ambiguity, which is closely related with the context. In summary, two kinds of phrases refer to different source information. Based on this feature, we propose a new coarse-to-fine model, as shown in Figure 6. Similar ideas have been successfully applied to semantic parsing (Dong and Lapata, 2018). The proposed model consists of a template generating module Tθ and an entity rendering module Rφ . Tθ first generates a template containing pattern phrases and the symbolic representation of the entity phrases. Then, the symbolized entities contained in the generated template are further properly rendered by the entity rendering module Rφ to reconstruct complete entity information. Since the annotated clarification questions explicitly separate entity phrases and pattern phrases, we can easily build training data for these two modules. For clarity, the template is constructed by replacing en"
D19-1172,D18-1188,1,0.829182,"ll dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification-based question answering. We implement representative neural networks as baselines for three tasks and propose a new generation model. The detailed analysis shows that our dataset brings new challenges. More powerful models and reasonable evaluation metrics need further explored. In the future, we plan to"
D19-1172,D18-1361,0,0.0212818,"rification Question in Other Tasks There are several studies on asking clarification questions. Stoyanchev et al. (2014) randomly drop one phrase from a question and require annotators to ask a clarification question toward the dropped information, e.g.“Do you know the birth data of XXX”. However, the small dataset size makes it hard to help downstream tasks. Following this work, Guo et al. (2017) provide a larger synthetic dataset QRAQ by replacing some of entities with variables. Li et al. (2017) use misQuestion Generation For different purposes, there are various question generation tasks. Hu et al. (2018) aim to ask questions to play the 20 question game. Dhingra et al. (2017) teach models to ask questions to limit the number of answer candidates in task-oriented dialogues. Ke et al. (2018) train models to ask questions in open-domain conversational systems to better interact with people. Guo et al. (2018) develop a sequence-to-sequence model to generate natural language questions. 7 Conclusion and Future Work In this work, we construct a clarification question dataset for KBQA. The dataset supports three tasks, clarification identification, clarification question generation, and clarification"
D19-1172,P18-1255,0,0.125162,"Missing"
D19-1172,N16-1174,0,0.0245396,"late generating module and an entity rendering module. The former is used to generate a clarification template based on ambiguous question, e.g., “When you say the source code language used in the program Midori, are you referring to [A] or [B]?”. The latter is used to fill up the generated template with detailed entity information. Structured models use separate structures to encode different source information and adopt an additional structure to model the inter-relation of the source information. Specifically, we use two representative neural networks, Hierarchical Attention Network (HAN) (Yang et al., 2016) and Dynamic Memory Network (DMN) (Kumar et al., 2016), as our structured baselines. The details of the models are shown in Supplementary Materials. 4.2 Clarification Question Generation Models The input of the generation model is the ambiguous context and entity information. In single-turn cases, the ambiguous context is current question Qa . In multi-cases, the input is current question and the previous conversation turn {Qp , Rp , Qa }. We use [S] to concatenate all entity information together, and use [SEL] to concatenate entity information and context information into a long sequence. We"
D19-1248,P13-2009,0,0.0266995,"cal form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and"
D19-1248,Q13-1005,0,0.0265942,"s the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example,"
D19-1248,P17-1005,0,0.0132889,"2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. D"
D19-1248,P16-1004,0,0.197334,"n intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., sema"
D19-1248,P18-1068,0,0.079013,"seen the development of AIdriven personal assistants (e.g., Siri, Alexa, Cortana, and Google Now) that often need to answer factorial questions. Meanwhile, large-scale knowledge base (KB) like DBPedia (Auer et al., 2007) or Freebase (Bollacker et al., 2008) has been built to store world’s facts in a structure database, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entit"
D19-1248,P16-1002,0,0.121012,"distribution over V to score candidates2 . Then, a FFN(·) or a pointer network (Vinyals et al., 2015) is utilized to predict instantiation for entry semantic category (i.e., e, p, tp or u num in V(vec) ) if it is necessary. (n) pj = softmax(sTj W (n) H:,1:n−1 ), (5) where H:,1:n−1 is contextual embedding of tokens in the question except [CTX], W (e) and W (n) are weights of pointer-network for (e) (n) entity and number, pj , pj ∈ Rn−1 are the resulting distributions over positions of input question, and n is the length of the question. The pointer network is also used for semantic parsing in (Jia and Liang, 2016), where the pointer aims at copying out-of-vocabulary words from a question over small-scale KB. Different from that, the pointer used here aims at locating the targeted entity and number in a question, which has two advantages. First, it handles the coreference problem by considering the context of entity mentions in the question. Second, it solves the problem caused by huge entity vocabulary, which reduces the size of decoding vocabulary from several million (i.e., the number of entities in KB) to several dozen (i.e., the length of the question). 3.2.3 Entity Detection and Linking • For pred"
D19-1248,D17-1160,0,0.0176575,"s, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based"
D19-1248,D11-1140,0,0.031991,"n example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng"
D19-1248,P16-1057,0,0.0379427,"Missing"
D19-1248,P16-1138,0,0.0291545,"while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis a"
D19-1248,D16-1147,0,0.0443322,"Missing"
D19-1248,P17-1105,0,0.0137173,"7; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a quest"
D19-1248,Q14-1030,0,0.103666,"Missing"
D19-1248,P07-1121,0,0.0543536,"rold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (R"
D19-1248,1983.tc-1.13,0,0.382158,"Missing"
D19-1248,P16-1127,0,0.19754,"e, which is used to support open-domain question answering (QA) in those assistants. Neural semantic parsing based approach (Jia and Liang, 2016; Reddy et al., 2014; Dong and ∗ Work done while the author was an intern at Microsoft, Beijing, China. Lapata, 2016; Liang et al., 2016; Dong and Lapata, 2018; Guo et al., 2018) is gaining rising attention for knowledge-based question answer (KBQA) in recent years since it does not rely on handcrafted features and is easy to adapt across domains. Traditional approaches usually retrieve answers from a small KB (e.g., small table) (Jia and Liang, 2016; Xiao et al., 2016) and are difficult to handle large-scale KBs. Many recent neural semantic parsing based approaches for KB-QA take a stepwise framework to handle this issue. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first use an entity linking system to find entities in a question, and then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the questi"
D19-1248,P16-1220,0,0.030023,"to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedings of the 2019 Conference on Empirical M"
D19-1248,P15-1128,0,0.0730713,"d then learn a model to map the question to logical form based on that. Dong and Lapata (2018) decompose the semantic parsing process into two stages. They first generate a rough sketch of logical form based on low-level features, and then fill in missing details by considering both the question and the sketch. However, these stepwise approaches have two issues. First, errors in upstream subtasks (e.g., entity detection and linking, relation classification) are propagated to downstream ones (e.g., semantic parsing), resulting in accumulated errors. For example, case studies in previous works (Yih et al., 2015; Dong and Lapata, 2016; Xu et al., 2016; Guo et al., 2018) show that entity linking error is one of the major errors leading to wrong predictions in KB-QA. Second, since models for the subtasks are learned independently, the supervision signals cannot be shared among the models for mutual benefits. To tackle issues mentioned above, we propose a novel multi-task semantic parsing framework for KB-QA. Specifically, an innovative pointerequipped semantic parsing model is first designed for two purposes: 1) built-in pointer network toward positions of entity mentions in the question 2442 Proceedin"
D19-1248,P17-1041,0,0.014099,"., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017; Yin and Neubig, 2017). For example, Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) leverage an attention-based encoder-decoder framework to translate a natural language question to tree-structured logical form. Recently, to handle huge entity vocabulary existing in a large-scale knowledge base, many works take a stepwise approach. For example, Liang et al. (2016), Dong and Lapata (2016), and Guo et al. (2018) first process questions using a name entity linking system to find entity candidates, and then learn a model to map a question to a logical form based on the candidates. Dong and Lapata (2018) d"
D19-1248,D07-1071,0,0.0686871,"uestion “Which sexes do King Harold, Queen Lillian and Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syn"
D19-1248,P09-1110,0,0.0855188,"Arthur Pendragon possess” as an example, a spurious logical form only retrieves the genders of “King Harold” and “Queen Lillian”, while it gets correct answers for the question. Spurious logical forms accidentally introduce noises into training data and thus negatively affect the performance of KB-QA. 5 Related Work Our work is aligned with semantic parsing based approach for KB-QA. Traditional semantic parsing systems typically learn a lexicon-based parser and a scoring model to construct a logical form given a natural language question (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2011; Andreas et al., 2013; Artzi and Zettlemoyer, 2013; Zhao and Huang, 2014; Long et al., 2016). For example, Zettlemoyer and Collins (2009) and Artzi and Zettlemoyer (2013) learn a CCG parser, and Long et al. (2016) develop a shift-reduce parser to construct logical forms. Neural semantic parsing approaches have been gaining rising attention in recent years, eschewing the need for extensive feature engineering (Jia and Liang, 2016; Ling et al., 2016; Xiao et al., 2016). Some efforts have been made to utilize the syntax of logical forms (Rabinovich et al., 2017; Krishna"
D19-1252,W18-6317,0,0.0174688,"sentences as a sequence and input it to Unicoder. The representation of the first token in the final layer will be used for the paraphrase classification task. This procedure is illustrated in Figure 1.b. We created the cross-lingual paraphrase classification dataset from machine translation dataset. Each bilingual sentence pair (X, Y ) servers as a positive sample. For negative samples, the most straight forward method is to replace Y to a ran2487 dom sampled sentence from target language. But this will make the classification task too easy. So we introduce the hard negative samples followed Guo et al. (2018). First, we train a light-weight paraphrase model with random negative samples. Then we use this model to select sentence with high similarity score to X but doesn’t equal to Y as hard negative samples. We choose DAN (Iyyer et al., 2015) as the light model. We create positive and negative samples in 1:1. Cross-lingual Masked Language Model Previous successful pre-training language model (Devlin et al., 2018; Radford et al., 2018) is conducted on document-level corpus rather than sentence-level corpus. The language model perplexity on document also is much lower than sentence (Peters et al., 20"
D19-1252,P15-1162,0,0.0938242,"Missing"
D19-1252,P07-2045,0,0.0085558,"inese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,0"
D19-1252,W08-0336,0,0.0235743,"Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M 15.5M 0.6M 12.6M 0.2M 0.8M 1.8M 0.5M 3.8M 5.5M 9.8M 0.6M 9.3M 4.0M 11.4M 13.2M 1.6M 11.7M 0.2M 3.3M 0.5M 0.7M 3.5M 9.6M Table 1: Sentence number we used in pre-training. Training Details Model Structure Our Unicoder is a 12-layer transformer with 1024 hidden units, 16 heads, GELU activation (Hendrycks and Gimpel, 2017). we set dropout to 0.1. The vocabulary size is 95,000. Pre-training deta"
D19-1252,D18-1269,0,0.21585,"corpus in which each sentence and its translation is well aligned. We construct cross-lingual document by replacing the sentences with even index to its translation as illustrated in Figure 1.c. We truncate the cross-lingual document by 256 sequence length and feed it to Unicoder for masked language modeling. Multi-language Fine-tuning A typical setting of cross-lingual language understanding is only one language has training data, but the test is conducted on other languages. We denote the language has training data as source language, and other languages as target languages. A scalable way (Conneau et al., 2018) to address this problem is through Cross-lingual TEST, in which a pre-trained encoder is trained on data in source language and directly evaluated on data in target languages. There are two other machine translation methods that make training and test belong to the same language. TRANSLATE-TRAIN translates the source language training data to a target language and fine-tunes on this pseudo training data. TRANSLATE-TEST fine-tunes on source language training data, but translates the target language test data to source language and test on it. Inspired by multi-task learning (Liu et al., 2018,"
D19-1252,eisele-chen-2010-multiun,0,0.0200481,"ion, we describe the data processing and training details. Then we compare the Unicoder with the current state of the art approaches on two tasks: XNLI and XQA. 2488 Data Processing Our model is pre-trained on 15 languages, including English(en), French(fr), Spanish(es), German(de), Greek(el), Bulgarian(bg), Russian(ru), Turkish(tr), Arabic(ar), Vietnamese(vi), Thai(th), Chinese(zh), Hindi(hi), Swahili(sw) and Urdu(ur). For MLM, we use the Wikipedia from these languages. The other four tasks need MT dataset. We use same MT dataset as Lample and Conneau (2019) which are collected from MultiUN (Eisele and Chen, 2010), IIT Bombay corpus (Kunchukuttan et al., 2017), OpenSubtitles 2018, EUbookshop corpus and GlobalVoices. In the MT corpus, 13 of 14 languages (except IIT Bombay corpus) are from parallel document and could be used to train cross-lingual document language model. The number of data we used is reported at table 1. For tokenization, we follows the line of Koehn et al. (2007); Chang et al. (2008) for each language. We use byte-pair encoding (BPE) to process the corpus and build vocabulary. language ar bg de el en es fr hi ru sw th tr ur vi zh mono-lingual bi-lingual 3.8M 1.5M 17.4M 1.3M 43.2M 11.3M"
D19-1252,N18-1202,0,0.232254,"rman) is obtained. In short, our contributions are 4-fold. First, 3 new cross-lingual pre-training tasks are proposed, which can help to learn a better languageindependent encoder. Second, a cross-lingual question answering (XQA) dataset is built, which can be used as a new cross-lingual benchmark dataset. Third, we verify that by fine-tuning multiple languages together, significant improvements can be obtained. Fourth, on the XNLI dataset, new state-of-the-art results are achieved. Related work Monolingual Pre-training Recently, pretraining an encoder by language model (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2018) and machine translation (McCann et al., 2017) have shown significant improvement on various natural language understanding (NLU) tasks, like tasks in GLUE (Wang et al., 2018). The application scheme is to fine-tune the pre-trained encoder on single sentence classification task or sequential labeling task. If the tasks have multiple inputs, just concatenate them into one sentence. This approach enables one model to be generalized to different language understanding tasks. Our approach also is contextual pre-training so it could been applied to various NLU tasks. Cross-lin"
D19-1252,P16-1162,0,0.0211415,"Keeping the same setting, XLM proposed a new task TLM, which uses a concatenation of the parallel sentences into one sample for masked language modeling. Besides these two tasks, we proposed three new cross-lingual pre-training tasks for building a better language-independent encoder. Approach This section will describe details of Unicoder, including tasks used in the pre-training procedure and its fine-tuning strategy. Model Structure Unicoder follows the network structure of XLM (Lample and Conneau, 2019). A shared vocabulary is constructed by running the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016) on corpus of all languages. We also down sample the rich-resource languages corpus, to prevent words of target languages from being split too much at the character level. Pre-training Tasks in Unicoder Both masked language model and translation language model are used in Unicoder by default, as they have shown strong performance in XLM. Motivated by Liu et al. (2019), which shows that pre-trained models can be further improved by involving more tasks in pre-training, we introduce three new cross-lingual tasks in Unicoder. All training data for these three tasks are acquired from the existing"
D19-1252,W18-5446,0,0.0675618,"Missing"
D19-1252,D19-1077,0,0.089503,"Missing"
D19-1252,N15-1104,0,0.121749,"Missing"
D19-1626,D15-1075,0,0.552415,"presentation of the premise sentence and the representation of the current token are fed into LSTM. We jointly train the aggregation layer and pre-trained layer for sequence matching. We conduct an experiment on two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significant improvement compared with state-of-the-art methods on both datasets. 1 Introduction Text sequence matching aims to infer the relationship of two text sequences which is a critical problem used in many NLP tasks such as answer selection (Yang et al., 2015), natural language inference (Bowman et al., 2015), and paraphrase identification (Dolan et al., 2004). Recent years, various deep learning models have been proposed for text sequence matching tasks (Wang and Jiang, 2016; Tay et al., 2018; Tymoshenko and Moschitti, 2018; Kim et al., 2018). Most previous works are learned from task specific supervised datasets, which are always limited to the amount, due to the cost of annotation. Recently, learning a general representation of words based on language models attracts great attention, and are successfully applied to a range of NLP tasks (McCann et al., 2017; Peters et al., 2018, 2017; Alec Radfo"
D19-1626,C04-1051,0,0.0322765,"tation of the current token are fed into LSTM. We jointly train the aggregation layer and pre-trained layer for sequence matching. We conduct an experiment on two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significant improvement compared with state-of-the-art methods on both datasets. 1 Introduction Text sequence matching aims to infer the relationship of two text sequences which is a critical problem used in many NLP tasks such as answer selection (Yang et al., 2015), natural language inference (Bowman et al., 2015), and paraphrase identification (Dolan et al., 2004). Recent years, various deep learning models have been proposed for text sequence matching tasks (Wang and Jiang, 2016; Tay et al., 2018; Tymoshenko and Moschitti, 2018; Kim et al., 2018). Most previous works are learned from task specific supervised datasets, which are always limited to the amount, due to the cost of annotation. Recently, learning a general representation of words based on language models attracts great attention, and are successfully applied to a range of NLP tasks (McCann et al., 2017; Peters et al., 2018, 2017; Alec Radford, 2018; Devlin et al., 2018). In these works, they"
D19-1626,P17-2081,0,0.0840442,"ation for Computational Linguistics • We compare various aggregation layers to extend BERT, with the MatchLSTM layer achieving the best performance. It gets more powerful representations than the original output from BERT for sequence matching. • Experiments on WikiQA and SNLI datasets show that our model achieves better performance than state-of-the-art methods. 2 Related Work Sequence Matching task has attracted lots of attention in the past decades. There are many related works on Question Answering(QA) (Yin et al., 2015; Tay et al., 2018; Wang et al., 2017; Tymoshenko and Moschitti, 2018; Min et al., 2017), Natural Language Inference(NLI) (Peters et al., 2018; Kim et al., 2018; Radford et al., 2018) and so on. (Yin et al., 2015) use attention mechanism with convolutional layer to model sentence pairs. In (Tymoshenko and Moschitti, 2018), they combine the similarity features of members within the same pair and traditional sentence pair similarity and achieve state-of-the-art results on several answer selection datasets including WikiQA (Yang et al., 2015). (Peters et al., 2018) and (Radford et al., 2018) incorporate pretrained language models to text sequence matching task and achieve performanc"
D19-1626,P17-1161,0,0.183354,"acts great attention, and are successfully applied to a range of NLP tasks (McCann et al., 2017; Peters et al., 2018, 2017; Alec Radford, 2018; Devlin et al., 2018). In these works, they first learn general language representations through training a language model using large scale unlabeled data. Then they use these representations to the downstream tasks. Previous works show that the representations with linguistic information perform obvious effectiveness and robustness applied to downstream tasks. In these methods, there are two typical strategies using pre-trained models, feature-based (Peters et al., 2017) and fine-tuning approach (Devlin et al., 2018; Radford et al., 2018). Our model is based on the fine-tuning approach and use BERT (Devlin et al., 2018) as our pre-trained language model. BERT has been directly used in many tasks and achieved significantly improvement compared with other methods. However, BERT only uses a standard softmax layer for many different specific tasks. Intuitively, for more complex tasks, like sequence matching in this work, we need a better strategy for extending the BERT to infer the relationship of the sequence pair. Thus, we incorporate a MatchLSTM layer (Wang an"
D19-1626,N18-1202,0,0.522195,"guage inference (Bowman et al., 2015), and paraphrase identification (Dolan et al., 2004). Recent years, various deep learning models have been proposed for text sequence matching tasks (Wang and Jiang, 2016; Tay et al., 2018; Tymoshenko and Moschitti, 2018; Kim et al., 2018). Most previous works are learned from task specific supervised datasets, which are always limited to the amount, due to the cost of annotation. Recently, learning a general representation of words based on language models attracts great attention, and are successfully applied to a range of NLP tasks (McCann et al., 2017; Peters et al., 2018, 2017; Alec Radford, 2018; Devlin et al., 2018). In these works, they first learn general language representations through training a language model using large scale unlabeled data. Then they use these representations to the downstream tasks. Previous works show that the representations with linguistic information perform obvious effectiveness and robustness applied to downstream tasks. In these methods, there are two typical strategies using pre-trained models, feature-based (Peters et al., 2017) and fine-tuning approach (Devlin et al., 2018; Radford et al., 2018). Our model is based on the"
D19-1626,D18-1240,0,0.274152,"n two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significant improvement compared with state-of-the-art methods on both datasets. 1 Introduction Text sequence matching aims to infer the relationship of two text sequences which is a critical problem used in many NLP tasks such as answer selection (Yang et al., 2015), natural language inference (Bowman et al., 2015), and paraphrase identification (Dolan et al., 2004). Recent years, various deep learning models have been proposed for text sequence matching tasks (Wang and Jiang, 2016; Tay et al., 2018; Tymoshenko and Moschitti, 2018; Kim et al., 2018). Most previous works are learned from task specific supervised datasets, which are always limited to the amount, due to the cost of annotation. Recently, learning a general representation of words based on language models attracts great attention, and are successfully applied to a range of NLP tasks (McCann et al., 2017; Peters et al., 2018, 2017; Alec Radford, 2018; Devlin et al., 2018). In these works, they first learn general language representations through training a language model using large scale unlabeled data. Then they use these representations to the downstream"
D19-1626,W18-5446,0,0.0771855,"Missing"
D19-1626,1983.tc-1.13,0,0.116629,"Missing"
D19-1626,D15-1237,0,0.0790689,"Missing"
D19-1626,D16-1264,0,0.0359574,"on SNLI (Bowman et al., 2015). Recently, pre-trained language models have been successfully used in NLP tasks. ELMo (Peters et al., 2017) is trained as a bidirectional language model. OpenAI GPT (Alec Radford, 2018) uses a basic left-to-right transformer to learn a language model. BERT, used in this paper, is based on the architecture of a bidirectional Transformer and per-trained on Masked Language Model task and Next Sentence Prediction. Using the pre-trained parameters, BERT (Devlin et al., 2018) achieves state-of-the-art performance on the GLUE benchmark (Wang et al., 2018) and SQuAD 1.1 (Rajpurkar et al., 2016) by fine-tuning in corresponding supervised data. In this work, we design a sequence matching model based on BERT. 3 Our Model In this section, we will introduce our sequence matching model. Our model is combined BERT encoder with the MatchLSTM layer. 3.1 BERT Encoder Given two natural language sentences A, B, the tokens of both sentences are packed into a token sequence S as “[CLS] A [SEP] B [SEP]”, where [CLS], [SEP] are special tokens. The representation of each token is the sum of three types of embedding: 1). WordPiece embedding(Wu et al., 2016) is used in BERT to represent the input sequ"
N18-1141,D13-1160,0,0.066233,"ning when to regard generated questions as positive instances (collaborative) could improve the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks"
N18-1141,D17-1091,0,0.0227649,", question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algor"
N18-1141,P17-1123,0,0.0582539,"arning of question answering and question generation. Question answering (QA) and question generation (QG) are closely related natural language processing tasks. The goal of QA is to obtain an answer given a question. The goal of QG is almost reverse which is to generate a question from the answer. In this work, we consider answer selection (Yang et al., 2015; Balakrishnan et al., 2015) as the QA task, which assigns a numeric score to each candidate answer, and selects the top ranked one as the answer. We consider QG as a generation problem and exploit sequence-to-sequence learning (Seq2Seq) (Du et al., 2017; Zhou et al., 2017) as the backbone of the QG model. The key idea of this work is that QA and QG are two closely tasks and we seek to leverage the connection between these two tasks to improve both QA and QG. Our primary motivations are twofolds. On one hand, the Seq2Seq based QG model is trained by maximizing the literal similarity between the generated sentence and the ground truth sentence with maximum-likelihood estimation objective function (Du et al., 2017). However, there is no signal indicating whether or not the generated sentence could be correctly answered by the input. This proble"
N18-1141,D17-1090,1,0.740288,"g (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary task and use auxiliary task, such as question generation and question paraphrasing, to improve the primary task. This is one part of our goal and our another goal is to improve the QG model with the QA system and further to increasingly improve both tasks in a loop. In terms of assigning category label to the generated question, Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 1565 2017) and Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) could be viewed as special cases of our algorithm. Our algorithm learns when to"
N18-1141,P16-1154,0,0.0765019,"Missing"
N18-1141,P16-1014,0,0.026262,"vector, header vector, and the cell vector. The backbone of the decoder is an attention based GRU RNN, which generates a word at each time step and repeats the process until generating the end-of-sentence symbol. We made two modifications to adapt the decoder to the table structure. The first modification is that the attention model is calculated over the headers, cells and the caption of a table. Ideally, the decoder should learn to focus on a region of the table when generating a word. The second modification is a table based copying mechanism. It has been proven that the copying mechanism (Gulcehre et al., 2016; Gu 1568 et al., 2016) is an effective way to replicate lowfrequent words from the source to the target sequence in sequence-to-sequence learning. In the decoding process, a word is generated either from the target vocabulary via standard sof tmax or from a table via the copy mechanism. A neural gate gt is used to trade-off between generating from the target vocabulary and copying from the table. The probability of generating a word y calculated as follows, where αt (y) is the attention probability of the word y from the table at time step t and βt (y) is the probability of predicting the wor"
N18-1141,P17-1019,0,0.0168286,"ion-answer pairs are correct and some are wrong. However, this kind of dataset is hard to obtain in most situations because of the lack of manual annotation efforts. From this perspective, the QA model could exactly benefit from the QG model through incorporating additional questionanswer pairs whose questions are automatically generated by the QG model1 . To achieve this goal, we present a training algorithm that improves the QA model and the 1 An alternative way is to automatically generate answers for each question. Solving the problem in this condition requires an answer generation model (He et al., 2017), which is out of the focus of this work. Our algorithm could also be adapted to this scenario. 1564 Proceedings of NAACL-HLT 2018, pages 1564–1574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics QG model in a loop. The QA model improves QG through introducing an additional QA-specific loss function, the objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating"
N18-1141,N03-1017,0,0.0990867,"A and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25 algorithm. WordCnt uses the number of co-occurred words in query-caption pair, query-header pair, and query-cell pair, respectively. MT based PP is a phrase-level feature. The features come from a phrase table which is extracted from bilingual corpus via statistical machine translation approach (Koehn et al., 2003). LambdaMART (Burges, 2010) is used to train the ranker. CNN uses convolutional neural network to measure the similarity between the query and table caption, table headers, and table cells, respectively. TQNN is the table-based QA model implemented in this work, which is regard as the baseline for the joint learning algorithm. Results of single systems are given in Table 1. We can see that BM25 is a simple yet very effective baseline method. Our basic model performs better than all the single models in terms of MAP. Method BM25 WordCnt MT based PP CNN TQNN (baseline) Seq2SeqPara GCN (competiti"
N18-1141,D16-1127,0,0.264999,"rks on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when to collaborate and yields empirical improvements on two QA tasks. This work relates to recent studies which attempt to improve the performance of a discriminative QA model with generative models (Wang et al., 2017; Yang et al., 2017; Dong et al., 2017; Duan et al., 2017). These works regard QA as the primary tas"
N18-1141,P17-1103,0,0.0185062,"49M query-table pairs. An example of the data is given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity be"
N18-1141,P16-1170,0,0.0518732,"simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from different resources, including a sentence(Heilman, 2011), a topic (Chali and Hasan, 2015), a fact from knowledge base (Serban et al., 2016), etc. In computer vision community, there are also recent studies on generating questions from an image (Mostafazadeh et al., 2016). Our QG model belongs to sentence-based question generation. GAN has been successfully applied in computer vision tasks (Goodfellow et al., 2014). There are also some recent trials that adapt GAN to text generation (Yu et al., 2017), question answering (Wang et al., 2017), dialogue generation (Li et al., 2016), etc. The relationship of the discriminator and the generator in GAN are competitive. The key finding of this work is that, directly applying the idea of “competitive” in GAN does not improve the accuracy of a QA model. We contribute a generative collaborative network that learns when t"
N18-1141,P02-1040,0,0.10243,"given in Figure 2. We randomly select 1.29M as the training set, 0.1M as the dev set and 0.1M as the test set. We evaluate the performance on table-based QA with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). We use the same candidate retrieval adopted in (Yan et al., 2017), namely representing a table as bag-ofwords, to guarantee the efficiency of the approach. Each query has 50 candidate tables on average. It is still an open problem to automatically evaluate the performance of a natural language generation system (Lowe et al., 2017). In this work, we use BLEU-4 (Papineni et al., 2002) score as the evaluation metric, which measures the overlap between the generated question and the referenced question. The hyper parameters are tuned on the validation set and the performance is reported on the test set. Results and Analysis We report the results and our analysis on table-based QA and QG respectively in this part. We first report the results of single systems on table-based QA. We compare to four single systems implemented by (Yan et al., 2017). In BM25, each table is represented as a flattened vector, and the similarity between a query and a table is calculated with the BM25"
N18-1141,D16-1264,0,0.0851505,"e the accuracy of the QA model. 2 Related Work Our work connects to existing works on question answering (QA), question generation (QG), and the use of generative adversarial nets in question answering and text generation. We consider two kinds of answer selection tasks in this work, one is table as the answer (Balakrishnan et al., 2015) and another is sentence as the answer (Yang et al., 2015). In natural language processing community, there are also other types of QA tasks including knowledge based QA (Berant et al., 2013), community based QA (Qiu and Huang, 2015) and reading comprehension (Rajpurkar et al., 2016). We believe that our algorithm is generic and could also be applied to these tasks with dedicated QA and QG model architectures. Despite the use of sophisticated features could learn a more accurate QA model, in this work we implement a simple yet effective neural network based QA model, which could be conventionally jointly learned with the QG model through back propagation. Question generation draws a lot of attentions recently, which is partly influenced by the remarkable success of neural networks in natural language generation. There are several works on generating questions from differe"
N18-1141,P17-1096,0,0.474587,"objective of which is to maximize the expectation of the QA scores of the generated question-answer pairs. Policy gradient method (Williams, 1992; Yu et al., 2017) is used to update the QG model. In turn, the QG model improves QA through incorporating additional training instances. Here the key problem is how to label the generated question-answer pair. The application of Generative Adversarial Network (GAN) (Goodfellow et al., 2014; Wang et al., 2017) in this scenario regards every generated question-answer pair as a negative instance. On the contrary, Generative Domain-Adaptive Nets (GDAN) (Yang et al., 2017) regards every generated question-answer pair appended with special domain tag as a positive instance. However, it is non-trivial to label the generated question-answer pairs because some of which are good paraphrases of the ground truth yet some might be negative instances with similar utterances. To address this, we bring in a collaboration detector, which takes two question-answer pairs as the input and determines their relation as collaborative or competitive. The output of the collaboration detector is regarded as the label of the generated questionanswer pair. We conduct experiments on b"
P08-1011,C02-1126,0,0.0131409,"ed in a similar way as does with target features. The source head word feature is defined to be a function f4 to indicate whether a word ei is the source head word in English according to a parse tree of the source sentence. Similar to the definition of lexical features, we also use a set of features based on POS tags of source language. 3 3.1 Model Training and Application Training We parsed English and Chinese sentences to get training samples for measure word generation model. Based on the source syntax parse tree, for each measure word, we identified its head word by using a toolkit from (Chiang and Bikel, 2002) which can heuristically identify head words for sub-trees. For the bilingual corpus, we also perform word alignment to get correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we ne"
P08-1011,P05-1033,0,0.134325,"ls and indefinite articles are directly followed by countable nouns to denote the quantity of objects. Therefore, in the English-to-Chinese machine translation task we need to take additional efforts to generate the missing measure words in Chinese. For example, when translating the English phrase three books into the Chinese phrases “三本书”, where three corresponds to the numeral “三” and books corresponds to the noun “书”, the Chinese measure word “本” should be generated between the numeral and the noun. In most statistical machine translation (SMT) models (Och et al., 2004; Koehn et al., 2003; Chiang, 2005), some of measure words can be generated without modification or additional processing. For example, in above translation, the phrase translation table may suggest the word three be translated into “三”, “三本”, “三只”, etc, and the word books into “书”, “书本”, “名册” (scroll), etc. Then the SMT model selects the most likely combination “三本书” as the final translation result. In this example, a measure word candidate set consisting of “本” and “只” can be generated by bilingual phrases (or synchronous translation rules), and the best measure word “本” from the measure 2 1 There are some exceptional cases,"
P08-1011,N03-1017,0,0.0412262,"Missing"
P08-1011,P07-1017,0,0.0485184,"Missing"
P08-1011,P00-1056,0,0.0252377,"s and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in previous sections, we apply our measure word generation module into SMT output as a post-processing ste"
P08-1011,J04-4002,0,0.307392,"Missing"
P08-1011,P02-1040,0,0.0737285,"Missing"
P08-1011,N07-1051,0,0.0125043,"et correspondences between source and target words. Then, the collocation between measure words and head words and their surrounding contextual information are extracted to train the measure word selection models. According to word alignment results, we classify measure words into two classes based on whether they have non-null translations. We map Chinese measure words having non-null translations to a unified symbol {NULL} as mentioned in Section 2.4, indicating that we need not generate these kind of measure words since they can be translated from English. In our work, the Berkeley parser (Petrov and Klein, 2007) was employed to extract syntactic knowledge from the training corpus. We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions with IBM model 4, and then applied the refinement rule described in (Koehn et al., 2003) to obtain a many-to-many word alignment for each sentence pair. We used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a fivegram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The Maximum Entropy training toolkit from (Zhang, 2006) was employed to train the measure word selection model. 3.2 Measure word generation As mentioned in"
P09-1066,C08-1005,0,0.0599862,"ngalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Stati"
P09-1066,P05-1033,0,0.476399,"e rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a loglinear model aims to"
P09-1066,D08-1011,0,0.382928,"sensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire sea"
P09-1066,2008.amta-srw.3,0,0.391125,"y, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better alignment, Rosti et al. (2008) introduced an incremental strategy for confusion network construction; and Hildebrand and Vogel (2008) proposed a hypotheses reranking model for multiple systems’ outputs with more features including word translation probability and n-gram agreement statistics. A common property of all the work mentioned above is that the combination models work on the basis of n-best translation lists (full hypotheses) of existing machine translation systems. However, the n-best list only presents a very small portion of the entire search space of a Statistical Machine Translation (SMT) model while a majority of the space, within which there are many potentially good translations, is pruned away in decoding."
P09-1066,N04-1022,0,0.0804613,"explored in decoding. Experimental results on data sets for NIST Chinese-to-English machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders, and the outputs from co-decoding can be used to further improve the result of system combination. 1 Introduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated"
P09-1066,P06-1077,0,0.0243484,"m step 2 to step 4 until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search sp"
P09-1066,E06-1005,0,0.0485259,"oduction Recent research has shown substantial improvements can be achieved by utilizing consensus statistics obtained from outputs of multiple machine translation systems. Translation consensus can be measured either at sentence level or at word level. For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list tries to find a hypothesis with lowest expected loss with respect to all the other translations, which can be viewed as sentence-level consensus-based decoding. Word based methods proposed range from straightforward consensus voting (Bangalore et al., 2001; Matusov et al., 2006) to more complicated word-based system combination model (Rosti et al., 2007; Sim et al., 2007). Typically, the resulting systems take outputs of individual machine translation systems as input, and build a new confusion network for second-pass decoding. There have been many efforts dedicated to advance the state-of-the-art performance by combining multiple systems’ outputs. Most of the work focused on seeking better word alignment for consensus-based confusion network decoding (Matusov et al., 2006) or word-level system combination (He et al., 2008; Ayan et al., 2008). In addition to better a"
P09-1066,P02-1038,0,0.124274,"Missing"
P09-1066,P03-1021,0,0.0111584,"egation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences in ℋ? ? of all n-grams in e: ? −?+1 ?? + ?, ?′ = ?=1 ?? − ?, ? ′ is the n-gram disagreement measure function which is complementary to ?? + ?, ? ′ : ? −?+1 ?=1 Model Training We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Let ?? be the feature weight vector for member decoder ?? , the training procedure proceeds as follows: 1. Choose initial values for ?1 , … , ?? 2. Perform co-decoding using all member decoders on a development set D with ?1 , … , ?? . For each decoder ?? , find a new feature weight vector ?′? which optimizes the specified evaluation criterion L on D using the MERT algorithm based on the n-best list ℋ? generated by ?? : ?′? = argmax? ? (?|?, ℋ? , ?)) where T denotes the translations selected by re-ranking the translations"
P09-1066,J04-4002,0,0.162557,"forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline model, the only change is hypothesis scoring. By rerunning a complete decoding process, member model can be applied to re-score all hypotheses explored by a decoder. Therefore step 3 can be viewed as full-scale hypothesis re-ranking because the re-ranking scope is beyond the limited n-best hypotheses currently cached in ℋ?"
P09-1066,W04-3250,0,0.67335,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,P08-1066,0,0.047141,"cal phrase-based decoder. Phrasal rules are extracted from all bilingual sentence pairs, while rules with variables are extracted only from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a BTG decoder with lexicalized reordering model based on maximum entropy principle as proposed by Xiong et al. (2006). We use all the bilingual data to extract phrases up to length 3. The third one (SYS3) is a string-to-dependency tree –based decoder as proposed by Shen et al. (2008). For rule extraction we use the same setting as in SYS1. We parsed the language model training data with Berkeley parser, and then trained a dependency language model based on the parsing output. All baseline decoders are extended with n-gram consensus –based co-decoding features to construct member decoders. By default, the beam size of 20 is used for all decoders in the experiments. We run two iterations of decoding for each member decoder, and hold the value of ? in Equation 5 as a constant 0.05, which is tuned on the test data of NIST 2004 Chinese-toEnglish machine translation task. 3.3 D"
P09-1066,koen-2004-pharaoh,0,0.617964,"ltiple n-best lists. The rest of the paper is structured as follows. Section 2 gives a formal description of the codecoding model, the strategy to apply consensus information and hypotheses ranking in decoding. In Section 3, we make detailed comparison between co-decoding and related work such as system combination and hypotheses selection out of multiple systems. Experimental results and discussions are presented in Section 4. Section 5 concludes the paper. 2 2.1 Collaborative Decoding Overview Collaborative decoding does not present a full SMT model as other SMT decoders do such as Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). Instead, it provides a framework that accommodates and coordinates multiple MT decoders. Conceptually, collaborative decoding incorporates the following four constituents: 1. Co-decoding model. A co-decoding model consists of a set of member models, which are a set of augmented baseline models. We call decoders based on member models member decoders, and those based on baseline models baseline decoders. In our work, any Maximum A Posteriori (MAP) SMT model with log-linear formulation (Och, 2002) can be a qualified candidate for a baseline model. The requirement for a"
P09-1066,D08-1065,0,0.341537,"?′ ?? ?? (?, ?′) (4) ?′ ∈ℋ? ? where e is a translation of f by decoder ?? (? ≠ ?), ? ′ is a translation in ℋ? ? and ? ?′ ?? is the posterior probability of translation ? ′ determined by decoder ?? given source sentence f. ?? (?, ?′) is a consensus measure defined on e and ?′, by varying which different feature functions can be obtained. 587 Referring to the log-linear model formulation, the translation posterior ? ?′ ?? can be computed as: ? ?′ ?? = exp ??? ?′ ?′′ ∈ℋ? ? exp ??? ?′′ (5) 2.5 where ?? (∙) is the score function given in Equation 2, and ? is a scaling factor following the work of Tromble et al. (2008) To compute the consensus measures, we further decompose each ?? ?, ?′ into n-gram matching statistics between e and ?′. Here we do not discriminate among different lexical n-grams and are only concerned with statistics aggregation of all n-grams of the same order. For each ngram of order n, we introduce a pair of complementary consensus measure functions ?? + ?, ?′ and ?? − ?, ?′ described as follows: ?? + ?, ? ′ is the n-gram agreement measure function which counts the number of occurrences in ? ′ of n-grams in e. So the corresponding feature value will be the expected number of occurrences"
P09-1066,P07-2045,0,0.00862222,"Missing"
P09-1066,P06-1066,0,0.614687,"until a preset iteration limit is reached. In the iterative decoding procedure described above, hypotheses of different decoders can be mutually improved. For example, given two decoders ?1 and ?2 with hypotheses sets ℋ1 and ℋ2 , improvements on ℋ1 enable ?2 to improve ℋ2 , and in turn ℋ1 benefits from improved ℋ2 , and so forth. Step 2 is used to facilitate the computation of feature functions ℎ?,? (?, ℋ? ∙ ) , which require both e and every hypothesis in ℋ? ∙ should be translations of the same set of source words. This step seems to be redundant for CKY-style MT decoders (Liu et al., 2006; Xiong et al., 2006; Chiang, 2005) since the grouping is immediately available from decoders because all hypotheses spanning the same range of source sentence have been stacked together in the same chart cell. But to be a general framework, this step is necessary for some state-of-the-art phrase-based decoders (Koehn, 2007; Och and Ney, 2004) because in these decoders, hypotheses with different coverage vectors can co-exist in the same bin, or hypotheses associated with the same coverage vector might appear in different bins. Note that a member model does not enlarge the theoretical search space of its baseline"
P09-1066,W08-0329,0,\N,Missing
P09-1066,N07-1029,0,\N,Missing
P11-1126,P05-1033,0,0.734658,"y the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest"
P11-1126,P10-1146,0,0.0227603,"ndent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus dec"
P11-1126,C10-2025,1,0.718798,"ures from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction brings high computational complexity. What’s more, partial hypotheses generated by confusion network decoding cannot be assigned exact feature values for future use in higher level decoding, and they only use feature values of 1-best hypothesis as an approximation. HM decoding, on the other hand, leverages a set of enriched features, which 1263 are computable for all the hypotheses gene"
P11-1126,P09-1064,0,0.0171445,"ms as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consi"
P11-1126,N10-1141,0,0.352458,"ft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from d"
P11-1126,C10-1036,1,0.617604,"oring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from different SMT systems"
P11-1126,P06-1121,0,0.0316772,"er itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the"
P11-1126,D08-1011,0,0.0195808,"demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram co"
P11-1126,W04-3250,0,0.0206599,"ontain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dep"
P11-1126,N04-1022,0,0.0272922,"between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although the"
P11-1126,P09-1019,0,0.184738,"mbination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements ov"
P11-1126,P09-1066,1,0.802998,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1107,0,0.280871,"ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hyp"
P11-1126,P09-1065,0,0.0268883,"ty to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction b"
P11-1126,P03-1021,0,0.0152193,"ience: : the word count feature. 3) : the n-gram posterior feature of computed based on the mixture search space generated by the HM decoder: is the posterior probability of an n-gram in , is the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecuti"
P11-1126,J04-4002,0,0.0494598,"of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.co"
P11-1126,P02-1040,0,0.0823569,"newswire portions of the NIST 2006 (MT06) and 2008 (MT08) data sets. All bilingual corpora available for the NIST 2008 constrained data track of Chinese-to-English MT task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 20"
P11-1126,P08-1066,0,0.154677,"set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations."
P11-1126,P07-1040,0,0.0207307,"ated techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or e"
P11-1126,D08-1065,0,0.0189733,"one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China {muli,mingzhou}@microsoft.com the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approa"
P11-1126,J97-3002,0,0.018326,"s the posterior probability of one translation given based on . 4) 1) is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by , and 0 otherwise. 6) : the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces: not exist in equals to 1 when does , and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:  Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order.  Inverted rule . It combines translations of two consecutive blocks into a single larger block in an inverted order. These two rules are used bottom-up until the whole source sentence is fully covered. We use two reordering rule penalty features,"
P11-1126,P06-1066,0,0.0212564,"ni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Data Set MT04(dev) MT05 MT06 MT08 #Sentence 1,788 1,082 616 691 #Word 48,215 29,263 17,316 17,424 Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only:  PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle.  DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from"
P13-2008,N03-1017,0,0.0365144,"Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD (Q, Q0 ), which is defined as |Q0 |− |Q|. • Edit Distance feature hED (Q, Q0 ), which is defined as the character-level edit distance between Q and Q0 . Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Besides, a set of traditional SMT features (Koehn et al., 2003) are also used in our paraphrasing model, including translation probability, lexical weight, word count, paraphrase rule count3 , and language model feature. ˆ"
P13-2008,P05-1074,0,0.105085,"ided by the paraphrase candidates of the original queries. 2.1 • Word Deletion feature hW DEL (Q, Q0 ), which is defined as the number of words in the original query Q without being aligned to any word in the paraphrase candidate Q0 . • Word Overlap feature hW O (Q, Q0 ), which is defined as the number of word pairs that align identical words between Q and Q0 . Paraphrase Extraction • Word Alteration feature hW A (Q, Q0 ), which is defined as the number of word pairs that align different words between Q and Q0 . Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al."
P13-2008,P00-1037,0,0.159318,"Missing"
P13-2008,P00-1056,0,0.131701,"er of word pairs that align different words between Q and Q0 . Paraphrases can be mined from various resources. Given a bilingual corpus, we use Bannard and Callison-Burch (2005)’s pivot-based approach to extract paraphrases. Given a monolingual corpus, Lin and Pantel (2001)’s method is used to extract paraphrases based on distributional hypothesis. Additionally, human annotated data can also be used as high-quality paraphrases. We use Miller (1995)’s approach to extract paraphrases from the synonym dictionary of WordNet. Word alignments within each paraphrase pair are generated using GIZA++ (Och and Ney, 2000). 2.2 • Word Reorder feature hW R (Q, Q0 ), which is modeled by a relative distortion probability distribution, similar to the distortion model in (Koehn et al., 2003). • Length Difference feature hLD (Q, Q0 ), which is defined as |Q0 |− |Q|. • Edit Distance feature hED (Q, Q0 ), which is defined as the character-level edit distance between Q and Q0 . Search-Oriented Paraphrasing Model Similar to statistical machine translation (SMT), given an input query Q, our paraphrasing engine generates paraphrase candidates1 based on a linear model. Besides, a set of traditional SMT features (Koehn et al"
P13-2008,P03-1021,0,0.0343509,"Ming Zhou Microsoft Research Asia mingzhou@microsoft.com Ming Zhang School of EECS Peking University mzhang@net.pku.edu.cn Abstract an in-depth study on adapting paraphrasing to web search. First, we propose a search-oriented paraphrasing model, which includes specifically designed features for web queries that can enable a paraphrasing engine to learn preferences on different paraphrasing strategies. Second, we optimize the parameters of the paraphrasing model according to the Normalized Discounted Cumulative Gain (NDCG) score, by leveraging the minimum error rate training (MERT) algorithm (Och, 2003). Third, we propose an enhanced ranking model by using augmented features computed on paraphrases of original queries. Many query reformulation approaches have been proposed to tackle the query-document mismatch issue, which can be generally summarized as query expansion and query substitution. Query expansion (Baeza-Yates, 1992; Jing and Croft, 1994; Lavrenko and Croft, 2001; Cui et al., 2002; Yu et al., 2003; Zhang and Yu, 2006; Craswell and Szummer, 2007; Elsas et al., 2008; Xu et al., 2009) adds new terms extracted from different sources to the original query directly; while query substitu"
P13-2008,P02-1040,0,0.0855744,"s are computed based on Q0n and DQ , instead of Q and DQ . In this way, the paraphrase candidates act as hidden variables and expanded matching features between queries and documents, making our ranking model more tunable and flexible for web search. Q 43 3.2 Baseline Systems score only, without considering characteristics of mismatches in search. The baselines of the paraphrasing and the ranking model are described as follows: The paraphrasing baseline is denoted as BLPara, which only uses traditional SMT features described at the end of Section 2.2. Weights are optimized by MERT using BLEU (Papineni et al., 2002) as the error criterion. Development data are generated based on the English references of NIST 2008 constrained track of Chinese-to-English machine translation task. We use the first reference as the source, and the rest as its paraphrases. The ranking model baseline (Liu et al., 2007) is denoted as BL-Rank, which only uses matching features computed based on original queries and different meta-streams of web pages, including URL, page title, page body, meta-keywords, metadescription and anchor texts. The feature functions we use include unigram/bigram/trigram BM25 and original/normalized Per"
P13-2008,P05-1033,0,0.0605837,"otators. MERT is used to optimize feature weights of our linear-formed paraphrasing model. For H(Q) is the hypothesis space containing all paraphrase candidates of Q, hm is the mth feature function with weight λm , Q0 denotes one candidate. In order to enable our paraphrasing model to learn the preferences on different paraphrasing strategies according to the characteristics of web queries, we design search-oriented features2 based on word alignments within Q and Q0 , which can be described as follows: 1 We apply CYK algorithm (Chappelier and Rajman, 1998), which is most commonly used in SMT (Chiang, 2005), to generating paraphrase candidates. 2 Similar features have been demonstrated effective in (Jones et al., 2006). But we use SMT-like model to generate query reformulations. 3 Paraphrase rule count is the number of rules that are used to generate paraphrase candidates. 4 The ranking model R (Liu et al., 2007) uses matching features computed based on original queries and documents. 42 each query Qi in {Qi }Si=1 , we first generate Nbest paraphrase candidates {Qji }N j=1 , and compute NDCG score for each paraphrase based on documents ranked by the ranker R and labeled documents DiLabel . We th"
P13-2008,W04-3219,0,0.10147,"Missing"
P13-2008,P09-1094,0,0.0324077,"Missing"
P13-2075,N03-1004,0,0.0233236,"sing, speech recognition and etc. As far as we know, this is the first work that applies MBR principle to QA. Yaman et al. (2009) proposed a classification based method for QA task that jointly uses multiple 5-W QA systems by selecting one optimal QA system for each question. Comparing to their work, our MBRAR approaches assume few about the question types, and all QA systems contribute in the re-ranking model. Tellez-Valero et al. (2008) presented an answer validation method that helps individual QA systems to automatically detect its own errors based on information from multiple QA systems. Chu-Carroll et al. (2003) presented a multi-level answer resolution algorithm to merge results from the answering agents at the question, passage, and answer levels. Grappy et al. Second, the hypothesis distribution is defined as a probability distribution over the combined search space of N component QA systems and computed as a weighted sum of component model distributions: N ∑ ∑ αi · P (A|Hi (Q)) where α1 , ..., αN are coefficients with following ∑ constraints holds1 : 0 ≤ αi ≤ 1 and N i=1 αi = 1, P (A|Hi (Q)) is the posterior probability of A estimated on the ith QA system’s search space Hi (Q). Third, the feature"
P13-2075,W12-0512,0,0.042559,"Missing"
P13-2075,N04-1022,0,0.016789,"es of QA systems with different pros and cons. Section 3 presents two MBRAR approaches that can re-rank the answer candidates from single and multiple QA systems respectively. The relationship between our approach and previous work is discussed in Section 4. Section 5 evaluates our methods on large scale questions selected from two domains (Jeopardy! and Web) and shows promising results. Section 6 concludes this paper. 1 Introduction Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This work makes further exploration along this line of research, by applying MBR technique to question answering (QA). The function of a typical factoid question answering system is to automatically give answers to questions in most case asking about entities, which usually consists of three key components: question understanding, passage retrieval, and answer extraction. In this paper, we propose two MBRbased Answer Re-ranking (MBRAR) approaches, aiming to re-rank answer candidates from either sin"
P13-2075,E06-1050,0,0.0207103,"Missing"
P14-1091,D13-1161,0,0.148238,"rd to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Da"
P14-1091,D11-1039,0,0.0142537,"d over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗ This work was finished while the author was visiting Microsoft Research Asia. 967 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Hanks, Film.Actor.Film, Forrest Gumpi62 and hForrest Gump, Film.Film.Director, Robert Zemeckisi60 are three ordered formal triples corresponding to the three translation steps in Figure 1. We define the task of transforming question spans into formal triples as question translation."
P14-1091,P11-1060,0,0.199678,"um error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗ This work was finished while the author was visiting Microsoft Research Asia. 967 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Hanks, Film.Actor.Film, Forrest Gumpi62 and hForrest Gump, Film.Film.Director, Robert Zemeckisi60 are three ordered formal triples corresponding to the three translation steps in Figure 1. We define the task of transforming question spans into formal triples as question translation. A denotes one final"
P14-1091,P13-5002,0,0.0191882,"Missing"
P14-1091,J13-2005,0,0.357304,"and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce th"
P14-1091,D13-1160,0,0.380905,"unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Data Set WEBQUESTIONS Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an join"
P14-1091,W12-3016,0,0.0735886,"t it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Data Set WEBQUESTIONS Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. # Questions 5,810 # Words 6.7 Table 1: Statistics of evaluation set. #"
P14-1091,P13-1042,0,0.508703,"Missing"
P14-1091,P03-1021,0,0.0585009,"sing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions. The final answers can be obtained from the root cell. Derivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT) (Och, 2003) is used to tune feature weights based on a set of question-answer pairs. Figure 1 shows an example: the question director of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps: (i) translate director of to director of ; (ii) translate movie starred by Tom Hanks to one of its answers Forrest Gump; (iii) translate director of Forrest Gump to a final answer Robert Zemeckis. Note that the updated question covered by Cell[0, 6] is obtained by combining the answers to question spans covered by Cell[0, 1] and Cell[2, 6]. The contributions of this work"
P14-1091,J10-3005,0,0.00981963,"contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, wh"
P14-1091,P13-1092,0,0.0834723,"hastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. 4 4.1 Experiment Data"
P14-1091,P03-1003,0,0.0535075,"rsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 201"
P14-1091,N06-1056,0,0.0739044,"osition component. ref ˆ ˆ M Err(Aref i , Ai ; λ1 ) = 1 − δ(Ai , Ai ) ˆ where δ(Aref i , Ai ) is an indicator function which equals 1 when Aˆi is included in the reference set Aref i , and 0 otherwise. • htriple (·), which counts the number of triples in D, whose predicates are not N ull. 3 • htripleweight (·), which P sums the scores of all triples {ti } in D as ti ∈D ti .score. Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explai"
P14-1091,W12-0103,0,0.0349442,"Missing"
P14-1091,P07-1121,0,0.270694,"Missing"
P14-1091,D11-1142,0,0.016949,"Missing"
P14-1091,P13-1158,0,0.0557123,"of formal triples T . Each triple t ∈ T is in the form of {esbj , p, eobj }, where esbj ’s mention3 occurs in Q, p is a predicate that denotes the meaning expressed by the context of esbj in Q, eobj is an answer to Q retrieved from KB using a triple query q = {esbj , p, ?}. Note that if no predicate p or answer eobj can be generated, {Q, N ull, Q} will be returned as a special triple, which sets eobj to be Q itself, and p to be N ull. This makes sure the un-answerable spans can be passed on to the higher-level operations. Question translation assumes each span Q is a single-relation question (Fader et al., 2013). Such assumption simplifies the efforts of semantic parsing to the minimum question units, while leaving the capability of handling multiple-relation questions (Figure 1 gives one such example) to the outer CYK-parsing based translation procedure. Two question translation methods are presented in the rest of this subsection, which are based on question patterns and relation expressions respectively. 2.3.1 Question Pattern-based Translation A question pattern QP includes a pattern string QP pattern , which is composed of words and a slot 3 For simplicity, a cleaned entity dictionary dumped fro"
P14-1091,D07-1071,0,0.0608809,"ˆ ˆ M Err(Aref i , Ai ; λ1 ) = 1 − δ(Ai , Ai ) ˆ where δ(Aref i , Ai ) is an indicator function which equals 1 when Aˆi is included in the reference set Aref i , and 0 otherwise. • htriple (·), which counts the number of triples in D, whose predicates are not N ull. 3 • htripleweight (·), which P sums the scores of all triples {ti } in D as ti ∈D ti .score. Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, • hQP count (·), which counts the number of triples in D that are generated by QP-based question translation method. • hRE count (·), which counts the number of triples in D that are generated by RE-based question translation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an"
P14-1091,D10-1119,0,0.079168,"lation method. 5 The static rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs"
P14-1091,D11-1140,0,0.0182397,"c rank score of an entity represents a general indicator of the overall quality of that entity. 972 best translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependen"
P16-1049,P05-1074,0,0.0295791,"Missing"
P16-1049,J93-2003,0,0.0563193,"hich will be introduced below. • response ranking, which ranks all response candidates in C and selects the most possible ˆ response candidate as S: Sˆ = arg max Rank(S, Q) S∈C • response triggering, which decides whether ˆ it is confident enough to response Q using S: 4.1 Word-level Feature We define three word-level features in this work: (1) hW M (S, Q) denotes a word matching feature that counts the number (weighted by the IDF value of each word in S) of non-stopwords shared by S and Q. (2) hW 2W (S, Q) denotes a word-toword translation-based feature that calculates the IBM model 1 score (Brown et al., 1993) of S and Q based on word alignments trained on ‘questionrelated question’ pairs using GIZA++ (Och and Ney, 2003). (3) hW 2V (S, Q) denotes a word embedding-based feature that calculates the average cosine distance between word embeddings of all non-stopword pairs hvSj , vQi i. vSj represent the word vector of j th word in S and vQj represent the word vector of ith word in Q. ˆ Q) I = T rigger(S, where I is a binary value. When I equals to true, let the response R = Sˆ and output R; otherwise, output nothing. In the following three sections, we will describe solutions of these three components"
P16-1049,P13-1158,0,0.0120646,"ng set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 denotes the current sentence is a correct answer sentence, and 0 denotes the opposite meaning. Given a question, the task of WikiQA is to select answer sentences from all sentences in a question’s corresponding document. The training data settings of response ranking features are described below. 6 521 http://aka.ms/WikiQA Features Fw Fp Fs Fd Fr Fty Fto Fw denotes 3 word-level features, hW M , hW 2W and hW 2V . For hW 2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers.7 . For hW 2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English. Fp denotes 2 phrase-level features, hP P and hP T . For hP P , bilingual data8 is used to extract a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Section 4.2.1). For hP T , GIZA++ trains word alignments on 4M ‘question-answer’ pairs9 crawled from Yahoo Answers10 , and then a phrase table is extracted from word alignments using the intersect-diag-grow refinement. Fs denotes 2 sentence-level features, hSCR a"
P16-1049,P15-1152,0,0.0316726,"Missing"
P16-1049,N10-1145,0,0.0135467,"hniques. However, collecting enough Q-R pairs to build chatbots is often intractable for many domains. Compared to previous methods, DocChat learns internal relationships between utterances and responses based on statistical models at different levels of granularity, and relax the dependency on Q-R pairs as response sources. These make DocChat as a general response generation solution to chatbots, with high adaptation capability. For answer sentence selection. Prior work in measuring the relevance between question and answer is mainly in word-level and syntactic-level (Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013). Learning representation by neural network architecture (Yu et al., 2014; Wang and Nyberg, 2015; Severyn and Moschitti, 2015) has become a hot research topic to go beyond word-level or phrase-level methods. Compared to previous works we find that, (i) Large scale existing resources with noise have more advantages as training data. (ii) Knowledge-based semantic models can play important roles. at least one suitable response, but response ranking will output the best possible candidate all the time. So, we have to decide which responses are confident enough to be output, and"
P16-1049,C10-1131,0,0.0206887,"Missing"
P16-1049,P15-2116,0,0.0100083,"Missing"
P16-1049,N03-1017,0,0.0225488,"g document. The training data settings of response ranking features are described below. 6 521 http://aka.ms/WikiQA Features Fw Fp Fs Fd Fr Fty Fto Fw denotes 3 word-level features, hW M , hW 2W and hW 2V . For hW 2W , GIZA++ is used to train word alignments on 11.6M ‘question-related question’ pairs (Fader et al., 2013) crawled from WikiAnswers.7 . For hW 2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English. Fp denotes 2 phrase-level features, hP P and hP T . For hP P , bilingual data8 is used to extract a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Section 4.2.1). For hP T , GIZA++ trains word alignments on 4M ‘question-answer’ pairs9 crawled from Yahoo Answers10 , and then a phrase table is extracted from word alignments using the intersect-diag-grow refinement. Fs denotes 2 sentence-level features, hSCR and hSDR . For hSCR , 4M ‘question-answer’ pairs (the same to hP T ) is used to train the CNN model. For hSDR , we randomly select 0.5M ‘sentence-next sentence’ pairs from English Wikipedia. Fd denotes document-level feature hDM . Here, we didn’t train a new model. Instead, we just reuse the CNN m"
P16-1049,D07-1003,0,0.00742056,"existing resources are readily available (such as Q-Q pairs, Q-A pairs, ‘sentence-next sentence’ pairs, and etc.), instead of requiring manually annotated data (such as WikiQA and QASent). Training of the response ranking model does need labeled data, but the size demanded is acceptable. Second, as the training data used in our approach come from open domain resources, we can expect a high adaptation capability and comparable results on other WikiQAlike tasks, as our models are task-independent. To verify the second advantage, we evaluate DocChat on another answer selection data set, QASent (Wang et al., 2007), and list results in Table 3. CN NW ikiQA and CN NQASent refer to the results of Yang et al. (2015)’s method, where the CNN models are trained on WikiQA’s training set and QASent’s training set respectively. All these three methods train feature weights using QASent’s development set. Table 3 tells, DocChat outperforms CN NW ikiQA in terms of MAP and MRR, and achieves comparable results compared to CN NQASent . The comparisons results show a good adaptation capability of DocChat. Table 4 evaluates the contributions of features at different levels of granularity. To highlight the differences,"
P16-1049,W00-0304,0,0.019256,"also, besides, moreover and etc., as the contents of sentences starting with such phrases usually depend on their context sentences, and they are not suitable for responses. 6 7 Experiments 7.1 Evaluation on QA (English) Take into account response ranking task and answer selection task are similar, we first evaluate DocChat in a QA scenario as a simulation. Here, response ranking is treated as the answer selection task, and response triggering is treated as the answer triggering task. Related Work For modeling dialogue. Previous works mainly focused on rule-based or learning-based approaches (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007). These methods require efforts on designing rules or labeling data for training, which suffer the coverage issue. For short text conversation. With the fast development of social media, such as microblog and CQA services, large scale conversation data and data-driven approaches become possible. Ritter et al. (2011) proposed an SMT based method, which treats response generation as a machine translation task. Shang et al. (2015) presented an RNN based method, which is trained based on a large number of single round conversation data. Grammatic"
P16-1049,D15-1237,0,0.0100947,"Missing"
P16-1049,P13-1171,0,0.046252,"ing enough Q-R pairs to build chatbots is often intractable for many domains. Compared to previous methods, DocChat learns internal relationships between utterances and responses based on statistical models at different levels of granularity, and relax the dependency on Q-R pairs as response sources. These make DocChat as a general response generation solution to chatbots, with high adaptation capability. For answer sentence selection. Prior work in measuring the relevance between question and answer is mainly in word-level and syntactic-level (Wang and Manning, 2010; Heilman and Smith, 2010; Yih et al., 2013). Learning representation by neural network architecture (Yu et al., 2014; Wang and Nyberg, 2015; Severyn and Moschitti, 2015) has become a hot research topic to go beyond word-level or phrase-level methods. Compared to previous works we find that, (i) Large scale existing resources with noise have more advantages as training data. (ii) Knowledge-based semantic models can play important roles. at least one suitable response, but response ranking will output the best possible candidate all the time. So, we have to decide which responses are confident enough to be output, and which are not. In t"
P16-1049,P14-2105,0,0.0136102,"nguage) with its answer can be first parsed into a fact formatted as hesbj , rel, eobj i, where esbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, eobj denotes an object entity found from the knowledge base based on esbj and rel. Then we can get hQ, reli pairs. This rel can help for modeling semantic relationships between Q and R. For example, the Q-A pair hWhat does Jimmy Neutron do? − inventori can be parsed into hJimmy Neutron, fictional character occupation, inventori where the rel is fictional character occupation. Similar to Yih et al. (2014), We use hQ, reli pairs as training data, and learn a rel-CNN model, which can encode each question Q (or each relation rel) into a relation embedding. For a given question Q, the corresponding relation rel+ is L = max{0, M − cosine(y(SX ), y(SY )) +cosine(y(SX ), y(SY− ))} where M is a constant, SY− is a negative instance. 4.3.1 Document-level Feature Causality Relationship Modeling We train the first attention-based sentence embedding model based on a set of ‘question-answer’ pairs as input sentence pairs, and then design a causality relationship-based feature as: hSCR (S, Q) = cosine(ySCR ("
P16-1049,J03-1002,0,0.0490153,"ossible ˆ response candidate as S: Sˆ = arg max Rank(S, Q) S∈C • response triggering, which decides whether ˆ it is confident enough to response Q using S: 4.1 Word-level Feature We define three word-level features in this work: (1) hW M (S, Q) denotes a word matching feature that counts the number (weighted by the IDF value of each word in S) of non-stopwords shared by S and Q. (2) hW 2W (S, Q) denotes a word-toword translation-based feature that calculates the IBM model 1 score (Brown et al., 1993) of S and Q based on word alignments trained on ‘questionrelated question’ pairs using GIZA++ (Och and Ney, 2003). (3) hW 2V (S, Q) denotes a word embedding-based feature that calculates the average cosine distance between word embeddings of all non-stopword pairs hvSj , vQi i. vSj represent the word vector of j th word in S and vQj represent the word vector of ith word in Q. ˆ Q) I = T rigger(S, where I is a binary value. When I equals to true, let the response R = Sˆ and output R; otherwise, output nothing. In the following three sections, we will describe solutions of these three components one by one. 3 X Response Retrieval Given a user utterance Q, the goal of response retrieval is to efficiently fi"
P16-1049,D11-1054,0,0.0213887,"sponse ranking is treated as the answer selection task, and response triggering is treated as the answer triggering task. Related Work For modeling dialogue. Previous works mainly focused on rule-based or learning-based approaches (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007). These methods require efforts on designing rules or labeling data for training, which suffer the coverage issue. For short text conversation. With the fast development of social media, such as microblog and CQA services, large scale conversation data and data-driven approaches become possible. Ritter et al. (2011) proposed an SMT based method, which treats response generation as a machine translation task. Shang et al. (2015) presented an RNN based method, which is trained based on a large number of single round conversation data. Grammatical and fluency problems are the biggest issue for such generation-based approaches. Retrievalbased methods selects the most suitable response 7.1.1 Experiment Setup We select WikiQA6 as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which contains 2,118 ‘question-document’ pairs in the training set, 29"
P18-1034,Q13-1005,0,0.0484716,"udies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model paramet"
P18-1034,D13-1160,0,0.522838,"Missing"
P18-1034,D14-1179,0,0.0149909,"Missing"
P18-1034,H94-1010,0,0.425333,"he input, and outputs a SQL query y. We do not consider the join operation over multiple relational tables, which we leave in the future work. We use WikiSQL (Zhong et al., 2017), the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of S"
P18-1034,P17-1174,1,0.889119,"Missing"
P18-1034,N16-1024,0,0.0300555,"imum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect t"
P18-1034,P17-1089,0,0.106833,"mn names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti"
P18-1034,C12-2040,0,0.0387214,", the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL que"
P18-1034,P17-1167,0,0.0492084,"Missing"
P18-1034,P18-1168,0,0.060629,"Missing"
P18-1034,P16-1154,0,0.051477,"etworks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One advantage of this architecture is that it"
P18-1034,P16-1002,0,0.0341733,"al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. questi"
P18-1034,P16-1014,0,0.0177017,"y We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One"
P18-1034,P16-1086,0,0.028017,"s a probability distribution over the tokens from one of the three channels. X p(yt |y&lt;t , x) = pw (yt |zt , y&lt;t , x)pz (zt |y&lt;t , x) Methodology We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and ge"
P18-1034,P17-1097,0,0.143076,"is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu"
P18-1034,D16-1116,0,0.0360868,"Missing"
P18-1034,D17-1160,0,0.26447,"sign of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4.2 STAMP: Syntax- and Table- Aware seMantic Parser Figure 2 illustrates an overview of the prop"
P18-1034,Q13-1016,0,0.0505279,"tional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) th"
P18-1034,D11-1140,0,0.0801846,"Missing"
P18-1034,P15-1142,0,0.163748,"et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Z"
P18-1034,P16-1003,0,0.0875917,"s in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for le"
P18-1034,P17-1003,0,0.0189993,"tional databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus base"
P18-1034,D14-1162,0,0.0809332,"ells, and weighted average two cell distributions, which is calculated as follows. cell pcell pcell w (j) = λˆ w (j) + (1 − λ)αj 4.5 As the WikiSQL data contains rich supervision of question-SQL pairs, we use them to train model parameters. The model has two cross-entropy loss functions, as given below. One is for the switching gate classifier (pz ) and another is for the attentional probability distribution of a channel (pw ). l=− X X logpw (yt |zt , y&lt;t , x) logpz (zt |y&lt;t , x)− t t (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3 . The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-fitting. We set the dimension of encoder and decoder hidden state as 200. During training, we randomize model parameters from a uniform distribution with fan-in and fan-out, set batch size as 64, set the learning rate of SGD as 0.5, and update the model with stochastic gradient descent. Greedy search is used in the inference process. We use the model trained from question-SQL pairs as initializat"
P18-1034,P13-1092,0,0.0220016,"ses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic exec"
P18-1034,P11-1060,0,0.149695,"set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coup"
P18-1034,P17-1105,0,0.048093,"ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has n"
P18-1034,P17-2034,0,0.0222559,"to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic p"
P18-1034,D07-1071,0,0.600848,"Missing"
P18-1034,P07-1121,0,0.306045,"Missing"
P18-1034,D17-1125,0,0.635741,"Missing"
P18-1034,P17-1065,1,0.837395,"17; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4"
P18-1034,P16-1127,0,0.111172,"Missing"
P18-1034,P17-1041,0,0.227409,"the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input se"
P18-1034,N18-2093,0,0.393921,"Missing"
P18-1034,P16-1004,0,\N,Missing
P18-1034,P16-1138,0,\N,Missing
P19-1131,D17-1209,0,0.025975,"learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model based on GCN to perform joint type inference for entity relation extraction task. Compar"
P19-1131,P11-1056,0,0.147557,"republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mech"
P19-1131,P18-2014,0,0.141109,"tifies a PHYS relation between “[units]PER ” and “[captial]GPE ”, while the “NN” does not find this relation even the entities are correct. However, both models do not identify the relation ART between “[units]PER ” and “[weapons]WEA ”. We think advanced improvement methods which use more powerful graph neural network might be helpful in this situation. 4.2 Golden Entity Results on ACE05 In order to compare with relation classification methods, we evaluate our models with golden entities on ACE05 corpus in Table 4. We use the same data split to compare with their model (Miwa and Bansal, 2016; Christopoulou et al., 2018). We do not tune hyperparameters extensively. For example, we use the same setting in both end-to-end and golden entity rather than tune parameters on each of them. The baseline systems are (Miwa and Bansal, 2016) and (Christopoulou et al., 2018). In general, our “NN” is competitive, comparing to the dependency tree-based state-of-the-art model (Miwa and Bansal, 2016). It shows that our CNN-based neural networks are able to extract more powerful features to help relation extraction task. After adding GCN, our GCN-based models achieve the better performance. This indicates that the proposed mod"
P19-1131,P82-1020,0,0.824741,"Missing"
P19-1131,P16-1087,0,0.0251959,"tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great"
P19-1131,P17-1085,0,0.448054,"e first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By inspecting the performance of"
P19-1131,P14-1038,0,0.0836562,"el as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 20"
P19-1131,P16-1200,0,0.055208,"AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint"
P19-1131,D17-1159,0,0.0261108,"ation between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battaglia et al., 2018; Zhou et al., 2018). Graph Convolutional Network (GCN) is one of the typical variants of GNN (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2017). It has been successfully applied to many NLP tasks such as text classification (Yao et al., 2018), semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018) machine translation (Bastings et al., 2017) and knowledge base completion (Shang et al., 2018). We note that most previous applications of GCN focus on a single job, while the joint entity relation extraction consists of multiple sub-tasks. Investigating GCN in joint learning scenarios is the main topic of this work. A closely related work is (Christopoulou et al., 2018), which focuses on relation extraction with golden entities. Our work can be viewed as an end-to-end extension of their work. 6 Conclusion We propose a novel and concise joint model ba"
P19-1131,P16-1105,0,0.140999,"two stages: entities are first extracted by an entity model, and then these extracted entities are used as the inputs of a relation model. Pipeline models often ignore interactions between the two models and they suffer from error propagation. Joint models integrate information between entities and relations into a single model with the joint training, and have achieved better ∗ Work done while this author was an intern at Microsoft Research Asia. results than the pipeline models. In this paper, we focus on joint models. More and more joint methods have been applied to this task. Among them, Miwa and Bansal (2016); Katiyar and Cardie (2017) identify the entity with a sequence labelling model, and identify the relation type with a multi-class classifier. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. In addition, some complex joint decoding algorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully investigated, including Li and Ji (2014); Zhang et al. (2017); Zheng et al. (2017); Wang et al. (2018). They jointly handle span detection and type inference to achieve more interactions. By in"
P19-1131,D09-1013,0,0.0833432,"♠ PER:♥♣♠ ORG:♥♣♠ [republican gurad]ORG-AFF-2:♥♣♠ [units]PHYS-1:♥♠|ORG-AFF-1:♥♣♠|ART-1:♥ ordered to use chemical [weapons]WEA:♥♣♠ ART-2:♥ once u.s. and allied troops cross it . Table 5: Examples from the ACE05 dataset with label annotations from “NN” model and “GCN” model for comparison. The ♥ is the gold standard, and the ♣, ♠ are the output of the “NN” ,“GCN” model respectively. 5 Related Work There have been extensive studies for entity relation extraction task. Early work employs a pipeline of methods that extracts entities first, and then determines their relations (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint entity relation extraction. Parameter sharing is a basic strategy for joint extraction. For example, Miwa and Bansal (2016) propose a neural method comprised of a sentencelevel RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN usi"
P19-1131,D14-1162,0,0.0870553,"s work in Table 1. In general, our “GCN” achieves the best entity performance 84.2 percent comparing with existing joint models. For relation performance, our “GCN” significantly outperforms all joint models except for (Sun et al., 2018) which uses more complex joint decoder. Comparing with our basic neural network “NN”, our “GCN” has large improvement both on entities and relations. Those observations demonstrate the effectiveness of our “GCN” for capturing information on multiple entity types and relation types from a sentence. 5 Our word embeddings is initialized with 100dimensional glove (Pennington et al., 2014) word embeddings. The dimensionality of the hidden units and node embedding are set to 128. For all CNN in our network, the kernel sizes are 2 and 3, and the output channels are 25. 1365 Model P Entity R F P Relation R F L&J (2014) Zhang (2017) Sun (2018) 85.2 83.9 76.9 83.2 80.8 83.5 83.6 65.4 64.9 39.8 55.1 49.5 57.5 59.6 M&B (2016) K&C (2017) NN GCN 82.9 84.0 85.7 86.1 83.9 81.3 82.1 82.4 83.4 82.6 83.9 84.2 57.2 55.5 65.6 68.1 54.0 51.8 50.7 52.3 55.6 53.6 57.2 59.1 Table 1: Results on the ACE05 test data. Li and Ji (2014) Zhang et al. (2017) and Sun et al. (2018) are joint decoding algori"
P19-1131,P17-1113,0,0.190846,"es hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been receiving more and more attention because of the great expressive power of graphs (Cai et al., 2018; Battagli"
P19-1131,D18-1249,1,0.863882,"oncise joint model to handle the joint type inference problem based on graph convolutional network (GCN). 1362 In this work, we decompose the joint entity relation extraction task into two parts, namely, entity span detection and entity relation type deduction. We first treat entity span detection as a sequence labelling task (Section 3.1), and then construct an entity-relation bipartite graph (Section 3.2) to perform joint type inference on entity nodes and relation nodes (Section 3.3). All submodels share parameters and are trained jointly. Different from existing joint learning algorithms (Sun et al., 2018; Zhang et al., 2017; Katiyar and Cardie, 2017; Miwa and Bansal, 2016), we propose a concise joint model to perform joint type inference on entities and relations based on GCNs. It considers interactions among multiple entity types and relation types simultaneously in a sentence. 3.1 Entity Span Detection To extract entity spans from a sentence (Figure 2), we adopt the BILOU sequence tagging scheme: B, I, L and O denote the begin, inside, last and outside of a target span, U denotes a single word span. For example, for a person (PER) entity “Patrick McDowell”, we assign B to “Patrick” and L to"
P19-1131,P13-1161,0,0.0695515,"el RNN for extracting entities, and a dependency tree-based RNN to predict relations. Their relation model takes hidden states of the entity model as features (i.e., the shared parameters). Similarly, Katiyar and Cardie (2017) use a simplified relation model based on the entity RNN using the attention mechanism. These joint methods do joint learning through sharing parameters and they have no explicit interaction in type inference. To further explore interactions between the entity decoder and the relation decoder, many of them focus on some joint decoding algorithms. ILP-based joint decoder (Yang and Cardie, 2013), CRF-based joint decoder (Katiyar and Cardie, 2016), joint sequence labelling tag set (Zheng et al., 2017), beam search (Li and Ji, 2014), global normalization (Zhang et al., 2017), and transition system (Wang et al., 2018) are investigated. Different from models there, we propose a novel and concise joint model to handle joint type inference based on graph convolutional networks, which can capture information between multiple entity types and relation types explicitly9 . 9 In addition, transfer learning(Sun and Wu, 2019), multiRecently, researches of graph neural networks (GNNs) have been re"
P19-1131,D17-1182,0,0.112899,"Missing"
P19-1131,D18-1244,0,0.123934,"information from their neighborhood over the bipartite graph. It helps us to concisely capture information among entities and relations. For example, in Figure 1, to predict the PER (“Toefting”), our joint model can pool the information of PER-SOC, PHYS, PER (“teammates”) and GPE (captital). To further utilize the structure of the graph, we also propose assigning different weights on graph edges. In particular, we introduce a binary relation classification task, which is to determine whether the two entities form a valid relation. Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary relation classification, which makes the proposed adjacency matrix more explanatory. To summarize, the main contributions of this work are 1 bipartite graph in a more efficient and interpretable way. • We show that the proposed joint model on ACE05 achieves best entity performance, and is competitive with the state-of-the-art in relation performance. 2 Background of GCN In this section, we briefly describe graph convolutional networks (GCNs). Given a graph with n nodes, the goal of GCNs is to learn structureaware node representat"
P19-1641,W05-0909,0,0.051152,"es. 6386 4 Experiment and Case Study 4.1 Evaluation Metrics We separately evaluate the procedure extraction and captioning module. For procedure extraction, we adopt the widely used mJacc (mean of Jaccard) (Bojanowski et al., 2014) and mIoU (mean of IoU) metrics for evaluating the procedure proposition. The Jaccard calculates the intersection of the predicted and ground-truth procedure proposals over the length of the latter. The IoU replaces the denominator part with the union of predicted and ground-truth procedures. For procedure captioning, we adopt BLEU4(Papineni et al., 2002) and METEOR(Banerjee and Lavie, 2005) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground-truth procedures. 4.2 Dataset In this paper, we use the YouCookII3 (Zhou et al., 2018a) dataset to conduct experiments. It contains 2000 videos dumped from YouTube which are all instructional cooking recipe videos. For each video, human annotators were asked to first label the starting and ending time of procedure segments, and then write captions for each procedure. This dataset contains pre-processed frame features (T = 500 frames for each video, each frame feature is a 512-d vector, ex"
P19-1641,N15-1017,0,0.196455,"ed instructional video during procedure extraction and captioning. 2. We employ the pre-trained BERT(Devlin et al., 2018) and self-attention(Vaswani et al., 2017) layer to embed transcript, and then integrate them to visual encoding during procedure extraction. 3. We adopt the sequence-to-sequence model to generate captions by merging tokens of the transcript with the aligned video frames. 2 Related Works Narrated Instructional Video Understanding Previous works aim to ground the description to the video. (Malmaud et al., 2015) adopted an HMM model to align the recipe steps to the narration. (Naim et al., 2015) utilize latent-variable based discriminative models (CRF, Structured Perceptron) for unsupervised alignment. Besides the alignment of transcripts with video, (Alayrac et al., 2016, 2018) propose to learn the main steps from a set of narrated instructional videos for five different tasks and formulate the problem into two clustering problems. Graph-based clustering is also adopted to learn the semantic storyline of instructional videos in (Sener et al., 2015). These works assume that ”one task” has the same procedures. Different from previous works, we focus on learning more complicated proced"
P19-1641,P02-1040,0,0.108101,"le based on the extracted procedures. 6386 4 Experiment and Case Study 4.1 Evaluation Metrics We separately evaluate the procedure extraction and captioning module. For procedure extraction, we adopt the widely used mJacc (mean of Jaccard) (Bojanowski et al., 2014) and mIoU (mean of IoU) metrics for evaluating the procedure proposition. The Jaccard calculates the intersection of the predicted and ground-truth procedure proposals over the length of the latter. The IoU replaces the denominator part with the union of predicted and ground-truth procedures. For procedure captioning, we adopt BLEU4(Papineni et al., 2002) and METEOR(Banerjee and Lavie, 2005) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground-truth procedures. 4.2 Dataset In this paper, we use the YouCookII3 (Zhou et al., 2018a) dataset to conduct experiments. It contains 2000 videos dumped from YouTube which are all instructional cooking recipe videos. For each video, human annotators were asked to first label the starting and ending time of procedure segments, and then write captions for each procedure. This dataset contains pre-processed frame features (T = 500 frames for each video, eac"
P19-1641,N15-1173,0,0.0262622,"to ours, which is designed to detect long complicated event proposals rather than actions. We adopt this framework and inject the textual transcript of narrated instructional videos as our first step. Dense video caption aims to generate descriptive sentences for all events in the video. Different from video captioning and paragraph generation, dense video caption requires segmenting of each video into a sequence of temporal proposals with corresponding captions. (Krishna et al., 2017) resorts to the DAP method (Escorcia et al., 2016) for event detection and apply the contextaware S2VT model (Venugopalan et al., 2015). (Yu et al., 2018) propose to generate long and detailed description for sport videos. (Li et al., 2018) train jointly on unifying the temporal proposal localization and sentence generation for dense video captioning. (Xiong et al., 2018) assembles temporally localized description to produce a descriptive paragraph. (Duan et al., 2018) propose weakly supervised dense event captioning, which does not require temporal segment annotations, and decomposes the problem into a pair of dual tasks. (Wang et al., 2018a) exploit both past and future context for predicting accurate event proposals. (Zhou"
P19-1641,N18-2125,0,0.194606,"Missing"
