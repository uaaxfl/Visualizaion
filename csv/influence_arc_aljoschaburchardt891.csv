2011.eamt-1.36,P08-1087,0,0.0833238,"Missing"
2011.eamt-1.36,W10-1703,0,0.0895181,"Missing"
2011.eamt-1.36,E09-1049,0,0.0159472,"r modification improve some aspect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for"
2011.eamt-1.36,W09-0433,0,0.0143606,"pect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for the estimation of"
2011.eamt-1.36,2005.eamt-1.13,0,0.0558952,"Missing"
2011.eamt-1.36,H05-2007,0,0.0301208,"cation has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for the estimation of inflectional and reordering errors. A method based on WER and PER decomposition for discovering inflectional errors and missing words is presented in (Popovi´c and Ney, 2007). Zhou (2008) proposed a diagnostic evaluation of linguistic check-points obtained automatically by aligning parsed sourc"
2011.eamt-1.36,2008.eamt-1.17,0,0.0176041,"? Does a particular modification improve some aspect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed"
2011.eamt-1.36,W07-0707,1,0.92829,"Missing"
2011.eamt-1.36,W06-3101,1,0.864371,"Missing"
2011.eamt-1.36,vilar-etal-2006-error,0,0.546589,"Missing"
2011.eamt-1.36,C08-1141,0,0.296812,"Missing"
2011.eamt-1.36,W09-0401,0,\N,Missing
2012.eamt-1.14,W05-0820,0,0.0173523,"ngly the economic feasibility of MT and the fitness for real-world needs of professional translators and Language Service Providers (LSPs) have been hardly analysed so far. The MT community tries to broaden the domains the translation systems are applied to. In the early years, research on statistical machine translation concentrated on restricted domains, the touristic domain being a typical example. As the quality of the translations got better, the difficulty of the task was increased by moving to richer domains. The WMT evaluations are another example of this trend. In the first editions (Koehn and Monz, 2005) the data the systems were trained and evaluated on consisted only of the proceedings of the European Parliament. In more recent editions (Callison-Burch et al., 2011) the (parallel) training data still mostly consists of europarl data, but the evaluation has moved to the news domain, with a much wider variety of topics. 2 From TM to MT: The LSP’s starting point A LSP always has to keep a good balance between prices, linguistic quality, and time, all for the benefit of the client. Especially in the area of trainc 2012 European Association for Machine Translation. 73 ing material the price pres"
2012.eamt-1.14,moore-2002-fast,0,0.122004,"Missing"
2012.eamt-1.14,W10-1738,1,0.881638,"Missing"
2012.eamt-1.14,2002.tmi-tutorials.2,0,0.0980944,"Missing"
2013.mtsummit-posters.4,2010.iwslt-papers.1,0,0.0149183,"h means that the verb read needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which hel"
2013.mtsummit-posters.4,2012.eamt-1.44,0,0.0167737,"ad needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which helps 225 to estimate its gr"
2013.mtsummit-posters.4,W11-2104,1,0.915377,"to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and tar"
2013.mtsummit-posters.4,J99-2004,0,0.0441188,"al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to boost their performance over the baseline system. 224 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags, cf. Bangalore and Joshi (1999)) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that specifies the local syntactic context of the word at the lexical level in the form of a set of arguments. CCG builds a parse tree for a sentence by combining CCG categories using a set of binary combinatory rules. Most of the CCG grammar is contained in the lexicon, which is why CCG has simpler rules compared to CFG productions. CCG categories are divided into atomic and complex categories. Examples of atomic categories are S (sentence), N (noun), NP (noun phrase), et"
2013.mtsummit-posters.4,C04-1046,0,0.0459933,"syntactic categories, we were able to extract grammaticality QE features based on recognising grammatical chunks and examining sequences of CCG categories in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum En"
2013.mtsummit-posters.4,W10-1703,0,0.0690335,"Missing"
2013.mtsummit-posters.4,W12-3102,0,0.0576051,"Missing"
2013.mtsummit-posters.4,W12-3110,0,0.0115413,"et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Specia (2012) compare the performance of a set of linguistic features extracted from source and target sentences constituency and dependency trees with the baseline system of the WMT 2012 evaluation campaign (Callison-Burch et al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to bo"
2013.mtsummit-posters.4,2011.eamt-1.32,0,0.0415548,"Missing"
2013.mtsummit-posters.4,P07-1037,0,0.0606055,"Missing"
2013.mtsummit-posters.4,D09-1123,0,0.0510684,"Missing"
2013.mtsummit-posters.4,W12-3117,0,0.0190571,"features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Spe"
2013.mtsummit-posters.4,2006.amta-papers.25,0,0.126803,"Missing"
2013.mtsummit-posters.4,2010.jec-1.5,0,0.0451371,"Missing"
2013.mtsummit-posters.4,2009.mtsummit-papers.16,0,0.0118631,"s in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic fea"
2013.mtsummit-posters.4,2011.mtsummit-papers.58,0,0.160181,"trate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a"
2013.mtsummit-posters.4,2011.eamt-1.12,0,0.0120636,"uk translation and sometimes from internal translation information output by the MT system. With the improvement of the quality of MT systems and their increasing use in real-world applications, MT QE has become increasingly more important. QE has been demonstrated to help in making the integration of MT systems in the translation pipeline more efficient. For example, using QE to filter out low-quality translations from the post-editing process has been shown to help in reducing post-editing time as low-quality translations might take more time to post-edit than to be translated from scratch (Specia, 2011). Furthermore, QE helps to enhance MT user experience by informing the user of the predicted quality of the translation produced by the MT system. Moreover, QE has been more and more used to enhance the quality of MT systems by integrating QE scores in n-best reranking and combining the translation of different MT systems. QE features estimate the quality of the translation by capturing the aspects which evaluate translation quality, namely fluency and adequacy, in addition to predicting the difficulty of the translation. Adequacy refers to the extent to which the meaning of the source sentenc"
2013.mtsummit-posters.4,P10-1062,0,0.0244835,"rk The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased"
2013.mtsummit-posters.5,2003.mtsummit-systems.1,0,0.0537752,"ence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the best general purpose MT engines, it has been included in order to allow us to assess the performance level of our SMT system and also to compare it directly with other MT ap"
2013.mtsummit-posters.5,W10-1703,0,0.0302962,"number of translators working on it. 2.1 Translation systems used The evaluated translation outputs presented in this work are produced by German-English, GermanFrench and German-Spanish machine translation Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranki"
2013.mtsummit-posters.5,W12-3102,0,0.0648542,"Missing"
2013.mtsummit-posters.5,vilar-etal-2006-error,1,0.893094,"Missing"
2013.mtsummit-posters.5,W10-1738,1,0.850961,"level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the be"
2013.mtsummit-posters.5,federmann-2010-appraise,0,0.0730217,"nslation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and tech"
2013.mtsummit-posters.5,P07-2045,0,0.0038879,"licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine"
2013.mtsummit-posters.5,W12-3123,0,0.0364801,"ent of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluation process has b"
2013.mtsummit-posters.5,2011.eamt-1.12,0,0.0194594,"rther improvement of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluati"
2013.mtsummit-wptp.2,2003.mtsummit-systems.1,0,0.0711019,"1 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), se"
2013.mtsummit-wptp.2,federmann-2010-appraise,0,0.0122073,"e sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), select the translation output which is easiest to post-edit and perform the editing. Post-edit all: for each source sentence in the selected subset (4070 sentences in total), postedit all four produced translation outputs. For both post-editing tasks, the translators"
2013.mtsummit-wptp.2,P10-1064,0,0.032193,"Missing"
2013.mtsummit-wptp.2,W12-3123,0,0.0120168,"proved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professi"
2013.mtsummit-wptp.2,2011.eamt-1.12,0,0.0122675,", five types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, si"
2013.mtsummit-wptp.2,2012.amta-wptp.9,0,0.0175217,"dding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by trans"
2013.mtsummit-wptp.2,W10-1738,1,0.708534,"n-de es-de fr-de Total News 1788 514 912 1744 101 1852 6911 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and p"
2014.eamt-1.38,W13-2201,0,0.100476,"Missing"
2014.eamt-1.38,W11-2107,0,0.017106,"n examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result of these limitations, there has been a"
2014.eamt-1.38,2010.eamt-1.12,0,0.209351,"Missing"
2014.eamt-1.38,1994.amta-1.9,0,0.935888,"lity Metric” MQM designed by the QTLaunchPad project (http://www.qt21.eu/launchpad). The metric was designed to facilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model,"
2014.eamt-1.38,P02-1040,0,0.0903186,"ticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result o"
2014.eamt-1.38,stymne-ahrenberg-2012-practice,0,0.494563,"cilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model, SAE J2450, TAUS DQF, ATA assessment, and various tool-specific metrics) that served as the basis for a descriptive framework f"
2014.eamt-1.38,vilar-etal-2006-error,0,0.85614,"Missing"
2014.eamt-1.41,2011.mtsummit-papers.17,0,0.162598,"– Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas removing additions ha"
2014.eamt-1.41,W12-3102,0,0.0667545,"Missing"
2014.eamt-1.41,2012.eamt-1.35,0,0.0190046,"(Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2012 ok 323 31 200 quality level edit+ edit edit1559 0 544 399 0 550 548 856 576 make the translation acceptable. Post-editing time is measured on the sentence level in a controlled way in order to isolate factors such as pauses between sentences. The technical effort is represented by following five types of edit operations: bad 99 20 74 Table 1: Corpus statistics: number of sentences assigned to each"
2014.eamt-1.41,W12-3123,0,0.0661415,"under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 191 More details about the technical effort can be obtained by analysing particular edit operations. (Blain et al., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al"
2014.eamt-1.41,J11-4002,1,0.862953,"Missing"
2014.eamt-1.41,specia-etal-2010-dataset,0,0.058975,"Missing"
2014.eamt-1.41,2011.eamt-1.12,0,0.111983,"szkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas"
2014.eamt-1.41,2009.mtsummit-posters.20,0,0.295234,"e Lommel, Aljoscha Burchardt, Eleftherios Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most c"
2014.eamt-1.41,2010.jec-1.6,0,0.0874217,"Missing"
2014.eamt-1.41,temnikova-2010-cognitive,0,0.116741,"Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most"
2014.eamt-1.41,2013.mtsummit-papers.15,0,0.0298537,"., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2"
2015.eamt-1.15,W05-0814,0,0.0385293,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.15,E03-1076,0,0.0450815,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.15,2005.mtsummit-papers.11,0,0.0120371,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.15,J11-4002,1,0.893851,"Missing"
2015.eamt-1.15,2011.eamt-1.12,0,0.0238497,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.15,E09-1087,0,0.0700919,"Missing"
2015.eamt-1.15,tiedemann-2012-parallel,0,0.0242552,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.15,W11-2103,0,\N,Missing
2020.wmt-1.38,N18-1118,0,0.0289868,"Missing"
2020.wmt-1.38,W18-6432,0,0.0950057,"Missing"
2020.wmt-1.38,W18-6433,0,0.104826,"Missing"
2020.wmt-1.38,W18-6434,0,0.0282019,"est suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various linguistic phenomena and supporting different language directions. These include work in grammatical contrasts (Cinkova and Bojar, 2018), discourse (Bojar et al., 2018), morphology (Burlot et al., 2018), pronouns (Guillou et al., 2018) and word sense disambiguation (Rios et al., 2018). When compared to the vast majority of the previous test suites, the one presented here is the only one 346 Proceedings of the 5th Conference on Machine Translation (WMT), pages 346–356 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Lexical Ambiguity Er las gerne Novellen. He liked to read novels. He liked to read novellas. Phrasal verb Warum starben die Dinosaurier aus? Why did the dinosaurs die? Why did the dinos"
2020.wmt-1.38,L16-1100,0,0.0594885,"n ways that cannot be seen by generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite trac"
2020.wmt-1.38,W18-6435,0,0.0332352,"Missing"
2020.wmt-1.38,D17-1263,0,0.035982,"Missing"
2020.wmt-1.38,C90-2037,0,0.743908,"re relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various lingu"
2020.wmt-1.38,2020.wmt-1.12,0,0.168789,"Missing"
2020.wmt-1.38,W18-6436,1,0.851368,"a source sentence and a set of correct and/or incorrect MT outputs. At the evaluation time, the test items are given as input to the MT systems and it is tested on whether the respective MT output consists a correct translation. By observing the amount of the test items that are translated correctly, one can calculate the performance of the MT systems regarding the respective phenomenon. The evaluation presented in this paper is based on the DFKI Test Suite for MT on German to English, which has been presented in Burchardt et al. (2017) and applied extensively in the WMT shared task of 2018 (Macketanz et al., 2018) and 2019 (Avramidis et al., 2019). The current version includes 5,560 test items in order to control 107 phenomena organised in 14 categories. Some sample test items can be seen in Table 1 whereas a more detailed list of test sentences with correct and incorrect translations can be found on GitHub1 . 3.1 Application of the test suite The construction of the test suite has been thoroughly explained in the papers from the previous years (Avramidis et al., 2018, 2019) and depicted in Figure 1 (steps a-c). The test items of the test suite are given as input to the MT systems (step d). Their MT ou"
2020.wmt-1.38,2020.wmt-1.25,0,0.313684,"Missing"
2020.wmt-1.38,W18-6307,0,0.0467561,"Missing"
2020.wmt-1.38,P02-1040,0,0.106035,"Missing"
2020.wmt-1.38,2020.acl-main.442,0,0.0159125,"generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of"
2020.wmt-1.38,W18-6437,0,0.044788,"Missing"
2020.wmt-1.38,2020.wmt-1.30,0,0.0546219,"Missing"
2020.wmt-1.38,P19-1164,0,0.0404761,"asing interest on several natural language processing (NLP) tasks. Focusing on particular issues gives the possibility to analyse the automatic output in ways that cannot be seen by generic metrics. This is of particular importance in the era of deep learning, which has led to high performances and differences that are relatively difficult to distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comp"
2020.wmt-1.38,1991.mtsummit-panels.5,0,0.636469,"o distinguish. Additionally, detailed evaluation can provide indications for the improvement of the systems and the data collection, or allow focusing on phenomena of the long tail that might be of particular interest for certain cases (e.g. social biases; Stanovsky et al., 2019). The most common method for fine-grained or focused evaluation are the test suites (also known as challenge sets or benchmarks; Guillou and Hardmeier, 2016; Ribeiro et al., 2020). These are test Related Work The use of test suites was introduced along with the early steps of MT in the 1990’s (King and Falkedal, 1990; Way, 1991; Heid and Hildenbrand, 1991). With the emergence of deep learning, recent works re-introduced test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The test suite track of the Conference of Machine Translation has already taken place two years in a row, allowing the presentation of several test suites, focusing on various linguistic pheno"
2020.wmt-1.38,2020.wmt-1.33,0,0.157472,"Missing"
avramidis-etal-2012-involving,eisele-chen-2010-multiun,0,\N,Missing
avramidis-etal-2012-involving,federmann-2010-appraise,1,\N,Missing
avramidis-etal-2012-involving,J11-4002,1,\N,Missing
avramidis-etal-2012-involving,P02-1040,0,\N,Missing
avramidis-etal-2012-involving,W10-1738,1,\N,Missing
avramidis-etal-2012-involving,P07-2045,0,\N,Missing
avramidis-etal-2012-involving,W11-2104,1,\N,Missing
avramidis-etal-2012-involving,W10-1703,0,\N,Missing
avramidis-etal-2012-involving,vilar-etal-2006-error,1,\N,Missing
avramidis-etal-2012-involving,W11-2100,0,\N,Missing
avramidis-etal-2012-involving,2011.eamt-1.36,1,\N,Missing
avramidis-etal-2012-involving,2010.amta-papers.27,0,\N,Missing
avramidis-etal-2014-taraxu,federmann-2010-appraise,0,\N,Missing
avramidis-etal-2014-taraxu,avramidis-etal-2012-involving,1,\N,Missing
avramidis-etal-2014-taraxu,W10-1738,1,\N,Missing
avramidis-etal-2014-taraxu,P07-2045,0,\N,Missing
avramidis-etal-2014-taraxu,2013.mtsummit-posters.5,1,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2006-shalmaneser,1,\N,Missing
burchardt-etal-2006-salsa,burchardt-etal-2006-salto,1,\N,Missing
burchardt-etal-2006-salsa,fliedner-2006-towards,0,\N,Missing
burchardt-etal-2006-salsa,W04-2703,0,\N,Missing
burchardt-etal-2006-salsa,H05-1047,0,\N,Missing
burchardt-etal-2006-salsa,P98-1013,0,\N,Missing
burchardt-etal-2006-salsa,C98-1013,0,\N,Missing
burchardt-etal-2006-salsa,J02-3001,0,\N,Missing
burchardt-etal-2006-salsa,J05-1004,0,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2004-powerful,1,\N,Missing
burchardt-etal-2006-salsa,W04-1906,1,\N,Missing
burchardt-etal-2006-salto,brants-plaehn-2000-interactive,0,\N,Missing
burchardt-etal-2006-salto,P97-1003,0,\N,Missing
burchardt-etal-2006-salto,P98-1013,0,\N,Missing
burchardt-etal-2006-salto,C98-1013,0,\N,Missing
burchardt-etal-2006-salto,P03-1068,1,\N,Missing
burchardt-etal-2006-salto,erk-pado-2004-powerful,1,\N,Missing
burchardt-pennacchiotti-2008-fate,burchardt-etal-2006-salto,1,\N,Missing
burchardt-pennacchiotti-2008-fate,burchardt-etal-2006-salsa,1,\N,Missing
burchardt-pennacchiotti-2008-fate,W07-1402,1,\N,Missing
burchardt-pennacchiotti-2008-fate,J03-4003,0,\N,Missing
burchardt-pennacchiotti-2008-fate,P98-1013,0,\N,Missing
burchardt-pennacchiotti-2008-fate,C98-1013,0,\N,Missing
burchardt-pennacchiotti-2008-fate,W05-1210,0,\N,Missing
burchardt-pennacchiotti-2008-fate,W07-1400,0,\N,Missing
I08-1051,burchardt-etal-2006-salsa,1,0.931521,"ularity offered by a given annotation layer may diverge considerably from the granularity that is needed for the integration of corpus-derived data in large symbolic processing architectures or general lexical resources. This problem is multiplied when more than one layer of annotation is considered, for example in the characterisation of interface phenomena. While it may be possible to obtain coarser-grained representations procedurally by collapsing categories, such procedures are not flexibly configurable. Figure 1 illustrates these difficulties with a sentence from the SALSA/TIGER corpus (Burchardt et al., 2006), a manually annotated German newspaper corpus which contains role-semantic analyses in the FrameNet paradigm (Fillmore et al., 2003) on top of syntactic structure (Brants et al., 2002).1 The se1 While FrameNet was originally developed for English, the majority of frames has been found to generalise well to other 389 which the official Croatia but in significant international-law difficulties bring would Figure 1: Multi-layer annotation of a German phrase with syntax and frame semantics (‘which would bring official Croatia into significant difficulties with international law’) mantic structure"
I08-1051,E03-1068,0,0.0163518,"namely 4 missing metaphor flags and 4 omitted underspecification links. On the semantic level, we extracted annotation instances (in context) for metaphorical vs. nonmetaphorical readings, or frames that are involved in underspecification in certain sentences, but not in others. While the result sets thus obtained still require manual inspection, they clearly illustrate how the detection of inconsistencies can be enhanced by a declarative formalisation of the annotation scheme. Another strategy could be to concentrate on frames or lemmas exhibiting proportionally high variation in annotation (Dickinson and Meurers, 2003). 6 Conclusion In this paper, we have constructed a Description Logics-based lexicon model directly from multi-layer linguistic corpus annotations. We have shown how such a model allows for explicit data modelling, and for flexible and fine-grained definition of various degrees of abstractions over corpus annotations. Furthermore, we have demonstrated that a powerful logical formalisation which integrates an underlying annotation scheme can be used to directly control consistency of the annotations using general KR techniques. It can also overcome limitations of current XML-based search tools"
I08-1051,J02-3001,0,0.00609142,"roblematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the most common use of such corpora is the acquisition of statistical models that make use of the “more shallow” levels to predict the “deeper” levels of annotation (Gildea and Jurafsky, 2002; Miltsakaki et al., 2005). While these models fill an important need for practical applications, they fall short of the general task of lexicon modelling, i.e., creating an abstracted and compact representation of the corpus information that lends itself to ’linguistically informed’ usages such as human interpretation or integration with other knowledge sources (e.g., deep grammar resources or ontologies). In practice, this task faces three major problems: ∗ At the time of writing, Sebastian Padó and Dennis Spohr were affiliated with Saarland University, and Anette Frank with DFKI Saarbrücken"
I08-1051,U04-1019,0,0.0261719,"heme. We present a general approach to formally modelling corpora with multi-layered annotation, thereby inducing a lexicon model in a typed logical representation language, OWL DL. This model can be interpreted as a graph structure that offers flexible querying functionality beyond current XML-based query languages and powerful methods for consistency control. We illustrate our approach by applying it to the syntactically and semantically annotated SALSA/TIGER corpus. 1 4 Dept. of Linguistics Stanford University Stanford, CA Querying multiple layers of linguistic annotation. A recent survey (Lai and Bird, 2004) found that currently available XML-based corpus query tools support queries operating on multiple linguistic levels only in very restricted ways. Particularly problematic are intersecting hierarchies, i.e., tree-shaped analyses on multiple linguistic levels. Introduction Over the years, much effort has gone into the creation of large corpora with multiple layers of linguistic annotation, such as morphology, syntax, semantics, and discourse structure. Such corpora offer the possibility to empirically investigate the interactions between different levels of linguistic analysis. Currently, the m"
I08-1051,W06-1007,0,0.0219647,"mantics of the model makes it possible to use general and efficient knowledge representation techniques for consistency control. Finally, we can extract specific subsets from a corpus by defining task-specific views on the graph. After a short discussion of related approaches in languages (Burchardt et al., 2006; Boas, 2005). Section 2, Section 3 provides details on our methodology. Sections 4 and 5 demonstrate the benefits of our strategy on a model of the SALSA/TIGER data. Section 6 concludes. 2 Related Work One recent approach to lexical resource modelling is the Lexical Systems framework (Polguère, 2006), which aims at providing a highly general representation for arbitrary kinds of lexica. While this is desirable from a representational point of view, the resulting models are arguably too generic to support strong consistency checks on the encoded data. A further proposal is the currently evolving Lexical Markup Framework (LMF; Francopoulo et al. (2006)), an ISO standard for lexical resource modelling, and an LMF version of FrameNet exists. However, we believe that our usage of a typed formalism takes advantage of a strong logical foundation and the notions of inheritance and entailment (cf."
I08-1051,W06-0609,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
L16-1001,J10-3004,0,0.0171198,"nal or books, or transcription of oral conversations. Furthermore, corpora with interrogatives are extremely rare, and most of them contain interrogatives that are artificially produced by manipulation over sentences that were originally declarative ones. The multilinguality of this corpus adds also to its importance. In the last years, some corpora were collected composed by chat conversations over the internet. These corpora typically contain informal conversations about personal topics (Forsyth and Martell, 2007). Other corpora are more focused on technical issues such as the LINUX corpus (Elsner and Charniak, 2010), the IPHONE/PHYSICS/PYTHON corpus (Adams, 2008) and the Ubuntu chat corpus (Uthus and Aha, 2013). These corpora differ from the one presented here as they include large amounts of utterances produce under social interaction, although the chats used as sources for these corpora were initially intended only for technical support. In all the referred corpora, the conversation threads involve several participants using an informal register. In almost all the cases (except for the Ubuntu corpus) the language addressed in these corpora is limited to English. The corpus was collected selecting the d"
L16-1296,W15-5705,1,0.838417,"Missing"
L16-1296,P02-1040,0,0.134922,"Missing"
L18-1142,2003.mtsummit-systems.1,0,0.331074,"Missing"
L18-1142,D16-1025,0,0.0177636,"pes of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that was involved in the evaluation procedure. In this paper we will present the n"
L18-1142,L16-1100,0,0.139109,"vi´c and Ney, 2011) have not yet become standard. For the detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that"
L18-1142,D17-1263,0,0.0946452,"e detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT systems on IWSLT data using a coarse-grained error typology where neural systems have been found to make fewer morphological, lexical and word-order errors. Using our own test suites, we have performed several comparative studies of different MT systems both in the general domain (Burchardt et al., 2017) and in the technical domain (Beyer et al., 2017). When presenting this work, one of the most (obvious) criticism we got was the huge amount of manual effort that was involved in the evaluation procedure. In this"
L18-1142,1995.mtsummit-1.35,0,0.455422,"tasks where test suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as dif"
L18-1142,C90-2037,0,0.870085,"be extended to other NLP tasks where test suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can b"
L18-1142,2001.mtsummit-papers.35,0,0.814551,"st suites can be used such as evaluating (one-shot) dialogue systems. Keywords: Machine Translation, Quality Evaluation, Test Suites 1. Introduction and Background In several areas of NLP evaluation, test suites have been used to analyze the strengths and weaknesses of systems. In contrast to “real-life” gold standard corpora, test suites can contain made-up or edited input-output pairs to isolate interesting or difficult phenomena. In Machine Translation (MT) research, broadly-defined test suites have not been used apart from several singular attempts (King and Falkedal, 1990; Isahara, 1995; Koh et al., 2001, etc.). One of the reasons for this might be the fear that the performance of statistical MT systems depends so much on the particular input data, parameter settings, etc., that relevant conclusions about the errors they make are difficult to obtain. Another concern is that “correct” MT output cannot be specified in the same way as the output of other language processing tasks like parsing or fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as difficult as the tran"
L18-1142,C96-2120,0,0.256926,"the manual evaluation procedure we have applied in the past. Section 3. describes the new TQ-AutoTest framework that supports the evaluation procedure. A use case of the TQ-AutoTest will be shown in Section 4.. Finally, in Section 5. we will conclude and give an outlook on future work. 2. Test Suites for German – English We have built a test suite for a fine-grained evaluation of MT quality for the language pair German – English. In brief, it contains segments selected from various parallel corpora and drawn from other sources such as grammatical resources, e.g., the TSNLP Grammar Test Suite (Lehmann et al., 1996) and online lists of typical translation errors. Each test sentence is annotated with a phenomenon category and the phenomenon it represents. An example showing these fields can be seen in Table 1 with the first column containing the source segment and the second and third column containing the phenomenon category and the phenomenon, respectively. The fourth column shows an example machine translation1 and the last column contains a 1 As example we have used the “old” Google Translate system that was used before Google changed to a neural system in September 2016, cf. https://research.googlebl"
L18-1142,J11-4002,0,0.0624529,"Missing"
L18-1142,W14-0821,0,0.0650591,"Missing"
L18-1142,W16-2323,0,0.0425685,"Missing"
L18-1142,W16-3409,0,0.0131884,"fact extraction where the expected results can be more or less clearly defined. Due to the variation of language, ambiguity, etc., checking and evaluating MT output can be almost as difficult as the translation itself. Today, MT quality is still usually assessed by shallow automatic comparisons of MT outputs with reference corpora resulting in a number. Early attempts to automatically classify errors based on post-edits or reference translations like (Popovi´c and Ney, 2011) have not yet become standard. For the detection of certain types of errors like grammar errors, parsers have been used (Tezcan et al., 2016). In other narrow domains, researchers have started to explore the differences between systems and between the development stages of one system in more linguistic detail. Especially the trend towards neural MT has renewed peoples’ interest in better and more analytical diagnostic methods for MT quality. Recent work based on specific test suites includes the study of verb-particle constructions (Schottm¨uller and Nivre, 2014), pronouns (Guillou and Hardmeier, 2016) or structural divergences (Isabelle et al., 2017). (Bentivogli et al., 2016) performed a comparison of neural- with phrasebased MT"
W07-1402,W05-1206,0,\N,Missing
W07-1402,P98-1013,0,\N,Missing
W07-1402,C98-1013,0,\N,Missing
W07-1402,P02-1035,0,\N,Missing
W11-2104,C04-1046,0,0.80923,"of every given sentence. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training"
W11-2104,W08-0309,0,0.121698,"ch metrics have been known as Confidence Estimation metrics and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that perfo"
W11-2104,W10-1703,0,0.363914,"s and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that performs similar to the human ranking: it is trained to rank sev"
W11-2104,W09-3712,0,0.0156559,"ng, the classifiers were used to perform ranking on a test set of 184 sentences which had been kept apart from the 2010 data, with the criterion that they do not contain contradictions among human judgments. In order to allow further comparison with other evaluation metrics, we performed an extended experiment: we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to per"
W11-2104,N07-1051,0,0.0080242,": we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to perform better given a smaller group of statistically independent features. Since exhaustive training/testing of all possible feature subsets was not possible, we performed feature selection based on the Relieff method (Kononenko, 1994; Kira and Rendell, 1992). Automatic ranking was performed based on the most"
W11-2104,P06-1055,0,0.00745077,"e pairwise comparison of system outputs ti and tj with respective ranks ri and rj , determined as:  1 ri &lt; rj c(ri , rj ) = −1 ri &gt; rj At testing time, after the classifier has made all the pairwise decisions, those need to be converted back to ranks. System entries are ordered, according to how many times each of them won in the pairwise comparison, leading to rank lists similar to the ones provided by human annotators. Note that this kind of decomposition allows for ties when there are equal times of winnings. 66 Acquiring features • Parsing: Processing features acquired from PCFG parsing (Petrov et al., 2006) for both source and target side include: – – – – parse log likelihood, number of n-best trees, confidence for the best parse, average confidence of all trees. Ratios of the above target features to their respective source features were included. • Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. In particular, NPs, VPs, PPs, NNs and punctuation occurences were counted. Then the ratio of the occurences of each tag in the target sentence by its occurences on the source sentence was also calculated. 2.4"
W11-2104,2009.eamt-1.15,0,0.0842697,"Missing"
W11-2104,N07-1029,0,0.0599919,"Missing"
W11-2104,2009.mtsummit-papers.16,0,0.0218978,"teria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating against this data. Evalu"
W11-2104,H05-1096,0,0.181409,"nce. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating aga"
W11-2104,W09-0401,0,\N,Missing
W11-2104,J06-4003,0,\N,Missing
W11-2109,W05-0909,0,0.0830542,"xplored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of ev"
W11-2109,J93-2003,0,0.0279455,"st be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of evaluating translation quality (e.g., completeness, ordering, domain fit, etc.) is transformed into a kind of paraphrase evaluation in the target language, which is a very difficult problem itself. In addition, the set of selected references always represents only a small subset of all good translations. To remedy these drawbacks, we propose a truly automatic evaluation metric which is based on the IBM 1 lexicon scores (Brown et al., 1993). The inclusion of IBM 1 scores in translation systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated"
W11-2109,W08-0309,0,0.0789113,"n experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P"
W11-2109,W10-1703,0,0.0538159,"have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P (f1J |eI1 ) = 3 Experiments on WMT 2008, WMT 2009 and WMT 2010 test data I J"
W11-2109,P02-1040,0,0.0908633,"BM 1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using refer"
W11-2109,E09-1087,0,0.0855911,"Missing"
W11-2109,W09-0401,0,\N,Missing
W11-2109,C04-1046,0,\N,Missing
W15-3004,W07-0726,0,0.0343284,"he QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17"
W15-3004,eisele-chen-2010-multiun,0,0.0119801,"t consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT sy"
W15-3004,W11-2104,1,0.900755,"Missing"
W15-3004,W12-3138,0,0.0126825,"t is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computat"
W15-3004,W10-1708,0,0.0233189,"1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015"
W15-3004,W12-0115,0,0.0126604,"bination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. 2.1.1 Empirical machine learning classifier (primary submission) The machine learning (ML) selection mechanism is based on encouraging results of previous projects including E URO M ATRIX + (Federmann and Hunsicker, 2011), META-NET (Federmann, 2012), QTL AUNCH PAD (Avramidis, 2013; Shah et al., 2013). It has been extended to include several features that can only be generated on a sentence level and would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In the architecture at hand, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source wit"
W15-3004,P02-1040,0,0.0935117,"Missing"
W15-3004,N07-1051,0,0.0411992,"l system combination produces a perfect translation. In this particular case, the machine translation is even better than the human reference (“W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus.”) as the latter is introducing a determiner for “table”, which is not justified by the source. 2.1 Feature sets We experimented with feature sets that performed well in previous experiments. In particular: • Basic syntax-based feature set: unknown words, count of tokens, count of alternative parse trees, count of verb phrases, PCFG parse log likelihood. The parsing was performed with the Berkeley Parser (Petrov and Klein, 2007) and features were extracted from both source and target. This feature set has performed well as a metric in WMT-11 metrics task (Avramidis et al., 2011). Sentence level selection • Basic feature set + 17 QuEst baseline features: this feature set combines the basic syntax-based feature set described above We present two methods for performing sentence level selection, one with pairwise classifier and one based on POS 4-gram IBM 1 models. 67 icality. Further analysis on this aspect may be required. with the baseline feature set of the QuEst toolkit (Specia et al., 2013) as per WMT-13 (Bojar et"
W15-3004,W11-2123,0,0.0411543,"o methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-"
W15-3004,W11-2110,1,0.867236,"Missing"
W15-3004,N07-1064,0,0.190898,"analysis phase, where the sourcelanguage text is parsed and a tree of the source language is constructed • the transfer phase, where the analysis tree is used for the transfer phase, where canonical forms and categories of the source are transferred into similar representations of the target language • the generation phase, where the target sentence is formed out of the transfered representations by employing inflection and agreement rules. LucyMoses As an alternative way of automatic post-editing of the transfer-based system, a serial transfer+SMT system combination is used, as described in (Simard et al., 2007). For building it, the first stage is translation of the source language part of the training corpus by the transfer-based system. In the second stage, an SMT system is trained using the transfer-based translation output as a source language and the target language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. In previous experiments, however, the method on its own could not outperform Moses trained on a large parallel corpus. The example in Figure 1 (taken from the QTL EAP corpus u"
W15-3004,P13-1135,0,0.0238802,"by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this pap"
W15-3004,P13-4014,0,0.0588687,"Missing"
W15-3004,J93-2003,0,\N,Missing
W15-3004,W05-0909,0,\N,Missing
W15-3004,W13-2201,0,\N,Missing
W15-3004,W11-2141,0,\N,Missing
W15-4914,W05-0814,0,0.0377031,"lish sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation output • ﬁrst four letters of the word (4let) The simplest way for word reduction is to use only its ﬁrst n letters. The choice of ﬁrst four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • ﬁrst two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more reﬁned method which splits words into stems and sufﬁxes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained whe"
W15-4914,E03-1076,0,0.0295937,"for lemmas, it would not be possible to detect any inﬂectional error thus setting the inﬂectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inﬂective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classiﬁcation results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The sufﬁx of each word is removed and only the stem is preserved. For calculation of stem and sufﬁx frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation o"
W15-4914,2005.mtsummit-papers.11,0,0.0288445,"” error rates. The best way for the assessment would be, of course, a comparison with human error classiﬁcation. Nevertheless, this has not been done for two reasons: ﬁrst, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable"
W15-4914,J11-4002,1,0.893667,"Missing"
W15-4914,2011.eamt-1.12,0,0.0230765,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using T"
W15-4914,E09-1087,0,0.0605874,"Missing"
W15-4914,tiedemann-2012-parallel,0,0.0606606,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenian"
W15-4914,W11-2103,0,\N,Missing
W15-5702,2003.mtsummit-systems.1,0,0.0185567,"(6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation occurs in three phases, namely analysis, transfer, and generation. All three phases consist of hand-coded linguistic rules which have shown to perform well for capturing the structural and semantic differences between German and other language"
W15-5702,W14-3302,0,0.0706052,"Missing"
W15-5702,eisele-chen-2010-multiun,0,0.0162457,"gure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we als"
W15-5702,W11-2141,0,0.122729,"dent target language structures. A RestAPI allows the different processing steps and/or intermediate results to be influenced. Deep features for empirical enhancement Although deep techniques indicate good coverage of a number of linguistic phenomena, each of the three phases may frequently encounter serious robustness issues and/or the inability to fully process a given sentence. Erroneous analysis from early phases may aggregate along the pipeline and cause further sub-optimal choices in later phases, thus severely deteriorating the quality of the produced translation. Preliminary analysis (Federmann and Hunsicker, 2011) has shown that such is the case for source sentences that are ungrammatical in the first place or that have a very shallow syntax with many specialized lexical entries. To tackle these issues, we combine the transfer-based component with our supportive SMT engine in the following two ways: (a) train a statistical machine translation to automatically post-edit the output of the transfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that perf"
W15-5702,W12-0115,0,0.0146068,"combination produces a perfect translation. In this particular case, the machine translation (W¨ahlen Sie im Einf¨ugen Men¨u Tabelle aus) is even better than the human reference (W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus) as the latter introduces a determiner for “table” that is not justified by the source. English Transfer-‐ based MT German* SMT German Figure 2: Serial System Combination en→de. 2.4 Parallel System Combination: Selection Mechanism The selection mechanism is based on encouraging results of previous projects including Euromatrix Plus (Federmann and Hunsicker, 2011), T4ME (Federmann, 2012), QTLaunchPad (Avramidis, 2013; Shah et al., 2013). It has been extended to include several deep features that can only be generated on a sentence level and that would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In System 1, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source with the st"
W15-5702,W11-2123,0,0.00929833,"l., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has sh"
W15-5702,popovic-ney-2006-pos,1,0.760166,"e pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation oc"
W15-5702,N07-1064,0,0.0303755,"ransfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that performs real-time analysis of the produced translations and automatically selects the output that is predicted to be of a better quality (Avramidis, 2011). Figure 1 shows the overall architecture of System 1 for en→de. 2.3 Serial System Combination: Lucy+Moses For automatic post-editing of the transfer-based system, a serial Transfer+SMT system combination is used, as described in (Simard et al., 2007) The first stage is translation of the source-language part of the training corpus by the transfer-based system. The second stage is training an SMT system with the transfer-based translation output as a source language and the target-language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. Figure 2 illustrates the architecture for translation direction en→de. Note that the notion of “German*” in the figure is meant to distinguish the input and output of the SMT system. “German*” is t"
W15-5702,P13-1135,0,0.0293718,"arts from each employed method. Figure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield,"
W15-5705,2013.iwslt-evaluation.1,0,0.0890239,"Missing"
W15-5705,P11-2075,0,0.0126863,"ingual patent retrieval, cross-lingual sentiment classification, collaborative work via idea exchange, speech-to-speech translation, and dialogue. The Patent Translation Task at the Seventh NTCIR Workshop employed search topics for cross-lingual patent retrieval, which was used to evaluate the contribution of machine translation for retrieving patent documents across languages (Fujii et al., 2008). They also analysed the relationship between the accuracy of MT and its effects on retrieval accuracy (Fujii et al., 2009), which comes closest to the evaluation of answer retrieval in our scenario. Duh et al. (2011) investigated the effect of Machine Translation on Cross-lingual Sentiment classification and suggested improvements to the adaptation problems that have been identified. Yamashita and Ishida (2006) started research on collaborative work using machine translation. Similarly, Wang et al. (2013) evaluated MT through idea exchange: in this scenario, pairs of one English and one Chinese speaker performed brainstorming tasks assisted by MT, which helped the non-native English speakers produce ideas; nevertheless comprehension problems were identified with MT output. In the early years of NLP, the V"
W15-5705,2010.iwslt-papers.16,0,0.0678872,"Missing"
W16-2329,2003.mtsummit-systems.1,0,0.743848,"ve search with all possible combinations of the modification above and the most indicative automatic scores are shown in table 3. Although automatic scores have in the past shown low performance when evaluating RBMT systems, our proposed modifications have a lexical impact that can be adequately measured with ngram based metrics. Our investigation and discussion is performed on Batch 2. The best combination of the suggested modifications achieves an overall improvement of 0.51 points BLEU and 0.68 points METEOR over the baseline. In particular: Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its state-of-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. This year’s work on RBMT focuses on issues revealed through manual inspection of its performance on the development set: • Adding quotes around menu items resulted in a significant drop of the automatic scores, so it was not used;"
W16-2329,P02-1040,0,0.0961403,"Missing"
W16-2329,W15-3004,1,0.893586,"Missing"
W16-2329,N07-1051,0,0.0242083,"Missing"
W16-2329,W07-0718,0,0.055134,"spective phrase-based models, using the first 1.1M sentences of Europarl and ommiting the entire Commoncrawl. We experimented with four different settings concerning the translation path. These settings with the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn,"
W16-2329,E06-1032,0,0.0392743,"Missing"
W16-2329,I08-2089,0,0.142696,"he entire Commoncrawl. We experimented with four different settings concerning the translation path. These settings with the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and que"
W16-2329,W11-2123,0,0.0209094,"training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 system variants 2.3 Syntax-enhanced SMT Motivated by the importance of grammar in the translation between English and German, we developed a syntax-enhanced SMT system. The process is similar to that of our baseline, but this version includes syntax-aware phrase extraction. Phrase pairs in the baseline SMT system were augmented with linguistically-motivated phrase pairs. These phrases were extracted by generating constituency and dependency parse trees"
W16-2329,N07-1064,0,0.0988575,"t pairwise recomposition (Avramidis, 2013) to reduce ties between the systems. Due to technical reasons, the version of the selection mechanism that is submitted to this task is only a pilot version that includes WSDSMT (section 2.2), baseline RBMT (section 2.4) and RBMT→SMT (section 2.5). When ties occurred, despite the soft recomposition, the system was selected based on a predefined system priority (WSD-SMT, RBMT, RBMT→SMT). The preSerial RBMT post-editing with SMT As an alternative to automatic post-editing of the RBMT system, a serial RBMT+SMT system combination is used, as described in (Simard et al., 2007). For building it, the first stage is translation of the source language part of the training corpus by the RBMT system. In the second stage, a SMT system is trained using the RBMT translation output as a source language and the target language part as a target language. Later, the test set is first translated by the RBMT system, and the obtained translation is translated by the SMT system. 418 defined order of the systems needs to be further confirmed as part of the future work. 3 the investigated phenomena. SMT performs well on terminology, menu items and quotation marks, but seems to suffer"
W16-2329,P03-1054,0,0.00782516,"Missing"
W16-2329,2009.mtsummit-posters.18,1,0.80462,"nd German, we developed a syntax-enhanced SMT system. The process is similar to that of our baseline, but this version includes syntax-aware phrase extraction. Phrase pairs in the baseline SMT system were augmented with linguistically-motivated phrase pairs. These phrases were extracted by generating constituency and dependency parse trees for both the source and target languages, followed by nodealigning the parallel parse trees using a statistical tree aligner (Zhechev, 2009). The syntax-aware phrase extraction algorithm obtains surface-level chunks (syntax-aware) from the aligned subtrees (Srivastava and Way, 2009). Intermediate experiments were conducted by using either constituency parsing or dependency SMT with Word Sense Disambiguation The word-sense-disambiguated SMT system is a factored phrase-based statistical system with two decoding paths, one basic and one alternative. In the basic path, all nouns of the source language (English) have been annotated with a WSD system (Weissenborn et al., 2015) that assigns BabelNet senses to nouns and has recently shown improvements over state-of-the-art results on several corpora. The sense labels are estimated based on the disambiguation analysis on the sent"
W16-2329,W08-0318,0,0.10178,"the corresponding automatic scores are depicted in Table 2, which includes the results on the development set 2. On this set, WSD does not show a positive effect over the baseline in terms of automatic scores. Table 1: Size of corpora used for SMT. technical (IT-domain) and Europarl corpora, plus one language model was trained on the targetlanguage news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based"
W16-2329,P15-1058,0,0.0431184,"Missing"
W16-2329,W07-0734,0,0.0528643,"put for every sentence. The architecture of the system is illustrated in figure 1. The core of the selection mechanism is a ranker which reproduces ranking by aggregating pairwise decisions by a binary classifier (Avramidis, 2013). Such a classifier is trained on binary comparisons in order to select the best one out of two different MT outputs given one source sentence at a time. As training material, we used the test-sets of WMT evaluation task (2008-2014). The rank labels for the training are automatically generated, after ordering the given MT outputs based on their sentence-level METEOR (Lavie and Agarwal, 2007) against the references. We have previously experimented with training on ranking provided by users, but experiments showed that for this task, ranks made out of sentence-level METEOR maximize all automatic scores on our development set, including other document-level ones, such as BLEU. output RBMT RBMT→SMT Figure 1: Architecture of the selection mechanism automatic scores, the difference does not seem significant, and manual inspection raised the concern that this may be because of the way this phrase has been translated in the references. We therefore conducted manual sentence selection on"
W16-2329,P03-1021,0,0.0374423,"the fact that different engines make different errors) solely based on the rough feedback provided by automatic scores. As scores like BLEU (Pap2 System components We hereby present the systems that appear in our submissions and our hybrid system: 2.1 Phrase-based SMT baseline The baseline system consists of a basic phrasebased SMT model, trained with the state-of-theart settings on both the generic and technical data. The translation table was trained on a concatenation of generic and technical data, filtering out the sentences longer than 80 words. Batch 1 was used as a tuning set for MERT (Och, 2003). One language model (monolingual) of order 5 was trained on the target side from both the 415 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 415–422, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics corpus Chromium browser Drupal Libreoffice help Libreoffice UI Ubuntu Saucy Europarl (mono) News (mono) Commoncrawl (parallel) Europarl (parallel) MultiUN (parallel) News Crawl (parallel) entries words 6.3K 4.7K 46.8K 35.6K 182.9K 55.1K 57.4K 1.1M 143.7K 1.6M 2.2M 89M 54.0M 1.7B 2.4M 1.9M 167.6K 201.3K 53.6M 50.1"
W16-2329,avramidis-etal-2012-involving,1,\N,Missing
W16-6404,2003.mtsummit-systems.1,0,0.0803587,"ns were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional work on RBMT focused on issues revealed through manual inspection of its performance on the QTLeap corpus (see also section 3): 2 http://metashare.metanet4u.eu/go2/qtleapcorpus 30 corpus Chromium browser Drupal Libreoffi"
W16-6404,W16-2329,1,0.864384,"have performed using a dedicated “test suite” that contains selected examples of relevant phenomena. While automatic scores show huge differences between the engines, the overall average number or errors they (do not) make is very similar for all systems. However, the detailed error breakdown shows that the systems behave very differently concerning the various phenomena. 1 Introduction This paper describes a hybrid Machine Translation (MT) system built for translating from English to German in the domain of technical documentation. The system builds upon the general architecture described in Avramidis et al. (2016), but in the current version several components have been improved or replaced. As detailed in the previous paper, the design of the system was driven by the assumptions that a) none of today’s common MT approaches, phrase-based statistical (PB-SMT) or rule-based (RBMT), is on its own capable of providing enough good translations to be useful in an outbound translation scenario without human intervention, and b) “deep” linguistic knowledge should help to improve translation quality. Instead of building a completely new system, our goal is to adjust and combine existing systems in a smart way u"
W16-6404,W14-4012,0,0.109025,"Missing"
W16-6404,W11-2123,0,0.0103712,"training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based on the same data and settings, unless stated otherwise. 2.2 Rule-based component The rule-based system Lucy (Alonso and Thurmair, 2003) is also part of our experiment, due to its stateof-the-art performance in the previous years. Additionally, manual inspection on the development set has shown that it provides better handling of complex grammatical phenomena particularly when translating into German, due to the fact that it operates based on transfer rules from the source to the target syntax tree. Additional w"
W16-6404,W08-0318,0,0.0344971,"s longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and queried with KenLM (Heafield, 2011). All statistical systems presented below are extensions of this system, also based"
W16-6404,W16-2361,0,0.029527,"Missing"
W16-6404,I08-2089,0,0.0251905,"cal data. The translation table was trained on a concatenation of generic and technical data, filtering out the sentences longer than 80 words. The first batch of the QTLeap corpus2 was used as a tuning set for MERT (Och, 2003), whereas the second batch was reserved for testing. One language model (monolingual) of order 5 was trained on the target side from both the technical (IT-domain) and Europarl corpora, plus one language model was trained on the target-language news corpus from the years 2007 to 2013 (Callison-Burch et al., 2007). All language models were interpolated on the tuning set (Schwenk and Koehn, 2008). The size of the training data is shown in Table 1. The text has been tokenized and truecased (Koehn et al., 2008) prior to the training and the decoding, and de-tokenized and de-truecased afterwards. A few regular expressions were added to the tokenizer, so that URLs are not tokenized before being translated. Normalization of punctuation was also included, mainly in order to fix several issues with variable typography on quotes. The phrase-based SMT system was trained with Moses (Koehn, 2010) using EMS (Koehn, 2010), whereas the language models were trained with SRILM (Stolcke, 2002) and que"
W18-6436,W07-0718,0,0.207348,"Missing"
W18-6436,L16-1100,0,0.150074,") and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim at comparing different MT technologies (Isabelle et al., 2017; Burchardt et al., 2017) and Quality Estimation methods (Avramidis et al., 2018). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either b"
W18-6436,D17-1263,0,0.157443,"Missing"
W18-6436,L18-1142,1,0.707008,"Missing"
W18-6436,1991.mtsummit-panels.5,0,0.873131,"spects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low. 1 2 Related Work The use of test suites in the evaluation of NLP applications (Balkan et al., 1995) and MT systems in particular (King and Falkedal, 1990; Way, 1991) has been proposed already in the 1990’s. For instance, test suites were employed to evaluate stateof-the-art rule-based systems (Heid and Hildenbrand, 1991). The idea of using test suites for MT evaluation was revived recently with the emergence of Neural MT (NMT) as the produced translations reached significantly better levels of quality, leading to a need for more fine-grained qualitative observations. Recent works include test suites that focus on the evaluation of particular linguistic phenomena (e.g. pronoun translation; Guillou and Hardmeier, 2016) or more generic test suites that aim a"
W18-6436,P02-1040,0,0.11164,"8). The previously presented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium"
W18-6436,2006.amta-papers.25,0,0.0953688,"ented papers differ in the amount of phenomena and the language pairs they cover. This paper extends the work presented in Burchardt et al. (2017) by including more test sentences and better coverage of phenomena. In conIntroduction The evaluation of Machine Translation (MT) has mostly relied on methods that produce a numerical judgment on the correctness of a test set. These methods are either based on the human perception of the correctness of the MT output (CallisonBurch et al., 2007), or on automatic metrics that compare the MT output with the reference translation (Papineni et al., 2002; Snover et al., 2006). In both cases, the evaluation is performed on a testset containing articles or small documents that are assumed to be a random representative sample of texts in this domain. Moreover, this kind of evaluation aims at producing average scores that express a generic sense of correctness for the entire test set and compare the performance of several MT systems. Although this approach has been proven valuable for the MT development and the assessment of 578 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 578–587 c Belgium, Brussels, October 31"
W18-6436,C90-2037,0,\N,Missing
W18-6436,E17-2058,0,\N,Missing
W18-6436,W18-2107,1,\N,Missing
